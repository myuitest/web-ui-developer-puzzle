cat: .: Is a directory
.idea
.project
.classpath
*.iml
target/
out/
*.log
tmp/
metastore_db/cat: ./allcode.txt: input file is output file
cat: ./clientlifecycle-common-core: Is a directory
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>com.hsbc.gbm.bd.clm</groupId>
        <artifactId>clientlifecycle-common</artifactId>
        <version>2.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <artifactId>clientlifecycle-common-core_${scala.binary.version}</artifactId>

    <profiles>
        <profile>
            <id>spark2</id>
            <dependencies>
                <dependency>
                    <groupId>com.hsbc.gbm.bd.clm</groupId>
                    <artifactId>clientlifecycle-common-hint_2.11</artifactId>
                    <version>${project.version}</version>
                </dependency>
                <dependency>
                    <groupId>com.hsbc.gbm.bd.datafactory</groupId>
                    <artifactId>datafactory-api_${scala.binary.version}</artifactId>
                </dependency>
            </dependencies>
        </profile>
        <profile>
            <id>spark3</id>
            <dependencies>
                <dependency>
                    <groupId>com.hsbc.gbm.bd.datafactory</groupId>
                    <artifactId>datafactory-api_${scala.binary.version}</artifactId>
                    <exclusions>
                        <exclusion>
                            <groupId>com.fasterxml.jackson.module</groupId>
                            <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
                        </exclusion>
                    </exclusions>
                </dependency>
                <dependency>
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>jackson-core</artifactId>
                    <version>2.10.0</version>
                </dependency>
            </dependencies>
        </profile>
    </profiles>

    <dependencies>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpcore</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>io.github.classgraph</groupId>
            <artifactId>classgraph</artifactId>
        </dependency>

        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch-spark-${elasticsearch-spark.version}_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
        </dependency>

        <dependency>
            <groupId>com.hubspot.jinjava</groupId>
            <artifactId>jinjava</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>jackson-databind</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-compiler</artifactId>
            <scope>provided</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-clean-plugin</artifactId>
                <version>3.0.0</version>
                <configuration>
                    <filesets>
                        <fileset>
                            <directory>.</directory>
                            <includes>
                                <include>**/*.ser</include>
                            </includes>
                        </fileset>
                    </filesets>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.5.1</version>
                <configuration>
                    <encoding>UTF-8</encoding>
                    <source>1.8</source>
                    <target>1.8</target>
                    <fork>true</fork>
                    <compilerArgs>
                        <arg>-XDignore.symbol.file=true</arg>
                        <arg>-Werror</arg>
                    </compilerArgs>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <scalaVersion>${scala.version}</scalaVersion>
                    <args>
                        <arg>-target:jvm-1.8</arg>
                    </args>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>
cat: ./clientlifecycle-common-core/src: Is a directory
cat: ./clientlifecycle-common-core/src/main: Is a directory
cat: ./clientlifecycle-common-core/src/main/resources: Is a directory
cat: ./clientlifecycle-common-core/src/main/resources/template: Is a directory
cat: ./clientlifecycle-common-core/src/main/resources/template/scheduling: Is a directory
#!/bin/bash
set -e

SCRIPT_PATH=`readlink -f $0`
SCRIPT_DIR=$(dirname $(dirname $SCRIPT_PATH))
CURRENT_TIMESTAMP=$(date -d "`date \"+%Y-%m-%d\"`" "+%s")
{% for item in local_env.items() -%}
{{item.key}}={{item.value}}
{% endfor %}

export CURRENT_USER=`whoami`
export KRB5CCNAME=FILE:/tmp/krb5cc_${CURRENT_TIMESTAMP}_`id -u`
{% for item in global_env.items() -%}
export {{item.key}}={{item.value}}
{% endfor %}

cd ${SCRIPT_DIR}/..
echo "$SCRIPT_PATH"
echo "$SCRIPT_DIR"

export app_yaml=${SCRIPT_DIR}/../config/$1/application.yml
export log_properties_file=${SCRIPT_DIR}/../config/$1/log4j.properties

[[ -f ${app_yaml} ]] || export app_yaml=${SCRIPT_DIR}/../config/application.yml
[[ -f ${log_properties_file} ]] || export log_properties_file=${SCRIPT_DIR}/../config/log4j.properties

echo "application.yml path: ${app_yaml}"
echo "log4j.properties path: ${log_properties_file}"

kinit -kt $KEYTAB_LOCATION $CURRENT_USER

source ${SCRIPT_DIR}/utils/parse-yaml.sh ${app_yaml}
source ${SCRIPT_DIR}/utils/ilm-tools.sh

echo "app env: ${app_env}"

declare -A index_names

if [[ x${app_write_es} == x"true" ]]; then
{%- for index_name,index_properties in indexs.items() %}
  {%- set x = loop.index - 1 %}
  {%-  for properties in index_properties.items() %}
  {%- if properties.key == "policy" %}
  create_or_update_policy \
     {{ "-n " + properties.value.name if properties.value.name != null }} \
     {{ "-i " + properties.value.delete_interval if properties.value.delete_interval != null }} \
     {{ "-s " + properties.value.max_size if properties.value.max_size != null }} \
     {{ "-a " + properties.value.max_age if properties.value.max_age != null }} \
     {{ "-d " + properties.value.max_docs if properties.value.max_docs != null }} \
     {{ "-f " if properties.value.force != null && properties.value.force == true }}
  {%- set policy_name = properties.value.name %}
  {%- endif %}
  {%- if properties.key == "template" %}
  create_or_update_template \
     {{ "-n " + index_name + "_template4all" }} \
     {{ "-i " + index_name + "-*" }} \
     -p {{ policy_name }} \
     {{ "-a " + properties.value.rollover_alias if properties.value.rollover_alias != null }} \
     {{ "-f " if properties.value.force != null && properties.value.force == true }}
  index_{{ x }}=`get_finally_index "{{ properties.value.rollover_alias }}" "{{ index_name }}"`
  [[ -z ${index_{{ x }}} ]] && index_{{ x }}=`get_finally_index "{{ properties.value.rollover_alias }}" "{{ index_name }}"`
  [[ -z ${index_{{ x }}} ]] && echo "index is None" && exit 2
    {%- endif %}
  {%- endfor %}
  index_names["{{ index_name }}"]="${index_{{ x }}}"
{%- endfor %}
fi

if [[ x${app_measure} == x"true" ]]; then
  hadoop fs -rm -r -f ${app_measurePath}/${app_name}
  hadoop fs -put ${SCRIPT_DIR}/../measure ${app_measurePath}/${app_name}
fi

cd ${SCRIPT_DIR}

indexs=$(for index_name in ${!index_names[@]};do
  echo -e " ${index_name}=${index_names["${index_name}"]} \c"
done)

set +e

sed -e '/.*{{submitCmd}}/,$s#"#\\"#g'\
    -e 's/.*{{submitCmd}}/CMD="&/'\
    -e '${s/.*/&"/}'\
    -e '$a\
\
echo "Executing : ${CMD}"\
eval ${CMD}\
\
if [[ "$?" == "0" ]]; then\
   exit 0\
else\
   exit 1\
fi' $SCRIPT_DIR/submit-job-fg.sh | bash -s {{ entrypoints[0].name }} ${indexs} ${@:2}

if [[ x${app_measure} == x"true" ]]; then
  hadoop fs -rm -r -f ${app_measurePath}/${app_name}
fi

if [[ ${app_write_es} == "true" ]]; then
  {%- for index_name,index_properties in indexs.items() %}
    {%-  for properties in index_properties.items() %}
      {%- if properties.key == "latest_alias_name" %}
  if [[ `get_index_count ${index_names["{{ index_name }}"]}` -gt 0 ]];then
    create_or_update_alias {{ properties.value }} ${index_names["{{ index_name }}"]}
  fi
      {%- endif %}
    {%- endfor %}
  {%- endfor %}
fi

exit 0cat: ./clientlifecycle-common-core/src/main/resources/template/submit-job-fg: Is a directory
cat: ./clientlifecycle-common-core/src/main/resources/template/submit-job-fg/v1_1: Is a directory
#!/bin/bash

SCRIPT_PATH=`readlink -f $0`
SCRIPT_DIR=`dirname $SCRIPT_PATH`

cd ${SCRIPT_DIR}/..

JOB_NAME=$1
APP_DIR=$PWD
APP_LIB_DIR=$APP_DIR/lib
APP_JAR_DIR=$APP_DIR/jar
APP_CONF_DIR=$APP_DIR/config
ARTIFACT_ID="{{artifact_id}}"
VERSION="{{version}}"

DEP_JARS=
for jar in $APP_LIB_DIR/*.jar
do
  DEP_JARS=$jar,$DEP_JARS
done
DEP_JARS_FINAL=${DEP_JARS%?}

declare -A JOB_CLASSES
declare -A DEFAULTS
declare -A PARAM_OVERRIDES

{% for entrypoint in entrypoints -%}
JOB_CLASSES["{{entrypoint.name}}"]="{{entrypoint.class}}"
{% endfor %}

{% for item in defaults.items() -%}
DEFAULTS["{{item.key}}"]="{{item.value}}"
{% endfor %}

{% for item in param_overrides.items() -%}
PARAM_OVERRIDES["{{item.key}}"]="{{item.value}}"
{% endfor %}

MASTER_NAME="${PARAM_OVERRIDES[${JOB_NAME}__master]:-${DEFAULTS[master]}}"
DEPLOY_MODE="${PARAM_OVERRIDES[${JOB_NAME}__deploy_mode]:-${DEFAULTS[deploy_mode]}}"
QUEUE_NAME="${PARAM_OVERRIDES[${JOB_NAME}__queue]:-${DEFAULTS[queue]}}"
NUM_EXECUTORS="${PARAM_OVERRIDES[${JOB_NAME}__num_executors]:-${DEFAULTS[num_executors]}}"
EXECUTOR_CORES="${PARAM_OVERRIDES[${JOB_NAME}__executor_cores]:-${DEFAULTS[executor_cores]}}"
EXECUTOR_MEMORY="${PARAM_OVERRIDES[${JOB_NAME}__executor_memory]:-${DEFAULTS[executor_memory]}}"
DRIVER_MEMORY="${PARAM_OVERRIDES[${JOB_NAME}__driver_memory]:-${DEFAULTS[driver_memory]}}"
JOB_CONFIG_FILE="${PARAM_OVERRIDES[${JOB_NAME}__config_file]:-${DEFAULTS[config_file]}}"

cd ${SCRIPT_DIR}

{% raw %}
if [ -z "${JOB_CLASSES[$JOB_NAME]}" ]
    then
        echo "Usage: LOGS_BASE_LOCATION=/path/to/logs/base/dir/ KEYTAB_LOCATION=/path/to/keytab submit-job.sh <job-name> [parameters]"
        echo "Available jobs:"
        for item in "${!JOB_CLASSES[@]}"
        do
	        echo "$item"
        done
        exit 1
fi

JOB_CLASS="${JOB_CLASSES[$JOB_NAME]}"
LOG_FILE="$LOGS_BASE_LOCATION/$CURRENT_USER/$ARTIFACT_ID-$VERSION-$JOB_NAME.log"
COMMAND_LINE_ARGS=($@)
TRIMMED_COMMAND_LINE_ARGS=(${COMMAND_LINE_ARGS[@]:1})
ARGS_LEN=${#TRIMMED_COMMAND_LINE_ARGS[@]}

# Substitute environment variable names with actual values
for ((i=0; i<${ARGS_LEN}; i++ ));
do
  CMD_ARG=${TRIMMED_COMMAND_LINE_ARGS[$i]}
  if [[ $CMD_ARG == %* ]] ;
  then
    TRIMMED_COMMAND_LINE_ARGS[$i]=`eval "echo ${CMD_ARG/\%/$}"`
  fi
done
{% endraw %}

echo "ARTIFACT_ID: ${ARTIFACT_ID}" > $LOG_FILE
echo "VERSION: ${VERSION}" >> $LOG_FILE
echo "CURRENT_USER: ${CURRENT_USER}" >> $LOG_FILE
echo "JOB_NAME: ${JOB_NAME}" >> $LOG_FILE
echo "JOB_CLASS: ${JOB_CLASS}" >> $LOG_FILE
echo "JOB_CONFIG_FILE: ${JOB_CONFIG_FILE}" >> $LOG_FILE
echo "MASTER_NAME: ${MASTER_NAME}" >> $LOG_FILE
echo "DEPLOY_MODE: ${DEPLOY_MODE}" >> $LOG_FILE
echo "QUEUE_NAME: ${QUEUE_NAME}" >> $LOG_FILE
echo "NUM_EXECUTORS: ${NUM_EXECUTORS}" >> $LOG_FILE
echo "EXECUTOR_CORES: ${EXECUTOR_CORES}" >> $LOG_FILE
echo "EXECUTOR_MEMORY: ${EXECUTOR_MEMORY}" >> $LOG_FILE
echo "DRIVER_MEMORY: ${DRIVER_MEMORY}" >> $LOG_FILE
echo "LOG_FILE: ${LOG_FILE}" >> $LOG_FILE
echo "COMMAND_LINE_ARGS: ${COMMAND_LINE_ARGS[@]}" >> $LOG_FILE
echo "TRIMMED_COMMAND_LINE_ARGS: ${TRIMMED_COMMAND_LINE_ARGS[@]}" >> $LOG_FILE
echo "KEYTAB_LOCATION: ${KEYTAB_LOCATION}" >> $LOG_FILE

{{spark_home}}/bin/{{ submitCmd }} \
--jars $DEP_JARS_FINAL \
--name "CLM -- $ARTIFACT_ID-$VERSION-$JOB_NAME -- PROD" \
--verbose \
--master ${MASTER_NAME} \
--deploy-mode ${DEPLOY_MODE} \
--queue ${QUEUE_NAME} \
--num-executors ${NUM_EXECUTORS} \
--executor-cores ${EXECUTOR_CORES} \
--executor-memory ${EXECUTOR_MEMORY} \
--driver-memory ${DRIVER_MEMORY} \
{% for conf_item in conf -%}
--conf "{{ conf_item }}" \
{% endfor -%}
--conf "spark.yarn.principal=$CURRENT_USER" \
--conf "spark.yarn.keytab=$KEYTAB_LOCATION" \
{% for file_item in files -%}
--files "{{ file_item }}" \
{% endfor -%}
--class $JOB_CLASS \
"$APP_JAR_DIR/$ARTIFACT_ID-$VERSION.jar" \
"$JOB_NAME" \
$JOB_CONFIG_FILE \
${TRIMMED_COMMAND_LINE_ARGS[@]}
cat: ./clientlifecycle-common-core/src/main/resources/utils: Is a directory
set -x

################################### tools ##############################
function get_curl_body() {
  local api=$1
  local request=$2
  local body="curl ${curl_ssl_options} --user ${spark_es_net_http_auth_user}:${PASSWORD} -s -X${request} ${protocol:-http}://${ADDRESS}${api}"
  echo $body
}

#return finally index name which been wrote
function get_finally_index() {
  local alias_name=$1
  local guess_init_index_name=`echo ${alias_name} | cut -d'-' -f1,2`
  local init_index_name=${2:-$guess_init_index_name}-000001
  local latest_index=$(echo `get_specific_index ${alias_name} 1` | cut -d' ' -f1)
  [[ -z ${latest_index} ]] &&  echo `create_index ${init_index_name} ${alias_name}` >> /dev/null
  [[ -z ${latest_index} ]] && local latest_index=${init_index_name} && local latest_index_count=0 || local latest_index_count=`get_index_count ${latest_index}`
  [[ ${latest_index_count} -eq 0 ]] && local index=${latest_index} || local index=`rollover ${alias_name}`
  echo $index
}

#manual rollover
function rollover() {
  local alias_name=$1
  local response=$(`get_curl_body /${alias_name}/_rollover POST` -H'Content-Type: application/json' -d '
{
  "conditions": {
    "max_age": "10s"
  }
}
')
  local status=$(echo ${response} | grep -Po '"rolled_over":true' | cut -d':' -f2)
  local new_index=$(echo ${response} | grep -Po '"new_index".*?,' | cut -d':' -f2 | cut -d',' -f1 | cut -d'"' -f2)

  [[ $status == "true" ]] || echo "rollover ${alias_name} failed!!!" && exit 1
  echo ${new_index}
}

function get_index_count() {
    local index_name=$1
    local count=$(`get_curl_body /${index_name}/_count GET` | awk -F',' '{ print $1 }'|awk -F':' '{ print $2 }')
    echo $count
}

#query specific alias which point to many indexs, then sort desc then return specific index
#return format: indexname is_write_index
function get_specific_index(){
  local alias_name=$1
  local row_number=$2
  local latest_index=$(`get_curl_body /_cat/aliases/${alias_name} GET` | sort -k2 -n -r  | awk '{print $2,$6}' | sed -n "${row_number}p")
  echo $latest_index
}

function create_or_update_alias(){
  local alias_name=$1
  local index_pattern=$2
  local HTTP_STATUS=$(`get_curl_body /_aliases POST` -H'Content-Type: application/json' -d '
{
    "actions" : [
        {
          "remove" : {
            "index" : "'${index_pattern%-*}'-*",
            "alias" : "'${alias_name}'"
          }
        },
        {
          "add" : {
            "index" : "'${index_pattern}'",
            "alias" : "'${alias_name}'"
          }
        }
    ]
}
')
  check_execute_status $HTTP_STATUS "update alias ${alias_name} to ${index_pattern}"
}

function create_index(){
  local index_name=$1
  local alias_name=$2
  local HTTP_STATUS=$(`get_curl_body /${index_name} PUT` -H'Content-Type: application/json' -d '
{
  "aliases": {
    "'${alias_name}'": {
      "is_write_index":true
    }
  },
  "settings": {
    "number_of_shards": '${number_of_shards}',
    "number_of_replicas": '${number_of_replicas}'
  }
}
')
  check_execute_status $HTTP_STATUS "create index ${index_name}"
}

function create_or_update_policy(){
  local OPTIND
  while getopts ":a:s:d:i:n:f" opt; do
      case $opt in
          a)
             local max_age=$OPTARG
             ;;
          s)
             local max_size=$OPTARG
             ;;
          d)
             local max_docs=$OPTARG
             ;;
          i)
             local delete_interval=$OPTARG
             ;;
          n)
             local policy_name=$OPTARG
             ;;
          f)
             local force=true
             ;;
          \?)
             echo "Unknown parameter"
             exit 1
             ;;
          :)
             echo "None parameter"
             ;;
      esac
  done
  [[ -z $policy_name ]] && echo "Policy_name is None" && exit 3
  if [[ -z $force ]];then
    local policy_exist=`check_exist /_ilm/policy/${policy_name}`
    [[ $policy_exist == "true" ]] && echo "${policy_name} already exist before create, and force is false, so not update it" && return
  fi
  local condition=' '`combine max_size $max_size`' '`combine max_age $max_age`' '`combine max_docs $max_docs`' '
  local HTTP_STATUS=$(`get_curl_body /_ilm/policy/${policy_name} PUT` -H'Content-Type: application/json' -d '
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            '`echo ${condition} | tr ' ' ','`'
          }
        }
      },
      "delete": {
        "min_age": "'${delete_interval}'",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
')
  check_execute_status $HTTP_STATUS "create or update policy ${policy_name}"
}

function create_or_update_template(){
  local OPTIND
  while getopts ":n:i:p:a:f" opt; do
      case $opt in
          n)
             local template_name=$OPTARG
             ;;
          i)
             local index_patterns=$OPTARG
             ;;
          p)
             local policy_name=$OPTARG
             ;;
          a)
             local rollover_alias=$OPTARG
             ;;
          f)
             local force=true
             ;;
          \?)
             echo "Unknown parameter"
             exit 1
             ;;
          :)
             echo "None parameter"
             ;;
      esac
  done
  [[ -z $template_name ]] && echo "Template name is None" && exit 3
  if [[ -z $force ]];then
    local template_exist=`check_exist /_template/${template_name}`
    [[ $template_exist == "true" ]] && echo "${template_name} already exist before create, and force is false, so not update it" && return
  fi
  local HTTP_STATUS=$(`get_curl_body /_template/${template_name} PUT` -H'Content-Type: application/json' -d '
{
  "index_patterns": ["'${index_patterns}'"],
  "settings": {
    "number_of_shards": '${number_of_shards}',
    "number_of_replicas": '${number_of_replicas}',
    "index.lifecycle.name": "'${policy_name}'",
    "index.lifecycle.rollover_alias": "'${rollover_alias}'"
  }
}
')
  check_execute_status $HTTP_STATUS "create or update template ${template_name}"
}

function check_exist() {
  local api=$1
  local exist=$(`get_curl_body ${api} "GET -I -m 10 -s -o /dev/null -w %{http_code}"`)
  [[ $exist -eq 200 ]] && echo "true" || echo "false"
}

function check_execute_status() {
  local HTTP_STATUS=$1
  local abstract_log=$2
  if grep -v "$GOOD_STATUS" <<< "$HTTP_STATUS"; then
     echo "${abstract_log} failed! $HTTP_STATUS"
     exit 1
  fi
  echo "*****${abstract_log} successed! *****"
}

function combine(){
  local key=$1
  local value=$2
  local result=' '${value:+\"}''${value:+$key}''${value:+\"}''${value:+:}''${value:+\"}''${value}''${value:+\"}' '
  echo $result
}

#########################  init  ###################################
[[ ${spark_es_net_ssl:-false} == "true" ]] && curl_ssl_options="-k" && protocol="https"
GOOD_STATUS='"acknowledged":true'

if [[ -z $PASSWORD ]];then
  [[ ${app_env} != "prd" && ${app_env} != "prod" ]] &&
  PASSWORD=$(echo "RVZta0lpSExtRnpEemlKZQo=" | openssl enc -base64 -d) ||
  PASSWORD=$(echo "djRXVkt6TXhVSy83OHp2Nwo=" | openssl enc -base64 -d)
fi

[[ -z $number_of_replicas ]] && number_of_replicas=1
[[ -z $number_of_shards ]] && number_of_shards=3

function check_es_node() {
  if [[ -z $1 ]];then
    return 0
  fi
  http_code=$(`get_curl_body ${1} "GET -I -m 10 -s -o /dev/null -w %{http_code}"`)
  echo "$http_code"
}

es_nodes=(${spark_es_nodes//,/ })

for node in "${es_nodes[@]}"
do
  [[ $(check_es_node "${node}") -eq 200 ]] && ADDRESS=$node && break
done

[[ -z ${ADDRESS} ]] && echo "NoNodeAvailableException[None of the configured es nodes are available]" && exit 1

echo "connecting to host :$ADDRESS by user : ${spark_es_net_http_auth_user}"#!/bin/bash

#notes: when source this script then you can use value which setup in application.yml

function parse_yaml() {
    local yaml_file=$1
    local prefix=$2
    local s='[[:space:]]*'
    local w='[a-zA-Z0-9_.-]*'
    local fs="$(echo @|tr @ '\034')"
    (
        sed -ne '/^--/s|--||g; s/\s*$//g;' \
            -e "/#.*[\"\']/!s| #.*||g; /^#/s|#.*||g;" \
            -e  "s|^\($s\)\($w\)$s:$s\"\(.*\)\"$s\$|\1$fs\2$fs\3|p" \
            -e "s|^\($s\)\($w\)${s}[:-]$s\(.*\)$s\$|\1$fs\2$fs\3|p" |
        awk -F"$fs" '{
            indent = length($1)/2;
            if (length($2) == 0) { conj[indent]="+";} else {conj[indent]="";}
            vname[indent] = $2;
            for (i in vname) {if (i > indent) {delete vname[i]}}
                if (length($3) > 0) {
                    vn=""; for (i=0; i<indent; i++) {vn=(vn)(vname[i])("_")}
                    printf("%s%s%s%s=(\"%s\")\n", "'"$prefix"'",vn, $2, conj[indent-1],$3);
                }
            }' |
        sed -re 's/_=/+=/g' \
            -e ':1;s/[\.\-](.*=.*)/_\1/;t1'
    ) < "$yaml_file"
}
function create_variables() {
    local yaml_file="$1"
    eval "$(parse_yaml $yaml_file)"
}

yaml_file=$1
create_variables $yaml_filecat: ./clientlifecycle-common-core/src/main/scala: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm: Is a directory
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm/annotation: Is a directory
package com.hsbc.gbm.bd.clm.annotation;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
public @interface SparkJobName {
    String value();
}
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm/env: Is a directory
package com.hsbc.gbm.bd.clm.env

class AppConfig(config: Map[String, Any]) {

  def apply[T](key: String): T = {
    appConfig(key).asInstanceOf[T]
  }

  def apply[T](key: String, defaultValue: T): T = {
    appConfig.getOrElse(key, defaultValue).asInstanceOf[T]
  }

  private lazy val appConfig: Map[String, Any] = {
    config
      .filter({ case (k, v) => k.startsWith("app.") })
      .map({ case (k, v) => (k.substring("app.".length), v) })
  }
}
package com.hsbc.gbm.bd.clm.env

case class RawJobParams(rawParams: Array[String]) {

  val pairs: Array[(String, String)] = rawParams.map(param => toKeyValuePair(param))

  def apply(position: Int): Option[String] = {
    pairs.lift(position).map(_._2)
  }

  def apply(name: String): Option[String] = {
    val found = pairs.collectFirst({ case (k, v) if k.equals(name.toLowerCase) => v })
    if(found.isDefined) Option(found.get) else None
  }

  def apply(name: String, default: String): Option[String] = {
    val found = pairs.collectFirst({ case (k, v) if k.equals(name.toLowerCase) => v })
    if(found.isDefined) Option(found.get) else Option(default)
  }

  private def toKeyValuePair(param: String): (String, String) = {
    val fragments = param.split("=")
    if (fragments.length == 1) ("", fragments.head) else (fragments(0).toLowerCase, fragments(1))
  }

  override def toString: String = {
    val paramString = pairs.map(pair => if(pair._1 == "") s"${pair._2}" else  s"${pair._1}=${pair._2}").mkString(",")
    s"RawJobParams[$paramString]"
  }
}
package com.hsbc.gbm.bd.clm.env

import java._
import java.io.FileInputStream
import java.security.{InvalidAlgorithmParameterException, InvalidKeyException, NoSuchAlgorithmException}
import java.time.ZonedDateTime
import java.time.format.DateTimeFormatter
import java.util.UUID

import com.hsbc.gbm.bd.clm.utils.EncryptionUtils
import javax.crypto.{BadPaddingException, IllegalBlockSizeException, NoSuchPaddingException}
import org.apache.spark.SparkConf
import org.apache.spark.SPARK_VERSION
import org.apache.spark.sql.SparkSession
import org.yaml.snakeyaml.Yaml

import scala.collection.JavaConverters._

trait SparkEnv {

  def appName(): String

  def configLocation(): String

  private val formatter: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss.SSSXXXX")
  val timeBase: String = ZonedDateTime.now().format(formatter)
  val date: String = timeBase.substring(0, 10).replaceAll("[-|:'T]", "")
  val timestamp: String = timeBase.substring(0, 19).replaceAll("[-|:'T]", "")

  lazy val sparkConfig: SparkConf = {
    val sparkProperties: Map[String, String] = config
      .filter({ case (k, _) => k.startsWith("spark.") })
      .map({ case (k, v) =>
        val truncatedKey = k.substring("spark.".length)
        (truncatedKey, decryptValueIfApplicable(truncatedKey, v))
      })

    if (SPARK_VERSION.startsWith("3")) new SparkConf().setAll(sparkProperties)
    else {
      val kryoClasses = sparkKryoClasses(sparkProperties)
      new SparkConf().setAll(sparkProperties).registerKryoClasses(kryoClasses)
    }
  }

  lazy val appConfig: AppConfig = new AppConfig(config)

  lazy val spark: SparkSession =
    SparkSession
      .builder()
      .master(sparkConfig.get("spark.master", "local"))
      .appName(appName())
      .config(sparkConfig)
      .config("spark.runtime.config.time.date", date)
      .config("spark.runtime.config.time.timestamp", timestamp)
      .config("spark.sql.crossJoin.enabled", value = true)
      .enableHiveSupport()
      .getOrCreate()

  private lazy val config: Map[String, Any] = {
    val inputStream = new FileInputStream(configLocation())
    val valuesMap: util.Map[String, Object] = new Yaml().load(inputStream)
    flattenKeys("", valuesMap)
  }

  private def flattenKeys(parentKey: String, value: Any): Map[String, Any] = {
    val keySeparator = if (parentKey.isEmpty) "" else "."
    value match {
      case jMap: util.Map[String, Any] =>
        val sMap = jMap.asScala.toMap
        sMap.flatMap({ case (k, v) => flattenKeys(s"""$parentKey$keySeparator$k""", v) }) ++ Map((parentKey, sMap))
      case jList: util.List[Any] =>
        Map((parentKey, jList.asScala.toList))
      case _ =>
        Map((parentKey, value))
    }
  }

  private def sparkKryoClasses(sparkProperties: Map[String, String]): Array[Class[_]] = try {
    val kryoClassesString = sparkProperties.getOrElse("kryoserializer", "")
    kryoClassesString.split(",").map(className => Class.forName(className))
  } catch {
    case e: Exception =>
      throw new RuntimeException("Error loading kryo classes ", e)
  }

  private def decryptValueIfApplicable(key: String, value: Any): String = try
    key match {
      case "es.net.http.auth.pass" =>
        EncryptionUtils.decrypt(value.toString)
      case "es.xpack.security.user" =>
        val fragments = if (value != null) value.toString.split(":") else Array()
        if (fragments.length > 1) fragments(0) + ":" + EncryptionUtils.decrypt(fragments(1))
        else if (fragments.length > 0) fragments(0)
        else null
      case "javax.jdo.option.ConnectionURL" if value.toString.contains("derby") =>
        value.toString.replaceFirst("metastore_db", s"metastore_db/${UUID.randomUUID().toString}")
      case _ =>
        value.toString
    }
  catch {
    case e@(_: InvalidKeyException | _: NoSuchAlgorithmException | _: NoSuchPaddingException | _: InvalidAlgorithmParameterException | _: IllegalBlockSizeException | _: BadPaddingException) =>
      throw new RuntimeException(e)
  }
}
package com.hsbc.gbm.bd.clm.env

trait SparkJob extends SparkEnv {
  def process(): Unit

  def close(): Unit = spark.close()

  def applicationId: String = spark.sparkContext.applicationId
}
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm/renderer: Is a directory
package com.hsbc.gbm.bd.clm.renderer

import java.io.{File, InputStream}
import java.nio.file.{Files, Paths}

import com.hsbc.gbm.bd.clm.env.RawJobParams
import org.apache.commons.io.IOUtils
import org.yaml.snakeyaml.Yaml

import scala.collection.JavaConverters._

object GenericTemplateRenderer {

  def render(params: RawJobParams): Unit = {
    val yaml: Yaml = new Yaml
    val deploymentDescStream: InputStream = this.getClass.getClassLoader.getResourceAsStream(params("deploymentYamlPath").get)
    val context: Map[String, AnyRef] = yaml.load(deploymentDescStream).asInstanceOf[java.util.LinkedHashMap[String, AnyRef]].asScala.toMap

    if (params("templateVersion").isEmpty) throw new IllegalArgumentException("Template version is missing from the metadata section")

    if (params("templateId").isEmpty) throw new IllegalArgumentException("Template ID is missing from the metadata section")

    if (Files.notExists(Paths.get(params("outputDir").get))) new File(params("outputDir").get).getParentFile.mkdirs

    val templateInfo = TemplateInfo.getBy(params("templateId").get, params("templateVersion").get)
    val templatePath = templateInfo.templateFilePath
    val templateResource = this.getClass.getClassLoader.getResource(templatePath)
    val template = IOUtils.toString(templateResource)
    val renderer = templateInfo.renderer
    renderer.render(params, template, context)
  }

  def main(args: Array[String]): Unit = {
    // artifactId
    // artifactVersion
    // deploymentYamlPath
    // outputDir
    // templateId
    // templateVersion
    // printOutput
    // sparkHome
    val params = RawJobParams(args)
    render(params)
  }
}
package com.hsbc.gbm.bd.clm.renderer

import java.io.{File, FileWriter, IOException}

import com.hsbc.gbm.bd.clm.env.RawJobParams
import com.hubspot.jinjava.Jinjava
import org.apache.commons.io.IOUtils

import scala.collection.JavaConverters._

class SparkSubmitScriptRenderer extends TemplateRenderer {

  override def render(params: RawJobParams, template: String, context: Map[String, AnyRef]): Unit = {
    val jinjava: Jinjava = new Jinjava

    val paramOverrides: Map[String, AnyRef] = getParameterOverrides(context)

    val ctx: Map[String, AnyRef] = (context +
      ("version" -> params("artifactVersion").get) +
      ("artifact_id" -> params("artifactId").get) +
      ("spark_home" -> params("sparkHome").get) +
      ("submitCmd" -> params("submitCmd").getOrElse("spark-submit")) +
      ("param_overrides" -> paramOverrides)
      ).mapValues {
      case map: Map[String, AnyRef] =>
        map.asJava
      case x => x
    }

    val entryPointScriptContent = IOUtils.toString(this.getClass.getClassLoader.getResource("template/scheduling/clm-cda-work.j2"))

    val renderedTemplate: String = jinjava.render(template, ctx.asJava)
    val renderedTemplate2: String = jinjava.render(entryPointScriptContent, ctx.asJava)

    try {
      val file = new File(params("outputDir").get)
      val fw1: FileWriter = new FileWriter(file.getPath)
      val toolsPath = new File(file.getParent + "/utils")
      val schedulePath = new File(file.getParent + "/scheduling")
      if (!toolsPath.exists()) toolsPath.mkdirs()
      if (!schedulePath.exists()) schedulePath.mkdirs()
      val fw2: FileWriter = new FileWriter(toolsPath.getPath + "/ilm-tools.sh")
      val fw3: FileWriter = new FileWriter(toolsPath.getPath + "/parse-yaml.sh")
      val fw4: FileWriter = new FileWriter(schedulePath.getPath + "/clm-cda-work.sh")
      val ilmScriptContent = IOUtils.toString(this.getClass.getClassLoader.getResource("utils/ilm-tools.sh"))
      val parseYamlScriptContent = IOUtils.toString(this.getClass.getClassLoader.getResource("utils/parse-yaml.sh"))
      try {
        fw1.write(renderedTemplate)
        fw2.write(ilmScriptContent)
        fw3.write(parseYamlScriptContent)
        fw4.write(renderedTemplate2)
      }
      catch {
        case e: IOException => throw new RuntimeException(e)
      }
      finally {
        if (fw1 != null) fw1.close()
        if (fw2 != null) fw2.close()
        if (fw3 != null) fw3.close()
        if (fw4 != null) fw4.close()
      }
    }

    if (params("printOutput", "false").get.toBoolean) {
      println("Generated submit script:\n\n" + renderedTemplate)
    }
  }

  private def getParameterOverrides(context: Map[String, AnyRef]): Map[String, AnyRef] = {
    val entrypoints: List[Map[String, AnyRef]] = context("entrypoints").asInstanceOf[java.util.ArrayList[java.util.LinkedHashMap[String, AnyRef]]].asScala.toList.map(x => x.asScala.toMap)
    if (entrypoints == null) Map[String, AnyRef]() else entrypoints.flatMap(toEntrypointOverrides).toMap
  }

  private def toEntrypointOverrides(entrypoint: Map[String, AnyRef]): Map[String, AnyRef] = {
    val jobName: String = entrypoint("name").asInstanceOf[String]

    entrypoint.filter(x => !(x._1 == "name" || x._1 == "class"))
      .map(x => (s"${jobName}__${x._1}", x._2))
  }
}
package com.hsbc.gbm.bd.clm.renderer

object TemplateInfo extends Enumeration {
  val submitJobs = List(
    SubmitJob("submit-job-fg", "1.1", "template/submit-job-fg/v1_1/submit-job-fg.j2", new SparkSubmitScriptRenderer)
  )

  def getBy(templateName: String, templateVersion: String): SubmitJob = {
    submitJobs.filter(x => templateName.startsWith(x.templateName) && x.templateVersion == templateVersion).head
  }
}

case class SubmitJob(templateName: String, templateVersion: String, templateFilePath: String, renderer: TemplateRenderer)package com.hsbc.gbm.bd.clm.renderer

import com.hsbc.gbm.bd.clm.env.RawJobParams

trait TemplateRenderer {
  def render(params: RawJobParams, template: String, context: Map[String, AnyRef]): Unit
}
cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm/runner: Is a directory
package com.hsbc.gbm.bd.clm.runner

import com.hsbc.gbm.bd.clm.annotation.SparkJobName
import com.hsbc.gbm.bd.clm.env.{RawJobParams, SparkJob}
import io.github.classgraph.ClassGraph

import scala.collection.JavaConversions._
import scala.language.postfixOps

class GenericSparkJobRunner(jobName: String, configPath: String, params: Array[String]) {
  protected def closeJob(job: SparkJob): Unit = {
    job.close()
  }

  protected def run(): Unit = {
    val jobClassName = jobMappings()(jobName)
    val jobClass = Thread.currentThread.getContextClassLoader.loadClass(jobClassName)
    runScalaJob(jobClass.asInstanceOf[Class[SparkJob]])
  }

  protected def runScalaJob(jobClass: Class[SparkJob]): Unit = {
    val jobParams: RawJobParams = RawJobParams(params)

    val sparkJob: SparkJob = instantiateSparkJob(jobClass, jobParams)
    sparkJob.process()
    closeJob(sparkJob)
  }

  private def instantiateSparkJob(jobClass: Class[SparkJob], jobParams: RawJobParams): SparkJob = {
    jobClass
      .getConstructors
      .filter(_.getParameterTypes.length == 3)
      .find(constructor => {
        constructor.getParameterTypes()(0).isAssignableFrom(classOf[String]) &&
          constructor.getParameterTypes()(1).isAssignableFrom(classOf[String]) &&
          constructor.getParameterTypes()(2).isAssignableFrom(classOf[RawJobParams])
      })
      .get
      .newInstance(jobName, configPath, jobParams)
      .asInstanceOf[SparkJob]
  }

  protected def jobMappings(): Map[String, String] = {
    val scanResult = new ClassGraph().enableAllInfo.acceptPackages("com.hsbc.gbm.bd.clm").scan
    val classInfoList = scanResult.getClassesImplementing(classOf[SparkJob].getName)
    classInfoList.toList
      .map(classInfo => (classInfo.getAnnotationInfo(classOf[SparkJobName].getName), classInfo.getName))
      .filter(_._1 != null)
      .map(tuple => (tuple._1.getParameterValues.getValue("value").toString, tuple._2))
      .toMap
  }
}

object GenericSparkJobRunner {
  def main(args: Array[String]): Unit = {
    val jobName = args(0)
    val configPath = args(1)
    val params = args.slice(from = 2, until = args.length)

    new GenericSparkJobRunner(jobName, configPath, params).run()
  }
}



cat: ./clientlifecycle-common-core/src/main/scala/com/hsbc/gbm/bd/clm/utils: Is a directory
package com.hsbc.gbm.bd.clm.utils

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.SparkSession

object CsvUnitTestUtils {

  def csvToParquet(path: String, partition_col: Option[String] = None)(implicit spark: SparkSession): Unit = {
    val hadoopConf: Configuration = spark.sparkContext.hadoopConfiguration
    val fs: FileSystem = FileSystem.get(hadoopConf)

    val files = fs.listFiles(new Path(path), true)

    while (files.hasNext) {

      val path = files.next().getPath
      val df = spark.read.option("delimiter", "|").option("header", "true").csv(path.toString)
      df.storeAsParquet(path.getName.substring(0, if (path.getName.contains(".")) path.getName.lastIndexOf(".") else path.getName.length).concat(".parq"), partition_col)
    }
  }


  def csvToHiveTable(path: String, target: Option[String] = None)(implicit spark: SparkSession): Unit = {
    val hadoopConf: Configuration = spark.sparkContext.hadoopConfiguration
    val fs: FileSystem = FileSystem.get(hadoopConf)
    val files = fs.listFiles(new Path(path), true)

    while (files.hasNext) {

      val path = files.next().getPath
      val tableName = path.getName
      val dbName = path.getParent.getName
      val df = spark.read.option("delimiter", "|").option("header", "true").csv(path.toString)
      df.createOrReplaceTempView("temp")

      val location = target.map(t => s"$t/$dbName").getOrElse(dbName)

      val dbs = spark.sql(s"show databases").rdd.distinct().map(r => r(0).toString).collect.toList

      if (dbs.contains(dbName)) {
        spark.sql(s"""use $dbName""")
        spark.sql(s"drop table if exists $tableName")
        spark.sql(s" create table $tableName as select * from temp")
      } else {
        spark.sql(s"CREATE DATABASE $dbName location '$location' ")
        spark.sql(s"""use $dbName""")
        spark.sql(s"drop table if exists $tableName")
        spark.sql(s" create table $tableName as select * from temp")
      }
    }
  }
}
package com.hsbc.gbm.bd.clm.utils

import org.apache.commons.codec.binary.Base64
import javax.crypto._
import javax.crypto.spec.IvParameterSpec
import javax.crypto.spec.SecretKeySpec
import java.security._
import java.util

object EncryptionUtils {
  private val KEY = "DATA-PRODUCTS"
  private val DEFAULT_ALGORITHM = "AES"
  private val DEFAULT_CYPHER_TRANSFORMATION = "AES/CBC/PKCS5Padding"
  private val DEFAULT_HASHING = "HmacSHA256"
  private val IV_LENGTH = 16
  @transient private val secureRandom = new SecureRandom
  private val NONENCRYPTIONPREFIX = "NONENC_"

  private def getHmacSha256 = {
    val secret = "SecretForHmacSha256A"
    val sha256_HMAC = Mac.getInstance(DEFAULT_HASHING)
    val secret_key = new SecretKeySpec(secret.getBytes, DEFAULT_HASHING)
    sha256_HMAC.init(secret_key)
    sha256_HMAC.doFinal(KEY.getBytes)
  }

  def encrypt(password: String): String = {
    val initializationVector = getRandomInitializationVector
    val iv = new IvParameterSpec(initializationVector)
    val skeySpec = new SecretKeySpec(getHmacSha256, DEFAULT_ALGORITHM)
    val cipher = Cipher.getInstance(DEFAULT_CYPHER_TRANSFORMATION)
    cipher.init(Cipher.ENCRYPT_MODE, skeySpec, iv)
    val encrypted = cipher.doFinal(password.getBytes)
    val resultWithIv = new Array[Byte](IV_LENGTH + encrypted.length)
    System.arraycopy(initializationVector, 0, resultWithIv, 0, IV_LENGTH)
    System.arraycopy(encrypted, 0, resultWithIv, IV_LENGTH, encrypted.length)
    new String(Base64.encodeBase64String(resultWithIv))
  }

  private def getRandomInitializationVector = {
    new IvParameterSpec(secureRandom.generateSeed(IV_LENGTH)).getIV.clone.asInstanceOf[Array[Byte]]
  }

  def decrypt(encryptPassword: String): String = {
    if (encryptPassword != null && encryptPassword.indexOf(NONENCRYPTIONPREFIX) == 0) return encryptPassword.replaceFirst(NONENCRYPTIONPREFIX, "")
    val skeySpec = new SecretKeySpec(getHmacSha256, DEFAULT_ALGORITHM)
    val cipher = Cipher.getInstance(DEFAULT_CYPHER_TRANSFORMATION)
    val cipherText = Base64.decodeBase64(encryptPassword)
    val iv = new IvParameterSpec(util.Arrays.copyOfRange(cipherText, 0, IV_LENGTH))
    cipher.init(Cipher.DECRYPT_MODE, skeySpec, iv)
    val original = cipher.doFinal(cipherText, IV_LENGTH, cipherText.length - IV_LENGTH)
    new String(original)
  }

  private def exitIfSecurityPolicyIsLimited(): Unit = {
    try {
      val maxAllowedKeyLength = Cipher.getMaxAllowedKeyLength("AES/CBC/PKCS5Padding")
      if (maxAllowedKeyLength < 256) {
        println("JDK Security policy in " + System.getProperty("java.home") + " limits keys to " + maxAllowedKeyLength + " bits. Please upgrade the security policy on this JDK.")
        System.exit(1)
      }
      println("JDK security policy is unlimited!")
    } catch {
      case e: NoSuchAlgorithmException =>
        println("Not able to locate algorithms: " + e)
        System.exit(1)
    }
  }
}
package com.hsbc.gbm.bd.clm.utils

import java.net.URI
import java.security.cert.X509Certificate

import javax.net.ssl.{HostnameVerifier, SSLContext, SSLSession, X509TrustManager}
import org.apache.http.auth.{AuthScope, UsernamePasswordCredentials}
import org.apache.http.client.methods.{CloseableHttpResponse,HttpGet}
import org.apache.http.config.RegistryBuilder
import org.apache.http.conn.socket.{ConnectionSocketFactory, PlainConnectionSocketFactory}
import org.apache.http.conn.ssl.SSLConnectionSocketFactory
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.{BasicCredentialsProvider, CloseableHttpClient, HttpClients}
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager
import org.apache.http.util.EntityUtils
import org.apache.spark.SparkConf
import org.elasticsearch.hadoop.util.StringUtils

import scala.util.parsing.json.JSON

object EsHttpClientUtils {

  def getEsCount(indexName: String)(implicit sparkConf: SparkConf): Int = {
    val client = getHttpClient()


    val httpGet = new HttpGet()
    val esNodes = sparkConf.get("es.nodes")
    val esDefaultPort = sparkConf.get("es.port")

    @scala.annotation.tailrec
    def getHttpResponse(esNodes: String): CloseableHttpResponse = {
      if (!StringUtils.hasText(esNodes)) return null
      val node = esNodes.split(",").head
      val Array(host, port) = (if (node.contains(":")) node else s"${node.trim}:${esDefaultPort}").split(":")
      val esNode = s"${host}:${port}"
      val searchUrl = s"https://${esNode}/${indexName}/_count"

      httpGet.setURI(URI.create(searchUrl))

      val httpResponse = try {
        client.execute(httpGet)
      } catch {
        case e: Exception => null
      }

      httpResponse match {
        case x: CloseableHttpResponse if x.getStatusLine.getStatusCode == 200 => x
        case _ => getHttpResponse(if (esNodes.contains(",")) esNodes.substring(esNodes.indexOf(",") + 1) else "")
      }
    }

    val httpResponse = getHttpResponse(esNodes)
    if (httpResponse == null) throw new RuntimeException(s"httpResponse is null, it means all es node can not response a available result")

    val jsonOption = JSON.parseFull(EntityUtils.toString(httpResponse.getEntity, "UTF-8"))
    EntityUtils.consume(httpResponse.getEntity)
    httpResponse.close()
    client.close()

    def regJson(json: Option[Any]): Map[String, Any] = json match {
      case Some(map: Map[String, Any]) => map
    }

    @scala.annotation.tailrec
    def getFromJsonByXpath(input: Option[Any], xPath: String): Any = {
      val paths = xPath.split("\\.")
      val json = regJson(input).get(paths(0))
      if (paths.length > 1) getFromJsonByXpath(json, xPath.substring(xPath.indexOf(".") + 1)) else json.get
    }

    getFromJsonByXpath(jsonOption, "count").toString.toFloat.toInt
  }

  private def createSSLContextIgnoreVerifySSL(): SSLContext = {
    val sc = SSLContext.getInstance("TLSv1.2")
    val trustManager = new X509TrustManager() {
      override def checkClientTrusted(x509Certificates: Array[X509Certificate], s: String): Unit = {
      }

      override def checkServerTrusted(x509Certificates: Array[X509Certificate], s: String): Unit = {
      }

      override def getAcceptedIssuers: Array[X509Certificate] = null
    }
    sc.init(null, Array(trustManager), null)
    sc
  }

  //if you want to know why I use this way rather than dataframe/rdd api.
  // please through this link https://github.com/elastic/elasticsearch-hadoop/issues/867
  def getHttpClient()(implicit sparkConf: SparkConf): CloseableHttpClient = {
    val sslContext = createSSLContextIgnoreVerifySSL()

    val username = sparkConf.get("es.net.http.auth.user")
    val password = sparkConf.get("es.net.http.auth.pass")
    val credentials = new UsernamePasswordCredentials(username, password)
    val provider = new BasicCredentialsProvider()
    provider.setCredentials(AuthScope.ANY, credentials)

    val socketFactoryRegistry = RegistryBuilder.create[ConnectionSocketFactory]()
      .register("http", PlainConnectionSocketFactory.INSTANCE)
      .register("https", new SSLConnectionSocketFactory(sslContext, Array("TLSv1.1", "TLSv1.2", "TLSv1"), null, new HostnameVerifier {
        override def verify(s: String, sslSession: SSLSession): Boolean = true
      }))
      .build()

    val connManager = new PoolingHttpClientConnectionManager(socketFactoryRegistry)
    HttpClients.custom().setConnectionManager(connManager)
      .setDefaultCredentialsProvider(provider)
      .build()
  }
}
package com.hsbc.gbm.bd.clm.utils

import java.io.{File, FileInputStream, FileWriter}
import java.text.SimpleDateFormat
import java.util
import java.util.Date

import org.apache.commons.lang3.StringUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, FileUtil, Path}
import org.apache.spark.sql.{Column, Row, SparkSession}
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.yaml.snakeyaml.{DumperOptions, Yaml}

import scala.collection.{Map, Seq, immutable}
import scala.util.Random

object MockDataUtils {

  val randomParse: PartialFunction[String, String] = {
    case x if x.toLowerCase.contains("<@random") => {
      val pattern = "(<@random.*?/>)".r
      val first = if (pattern.findFirstIn(x).get.contains("(")) pattern.findFirstIn(x).get else pattern.findFirstIn(x).get.replace("random", "random(string, 5)")
      val firstRes = if (first.split(",").length == 2) {
        val tupleParams = """.*\(\s*(\S+)\s*,\s*(\d+)\s*\).*""".r
        val tupleParams(t, bit) = first
        t.trim.toLowerCase match {
          case "int" =>
            (for (_ <- 1 to bit.trim.toInt) yield Random.nextInt(10)).mkString("").replaceFirst("0", "1")
          case "string" =>
            val charTab = "abcdefghijklmnpqrstuvwxyzABCDEFGHIJKLMNPQRSTUVWXYZ"
            getRandom(charTab, bit.trim.toInt)
          case _ => throw new RuntimeException(s"Don't support type: ${t.trim}")
        }
      } else if (first.split("\\[").length == 2 && first.split("]").length == 2) {
        val pool = """.*\[\s*(.*)\s*\].*""".r
        val pool(list) = first
        getRandom(list.split(" ").filter(_ != ""))
      } else {
        throw new RuntimeException(s"Don't support: ${x}")
      }
      x.replaceFirst(pattern.findFirstIn(x).get.replace("(", """\(""")
        .replace(")", """\)""")
        .replace("[", """\[""")
        .replace("]", """\]""")
        , firstRes)
    }
  }

  val timestampParse: PartialFunction[String, String] = {
    case x if x.toLowerCase.contains("<@timestamp") =>
      val pattern = "(<@timestamp.*?/>)".r
      val first = if (pattern.findFirstIn(x).get.contains("(")) pattern.findFirstIn(x).get else pattern.findFirstIn(x).get.replace("timestamp", "timestamp(13)")
      val firstRes: String = if (first.contains("timestamp(13)")) {
        System.currentTimeMillis().toString
      } else if (first.contains("timestamp(10)")) {
        val sdf = new SimpleDateFormat("yyyyMMddHHmmss")
        sdf.parse(sdf.format(new Date())).getTime.toString
      } else {
        val pattern1 = """.*\((\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\).*""".r
        val pattern2 = """.*\((\d{14})\).*""".r
        val pattern3 = """.*\((\d{4}-\d{2}-\d{2})\).*""".r
        val pattern4 = """.*\((\d{8})\).*""".r

        val parsed = first match {
          case pattern1(x) => (x, "yyyy-MM-dd HH:mm:ss")
          case pattern2(x) => (x, "yyyyMMddHHmmss")
          case pattern3(x) => (x, "yyyy-MM-dd")
          case pattern4(x) => (x, "yyyyMMdd")
        }

        val sdf = new SimpleDateFormat(parsed._2)
        sdf.parse(parsed._1).getTime.toString
      }

      x.replaceFirst(pattern.findFirstIn(x).get.replace("(", """\(""").replace(")", """\)"""), firstRes)
  }

  def getRandom(pool: String, length: Int): String = {
    (for (_ <- 1 to length) yield pool.charAt(Random.nextInt(pool.length))).mkString("")
  }

  def getRandom(pool: Array[String]): String = {
    pool(Random.nextInt(pool.length))
  }

  def writeMockData(outputPath: String = "src/test/resources/mock-data-content/parq", processFunc: Option[PartialFunction[String, String]] = None, num: Int = 5, mappingPath: String = "src/main/resources/mapping.yml", mockRulePath: String = "src/test/resources/mock_rule.yml")(implicit spark: SparkSession): Unit = {
    import scala.collection.JavaConverters._

    if (!new File(mappingPath).exists()) throw new RuntimeException(s"mapping file not exist on $mappingPath")

    val mappingFIS = new FileInputStream(mappingPath)
    val mappingMap: util.Map[String, Object] = new Yaml().load(mappingFIS)
    val mapping: immutable.Map[String, Object] = mappingMap.asScala.toMap
    val allFields = mapping.mapValues(_.asInstanceOf[util.LinkedHashMap[String, String]].asScala.toMap.keySet.map(col).toList)

    writeMockRuleFile(allFields)
    val mockRuleFIS = new FileInputStream(mockRulePath)
    val mockRuleMap: util.Map[String, Object] = new Yaml().load(mockRuleFIS)
    val ruleMap: immutable.Map[String, Object] = mockRuleMap.asScala.toMap

    val processPartial = List(randomParse, timestampParse)
      .foldLeft(processFunc.getOrElse({
        case "" => ""
      }: PartialFunction[String, String]))((x, e) => x orElse e)

    val rule = ruleMap.mapValues(_.asInstanceOf[util.LinkedHashMap[String, String]].asScala.toMap)
    rule.mapValues(xs => {
      xs.map(x => {
        x._1 -> {
          if (x._2.trim.length == 0) duplicateRecord(x._1, processPartial, num)
          else duplicateRecord(x._2, processPartial, num)
        }
      })
    })
      .foreach(x => {
        val init = (for (_ <- 1 to num) yield List[String]()).toList
        val rows = x._2.values.foldLeft(List[List[String]](init: _*))((xs, e) => {
          xs.zip(e).map(m => m._1 ++ List(m._2))
        }).map(m => Row(m: _*)).asJava
        val st = StructType(x._2.keySet.toList.map(StructField(_, StringType, nullable = true)))
        spark.createDataFrame(rows, st)
          .write
          .option("delimiter", "|")
          .option("header", "true")
          .mode("Overwrite")
          .format("com.databricks.spark.csv")
          .save(s"${outputPath}/${x._1}")

        merge(s"${outputPath}/${x._1}", s"${outputPath}/${x._1}.csv")
      })
  }

  //TODO: when run on jenkins it will has more record as same as header, but run on local not appear
  def merge(srcPath: String, dstPath: String): Unit = {
    val dstFile = new File(dstPath)
    if (dstFile.exists()) dstFile.delete()

    val file = new File(srcPath)
    if (file.isDirectory) {
      file.listFiles().foreach(f => {
        if (!f.getName.endsWith("csv")) f.deleteOnExit()
      })
    }

    val hadoopConfig = new Configuration()
    val hdfs = FileSystem.get(hadoopConfig)
    FileUtil.copyMerge(hdfs, new Path(srcPath), hdfs, new Path(dstPath), true, hadoopConfig, null)

    if (file.getParentFile.isDirectory) {
      file.getParentFile.listFiles().foreach(f => {
        if (f.getName.endsWith("crc")) f.deleteOnExit()
      })
    }
  }

  def duplicateRecord(rule: String, processFunc: PartialFunction[String, String], num: Int = 5): Array[String] = {
    val seq = for (i <- 1 to num)
      yield fillByRule(rule, processFunc) + (if (rule.contains("<@")) "" else s"_$i")
    seq.toArray
  }

  @scala.annotation.tailrec
  def fillByRule(rule: String, processFunc: PartialFunction[String, String]): String = {
    rule match {
      case x: String if x.contains("<@") => fillByRule(processFunc(x), processFunc)
      case x => x
    }
  }

  def writeMockRuleFile(entity: Map[String, Seq[Column]], ruleFilePath: String = "src/test/resources/mock_rule.yml"): Unit = {
    import scala.collection.JavaConverters._

    val options = new DumperOptions()
    options.setIndent(2)
    options.setPrettyFlow(true)
    options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK)

    val yaml = new Yaml(options)

    val mockRuleFile = new File(ruleFilePath)
    val previousRule = {
      if (mockRuleFile.exists()) {
        val mockRuleFIS = new FileInputStream(ruleFilePath)
        val mockRuleMap: util.Map[String, Object] = new Yaml().load(mockRuleFIS)
        val ruleMap: immutable.Map[String, Object] = mockRuleMap.asScala.toMap
        ruleMap.mapValues(_.asInstanceOf[util.LinkedHashMap[String, String]])
      } else Map[String, util.Map[String, String]]()
    }.mapValues(_.asScala.toMap)

    val blankRule = entity.mapValues(xs => xs.map(_.expr.sql.split(" AS ")(0).replaceAll("`", "")).map(x => x -> "")).mapValues(_.toMap)

    val union = blankRule.map(x => {
      val history = previousRule.getOrElse(x._1, x._2)
      val currentKeys = x._2.keySet
      val finalMap = currentKeys.foldLeft(List[(String, String)]())(
        (xs, e) => {
          val t = history.getOrElse(e, x._2(e))
          xs :+ (e, t)
        }
      )
      (x._1, finalMap.toMap)
    }).mapValues(_.asJava).asJava

    val fw = new FileWriter(ruleFilePath)
    yaml.dump(union, fw)
  }
}
package com.hsbc.gbm.bd.clm

import java.io.FileInputStream

import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Column, DataFrame, Dataset, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.SPARK_VERSION
import org.slf4j.LoggerFactory
import com.hsbc.datafactory.api.DataFactoryAPI
import com.hsbc.datafactory.api.model.SnapshotMetaDataResponse
import org.apache.spark.sql.expressions.Window

import scala.annotation.tailrec
import scala.collection.{Map, mutable}
import org.elasticsearch.spark.sql._
import java.text.SimpleDateFormat
import java.time.ZonedDateTime
import java.time.format.DateTimeFormatter
import java.util
import java.util.{Calendar, Date}

import org.apache.commons.lang3.StringUtils
import org.yaml.snakeyaml.Yaml

import scala.util.{Failure, Success, Try}

package object utils {
  implicit def sparkDataFrameFunctions(df: DataFrame): ExtendedDataFrameFunctions = new ExtendedDataFrameFunctions(df)

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  sealed class ExtendedDataFrameFunctions(report: DataFrame) {
    lazy val reportCnt = report.count()
    lazy val repartitionReport = report.repartition(getPartitionNumberByDataSize(report))
    lazy val uid = report.sparkSession.sparkContext.getConf.getOption("uid")

    // this function just for data lineage, in fact, it is substantive effect
    def cut(aliasName: String)(implicit spark: SparkSession): DataFrame = {
      val env = spark.sparkContext.getConf.get("spark.master", "local")
      val randomFieldName = report.schema.filter(x => x.dataType.isInstanceOf[StringType]).head.name

      if (env.startsWith("local")) report.filter(s"`$randomFieldName` != '__useless_$aliasName'") else report
    }

    def except(right: DataFrame, ignoreColumns: Seq[String], ignoreNull: Boolean = false): DataFrame = {
      val matchColumns = report.drop(ignoreColumns: _*).columns

      if (ignoreNull) {
        val leftColumns = report.drop(ignoreColumns: _*)
        val rightColumns = right.drop(ignoreColumns: _*)
        val l = leftColumns.select(leftColumns.schema.map(nullStringColumnToBlank): _*)
        val r = rightColumns.select(leftColumns.schema.map(nullStringColumnToBlank): _*)

        l.join(r, matchColumns, "left_anti")
      } else {
        report.join(right, matchColumns, "left_anti")
      }
    }

    def exceptAll(right: DataFrame, ignoreColumns: Seq[String], ignoreNull: Boolean = false): DataFrame = {
      report.except(right, ignoreColumns, ignoreNull).withColumn("_except", lit("left"))
        .unionByName(
          right.except(report, ignoreColumns, ignoreNull).withColumn("_except", lit("right"))
        )
    }

    private def nullStringColumnToBlank(column: StructField): Column = {
      column.dataType match {
        case StringType =>
          when(col(column.name).isNull, lit(""))
            .otherwise(col(column.name))
            .alias(column.name)
        case _ => col(column.name)
      }
    }

    /**
     * rename df, if name not exist will throw Exception
     *
     * @param pairs
     * @return
     */
    def rename(pairs: Map[String, String], selected: Boolean = true): DataFrame =
      pairs.foldLeft(if (selected) report.select(pairs.keySet.toList.map(col): _*) else report) { (df, pair) => df.withColumnRenamed(pair._1, pair._2) }

    /**
     * skew join, implement of skew value specific process
     *
     * @param right
     * @param joinExprs
     * @param joinType
     * @param filterKey
     * @param skewValues
     * @return
     */
    def skewJoin(right: Dataset[_]
                 , joinExprs: Column
                 , joinType: String
                 , filterKey: Seq[String]
                 , skewValues: Seq[String])(implicit spark: SparkSession): DataFrame = {
      val env = spark.sparkContext.getConf.get("spark.master", "local")
      val joinResult = report.join(right, joinExprs, joinType)
      if (env.startsWith("local") || SPARK_VERSION.startsWith("3")) joinResult
      else joinResult.hint("SKEW_JOIN", s"filter_key(${filterKey.mkString(",")})", s"skew_values(${skewValues.mkString(",")})")
    }

    /**
     * skew join, implement of skew value specific process
     *
     * @param right
     * @param usingColumns
     * @param joinType
     * @param filterKey
     * @param skewValues
     * @return
     */
    def skewJoin(right: Dataset[_]
                 , usingColumns: Seq[String]
                 , joinType: String
                 , filterKey: Seq[String]
                 , skewValues: Seq[String])(implicit spark: SparkSession): DataFrame = {
      val env = spark.sparkContext.getConf.get("spark.master", "local")
      val joinResult = report.join(right, usingColumns, joinType)
      if (env.startsWith("local") || SPARK_VERSION.startsWith("3")) joinResult
      else joinResult.hint("SKEW_JOIN", s"filter_key(${filterKey.mkString(",")})", s"skew_values(${skewValues.mkString(",")})")
    }

    def getSkewValues(key: String, samplePercent: Int = 10, topN: Int = 100)(implicit spark: SparkSession): DataFrame = {
      report.createOrReplaceTempView("__temp__")

      //get skew key table
      spark.sql(
        s"""
           |select count(1) __cnt__
           |  ,${key}
           |from __temp__ TABLESAMPLE (${samplePercent} PERCENT)
           |group by ${key}
           |order by __cnt__ desc
           |limit $topN
           |""".stripMargin)
    }

    /**
     * just support left join and inner join
     * skew join, implement of skew value random distribute
     *
     * @param right    right table
     * @param leftKey  left table key which for filter
     * @param rightKey right table key which for filter
     * @param skewValues
     * @param spark
     * @return (leftTable, rightTable)
     */
    def preSkewJoin(right: DataFrame, leftKey: String, rightKey: String, skewValues: Seq[String], divide: Int = 100, force: Boolean = false)(implicit spark: SparkSession): (DataFrame, DataFrame) = {

      val env = spark.sparkContext.getConf.get("spark.master", "local")
      if (env.startsWith("local") || (SPARK_VERSION.startsWith("3") && !force)) return (report.withColumn(s"__${leftKey}__", col(leftKey)), right.withColumn(s"__${rightKey}__", col(rightKey)))

      val skewTable = spark.createDataFrame(skewValues.distinct.map((_, "active"))).toDF(leftKey, "__status__")

      //make the specific key random distribute
      val leftTable = report.join(skewTable, Seq(leftKey), "left")
        .withColumn(s"__${leftKey}__", when(skewTable("__status__").isNotNull, concat((rand() * divide).cast("int"), lit("_"), report(leftKey))).otherwise(report(leftKey)))
        .drop("__status__")

      //add __status__ to __side_table, as similar as add a flag
      val temp = right.join(skewTable, right(rightKey) === skewTable(leftKey), "left").drop(skewTable(leftKey))

      //make the side table explode
      val part1 = temp.filter(col("__status__").isNull).drop("__status__").withColumn(s"__${rightKey}__", col(rightKey))
      val part2 = temp.filter(col("__status__").isNotNull)
        .withColumn("__prefix__", explode_outer(split(lit((0 until divide).mkString(",")), ",")))
        .withColumn(s"__${rightKey}__", concat(col("__prefix__"), lit("_"), col(rightKey)))
        .drop("__status__", "__prefix__")
      val rightTable = part1.unionByName(part2)

      (leftTable, rightTable)
    }

    def dropDuplicateCols(rmDF: DataFrame): DataFrame = {
      val cols = report.columns.groupBy(identity).mapValues(_.length).filter(_._2 > 1).keySet.toSeq

      @tailrec
      def dropCol(df: DataFrame, cols: Seq[String]): DataFrame = {
        if (cols.isEmpty) df else dropCol(df.drop(rmDF(cols.head)), cols.tail)
      }

      dropCol(report, cols)
    }

    //drop duplicate columns, usually after join. btw, the parameter order is necessary, get left dataframe column when multiple  not null
    //just support join(right: Dataset[_], usingColumns: Seq[String], joinType: String)
    def dropDuplicateColumns(dfs: DataFrame*): DataFrame = {
      val total = report.columns.toSet
      val totalList = report.columns.toList
      total.foldLeft(report)((df, column) => {
        column match {
          case x if totalList.count(_ == x) > 1 =>
            dfs.filter(f => f.columns.contains(x))
              .map(_ (x))
              .tail
              .foldLeft(df)((f, c) => f.drop(c))
          case _ => df
        }
      }
      )
    }

    //drop duplicate columns, usually after join. btw, the parameter order is necessary, get left dataframe column when multiple  not null
    //just support join(right: Dataset[_], usingColumns: Seq[String], joinType: String)
    def dropDuplicateColumnsWithCoalesce(dfs: DataFrame*): DataFrame = {
      val total = report.columns.toSet
      val totalList = report.columns.toList
      total.foldLeft(report)((df, column) => {
        column match {
          case x if totalList.count(_ == x) > 1 =>
            df.withColumn(s"__${x}__random", coalesce(dfs.map(f => {
              if (f.columns.toSet.contains(x)) f(x) else lit(null)
            }): _*))
              .drop(x)
              .withColumnRenamed(s"__${x}__random", x)
          case _ => df
        }
      }
      )
    }

    def trimAllColumnsToNull: DataFrame = {
      report.select(
        report.schema.map(c =>
          trimOnlyStringColumnToNull(c)
        ): _*
      )
    }

    def trimAllColumnsButNotUidToNull: DataFrame = {
      report.select(
        report.schema.map(c => {
          val uniqueIds: List[String] = if (uid.isDefined) uid.get.split(",").toList.map(_.trim) else List[String]()
          if (uniqueIds.contains(c.name)) col(c.name)
          else trimOnlyStringColumnToNull(c)
        }
        ): _*
      )
    }

    def trimColumnsToNull(cols: String*): DataFrame = {
      cols.foldLeft(report)((df, c) => {
        if (df.columns.contains(c)) {
          df.withColumn(c, trimOnlyStringColumnToNull(df.schema(c)))
        } else {
          df
        }
      })
    }

    private def trimOnlyStringColumnToNull(column: StructField): Column = {
      column.dataType match {
        case StringType => when(trim(col(column.name)) === "", lit(null))
          .otherwise(trim(col(column.name)))
          .alias(column.name)
        case _ => col(column.name)
      }
    }

    /**
     * same as rename, but don't throw Exception when name not exist
     *
     * @param originalToRename
     * @return
     */
    def applyRenames(originalToRename: Map[String, String]): DataFrame =
      report.select(
        report.columns.map(c =>
          if (originalToRename.keySet.contains(c)) col(c) as originalToRename(c) else col(c)
        ): _*
      )

    def applyInserts(inserts: Map[String, String]): DataFrame = {
      inserts.foldLeft(report) { (df, key) =>
        df.withColumn(key._1, lit(key._2))
      }
    }

    def storeAsParquet(path: String, partition_col: Option[String] = None, retention: Int = 7)(implicit spark: SparkSession): Unit = {
      partition_col match {
        case Some(column) => {
          val formatter: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss.SSSXXXX")
          val date: String = ZonedDateTime.now().format(formatter).substring(0, 10).replaceAll("[-|:'T]", "")

          //write to hdfs
          repartitionReport.withColumn(column, lit(date))
            .write
            .mode("Overwrite")
            .option("header", value = true)
            .option("mapreduce.fileoutputcommitter.algorithm.version", 2)
            .partitionBy(column)
            .parquet(path)

          //delete other partitions
          spark.read.parquet(path)
            .select(col(column).cast("int"))
            .distinct
            .filter(s"$column < $date - $retention")
            .collect()
            .foreach(row => {
              val partition = row.getAs[Int](column)
              val fileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)
              val needRemovePath = new Path(s"$path/$column=$partition")
              if (fileSystem.exists(needRemovePath)) {
                fileSystem.delete(needRemovePath, true)
              }
            })
        }
        case Some(column) => repartitionReport.write.mode("Overwrite").option("header", value = true).partitionBy(column).parquet(path)
        case None => repartitionReport.write.mode("Overwrite").option("header", value = true).parquet(path)
      }
    }

    def write2Es(rawIndexName: String, cdaLocation: String): Unit = {
      write2Es(rawIndexName, cdaLocation, Option(Map("es.mapping.id" -> "uid")))
    }

    def write2Es(rawIndexName: String, cdaLocation: String, cfg: Option[scala.collection.Map[String, String]]): Unit = {
      val spark = report.sparkSession
      val formatter: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss.SSSXXXX")
      val processingTime: String = ZonedDateTime.now().format(formatter).substring(0, 19).replaceAll("[-|:'T]", "")

      val pattern = "<date>".r
      val indexName = pattern.replaceAllIn(rawIndexName, processingTime)

      val esReport = report
        .withColumn("Processing_Timestamp", lit(processingTime))
        .withColumn("CDA_Location", lit(cdaLocation))

      implicit val sparkConf: SparkConf = spark.sparkContext.getConf

      if (reportCnt < 100000000) {
        if (cfg.isDefined && uid.isDefined) {
          val p = """.*\((.*)\).*""".r
          val p(uniqueId) = uid.getOrElse(throw new RuntimeException("Please provide uid define on application yaml."))
          esReport
            .withColumn("uid", base64(concat_ws("-", uniqueId.split(",").map(x => coalesce(col(x.trim), lit("Null"))): _*)))
            .saveToEs(indexName, cfg.get)
        } else
          esReport.saveToEs(indexName)
      }
      else {
        throw new RuntimeException(s"The row count is over 0.1 billion, so we can't write it to ES")
      }

      //es.batch.write.refresh was set as false,it means index would not be update right now.
      //sleep 1s to make sure the index is totally update, the time should be fit with the index.refresh_interval
      //in test-application-scala.yml
      Thread.sleep(5 * 1000)

      val readCnt = EsHttpClientUtils.getEsCount(indexName)

      if (!(reportCnt - 100 < readCnt && readCnt < reportCnt + 100)) throw new RuntimeException(s"write es rows are not eq read rows, wrote:$reportCnt != read:$readCnt ")
    }

    def buildFullSchema(schema: StructType)(implicit spark: SparkSession): DataFrame = {
      schema.fields.foldLeft(report) { (newDF: DataFrame, field: StructField) =>
        if (report.columns.contains(field.name)) {
          newDF
        } else {
          newDF.withColumn(field.name, lit(null).cast(field.dataType))
        }
      }.select(schema.fieldNames.head, schema.fieldNames.tail: _*)
    }

    private def getPartitionNumberByDataSize(input: DataFrame): Int = {
      if (reportCnt == 0) return 1
      val takeN = 100
      val fraction = if (takeN / reportCnt >= 1) 1 else takeN.toFloat / reportCnt
      // 128M
      val blockSize = 128
      val byteToMBRatio = 1024 * 1024
      // parquet format compression ratio perhaps (1 - 0.19)
      val compressionRatio = 0.19
      val sampleData = input.sample(withReplacement = false, fraction, seed = 0).collect()
      val sampleCnt = sampleData.length
      val sampleDataByteSize = sampleData.foldLeft(0)((x, row) => {
        x + row.toString().getBytes("utf-8").length
      })
      val dataByte = (reportCnt / sampleCnt * sampleDataByteSize) * compressionRatio

      (dataByte / byteToMBRatio / blockSize).toInt + 1
    }

  }


  def getLatestSnapshotForBatchId(assetList: List[String], host: String, port: Int, initBatchId: String, span: Int = 7)(implicit spark: SparkSession): Map[String, String] = {
    val api = new DataFactoryAPI(host, port)

    val hadoopConf = spark.sparkContext.hadoopConfiguration
    val fs: FileSystem = FileSystem.get(hadoopConf)

    val metaData = (batchId: String) => {
      api.getSnapshot(batchId) match {
        case Success(x) => x
        case Failure(ex) => throw new RuntimeException(s"Could not find ${batchId} data from path $host:$port", ex)
      }
    }

    val assetsPath = (assets: List[String], batchId: String) => assets.foldLeft(Map[String, String]())(
      (m, ele) => {
        val element = if (ele.contains(":")) ele else s"$ele:$ele"
        val Array(source, asset) = element.split(":")
        val pathMap = source match {
          case x: String if "gda" == x.toLowerCase =>
            val t = Try(metaData(batchId).getGDAsForSource(asset).get(asset))
            (s"${x}:${asset}", if (t.isSuccess && t.get.isDefined) t.get.get.gdaPath else "")
          case _ =>
            val t = Try(metaData(batchId).getMDAsForSource(source).get(asset))
            (s"${source}:${asset}", if (t.isSuccess && t.get.isDefined) t.get.get.path else "")
        }
        m + pathMap
      }
    ).partition(x => if (StringUtils.isNotBlank(x._2)) fs.exists(new Path(x._2.trim)) else false)

    val Array(batchDate, erType) = initBatchId.split("_")
    val sdf = new SimpleDateFormat("yyyy-MM-dd")
    val date: Date = sdf.parse(batchDate)
    val cal = Calendar.getInstance()
    cal.setTime(date)

    val finalPath = (1 to span).toList.foldLeft(assetsPath(assetList, initBatchId))(
      (xs, x) => {
        if (xs._2.nonEmpty) {
          cal.add(Calendar.DATE, -x)
          val newDay = cal.getTime
          val batchId: String = sdf.format(newDay) + "_" + erType
          cal.setTime(date)
          val nextBatchAsset = assetsPath(xs._2.keySet.toList, batchId)
          (xs._1 ++ nextBatchAsset._1, nextBatchAsset._2)
        } else xs
      }
    )
    if (finalPath._2.nonEmpty) throw new RuntimeException(
      s"""
         |Below asset not exist, please check batchId($initBatchId) and span($span) length
         |    ${finalPath._2.mkString("\r\n")}
         |""".stripMargin)
    finalPath._1.map(x => (x._1.split(":")(1), x._2))
  }

  /**
   * get skew values
   *
   * @param pathEntry asset path
   * @param key       skew key
   * @param threshold skew count / count
   * @param topN      top N
   * @param spark     spark session
   * @return (skew key, (asset key, skew count, skew rate))
   */
  def getSkewValues(pathEntry: scala.collection.Map[String, String], key: String, threshold: Double = 0.05, topN: Int = 5)(implicit spark: SparkSession): Map[String, (String, Long, Double)] = {
    pathEntry.foldLeft(scala.collection.Map[String, (String, Long, Double)]())((x, xs) => {
      val df = spark.read.parquet(xs._2).cache
      val cnt = df.count
      val window = Window.partitionBy(key).orderBy(col("__cnt__").desc)
      val res = df
        .groupBy(key)
        .agg(count(key).as("__cnt__"))
        .withColumn("__rn__", row_number().over(window))
        .withColumn("__cnt", lit(cnt))
        .filter(s"(__cnt__ / __cnt) > $threshold")
        .withColumn("__rate__", col("__cnt__") / col("__cnt"))
        .filter(s"__rn__ <= $topN")
        .select(key, "__cnt__", "__rate__")
        .take(topN)
      df.unpersist()
      x ++ res.map(r => (r.getAs[String](key), (xs._1, r.getAs[Long]("__cnt__"), r.getAs[Double]("__rate__"))))
    })
  }
}
cat: ./clientlifecycle-common-core/src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./clientlifecycle-common-core/src/site/resources: Is a directory
cat: ./clientlifecycle-common-core/src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./clientlifecycle-common-core/src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>cat: ./clientlifecycle-common-core/src/test: Is a directory
cat: ./clientlifecycle-common-core/src/test/resources: Is a directory
cat: ./clientlifecycle-common-core/src/test/resources/config: Is a directory
spark:
  spark.master: "local[*]"
  spark.ui.enabled: "false"
  spark.app.id: "123456"
  spark.driver.host: "localhost"
  spark.local.dir : "tmp/spark-temp"
  spark.sql.catalogImplementation: hive
  spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
  derby.system.home: "hive/"
  javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=hive/metastore_db;create=true"
  kryoserializer: org.apache.spark.sql.execution.columnar.CachedBatch,[[B,org.apache.spark.sql.catalyst.expressions.GenericInternalRow

#  es.nodes: gbl20169872.systems.uk.hsbc:19201
  es.nodes: gbl20169876.systems.uk.hsbc:19201
  es.net.http.auth.user: clm-test
  es.net.http.auth.pass: cVUb30OEiZVKIes6g8MIpYso+zdyJybCnErMnM0Rl6c=
  es.port: 19201
  es.index.auto.create: true
  cluster.name: elasticsearch
  client.transport.sniff: true
  client.transport.ignore_cluster_name: true
  es.xpack.security.user: elastic:byiF7KeOP9EtCtkmgMlyttOWcO55La2WasVQb1oKnss=
  index.auto.create: true
  index.number_of_replicas: 0
  index.refresh_interval: 1s
  es.nodes.wan.only: true
  es.net.ssl: true

app:
  app_name: "dataproducts-common"
  write.es: "false"
  retention: 7
  es.index-name: "clm-test"

  monitoring:
    location: "tmp/monitoring"
    table.name: "monitoring"
  some-key:
    string-list:
      - item1
      - item2
      - item3
    int-list:
      - 101
      - 1045
      - 8625
    double-list:
      - 1.01
      - 104.5
      - 8625.16548
    map-of-strings:
      item1: value1
      item2: value2
      item3: value3
    map-of-ints:
      item1: 168
      item2: 2569
      item3: -14
    map-of-doubles:
      item1: 16.81256
      item2: 2.5690745
      item3: -0.1425456
    map-of-booleans:
      item1: true
      item2: false
      item3: true
  bool-property: true
  int-property: 123
  double-property: 456.789
  output: "csv_data.parq"
  db_table: "db_demo.table_demo"


defaults:
    master: yarn
    deploy_mode: cluster
    queue: uc-prod
    num_executors: 10
    executor_cores: 5
    executor_memory: 10G
    driver_memory: 6G
    config_file: application.yml

conf:
    - spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.resource=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.driver.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.file=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.sql.catalogImplementation=hive
    - spark.yarn.maxAppAttempts=1
    - spark.shuffle.service.enabled=true
    - spark.driver.maxResultSize=0g
    - spark.default.parallelism=300
    - spark.shuffle.file.buffer=128K
    - spark.sql.shuffle.partitions=300
    - spark.sql.autoBroadcastJoinThreshold=-1
    - spark.sql.join.preferSortMergeJoin=true
    - spark.sql.broadcastTimeout=300s
    - spark.executor.heartbeatInterval=20s
    - spark.dynamicAllocation.enabled=true
    - spark.dynamicAllocation.executorIdleTimeout=300s
    - spark.dynamicAllocation.cachedExecutorIdleTimeout=30min
    - spark.dynamicAllocation.initialExecutors=10
    - spark.dynamicAllocation.maxExecutors=10
    - spark.dynamicAllocation.minExecutors=5
    - spark.dynamicAllocation.schedulerBacklogTimeout=1s
    - spark.memory.fraction=0.6
    - spark.memory.storageFraction=0.3
    - spark.shuffle.sort.bypassMergeThreshold=1000

global_env:
    HDP_VERSION: 2.6.5.0-292
    LOGS_BASE_LOCATION: /hsbc-log/CLM/spark
    KEYTAB_LOCATION: /home/$CURRENT_USER/$CURRENT_USER.headless.keytab

local_env:

indexs:
    score5-successful:
        latest_alias_name: score5-successful-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-successful-alias-all
            force: true
    score5-failed:
        latest_alias_name: score5-failed-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-failed-alias-all
            force: true
    clm-csp-glcm:
        latest_alias_name: clm-csp-glcm-alias-latest
        policy:
            name: clm-policy_7d25g
            delete_interval: 7d
            max_size: 25gb
        template:
            rollover_alias: clm-csp-glcm-alias-all

files:
    - /usr/hdp/current/spark2-client/config/hive-site.xml
    - ${app_yaml}#$JOB_CONFIG_FILE

entrypoints:
    #GLCM CDA
    -   name: clm-glcm-cda
        class: com.hsbc.gbm.bd.clm.glcm.Main
    #GTRF CDA
    -   name: clm-gtrf-cda
        class: com.hsbc.gbm.bd.clm.gtrf.Main
    #FX CDA
    -   name: clm-fx-cda
        class: com.hsbc.gbm.bd.clm.fx.Main
    #GMnonFX CDA
    -   name: clm-gmnonfx-cda
        class: com.hsbc.gbm.bd.clm.gmnonfx.Main
    #HSS CDA
    -   name: clm-hss-cda
        class: com.hsbc.gbm.bd.clm.hss.Maindefaults:
    master: yarn
    deploy_mode: cluster
    queue: use-case
    num_executors: 10
    executor_cores: 5
    executor_memory: 10G
    driver_memory: 6G
    config_file: application.yml

conf:
    - spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.resource=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.driver.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.file=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.sql.catalogImplementation=hive
    - spark.yarn.maxAppAttempts=1
    - spark.shuffle.service.enabled=true
    - spark.driver.maxResultSize=0g
    - spark.default.parallelism=300
    - spark.shuffle.file.buffer=128K
    - spark.sql.shuffle.partitions=300
    - spark.sql.autoBroadcastJoinThreshold=-1
    - spark.sql.join.preferSortMergeJoin=true
    - spark.sql.broadcastTimeout=300s
    - spark.executor.heartbeatInterval=20s
    - spark.dynamicAllocation.enabled=true
    - spark.dynamicAllocation.executorIdleTimeout=300s
    - spark.dynamicAllocation.cachedExecutorIdleTimeout=30min
    - spark.dynamicAllocation.initialExecutors=10
    - spark.dynamicAllocation.maxExecutors=10
    - spark.dynamicAllocation.minExecutors=5
    - spark.dynamicAllocation.schedulerBacklogTimeout=1s
    - spark.memory.fraction=0.6
    - spark.memory.storageFraction=0.3
    - spark.shuffle.sort.bypassMergeThreshold=1000

global_env:
    HDP_VERSION: 3.1.5.0-152
    LOGS_BASE_LOCATION: /upload/0/clm/logs
    KEYTAB_LOCATION: /home/$CURRENT_USER/$CURRENT_USER.headless.keytab

local_env:

indexs:
    score5-successful:
        latest_alias_name: score5-successful-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-successful-alias-all
            force: true
    score5-failed:
        latest_alias_name: score5-failed-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-failed-alias-all
            force: true

files:
    - /usr/hdp/current/spark2-client/config/hive-site.xml
    - ${app_yaml}#$JOB_CONFIG_FILE

entrypoints:
    #GLCM CDA
    -   name: clm-glcm-cda
        class: com.hsbc.gbm.bd.clm.glcm.Main
    #GTRF CDA
    -   name: clm-gtrf-cda
        class: com.hsbc.gbm.bd.clm.gtrf.Main
    #FX CDA
    -   name: clm-fx-cda
        class: com.hsbc.gbm.bd.clm.fx.Main
    #GMnonFX CDA
    -   name: clm-gmnonfx-cda
        class: com.hsbc.gbm.bd.clm.gmnonfx.Main
    #HSS CDA
    -   name: clm-hss-cda
        class: com.hsbc.gbm.bd.clm.hss.Maindefaults:
    master: yarn
    deploy_mode: cluster
    queue: default
    num_executors: 10
    executor_cores: 5
    executor_memory: 10G
    driver_memory: 6G
    config_file: application.yml

conf:
    - spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.resource=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.driver.extraJavaOptions=-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Dconfig.file=${JOB_CONFIG_FILE} -Dlog4j.configuration=log4j
    - spark.sql.catalogImplementation=hive
    - spark.yarn.maxAppAttempts=1
    - spark.shuffle.service.enabled=true
    - spark.driver.maxResultSize=0g
    - spark.default.parallelism=300
    - spark.shuffle.file.buffer=128K
    - spark.sql.shuffle.partitions=300
    - spark.sql.autoBroadcastJoinThreshold=-1
    - spark.sql.join.preferSortMergeJoin=true
    - spark.sql.broadcastTimeout=300s
    - spark.executor.heartbeatInterval=20s
    - spark.dynamicAllocation.enabled=true
    - spark.dynamicAllocation.executorIdleTimeout=300s
    - spark.dynamicAllocation.cachedExecutorIdleTimeout=30min
    - spark.dynamicAllocation.initialExecutors=10
    - spark.dynamicAllocation.maxExecutors=10
    - spark.dynamicAllocation.minExecutors=5
    - spark.dynamicAllocation.schedulerBacklogTimeout=1s
    - spark.memory.fraction=0.6
    - spark.memory.storageFraction=0.3
    - spark.shuffle.sort.bypassMergeThreshold=1000

global_env:
    CDH_VERSION: 7.1.6-1
    LOGS_BASE_LOCATION: /upload/0/clm/logs
    KEYTAB_LOCATION: /home/$CURRENT_USER/$CURRENT_USER.headless.keytab

local_env:

indexs:
    score5-successful:
        latest_alias_name: score5-successful-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-successful-alias-all
            force: true
    score5-failed:
        latest_alias_name: score5-failed-alias-latest
        policy:
            name: score5_policy_7d25g
            delete_interval: 7d
            max_size: 25gb
            force: true
        template:
            rollover_alias: score5-failed-alias-all
            force: true

files:
    - /usr/hdp/current/spark2-client/config/hive-site.xml
    - ${app_yaml}#$JOB_CONFIG_FILE

entrypoints:
    #GLCM CDA
    -   name: clm-glcm-cda
        class: com.hsbc.gbm.bd.clm.glcm.Main
    #GTRF CDA
    -   name: clm-gtrf-cda
        class: com.hsbc.gbm.bd.clm.gtrf.Main
    #FX CDA
    -   name: clm-fx-cda
        class: com.hsbc.gbm.bd.clm.fx.Main
    #GMnonFX CDA
    -   name: clm-gmnonfx-cda
        class: com.hsbc.gbm.bd.clm.gmnonfx.Main
    #HSS CDA
    -   name: clm-hss-cda
        class: com.hsbc.gbm.bd.clm.hss.Main  spark:
    spark.master: "local[*]"
    spark.ui.enabled: "false"
    spark.app.id: "123456"
    spark.driver.host: "localhost"
    spark.local.dir : "tmp/spark-temp"
    spark.sql.catalogImplementation: hive
    spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
    derby.system.home: "hive/"
    javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=hive/metastore_db;create=true"
    kryoserializer: org.apache.spark.sql.execution.columnar.CachedBatch,[[B,org.apache.spark.sql.catalyst.expressions.GenericInternalRow

    es.nodes: localhost:9200
    es.net.http.auth.user: elastic
    es.net.http.auth.pass: NONENC_changeme
    es.port: 9200
    es.index.auto.create: true
    cluster.name: elasticsearch
    client.transport.sniff: true
    client.transport.ignore_cluster_name: true
#    es.xpack.security.user: elastic:byiF7KeOP9EtCtkmgMlyttOWcO55La2WasVQb1oKnss=
    index.auto.create: true
    index.number_of_replicas: 0
    index.refresh_interval: 5s

    uid:
      - id
      - product

  app:
    app_name: "dataproducts-common"
    monitoring:
      location: "tmp/monitoring"
      table.name: "monitoring"
    some-key:
      string-list:
        - item1
        - item2
        - item3
      int-list:
        - 101
        - 1045
        - 8625
      double-list:
        - 1.01
        - 104.5
        - 8625.16548
      map-of-strings:
          item1: value1
          item2: value2
          item3: value3
      map-of-ints:
          item1: 168
          item2: 2569
          item3: -14
      map-of-doubles:
          item1: 16.81256
          item2: 2.5690745
          item3: -0.1425456
      map-of-booleans:
          item1: true
          item2: false
          item3: true
    bool-property: true
    int-property: 123
    double-property: 456.789
    output: "csv_data.parq"
    db_table: "db_demo.table_demo"

spark:
  spark.master: "local"
  spark.local.dir: "/tmp/spark-temp"
  spark.sql.catalogImplementation: hive
  spark.sql.sources.partitionOverwriteMode: "dynamic"
  spark.sql.warehouse.dir: "/user/hive/warehouse"
  hive.metastore.warehouse.dir: "/user/hive/warehouse"
  derby.system.home: "/user/hive"
  kryoserializer: org.apache.spark.sql.execution.columnar.CachedBatch,[[B,org.apache.spark.sql.catalyst.expressions.GenericInternalRow
  es.nodes: "localhost:9201,localhost:9200"
  es.net.http.auth.user: elastic
  es.net.http.auth.pass: "4NaIad19p2IzxmWZzo7Vhl2vtatpD6Bv0GpK0EXAPPQ+PbBHXlxsep2lHlX9ZEZW"
  es.port: "9200"
  es.index.auto.create: "true"
  cluster.name: "elasticsearch"
  client.transport.sniff: "true"
  client.transport.ignore_cluster_name: "true"
  es.nodes.wan.only: true
  es.xpack.security.user: "elastic:4NaIad19p2IzxmWZzo7Vhl2vtatpD6Bv0GpK0EXAPPQ+PbBHXlxsep2lHlX9ZEZW"
  es.net.ssl: true
app:
  name: "mock-test"
  env: "dev"
  write.es: "false"
  retention: 7
  outputPath: "wda-acct.parq"
  measure: "false"
  skew_values:
    - "tid1"
    - "tid2"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

log4j.rootLogger=WARN, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p [%c] - %m%n

log4j.logger.org.apache.spark=WARNwda-acct:
  tresataId_sub:
  Account_Name: account_name
  Account_Number: account_number
  WS_CC_NO: local_cin
  Product: account_product_type
  Banking_Account_Active: account_status_orig
  WS_ACCT_STA_CHG_DTE: account_start_date
  Account_Closure_Date: account_closed_date
  Account_Currency: account_currency
  Account_Balance: account_balance
  Account_Balance_Date: account_balance_date
  source_input_path:
  source_date:
  Booking_Country: booking_country
  WS_OFCR_RESP_CDE: hsbc_booking_legal_entity
  WS_COA_NO: account_serial_number
  WS_PROC_GRP_CDE: account_system_code
rps-acnt:
  Account_Name: account_name
  Account_Number: account_number
  prod_cod_des: account_product_desc
  prod_cod: account_product_type
  Banking_Account_Active: account_status_orig
  Account_Currency: account_currency
  Account_Open_Date: account_start_date
  Account_Closure_Date: account_closed_date
  Account_Balance: account_balance
  Account_Balance_Date: account_balance_date
  source_input_path:
  source_date:
  source: source_system_name
  Booking_Country: booking_country
  br_no: hsbc_booking_branch_number
  br_no_des: hsbc_booking_branch_name
  imac_advd_lmt_amt: approved_limit
  imac_advd_lmt_strt_dt: date_first_activity
  imac_advd_lmt_expr_dt: date_last_activity
  imac_ulim_amt: utilization
  imac_rec_type: entity_typecat: ./clientlifecycle-common-core/src/test/resources/mock-data-content: Is a directory
cat: ./clientlifecycle-common-core/src/test/resources/mock-data-content/parq: Is a directory
Banking_Account_Active|Account_Balance_Date|source|source_date|Account_Currency|Account_Number|imac_advd_lmt_amt|prod_cod|imac_ulim_amt|br_no|Booking_Country|imac_advd_lmt_expr_dt|Account_Balance|Account_Closure_Date|Account_Open_Date|prod_cod_des|imac_rec_type|Account_Name|br_no_des|imac_advd_lmt_strt_dt|source_input_path
Banking_Account_Active_1|Account_Balance_Date_1|source_1|source_date_1|Account_Currency_1|Account_Number_1|imac_advd_lmt_amt_1|prod_cod_1|imac_ulim_amt_1|br_no_1|Booking_Country_1|imac_advd_lmt_expr_dt_1|Account_Balance_1|Account_Closure_Date_1|Account_Open_Date_1|prod_cod_des_1|imac_rec_type_1|Account_Name_1|br_no_des_1|imac_advd_lmt_strt_dt_1|source_input_path_1
Banking_Account_Active_2|Account_Balance_Date_2|source_2|source_date_2|Account_Currency_2|Account_Number_2|imac_advd_lmt_amt_2|prod_cod_2|imac_ulim_amt_2|br_no_2|Booking_Country_2|imac_advd_lmt_expr_dt_2|Account_Balance_2|Account_Closure_Date_2|Account_Open_Date_2|prod_cod_des_2|imac_rec_type_2|Account_Name_2|br_no_des_2|imac_advd_lmt_strt_dt_2|source_input_path_2
Banking_Account_Active_3|Account_Balance_Date_3|source_3|source_date_3|Account_Currency_3|Account_Number_3|imac_advd_lmt_amt_3|prod_cod_3|imac_ulim_amt_3|br_no_3|Booking_Country_3|imac_advd_lmt_expr_dt_3|Account_Balance_3|Account_Closure_Date_3|Account_Open_Date_3|prod_cod_des_3|imac_rec_type_3|Account_Name_3|br_no_des_3|imac_advd_lmt_strt_dt_3|source_input_path_3
Banking_Account_Active_4|Account_Balance_Date_4|source_4|source_date_4|Account_Currency_4|Account_Number_4|imac_advd_lmt_amt_4|prod_cod_4|imac_ulim_amt_4|br_no_4|Booking_Country_4|imac_advd_lmt_expr_dt_4|Account_Balance_4|Account_Closure_Date_4|Account_Open_Date_4|prod_cod_des_4|imac_rec_type_4|Account_Name_4|br_no_des_4|imac_advd_lmt_strt_dt_4|source_input_path_4
Banking_Account_Active_5|Account_Balance_Date_5|source_5|source_date_5|Account_Currency_5|Account_Number_5|imac_advd_lmt_amt_5|prod_cod_5|imac_ulim_amt_5|br_no_5|Booking_Country_5|imac_advd_lmt_expr_dt_5|Account_Balance_5|Account_Closure_Date_5|Account_Open_Date_5|prod_cod_des_5|imac_rec_type_5|Account_Name_5|br_no_des_5|imac_advd_lmt_strt_dt_5|source_input_path_5
Banking_Account_Active_6|Account_Balance_Date_6|source_6|source_date_6|Account_Currency_6|Account_Number_6|imac_advd_lmt_amt_6|prod_cod_6|imac_ulim_amt_6|br_no_6|Booking_Country_6|imac_advd_lmt_expr_dt_6|Account_Balance_6|Account_Closure_Date_6|Account_Open_Date_6|prod_cod_des_6|imac_rec_type_6|Account_Name_6|br_no_des_6|imac_advd_lmt_strt_dt_6|source_input_path_6
Banking_Account_Active_7|Account_Balance_Date_7|source_7|source_date_7|Account_Currency_7|Account_Number_7|imac_advd_lmt_amt_7|prod_cod_7|imac_ulim_amt_7|br_no_7|Booking_Country_7|imac_advd_lmt_expr_dt_7|Account_Balance_7|Account_Closure_Date_7|Account_Open_Date_7|prod_cod_des_7|imac_rec_type_7|Account_Name_7|br_no_des_7|imac_advd_lmt_strt_dt_7|source_input_path_7
Banking_Account_Active_8|Account_Balance_Date_8|source_8|source_date_8|Account_Currency_8|Account_Number_8|imac_advd_lmt_amt_8|prod_cod_8|imac_ulim_amt_8|br_no_8|Booking_Country_8|imac_advd_lmt_expr_dt_8|Account_Balance_8|Account_Closure_Date_8|Account_Open_Date_8|prod_cod_des_8|imac_rec_type_8|Account_Name_8|br_no_des_8|imac_advd_lmt_strt_dt_8|source_input_path_8
Banking_Account_Active_9|Account_Balance_Date_9|source_9|source_date_9|Account_Currency_9|Account_Number_9|imac_advd_lmt_amt_9|prod_cod_9|imac_ulim_amt_9|br_no_9|Booking_Country_9|imac_advd_lmt_expr_dt_9|Account_Balance_9|Account_Closure_Date_9|Account_Open_Date_9|prod_cod_des_9|imac_rec_type_9|Account_Name_9|br_no_des_9|imac_advd_lmt_strt_dt_9|source_input_path_9
Banking_Account_Active_10|Account_Balance_Date_10|source_10|source_date_10|Account_Currency_10|Account_Number_10|imac_advd_lmt_amt_10|prod_cod_10|imac_ulim_amt_10|br_no_10|Booking_Country_10|imac_advd_lmt_expr_dt_10|Account_Balance_10|Account_Closure_Date_10|Account_Open_Date_10|prod_cod_des_10|imac_rec_type_10|Account_Name_10|br_no_des_10|imac_advd_lmt_strt_dt_10|source_input_path_10
Banking_Account_Active_11|Account_Balance_Date_11|source_11|source_date_11|Account_Currency_11|Account_Number_11|imac_advd_lmt_amt_11|prod_cod_11|imac_ulim_amt_11|br_no_11|Booking_Country_11|imac_advd_lmt_expr_dt_11|Account_Balance_11|Account_Closure_Date_11|Account_Open_Date_11|prod_cod_des_11|imac_rec_type_11|Account_Name_11|br_no_des_11|imac_advd_lmt_strt_dt_11|source_input_path_11
Banking_Account_Active_12|Account_Balance_Date_12|source_12|source_date_12|Account_Currency_12|Account_Number_12|imac_advd_lmt_amt_12|prod_cod_12|imac_ulim_amt_12|br_no_12|Booking_Country_12|imac_advd_lmt_expr_dt_12|Account_Balance_12|Account_Closure_Date_12|Account_Open_Date_12|prod_cod_des_12|imac_rec_type_12|Account_Name_12|br_no_des_12|imac_advd_lmt_strt_dt_12|source_input_path_12
Banking_Account_Active_13|Account_Balance_Date_13|source_13|source_date_13|Account_Currency_13|Account_Number_13|imac_advd_lmt_amt_13|prod_cod_13|imac_ulim_amt_13|br_no_13|Booking_Country_13|imac_advd_lmt_expr_dt_13|Account_Balance_13|Account_Closure_Date_13|Account_Open_Date_13|prod_cod_des_13|imac_rec_type_13|Account_Name_13|br_no_des_13|imac_advd_lmt_strt_dt_13|source_input_path_13
Banking_Account_Active_14|Account_Balance_Date_14|source_14|source_date_14|Account_Currency_14|Account_Number_14|imac_advd_lmt_amt_14|prod_cod_14|imac_ulim_amt_14|br_no_14|Booking_Country_14|imac_advd_lmt_expr_dt_14|Account_Balance_14|Account_Closure_Date_14|Account_Open_Date_14|prod_cod_des_14|imac_rec_type_14|Account_Name_14|br_no_des_14|imac_advd_lmt_strt_dt_14|source_input_path_14
Banking_Account_Active_15|Account_Balance_Date_15|source_15|source_date_15|Account_Currency_15|Account_Number_15|imac_advd_lmt_amt_15|prod_cod_15|imac_ulim_amt_15|br_no_15|Booking_Country_15|imac_advd_lmt_expr_dt_15|Account_Balance_15|Account_Closure_Date_15|Account_Open_Date_15|prod_cod_des_15|imac_rec_type_15|Account_Name_15|br_no_des_15|imac_advd_lmt_strt_dt_15|source_input_path_15
Banking_Account_Active_16|Account_Balance_Date_16|source_16|source_date_16|Account_Currency_16|Account_Number_16|imac_advd_lmt_amt_16|prod_cod_16|imac_ulim_amt_16|br_no_16|Booking_Country_16|imac_advd_lmt_expr_dt_16|Account_Balance_16|Account_Closure_Date_16|Account_Open_Date_16|prod_cod_des_16|imac_rec_type_16|Account_Name_16|br_no_des_16|imac_advd_lmt_strt_dt_16|source_input_path_16
Banking_Account_Active_17|Account_Balance_Date_17|source_17|source_date_17|Account_Currency_17|Account_Number_17|imac_advd_lmt_amt_17|prod_cod_17|imac_ulim_amt_17|br_no_17|Booking_Country_17|imac_advd_lmt_expr_dt_17|Account_Balance_17|Account_Closure_Date_17|Account_Open_Date_17|prod_cod_des_17|imac_rec_type_17|Account_Name_17|br_no_des_17|imac_advd_lmt_strt_dt_17|source_input_path_17
Banking_Account_Active_18|Account_Balance_Date_18|source_18|source_date_18|Account_Currency_18|Account_Number_18|imac_advd_lmt_amt_18|prod_cod_18|imac_ulim_amt_18|br_no_18|Booking_Country_18|imac_advd_lmt_expr_dt_18|Account_Balance_18|Account_Closure_Date_18|Account_Open_Date_18|prod_cod_des_18|imac_rec_type_18|Account_Name_18|br_no_des_18|imac_advd_lmt_strt_dt_18|source_input_path_18
Banking_Account_Active_19|Account_Balance_Date_19|source_19|source_date_19|Account_Currency_19|Account_Number_19|imac_advd_lmt_amt_19|prod_cod_19|imac_ulim_amt_19|br_no_19|Booking_Country_19|imac_advd_lmt_expr_dt_19|Account_Balance_19|Account_Closure_Date_19|Account_Open_Date_19|prod_cod_des_19|imac_rec_type_19|Account_Name_19|br_no_des_19|imac_advd_lmt_strt_dt_19|source_input_path_19
Banking_Account_Active_20|Account_Balance_Date_20|source_20|source_date_20|Account_Currency_20|Account_Number_20|imac_advd_lmt_amt_20|prod_cod_20|imac_ulim_amt_20|br_no_20|Booking_Country_20|imac_advd_lmt_expr_dt_20|Account_Balance_20|Account_Closure_Date_20|Account_Open_Date_20|prod_cod_des_20|imac_rec_type_20|Account_Name_20|br_no_des_20|imac_advd_lmt_strt_dt_20|source_input_path_20
Banking_Account_Active_21|Account_Balance_Date_21|source_21|source_date_21|Account_Currency_21|Account_Number_21|imac_advd_lmt_amt_21|prod_cod_21|imac_ulim_amt_21|br_no_21|Booking_Country_21|imac_advd_lmt_expr_dt_21|Account_Balance_21|Account_Closure_Date_21|Account_Open_Date_21|prod_cod_des_21|imac_rec_type_21|Account_Name_21|br_no_des_21|imac_advd_lmt_strt_dt_21|source_input_path_21
Banking_Account_Active_22|Account_Balance_Date_22|source_22|source_date_22|Account_Currency_22|Account_Number_22|imac_advd_lmt_amt_22|prod_cod_22|imac_ulim_amt_22|br_no_22|Booking_Country_22|imac_advd_lmt_expr_dt_22|Account_Balance_22|Account_Closure_Date_22|Account_Open_Date_22|prod_cod_des_22|imac_rec_type_22|Account_Name_22|br_no_des_22|imac_advd_lmt_strt_dt_22|source_input_path_22
Banking_Account_Active_23|Account_Balance_Date_23|source_23|source_date_23|Account_Currency_23|Account_Number_23|imac_advd_lmt_amt_23|prod_cod_23|imac_ulim_amt_23|br_no_23|Booking_Country_23|imac_advd_lmt_expr_dt_23|Account_Balance_23|Account_Closure_Date_23|Account_Open_Date_23|prod_cod_des_23|imac_rec_type_23|Account_Name_23|br_no_des_23|imac_advd_lmt_strt_dt_23|source_input_path_23
Banking_Account_Active_24|Account_Balance_Date_24|source_24|source_date_24|Account_Currency_24|Account_Number_24|imac_advd_lmt_amt_24|prod_cod_24|imac_ulim_amt_24|br_no_24|Booking_Country_24|imac_advd_lmt_expr_dt_24|Account_Balance_24|Account_Closure_Date_24|Account_Open_Date_24|prod_cod_des_24|imac_rec_type_24|Account_Name_24|br_no_des_24|imac_advd_lmt_strt_dt_24|source_input_path_24
Banking_Account_Active_25|Account_Balance_Date_25|source_25|source_date_25|Account_Currency_25|Account_Number_25|imac_advd_lmt_amt_25|prod_cod_25|imac_ulim_amt_25|br_no_25|Booking_Country_25|imac_advd_lmt_expr_dt_25|Account_Balance_25|Account_Closure_Date_25|Account_Open_Date_25|prod_cod_des_25|imac_rec_type_25|Account_Name_25|br_no_des_25|imac_advd_lmt_strt_dt_25|source_input_path_25
Banking_Account_Active_26|Account_Balance_Date_26|source_26|source_date_26|Account_Currency_26|Account_Number_26|imac_advd_lmt_amt_26|prod_cod_26|imac_ulim_amt_26|br_no_26|Booking_Country_26|imac_advd_lmt_expr_dt_26|Account_Balance_26|Account_Closure_Date_26|Account_Open_Date_26|prod_cod_des_26|imac_rec_type_26|Account_Name_26|br_no_des_26|imac_advd_lmt_strt_dt_26|source_input_path_26
Banking_Account_Active_27|Account_Balance_Date_27|source_27|source_date_27|Account_Currency_27|Account_Number_27|imac_advd_lmt_amt_27|prod_cod_27|imac_ulim_amt_27|br_no_27|Booking_Country_27|imac_advd_lmt_expr_dt_27|Account_Balance_27|Account_Closure_Date_27|Account_Open_Date_27|prod_cod_des_27|imac_rec_type_27|Account_Name_27|br_no_des_27|imac_advd_lmt_strt_dt_27|source_input_path_27
Banking_Account_Active_28|Account_Balance_Date_28|source_28|source_date_28|Account_Currency_28|Account_Number_28|imac_advd_lmt_amt_28|prod_cod_28|imac_ulim_amt_28|br_no_28|Booking_Country_28|imac_advd_lmt_expr_dt_28|Account_Balance_28|Account_Closure_Date_28|Account_Open_Date_28|prod_cod_des_28|imac_rec_type_28|Account_Name_28|br_no_des_28|imac_advd_lmt_strt_dt_28|source_input_path_28
Banking_Account_Active_29|Account_Balance_Date_29|source_29|source_date_29|Account_Currency_29|Account_Number_29|imac_advd_lmt_amt_29|prod_cod_29|imac_ulim_amt_29|br_no_29|Booking_Country_29|imac_advd_lmt_expr_dt_29|Account_Balance_29|Account_Closure_Date_29|Account_Open_Date_29|prod_cod_des_29|imac_rec_type_29|Account_Name_29|br_no_des_29|imac_advd_lmt_strt_dt_29|source_input_path_29
Banking_Account_Active_30|Account_Balance_Date_30|source_30|source_date_30|Account_Currency_30|Account_Number_30|imac_advd_lmt_amt_30|prod_cod_30|imac_ulim_amt_30|br_no_30|Booking_Country_30|imac_advd_lmt_expr_dt_30|Account_Balance_30|Account_Closure_Date_30|Account_Open_Date_30|prod_cod_des_30|imac_rec_type_30|Account_Name_30|br_no_des_30|imac_advd_lmt_strt_dt_30|source_input_path_30
Banking_Account_Active_31|Account_Balance_Date_31|source_31|source_date_31|Account_Currency_31|Account_Number_31|imac_advd_lmt_amt_31|prod_cod_31|imac_ulim_amt_31|br_no_31|Booking_Country_31|imac_advd_lmt_expr_dt_31|Account_Balance_31|Account_Closure_Date_31|Account_Open_Date_31|prod_cod_des_31|imac_rec_type_31|Account_Name_31|br_no_des_31|imac_advd_lmt_strt_dt_31|source_input_path_31
Banking_Account_Active_32|Account_Balance_Date_32|source_32|source_date_32|Account_Currency_32|Account_Number_32|imac_advd_lmt_amt_32|prod_cod_32|imac_ulim_amt_32|br_no_32|Booking_Country_32|imac_advd_lmt_expr_dt_32|Account_Balance_32|Account_Closure_Date_32|Account_Open_Date_32|prod_cod_des_32|imac_rec_type_32|Account_Name_32|br_no_des_32|imac_advd_lmt_strt_dt_32|source_input_path_32
Banking_Account_Active_33|Account_Balance_Date_33|source_33|source_date_33|Account_Currency_33|Account_Number_33|imac_advd_lmt_amt_33|prod_cod_33|imac_ulim_amt_33|br_no_33|Booking_Country_33|imac_advd_lmt_expr_dt_33|Account_Balance_33|Account_Closure_Date_33|Account_Open_Date_33|prod_cod_des_33|imac_rec_type_33|Account_Name_33|br_no_des_33|imac_advd_lmt_strt_dt_33|source_input_path_33
Banking_Account_Active_34|Account_Balance_Date_34|source_34|source_date_34|Account_Currency_34|Account_Number_34|imac_advd_lmt_amt_34|prod_cod_34|imac_ulim_amt_34|br_no_34|Booking_Country_34|imac_advd_lmt_expr_dt_34|Account_Balance_34|Account_Closure_Date_34|Account_Open_Date_34|prod_cod_des_34|imac_rec_type_34|Account_Name_34|br_no_des_34|imac_advd_lmt_strt_dt_34|source_input_path_34
Banking_Account_Active_35|Account_Balance_Date_35|source_35|source_date_35|Account_Currency_35|Account_Number_35|imac_advd_lmt_amt_35|prod_cod_35|imac_ulim_amt_35|br_no_35|Booking_Country_35|imac_advd_lmt_expr_dt_35|Account_Balance_35|Account_Closure_Date_35|Account_Open_Date_35|prod_cod_des_35|imac_rec_type_35|Account_Name_35|br_no_des_35|imac_advd_lmt_strt_dt_35|source_input_path_35
Banking_Account_Active_36|Account_Balance_Date_36|source_36|source_date_36|Account_Currency_36|Account_Number_36|imac_advd_lmt_amt_36|prod_cod_36|imac_ulim_amt_36|br_no_36|Booking_Country_36|imac_advd_lmt_expr_dt_36|Account_Balance_36|Account_Closure_Date_36|Account_Open_Date_36|prod_cod_des_36|imac_rec_type_36|Account_Name_36|br_no_des_36|imac_advd_lmt_strt_dt_36|source_input_path_36
Banking_Account_Active_37|Account_Balance_Date_37|source_37|source_date_37|Account_Currency_37|Account_Number_37|imac_advd_lmt_amt_37|prod_cod_37|imac_ulim_amt_37|br_no_37|Booking_Country_37|imac_advd_lmt_expr_dt_37|Account_Balance_37|Account_Closure_Date_37|Account_Open_Date_37|prod_cod_des_37|imac_rec_type_37|Account_Name_37|br_no_des_37|imac_advd_lmt_strt_dt_37|source_input_path_37
Banking_Account_Active_38|Account_Balance_Date_38|source_38|source_date_38|Account_Currency_38|Account_Number_38|imac_advd_lmt_amt_38|prod_cod_38|imac_ulim_amt_38|br_no_38|Booking_Country_38|imac_advd_lmt_expr_dt_38|Account_Balance_38|Account_Closure_Date_38|Account_Open_Date_38|prod_cod_des_38|imac_rec_type_38|Account_Name_38|br_no_des_38|imac_advd_lmt_strt_dt_38|source_input_path_38
Banking_Account_Active_39|Account_Balance_Date_39|source_39|source_date_39|Account_Currency_39|Account_Number_39|imac_advd_lmt_amt_39|prod_cod_39|imac_ulim_amt_39|br_no_39|Booking_Country_39|imac_advd_lmt_expr_dt_39|Account_Balance_39|Account_Closure_Date_39|Account_Open_Date_39|prod_cod_des_39|imac_rec_type_39|Account_Name_39|br_no_des_39|imac_advd_lmt_strt_dt_39|source_input_path_39
Banking_Account_Active_40|Account_Balance_Date_40|source_40|source_date_40|Account_Currency_40|Account_Number_40|imac_advd_lmt_amt_40|prod_cod_40|imac_ulim_amt_40|br_no_40|Booking_Country_40|imac_advd_lmt_expr_dt_40|Account_Balance_40|Account_Closure_Date_40|Account_Open_Date_40|prod_cod_des_40|imac_rec_type_40|Account_Name_40|br_no_des_40|imac_advd_lmt_strt_dt_40|source_input_path_40
Banking_Account_Active_41|Account_Balance_Date_41|source_41|source_date_41|Account_Currency_41|Account_Number_41|imac_advd_lmt_amt_41|prod_cod_41|imac_ulim_amt_41|br_no_41|Booking_Country_41|imac_advd_lmt_expr_dt_41|Account_Balance_41|Account_Closure_Date_41|Account_Open_Date_41|prod_cod_des_41|imac_rec_type_41|Account_Name_41|br_no_des_41|imac_advd_lmt_strt_dt_41|source_input_path_41
Banking_Account_Active_42|Account_Balance_Date_42|source_42|source_date_42|Account_Currency_42|Account_Number_42|imac_advd_lmt_amt_42|prod_cod_42|imac_ulim_amt_42|br_no_42|Booking_Country_42|imac_advd_lmt_expr_dt_42|Account_Balance_42|Account_Closure_Date_42|Account_Open_Date_42|prod_cod_des_42|imac_rec_type_42|Account_Name_42|br_no_des_42|imac_advd_lmt_strt_dt_42|source_input_path_42
Banking_Account_Active_43|Account_Balance_Date_43|source_43|source_date_43|Account_Currency_43|Account_Number_43|imac_advd_lmt_amt_43|prod_cod_43|imac_ulim_amt_43|br_no_43|Booking_Country_43|imac_advd_lmt_expr_dt_43|Account_Balance_43|Account_Closure_Date_43|Account_Open_Date_43|prod_cod_des_43|imac_rec_type_43|Account_Name_43|br_no_des_43|imac_advd_lmt_strt_dt_43|source_input_path_43
Banking_Account_Active_44|Account_Balance_Date_44|source_44|source_date_44|Account_Currency_44|Account_Number_44|imac_advd_lmt_amt_44|prod_cod_44|imac_ulim_amt_44|br_no_44|Booking_Country_44|imac_advd_lmt_expr_dt_44|Account_Balance_44|Account_Closure_Date_44|Account_Open_Date_44|prod_cod_des_44|imac_rec_type_44|Account_Name_44|br_no_des_44|imac_advd_lmt_strt_dt_44|source_input_path_44
Banking_Account_Active_45|Account_Balance_Date_45|source_45|source_date_45|Account_Currency_45|Account_Number_45|imac_advd_lmt_amt_45|prod_cod_45|imac_ulim_amt_45|br_no_45|Booking_Country_45|imac_advd_lmt_expr_dt_45|Account_Balance_45|Account_Closure_Date_45|Account_Open_Date_45|prod_cod_des_45|imac_rec_type_45|Account_Name_45|br_no_des_45|imac_advd_lmt_strt_dt_45|source_input_path_45
Banking_Account_Active_46|Account_Balance_Date_46|source_46|source_date_46|Account_Currency_46|Account_Number_46|imac_advd_lmt_amt_46|prod_cod_46|imac_ulim_amt_46|br_no_46|Booking_Country_46|imac_advd_lmt_expr_dt_46|Account_Balance_46|Account_Closure_Date_46|Account_Open_Date_46|prod_cod_des_46|imac_rec_type_46|Account_Name_46|br_no_des_46|imac_advd_lmt_strt_dt_46|source_input_path_46
Banking_Account_Active_47|Account_Balance_Date_47|source_47|source_date_47|Account_Currency_47|Account_Number_47|imac_advd_lmt_amt_47|prod_cod_47|imac_ulim_amt_47|br_no_47|Booking_Country_47|imac_advd_lmt_expr_dt_47|Account_Balance_47|Account_Closure_Date_47|Account_Open_Date_47|prod_cod_des_47|imac_rec_type_47|Account_Name_47|br_no_des_47|imac_advd_lmt_strt_dt_47|source_input_path_47
Banking_Account_Active_48|Account_Balance_Date_48|source_48|source_date_48|Account_Currency_48|Account_Number_48|imac_advd_lmt_amt_48|prod_cod_48|imac_ulim_amt_48|br_no_48|Booking_Country_48|imac_advd_lmt_expr_dt_48|Account_Balance_48|Account_Closure_Date_48|Account_Open_Date_48|prod_cod_des_48|imac_rec_type_48|Account_Name_48|br_no_des_48|imac_advd_lmt_strt_dt_48|source_input_path_48
Banking_Account_Active_49|Account_Balance_Date_49|source_49|source_date_49|Account_Currency_49|Account_Number_49|imac_advd_lmt_amt_49|prod_cod_49|imac_ulim_amt_49|br_no_49|Booking_Country_49|imac_advd_lmt_expr_dt_49|Account_Balance_49|Account_Closure_Date_49|Account_Open_Date_49|prod_cod_des_49|imac_rec_type_49|Account_Name_49|br_no_des_49|imac_advd_lmt_strt_dt_49|source_input_path_49
Banking_Account_Active_50|Account_Balance_Date_50|source_50|source_date_50|Account_Currency_50|Account_Number_50|imac_advd_lmt_amt_50|prod_cod_50|imac_ulim_amt_50|br_no_50|Booking_Country_50|imac_advd_lmt_expr_dt_50|Account_Balance_50|Account_Closure_Date_50|Account_Open_Date_50|prod_cod_des_50|imac_rec_type_50|Account_Name_50|br_no_des_50|imac_advd_lmt_strt_dt_50|source_input_path_50
Banking_Account_Active_51|Account_Balance_Date_51|source_51|source_date_51|Account_Currency_51|Account_Number_51|imac_advd_lmt_amt_51|prod_cod_51|imac_ulim_amt_51|br_no_51|Booking_Country_51|imac_advd_lmt_expr_dt_51|Account_Balance_51|Account_Closure_Date_51|Account_Open_Date_51|prod_cod_des_51|imac_rec_type_51|Account_Name_51|br_no_des_51|imac_advd_lmt_strt_dt_51|source_input_path_51
Banking_Account_Active_52|Account_Balance_Date_52|source_52|source_date_52|Account_Currency_52|Account_Number_52|imac_advd_lmt_amt_52|prod_cod_52|imac_ulim_amt_52|br_no_52|Booking_Country_52|imac_advd_lmt_expr_dt_52|Account_Balance_52|Account_Closure_Date_52|Account_Open_Date_52|prod_cod_des_52|imac_rec_type_52|Account_Name_52|br_no_des_52|imac_advd_lmt_strt_dt_52|source_input_path_52
Banking_Account_Active_53|Account_Balance_Date_53|source_53|source_date_53|Account_Currency_53|Account_Number_53|imac_advd_lmt_amt_53|prod_cod_53|imac_ulim_amt_53|br_no_53|Booking_Country_53|imac_advd_lmt_expr_dt_53|Account_Balance_53|Account_Closure_Date_53|Account_Open_Date_53|prod_cod_des_53|imac_rec_type_53|Account_Name_53|br_no_des_53|imac_advd_lmt_strt_dt_53|source_input_path_53
Banking_Account_Active_54|Account_Balance_Date_54|source_54|source_date_54|Account_Currency_54|Account_Number_54|imac_advd_lmt_amt_54|prod_cod_54|imac_ulim_amt_54|br_no_54|Booking_Country_54|imac_advd_lmt_expr_dt_54|Account_Balance_54|Account_Closure_Date_54|Account_Open_Date_54|prod_cod_des_54|imac_rec_type_54|Account_Name_54|br_no_des_54|imac_advd_lmt_strt_dt_54|source_input_path_54
Banking_Account_Active_55|Account_Balance_Date_55|source_55|source_date_55|Account_Currency_55|Account_Number_55|imac_advd_lmt_amt_55|prod_cod_55|imac_ulim_amt_55|br_no_55|Booking_Country_55|imac_advd_lmt_expr_dt_55|Account_Balance_55|Account_Closure_Date_55|Account_Open_Date_55|prod_cod_des_55|imac_rec_type_55|Account_Name_55|br_no_des_55|imac_advd_lmt_strt_dt_55|source_input_path_55
Banking_Account_Active_56|Account_Balance_Date_56|source_56|source_date_56|Account_Currency_56|Account_Number_56|imac_advd_lmt_amt_56|prod_cod_56|imac_ulim_amt_56|br_no_56|Booking_Country_56|imac_advd_lmt_expr_dt_56|Account_Balance_56|Account_Closure_Date_56|Account_Open_Date_56|prod_cod_des_56|imac_rec_type_56|Account_Name_56|br_no_des_56|imac_advd_lmt_strt_dt_56|source_input_path_56
Banking_Account_Active_57|Account_Balance_Date_57|source_57|source_date_57|Account_Currency_57|Account_Number_57|imac_advd_lmt_amt_57|prod_cod_57|imac_ulim_amt_57|br_no_57|Booking_Country_57|imac_advd_lmt_expr_dt_57|Account_Balance_57|Account_Closure_Date_57|Account_Open_Date_57|prod_cod_des_57|imac_rec_type_57|Account_Name_57|br_no_des_57|imac_advd_lmt_strt_dt_57|source_input_path_57
Banking_Account_Active_58|Account_Balance_Date_58|source_58|source_date_58|Account_Currency_58|Account_Number_58|imac_advd_lmt_amt_58|prod_cod_58|imac_ulim_amt_58|br_no_58|Booking_Country_58|imac_advd_lmt_expr_dt_58|Account_Balance_58|Account_Closure_Date_58|Account_Open_Date_58|prod_cod_des_58|imac_rec_type_58|Account_Name_58|br_no_des_58|imac_advd_lmt_strt_dt_58|source_input_path_58
Banking_Account_Active_59|Account_Balance_Date_59|source_59|source_date_59|Account_Currency_59|Account_Number_59|imac_advd_lmt_amt_59|prod_cod_59|imac_ulim_amt_59|br_no_59|Booking_Country_59|imac_advd_lmt_expr_dt_59|Account_Balance_59|Account_Closure_Date_59|Account_Open_Date_59|prod_cod_des_59|imac_rec_type_59|Account_Name_59|br_no_des_59|imac_advd_lmt_strt_dt_59|source_input_path_59
Banking_Account_Active_60|Account_Balance_Date_60|source_60|source_date_60|Account_Currency_60|Account_Number_60|imac_advd_lmt_amt_60|prod_cod_60|imac_ulim_amt_60|br_no_60|Booking_Country_60|imac_advd_lmt_expr_dt_60|Account_Balance_60|Account_Closure_Date_60|Account_Open_Date_60|prod_cod_des_60|imac_rec_type_60|Account_Name_60|br_no_des_60|imac_advd_lmt_strt_dt_60|source_input_path_60
Banking_Account_Active|Account_Balance_Date|source_date|Account_Currency|Account_Number|WS_PROC_GRP_CDE|Booking_Country|Account_Balance|WS_ACCT_STA_CHG_DTE|Account_Closure_Date|WS_CC_NO|WS_COA_NO|tresataId_sub|Account_Name|Product|source_input_path|WS_OFCR_RESP_CDE
ZxfwlA-82931|us|39735|uk_hk|-(1637766888440_uk)-|1637766888447|iRbvc|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_1|Account_Name_1|Product_1|source_input_path_1|WS_OFCR_RESP_CDE_1
nsuLFw-91551|hk|15158|uk_hk|-(1637766888440_uk)-|1637766888447|CgDHK|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_2|Account_Name_2|Product_2|source_input_path_2|WS_OFCR_RESP_CDE_2
fkVyhQ-12580|us|10490|us_uk|-(1637766888440_uk)-|1637766888447|liMFv|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_3|Account_Name_3|Product_3|source_input_path_3|WS_OFCR_RESP_CDE_3
ryPmeN-29199|us|65542|hk_hk|-(1637766888440_hk)-|1637766888447|CFQlB|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_4|Account_Name_4|Product_4|source_input_path_4|WS_OFCR_RESP_CDE_4
DGQuvu-42428|uk|77174|hk_uk|-(1637766888440_us)-|1637766888447|FDrup|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_5|Account_Name_5|Product_5|source_input_path_5|WS_OFCR_RESP_CDE_5
PPSTpJ-15657|uk|46873|us_us|-(1637766888440_hk)-|1637766888447|FUjPN|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_6|Account_Name_6|Product_6|source_input_path_6|WS_OFCR_RESP_CDE_6
iqBrKh-36289|hk|68539|us_us|-(1637766888440_hk)-|1637766888447|mQeaH|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_7|Account_Name_7|Product_7|source_input_path_7|WS_OFCR_RESP_CDE_7
XdPgpX-28165|us|69723|us_us|-(1637766888440_us)-|1637766888447|illBh|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_8|Account_Name_8|Product_8|source_input_path_8|WS_OFCR_RESP_CDE_8
AIBlTj-32594|us|84158|uk_hk|-(1637766888440_us)-|1637766888447|ShvhD|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_9|Account_Name_9|Product_9|source_input_path_9|WS_OFCR_RESP_CDE_9
VPIHzk-51322|hk|21541|hk_hk|-(1637766888441_uk)-|1637766888447|pBwCF|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_10|Account_Name_10|Product_10|source_input_path_10|WS_OFCR_RESP_CDE_10
iIvLjm-67311|hk|69663|us_hk|-(1637766888441_uk)-|1637766888447|wUsDa|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_11|Account_Name_11|Product_11|source_input_path_11|WS_OFCR_RESP_CDE_11
PJFZeD-27675|uk|23267|uk_uk|-(1637766888441_uk)-|1637766888447|atBrq|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_12|Account_Name_12|Product_12|source_input_path_12|WS_OFCR_RESP_CDE_12
kUtDfU-92100|us|59611|us_uk|-(1637766888441_uk)-|1637766888447|fqEWL|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_13|Account_Name_13|Product_13|source_input_path_13|WS_OFCR_RESP_CDE_13
MUpLIg-38146|us|15761|hk_hk|-(1637766888441_uk)-|1637766888447|GyKRA|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_14|Account_Name_14|Product_14|source_input_path_14|WS_OFCR_RESP_CDE_14
kbXiFw-11427|us|23691|hk_uk|-(1637766888441_uk)-|1637766888449|TbXJH|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_15|Account_Name_15|Product_15|source_input_path_15|WS_OFCR_RESP_CDE_15
iPPgGF-55165|us|42313|hk_us|-(1637766888441_uk)-|1637766888449|TeWjk|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_16|Account_Name_16|Product_16|source_input_path_16|WS_OFCR_RESP_CDE_16
SnQHVA-11966|us|56577|us_uk|-(1637766888441_hk)-|1637766888449|JWQgJ|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_17|Account_Name_17|Product_17|source_input_path_17|WS_OFCR_RESP_CDE_17
FLgjnb-18319|uk|67651|uk_uk|-(1637766888441_uk)-|1637766888449|nfUph|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_18|Account_Name_18|Product_18|source_input_path_18|WS_OFCR_RESP_CDE_18
lvaJyM-81189|us|55113|us_uk|-(1637766888441_hk)-|1637766888449|jpkrZ|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_19|Account_Name_19|Product_19|source_input_path_19|WS_OFCR_RESP_CDE_19
KcaiCW-19148|uk|69111|hk_uk|-(1637766888441_us)-|1637766888449|Xfhjh|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_20|Account_Name_20|Product_20|source_input_path_20|WS_OFCR_RESP_CDE_20
crFtzg-97941|hk|65319|hk_us|-(1637766888441_hk)-|1637766888449|bHFud|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_21|Account_Name_21|Product_21|source_input_path_21|WS_OFCR_RESP_CDE_21
QqZDbA-12495|us|52413|hk_uk|-(1637766888441_us)-|1637766888449|CHsqj|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_22|Account_Name_22|Product_22|source_input_path_22|WS_OFCR_RESP_CDE_22
kNsbpa-55158|us|29911|uk_uk|-(1637766888442_hk)-|1637766888449|wdkYP|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_23|Account_Name_23|Product_23|source_input_path_23|WS_OFCR_RESP_CDE_23
gJvjXP-89849|uk|74384|hk_hk|-(1637766888442_hk)-|1637766888449|gNFIB|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_24|Account_Name_24|Product_24|source_input_path_24|WS_OFCR_RESP_CDE_24
DPncrF-51528|uk|59587|hk_us|-(1637766888442_hk)-|1637766888449|UrUsJ|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_25|Account_Name_25|Product_25|source_input_path_25|WS_OFCR_RESP_CDE_25
CyikJp-54927|us|63239|hk_hk|-(1637766888442_us)-|1637766888449|KLaua|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_26|Account_Name_26|Product_26|source_input_path_26|WS_OFCR_RESP_CDE_26
GcdisN-26511|hk|31712|us_hk|-(1637766888442_us)-|1637766888449|izwzi|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_27|Account_Name_27|Product_27|source_input_path_27|WS_OFCR_RESP_CDE_27
FLbDwe-99151|uk|66811|us_us|-(1637766888442_uk)-|1637766888449|DCvpq|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_28|Account_Name_28|Product_28|source_input_path_28|WS_OFCR_RESP_CDE_28
TDdkbG-13126|hk|73455|uk_hk|-(1637766888442_us)-|1637766888449|gQunV|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_29|Account_Name_29|Product_29|source_input_path_29|WS_OFCR_RESP_CDE_29
qCqlKE-74691|hk|35615|hk_hk|-(1637766888442_us)-|1637766888449|QBJzB|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_30|Account_Name_30|Product_30|source_input_path_30|WS_OFCR_RESP_CDE_30
VKnHXT-61287|hk|84546|us_hk|-(1637766888442_hk)-|1637766888449|VbMTy|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_31|Account_Name_31|Product_31|source_input_path_31|WS_OFCR_RESP_CDE_31
JMfwnk-75344|hk|15318|us_us|-(1637766888442_us)-|1637766888449|YYaGZ|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_32|Account_Name_32|Product_32|source_input_path_32|WS_OFCR_RESP_CDE_32
CtkLzz-33343|uk|91113|uk_us|-(1637766888442_us)-|1637766888449|KJiNT|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_33|Account_Name_33|Product_33|source_input_path_33|WS_OFCR_RESP_CDE_33
PgimxN-15599|hk|94881|uk_us|-(1637766888444_hk)-|1637766888449|abFLK|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_34|Account_Name_34|Product_34|source_input_path_34|WS_OFCR_RESP_CDE_34
aJlTYq-12577|hk|85146|hk_us|-(1637766888444_hk)-|1637766888450|NHqUY|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_35|Account_Name_35|Product_35|source_input_path_35|WS_OFCR_RESP_CDE_35
GKmlVJ-84546|us|48219|us_hk|-(1637766888444_hk)-|1637766888450|biWpV|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_36|Account_Name_36|Product_36|source_input_path_36|WS_OFCR_RESP_CDE_36
vDGCHN-26188|uk|99897|us_hk|-(1637766888444_hk)-|1637766888450|NBEZW|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_37|Account_Name_37|Product_37|source_input_path_37|WS_OFCR_RESP_CDE_37
rZGlBE-29512|hk|29311|us_hk|-(1637766888444_hk)-|1637766888450|JYhyg|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_38|Account_Name_38|Product_38|source_input_path_38|WS_OFCR_RESP_CDE_38
wtxGWk-51643|us|53548|hk_hk|-(1637766888444_us)-|1637766888450|qcEKt|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_39|Account_Name_39|Product_39|source_input_path_39|WS_OFCR_RESP_CDE_39
hDcnuJ-89175|uk|10553|hk_hk|-(1637766888444_us)-|1637766888450|vnEPb|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_40|Account_Name_40|Product_40|source_input_path_40|WS_OFCR_RESP_CDE_40
SBHYIV-51847|us|61814|us_hk|-(1637766888444_uk)-|1637766888450|ueydi|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_41|Account_Name_41|Product_41|source_input_path_41|WS_OFCR_RESP_CDE_41
fddkeT-64179|us|12327|uk_hk|-(1637766888444_us)-|1637766888450|rDPCl|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_42|Account_Name_42|Product_42|source_input_path_42|WS_OFCR_RESP_CDE_42
yLUmwA-59381|us|95331|hk_uk|-(1637766888444_uk)-|1637766888450|sPNNk|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_43|Account_Name_43|Product_43|source_input_path_43|WS_OFCR_RESP_CDE_43
mAIdji-96386|uk|86177|uk_uk|-(1637766888444_us)-|1637766888450|bawNi|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_44|Account_Name_44|Product_44|source_input_path_44|WS_OFCR_RESP_CDE_44
PvRNAm-81291|uk|21517|hk_hk|-(1637766888444_us)-|1637766888450|PDaiG|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_45|Account_Name_45|Product_45|source_input_path_45|WS_OFCR_RESP_CDE_45
LBwVRn-69185|us|45245|uk_us|-(1637766888444_hk)-|1637766888450|Chxsl|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_46|Account_Name_46|Product_46|source_input_path_46|WS_OFCR_RESP_CDE_46
Nwywaa-29927|hk|12851|us_uk|-(1637766888445_hk)-|1637766888450|dNCXP|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_47|Account_Name_47|Product_47|source_input_path_47|WS_OFCR_RESP_CDE_47
svurZD-44114|hk|11097|uk_hk|-(1637766888445_us)-|1637766888450|AnefN|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_48|Account_Name_48|Product_48|source_input_path_48|WS_OFCR_RESP_CDE_48
JcHVmm-53494|hk|58653|us_hk|-(1637766888445_uk)-|1637766888450|RKGUx|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_49|Account_Name_49|Product_49|source_input_path_49|WS_OFCR_RESP_CDE_49
XRZanK-51676|us|21914|hk_us|-(1637766888445_hk)-|1637766888450|yGLcY|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_50|Account_Name_50|Product_50|source_input_path_50|WS_OFCR_RESP_CDE_50
fWbMKC-19557|us|23243|us_us|-(1637766888445_uk)-|1637766888450|QCMBP|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_51|Account_Name_51|Product_51|source_input_path_51|WS_OFCR_RESP_CDE_51
NNKZVc-13813|uk|12536|us_hk|-(1637766888445_us)-|1637766888450|GkwdE|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_52|Account_Name_52|Product_52|source_input_path_52|WS_OFCR_RESP_CDE_52
ZgTUqr-57868|hk|19813|uk_uk|-(1637766888445_uk)-|1637766888450|diUDb|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_53|Account_Name_53|Product_53|source_input_path_53|WS_OFCR_RESP_CDE_53
hytWhd-59153|hk|91751|uk_us|-(1637766888445_uk)-|1637766888450|KTgzl|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_54|Account_Name_54|Product_54|source_input_path_54|WS_OFCR_RESP_CDE_54
WsJACj-84213|hk|61055|us_uk|-(1637766888445_hk)-|1637766888450|twMIl|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_55|Account_Name_55|Product_55|source_input_path_55|WS_OFCR_RESP_CDE_55
FAcEeX-33659|uk|83167|uk_hk|-(1637766888445_hk)-|1637766888450|qiSvZ|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_56|Account_Name_56|Product_56|source_input_path_56|WS_OFCR_RESP_CDE_56
qvAXNA-65883|hk|39444|uk_uk|-(1637766888445_hk)-|1637766888450|hWBqN|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_57|Account_Name_57|Product_57|source_input_path_57|WS_OFCR_RESP_CDE_57
EcZkCz-61276|hk|81431|uk_us|-(1637766888445_uk)-|1637766888450|pbbjf|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_58|Account_Name_58|Product_58|source_input_path_58|WS_OFCR_RESP_CDE_58
FJrQQp-49348|hk|64314|us_hk|-(1637766888445_hk)-|1637766888450|LnuyR|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_59|Account_Name_59|Product_59|source_input_path_59|WS_OFCR_RESP_CDE_59
vQumrj-99169|us|97961|uk_uk|-(1637766888445_us)-|1637766888450|pTLML|1637766888000|1637078400000|1637134306000|1637134367000|just for test|tresataId_sub_60|Account_Name_60|Product_60|source_input_path_60|WS_OFCR_RESP_CDE_60
wda-acct:
  Banking_Account_Active: <@random(string, 6)/>-<@random(int,5)/>
  Account_Balance_Date: <@random([uk     us      hk]) />
  source_date: <@random(int,5) />
  Account_Currency: <@random([uk     us      hk])/>_<@random([uk     us      hk])/>
  Account_Number: -(<@timestamp/>_<@random([ uk  us   hk ])/>)-
  WS_PROC_GRP_CDE: <@timestamp />
  Booking_Country: <@random />
  Account_Balance: <@timestamp(10) />
  WS_ACCT_STA_CHG_DTE: <@timestamp(20211117) />
  Account_Closure_Date: <@timestamp(20211117153146) />
  WS_CC_NO: <@timestamp(2021-11-17 15:32:47) />
  WS_COA_NO: <@test/>
  tresataId_sub: ''
  Account_Name: ''
  Product: ''
  source_input_path: ''
  WS_OFCR_RESP_CDE: ''
rps-acnt:
  Banking_Account_Active: ''
  Account_Balance_Date: ''
  source: ''
  source_date: ''
  Account_Currency: ''
  Account_Number: ''
  imac_advd_lmt_amt: ''
  prod_cod: ''
  imac_ulim_amt: ''
  br_no: ''
  Booking_Country: ''
  imac_advd_lmt_expr_dt: ''
  Account_Balance: ''
  Account_Closure_Date: ''
  Account_Open_Date: ''
  prod_cod_des: ''
  imac_rec_type: ''
  Account_Name: ''
  br_no_des: ''
  imac_advd_lmt_strt_dt: ''
  source_input_path: ''
cat: ./clientlifecycle-common-core/src/test/resources/test-data: Is a directory
cat: ./clientlifecycle-common-core/src/test/resources/test-data/hive: Is a directory
cat: ./clientlifecycle-common-core/src/test/resources/test-data/hive/db_demo: Is a directory
id|name
1|Amy
2|Zipcat: ./clientlifecycle-common-core/src/test/resources/test-data/parq: Is a directory
tid|product
t1|prod1
t2|prod2cat: ./clientlifecycle-common-core/src/test/scala: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm/bd/clm: Is a directory
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm/bd/clm/renderer: Is a directory
package com.hsbc.gbm.bd.clm.renderer

import java.io.File

import com.hsbc.gbm.bd.clm.env.RawJobParams
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

@RunWith(classOf[JUnitRunner])
class GenericTemplateRendererTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll {

  val renderer: GenericTemplateRenderer.type = GenericTemplateRenderer

  override def beforeAll(): Unit = {
    val script = new File("src/test/resources/out")
    if (script.exists()) {
      script.delete()
    }
  }

  override def afterAll(): Unit = {

  }


  test("Test generate from SubmitJobFgHDP06") {
    val args = Array(
      "artifactId=test-artifact-id-hdp06"
      , "artifactVersion=1.0.0"
      , "deploymentYamlPath=config/deployment-hdp06.yml"
      , "outputDir=src/test/resources/out/submit-job-fg-hdp06.sh"
      , "templateId=submit-job-fg-hdp06"
      , "templateVersion=1.1"
      , "printOutput=false"
      , "sparkHome=/usr/hdp/current/spark2-client"
    )
    renderer.render(RawJobParams(args))
    val script = new File("src/test/resources/out/submit-job-fg-hdp06.sh")
    script should exist
  }

  test("Test generate from SubmitJobFgHDP45") {
    val args = Array(
      "artifactId=test-artifact-id-hdp45"
      , "artifactVersion=1.0.0"
      , "deploymentYamlPath=config/deployment-hdp45.yml"
      , "outputDir=src/test/resources/out/submit-job-fg-hdp45.sh"
      , "templateId=submit-job-fg-hdp45"
      , "templateVersion=1.1"
      , "printOutput=false"
      , "sparkHome=/tmp/clm/spark3.1.1"
      , "submitCmd=spark-submit.sh"
    )
    renderer.render(RawJobParams(args))
    val script = new File("src/test/resources/out/submit-job-fg-hdp45.sh")
    script should exist
  }

  test("Test generate from SubmitJobFgHDP48") {
    val args = Array(
      "artifactId=test-artifact-id-hdp48"
      , "artifactVersion=1.0.0"
      , "deploymentYamlPath=config/deployment-hdp48.yml"
      , "outputDir=src/test/resources/out/submit-job-fg-hdp48.sh"
      , "templateId=submit-job-fg-hdp48"
      , "templateVersion=1.1"
      , "printOutput=false"
      , "sparkHome=/tmp/clm/spark2.4.3"
      , "submitCmd=spark3-submit"
    )
    renderer.render(RawJobParams(args))
    val script = new File("src/test/resources/out/submit-job-fg-hdp48.sh")
    script should exist
  }

}
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm/bd/clm/runner: Is a directory
package com.hsbc.gbm.bd.clm.runner

import org.apache.log4j.{Level, Logger}
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

class GenericSparkJobRunnerTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll {

  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getRootLogger.setLevel(Level.ERROR)

  test("test GenericSparkJobRunner") {
    GenericSparkJobRunner.main(Array("SparkJobTest", "src/test/resources/config/test-application-scala.yml", "parameter0", "key1=parameter1"))
  }
}
cat: ./clientlifecycle-common-core/src/test/scala/com/hsbc/gbm/bd/clm/spark: Is a directory
package com.hsbc.gbm.bd.clm.spark

import com.hsbc.gbm.bd.clm.env.SparkEnv
import org.apache.spark.sql.SparkSession
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}
import com.hsbc.gbm.bd.clm.utils._
import org.apache.spark.sql.functions.{base64, col, concat_ws}

@RunWith(classOf[JUnitRunner])
class CommonFunctionsTest extends FunSuite
  with Matchers
  with SparkEnv
  with BeforeAndAfterAll {

  implicit val _spark_ : SparkSession = spark

  private val mainTable = spark.createDataFrame(Seq(
    ("1", "a"),
    ("1", "b"),
    ("1", "c"),
    ("1", "d1"),
    ("1", "d2"),
    ("1", "d3"),
    ("1", "d4"),
    ("1", "d5"),
    ("1", "d6"),
    ("2", "e")
  )).toDF("id", "product")
    .as("m")

  private val sideTable = spark.createDataFrame(Seq(
    ("1", "I"),
    ("2", "II"),
    ("4", "IIII"),
    ("3", null)
  )).toDF("id", "brand")
    .as("s")

  private val sideTable2 = spark.createDataFrame(Seq(
    ("1", "I"),
    ("2", "II"),
    ("3", null)
  )).toDF("id", "brand")
    .as("s")

  test("test joinDebaseSkew function") {
    val frame1 = mainTable.skewJoin(sideTable, Seq("id"), "left", Seq("m.id", "s.id"), Seq("1"))

    val tuple = mainTable.preSkewJoin(sideTable, "id", "id", Seq("1"))
    val frame2 = tuple._1.join(tuple._2, Seq("__id__"), "left").drop("__id__").drop(tuple._2("id"))

    frame1.collect() should contain allElementsOf frame2.collect()
  }

  test("test except function") {
    sideTable.except(sideTable2, Seq("run_date"), true)
      .select("id")
      .rdd
      .distinct
      .map(r => r(0).toString)
      .collect
      .toSeq should contain noElementsOf List("3")
    sideTable.except(sideTable2, Seq("run_date"), false)
      .select("id")
      .rdd
      .distinct
      .map(r => r(0).toString)
      .collect
      .toSeq should contain oneElementOf List("3")
  }

  def appName(): String = "spark-common-test"

  def configLocation(): String = "src/test/resources/config/test-application-scala.yml"
}
package com.hsbc.gbm.bd.clm.spark


import java.io.File

import com.hsbc.gbm.bd.clm.env.SparkEnv
import com.hsbc.gbm.bd.clm.utils.CsvUnitTestUtils
import org.apache.commons.io.FileUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

@RunWith(classOf[JUnitRunner])
class CsvUnitTestUtilsTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll
  with SparkEnv {

  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getRootLogger.setLevel(Level.ERROR)

  def appName(): String = "spark-common-test"

  def configLocation(): String = "src/test/resources/config/test-application-scala.yml"

  implicit val _spark_ : SparkSession = spark

  private def output: DataFrame = spark.read.parquet(appConfig("output"))

  override def beforeAll(): Unit = {
    CsvUnitTestUtils.csvToParquet(path = "src/test/resources/test-data/parq")
  }

  override def afterAll(): Unit = {
    FileUtils.deleteDirectory(new File("csv_data.parq"))
  }

  test("Test count") {
    output.show(false)
    output.count() shouldBe 2
  }

  test("Test column value") {
    output.select("product")
      .filter("tid = 't1'")
      .rdd.distinct.map(r => r(0).toString)
      .collect.toSeq should contain allElementsOf List("prod1")
  }
}
package com.hsbc.gbm.bd.clm.spark

import java.io.File

import com.hsbc.gbm.bd.clm.env.SparkEnv
import com.hsbc.gbm.bd.clm.utils.{CsvUnitTestUtils, MockDataUtils}
import org.apache.commons.io.FileUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

@RunWith(classOf[JUnitRunner])
class MockDataTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll
  with SparkEnv {

  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getRootLogger.setLevel(Level.ERROR)

  def appName(): String = "spark-mock-test"

  def configLocation(): String = "src/test/resources/config/test-mock-application.yml"

  implicit val _spark_ : SparkSession = spark

  private def output: DataFrame = spark.read.parquet(appConfig("outputPath"))

  override def beforeAll(): Unit = {
    val testParse: PartialFunction[String, String] = {
      case x if x.toLowerCase.contains("<@test") => {
        val pattern = "(<@test.*?/>)".r
        val first = pattern.findFirstIn(x).get

        x.replaceFirst(first.replace("(", """\(""")
          .replace(")", """\)""")
          , "just for test")
      }
    }

    //MockDataUtils.writeMockData(mappingPath = "src/test/resources/mapping.yml", num = 60, processFunc = Option(testParse))
    CsvUnitTestUtils.csvToParquet(path = "src/test/resources/mock-data-content/parq")
  }

  override def afterAll(): Unit = {
    FileUtils.deleteDirectory(new File("wda-acct.parq"))
    FileUtils.deleteDirectory(new File("rps-acnt.parq"))
  }

  test("Test count") {
    output.show(100, false)
    output.count() shouldBe 60
  }

  test("Test column value") {
    output.select("Account_Balance_Date")
      .rdd.distinct.map(r => r(0).toString)
      .collect.toSeq should contain allElementsOf List("uk", "us", "hk")
  }

  test("Test Partial Function") {
    output.select("WS_COA_NO")
      .rdd.distinct.map(r => r(0).toString)
      .collect.toSeq should contain allElementsOf List("just for test")
  }
}
package com.hsbc.gbm.bd.clm.spark

import com.hsbc.gbm.bd.clm.env.SparkEnv
import org.scalatest.FunSuite

class SparkEnvTest extends FunSuite with SparkEnv {

  def appName(): String = "spark-config-test"

  def configLocation(): String = "src/test/resources/config/test-application-scala.yml"


  test("Test getting a string value from appConfig") {
    val value: String = appConfig("monitoring.table.name")
    assert(value === "monitoring")
  }

  test("Test getting an integer value from appConfig") {
    val value: Int = appConfig("int-property")
    assert(value === 123)
  }

  test("Test getting a double value from appConfig") {
    val value: Double = appConfig("double-property")
    assert(value === 456.789)
  }

  test("Test getting a boolean value from appConfig") {
    val value: Boolean = appConfig("bool-property")
    assert(value === true)
  }

  test("Test getting a list of strings from appConfig") {
    val value: List[String] = appConfig("some-key.string-list")
    assert(value === List("item1", "item2", "item3"))
  }

  test("Test getting a map of strings from appConfig") {
    val value: Map[String, String] = appConfig("some-key.map-of-strings")
    assert(value === Map("item1" -> "value1", "item2" -> "value2", "item3" -> "value3"))
  }

  test("Test getting a map of ints from appConfig") {
    val value: Map[String, Int] = appConfig("some-key.map-of-ints")
    assert(value === Map("item1" -> 168, "item2" -> 2569, "item3" -> -14))
  }

  test("Test getting a map of doubles from appConfig") {
    val value: Map[String, Double] = appConfig("some-key.map-of-doubles")
    assert(value === Map("item1" -> 16.81256, "item2" -> 2.5690745, "item3" -> -0.1425456))
  }

  test("Test getting a map of booleans from appConfig") {
    val value: Map[String, Boolean] = appConfig("some-key.map-of-booleans")
    assert(value === Map("item1" -> true, "item2" -> false, "item3" -> true))
  }

  test("Test getting a list of integers from appConfig") {
    val value: List[Int] = appConfig("some-key.int-list")
    assert(value === List(101, 1045, 8625))
  }

  test("Test getting a list of doubles from appConfig") {
    val value: List[Double] = appConfig("some-key.double-list")
    assert(value === List(1.01, 104.5, 8625.16548))
  }

  test("Test non-existing key") {
    val thrown = intercept[NoSuchElementException] {
      val value: List[Double] = appConfig("invalid-key")
    }
    assert(thrown.getMessage === "key not found: invalid-key")
  }

  test("Test non-existing list key with default value") {
    val value: List[Int] = appConfig("invalid-key", List(1, 2, 3))
    assert(value === List(1, 2, 3))
  }

  test("Test non-existing string key with default value") {
    val value: String = appConfig("invalid-key", "DefaultValue")
    assert(value === "DefaultValue")
  }

}
package com.hsbc.gbm.bd.clm.spark

import com.hsbc.gbm.bd.clm.annotation.SparkJobName
import com.hsbc.gbm.bd.clm.env.{RawJobParams, SparkJob}

@SparkJobName("SparkJobTest")
class SparkJobTest(jobName: String, configPath: String, params: RawJobParams) extends SparkJob {
  override def process(): Unit = {

    val a = spark.createDataFrame(Seq(
      ("1", "a"),
      ("2", "b")
    )).toDF("id", "product")

    val b = spark.createDataFrame(Seq(
      ("1", "I"),
      ("2", "II")
    )).toDF("id", "level")

    a.join(b, Seq("id"), "inner").show(false)

    println(params(0))
    println(params("key1"))
  }

  override def appName(): String = jobName

  override def configLocation(): String = configPath
}
package com.hsbc.gbm.bd.clm

import com.hsbc.gbm.bd.clm.env.SparkEnv
import org.apache.spark.sql.SparkSession
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}
import com.hsbc.gbm.bd.clm.utils._

@RunWith(classOf[JUnitRunner])
class WriteValidTest extends FunSuite
  with Matchers
  with SparkEnv
  with BeforeAndAfterAll {

  implicit val _spark_ : SparkSession = spark

  test("test the speed of validate logic") {
    val csv_data = spark.read.option("header", "true").option("delimiter", "|").csv("src/test/resources/test-data/parq")
    csv_data.show()

    //write to Es
    if (appConfig("write.es").toString == "true") {
      csv_data.write2Es("clm-test", "clm-test.parquet")
    }
  }

  def appName(): String = "spark-common-test"

  def configLocation(): String = "src/test/resources/config/application.yml"
}cat: ./clientlifecycle-common-datalineage_2.11: Is a directory
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>com.hsbc.gbm.bd.clm</groupId>
        <artifactId>clientlifecycle-common</artifactId>
        <version>2.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <artifactId>clientlifecycle-common-datalineage_2.11</artifactId>

    <dependencies>
        <dependency>
            <groupId>com.hsbc.gbm.bd.clm</groupId>
            <artifactId>clientlifecycle-common-core_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
        </dependency>

        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>
cat: ./clientlifecycle-common-datalineage_2.11/src: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org/apache: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org/apache/spark: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org/apache/spark/api: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org/apache/spark/api/python: Is a directory
package org.apache.spark.api.python

import java.io.FileWriter

import org.apache.spark.sql.DataLineage.generateTempViewDataLineageString

object DataLineageApi {

  def save(viewName: String, optimize: Boolean = false, path: String): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(generateTempViewDataLineageString(viewName, optimize))
    } finally {
      writer.close()
    }
  }

  def get(viewName: String, optimize: Boolean = false): String = {
    generateTempViewDataLineageString(viewName, optimize)
  }

  def get(viewName: String): String = {
    generateTempViewDataLineageString(viewName, false)
  }
}
cat: ./clientlifecycle-common-datalineage_2.11/src/main/scala/org/apache/spark/sql: Is a directory
package org.apache.spark.sql

import java.util.concurrent.atomic.AtomicInteger

import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.HttpClients
import org.apache.http.util.EntityUtils

import scala.collection.mutable
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.LeftExistence
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}


trait AbstractDataLineage extends PredicateHelper with Logging {

  private val nextNodeId = new AtomicInteger(0)

  def generateSpecificDataFrameDataLineageString(df: DataFrame, viewName: String, optimize: Boolean = false)(implicit spark: SparkSession): String = {
    df.createOrReplaceTempView(viewName)
    generateTempViewDataLineageString(viewName, optimize)
  }

  def generateTempViewDataLineageString(viewName: String, optimize: Boolean = false): String = {
    val nodeMap = mutable.Map[String, String]()

    implicit val spark: SparkSession = SparkSession.getActiveSession.getOrElse(throw new RuntimeException(s"Active SparkSession not found"))

    val catalog = spark.sessionState.catalog
    val executor = if (optimize) spark.sessionState.optimizer else spark.sessionState.analyzer

    //get all temp views
    val tempViews = catalog.listTables("default").filter(catalog.isTemporaryTable).filter(_.table == viewName.toLowerCase).map { x =>
      val analyzed = executor.execute(catalog.getTempView(x.table).get)
      (x.table, analyzed)
    }

    //get all temp views node information
    tempViews.foreach { case (viewName, analyzed) =>
      nodeMap(viewName) = generateNodeString(analyzed, viewName, {
        if (isCached(analyzed)) "lightblue" else "lightyellow"
      })
    }

    val edges = tempViews.map { case (tempView, analyzed) =>
      val analyzer = {
        val plan = analyzed.transformUp {
          case p if isCached(p) => CachedNode(p) // Because of cache node is a leaf node, so we have to transform it, CachedNode is a custom node and it no longer leaf node
        }.transformDown {
          case s@SubqueryAlias(name, _) if tempViews.map(_._1).contains(name) => TempViewNode(name, s.output)
        } match {
          case f@Filter(condition, _) if condition.sql.contains("'__useless_") => f.child
          case x => x
        }

        executor.execute(plan)
      }

      val (inputNodeId, edges) = traversePlanRecursively(analyzer, nodeMap, isRoot = true) //temp view node is certainly root node in the logical tree
      val inputNodeToTempView = if (inputNodeId != tempView) {
        analyzer.output.indices.map { i =>
          s""""$inputNodeId":$i -> "$tempView":$i;"""
        }
      } else {
        Nil
      }
      edges ++ inputNodeToTempView
    }

    generateGraphString(nodeMap.values.toSeq, edges.flatten)
  }

  private def isCached(plan: LogicalPlan)(implicit spark: SparkSession): Boolean = {
    spark.sharedState.cacheManager.lookupCachedData(plan).isDefined
  }

  private def isCached(name: String)(implicit spark: SparkSession): Boolean = {
    spark.sessionState.catalog.getTempView(name).exists { p =>
      val analyzed = spark.sessionState.analyzer.execute(p)
      spark.sharedState.cacheManager.lookupCachedData(analyzed).isDefined
    }
  }

  protected def getNodeName(p: LogicalPlan): String = p match {
    case TempViewNode(name, _) => name
    case LogicalRelation(fs: HadoopFsRelation, _, _, false) => fs.location.rootPaths.head.getName
    case LogicalRelation(_, _, Some(table), false) => table.qualifiedName
    case HiveTableRelation(table, _, _) => table.qualifiedName
    case j: Join => s"${p.nodeName}_${j.joinType}_${nextNodeId.getAndIncrement()}"
    case sa: SubqueryAlias => s"Alias:${sa.alias}"
    case _@Filter(condition, _) if condition.sql.contains("'__useless_") =>
      val pattern = ".*__useless_(.*)'.*".r
      val pattern(aliasName) = condition.sql
      aliasName
    case _ => s"${p.nodeName}_${nextNodeId.getAndIncrement()}"
  }

  private def getNodeColor(plan: LogicalPlan)(implicit spark: SparkSession): String = plan match {
    case TempViewNode(name, _) if isCached(name) => "lightblue"
    case _: View | _: TempViewNode => "lightyellow"
    case _: LeafNode => "lightpink"
    case _@Filter(condition, _) if condition.sql.contains("'__useless_") => "lightpink"
    case _ => "lightgray"
  }

  private def normalizeForHtml(str: String): String = {
    str.replaceAll("&", "&amp;")
      .replaceAll("<", "&lt;")
      .replaceAll(">", "&gt;")
  }

  protected def tryCreateNode(
                               plan: LogicalPlan,
                               nodeMap: mutable.Map[String, String],
                               cached: Boolean = false)(implicit spark: SparkSession): String = {
    val nodeName = getNodeName(plan)
    if (plan.output.nonEmpty) {
      val nodeColor = if (cached) "lightblue" else ""
      if (!nodeMap.contains(nodeName)) nodeMap.put(nodeName, generateNodeString(plan, nodeName, nodeColor))
    }
    nodeName
  }

  // extract  node extra info
  protected def getExtraInfos(plan: LogicalPlan): Option[String] = {
    plan match {
      case j: Join => Option(j.condition.get.sql)
      case f@Filter(condition, _) if (!condition.sql.contains("'__useless_")) && (condition.sql.split("trim").length < 3) => Option(f.condition.sql)
      case s: Project if (!s.validConstraints.foldLeft("")((s, x) => s + x.sql).contains("'__useless_"))
        && (s.validConstraints.foldLeft("")((s, x) => s + x.sql).split("trim").length < 3) => Option(s.validConstraints.map(_.sql.replaceAll("<=>", "to")).mkString(" :: "))
      case _ => None
    }
  }

  protected def generateGraphString(nodes: Seq[String], edges: Seq[String])(implicit spark: SparkSession): String = {
    if (nodes.nonEmpty) {
      s"""
         |digraph {
         |  graph [pad="0.5", nodesep="0.5", ranksep="2", fontname="Helvetica"];
         |  node [shape=plain]
         |  rankdir=LR;
         |
         |  ${nodes.sorted.mkString("\n")}
         |  ${edges.sorted.mkString("\n")}
         |}
       """.stripMargin
    } else {
      ""
    }
  }

  protected def generateNodeString(plan: LogicalPlan, nodeName: String, nodeColor: String = "")(implicit spark: SparkSession): String = {
    val outputAttrs = plan.output.zipWithIndex.map { case (attr, i) =>
      s"""<tr><td port="$i">${normalizeForHtml(attr.name)}</td></tr>"""
    }

    val extraInfos = getExtraInfos(plan)

    s"""
       |"$nodeName" [label=<
       |<table border="1" cellborder="0" cellspacing="0">
       |  <tr><td bgcolor="${if (nodeColor.isEmpty) getNodeColor(plan) else nodeColor}" port="nodeName"><i>$nodeName</i></td></tr>
       | ${
      if (extraInfos.nonEmpty && extraInfos.get.length != 0 && !extraInfos.get.contains(" to ")) {
        if (extraInfos.get.contains("::")) {
          extraInfos.get.split(" :: ").grouped(2).map(_.mkString(" :: ")).map(x => s"""<tr><td bgcolor="lightblue"><i>${x}</i></td></tr>""").mkString("\n")
        } else
          s"""<tr><td bgcolor="lightblue"><i>${extraInfos.get}</i></td></tr>"""
      } else ""
    }
       |  ${outputAttrs.mkString("\n")}
       |</table>>];
     """.stripMargin
  }

  protected def getEdges(
                          plan: LogicalPlan,
                          currNodeName: String,
                          inputNodeNames: Seq[String],
                          nodeMap: mutable.Map[String, String])(implicit spark: SparkSession): Seq[String] = {
    val inputAttrSeq = plan.children.map(_.output).zip(inputNodeNames).map { case (attrs, nodeName) =>
      attrs.zipWithIndex.map { case (a, i) =>
        a -> s""""$nodeName":$i"""
      }
    }
    val inputAttrMap = AttributeMap(inputAttrSeq.flatten)
    val outputAttrWithIndex = plan.output.zipWithIndex

    val edges1 = plan match {
      case Aggregate(_, aggExprs, _) =>
        aggExprs.zip(outputAttrWithIndex).flatMap { case (ne, (_, i)) =>
          ne.references.filter(inputAttrMap.contains).map { attr =>
            s"""${inputAttrMap(attr)} -> "$currNodeName":$i;"""
          }
        }

      case Project(projList, _) =>
        val extraInfos = getExtraInfos(plan)
        projList.zip(outputAttrWithIndex).flatMap { case (ne, (_, i)) =>
          val labelMap = if (extraInfos.nonEmpty && extraInfos.get.length != 0)
            extraInfos.get.replaceAll("[()]", "")
              .split(" :: ")
              .filter(_.contains(" to "))
              .map(x => x.trim.split(" to "))
              .map(x => x(0) -> x(1))
              .toMap
              .map(x => x._1 -> s"[label=<<SUB>${x._1} to ${x._2}</SUB>>]")
          else Map[String, String]()
          ne.references.filter(inputAttrMap.contains).map { attr =>
            s"""${inputAttrMap(attr)} -> "$currNodeName":$i ${labelMap.getOrElse(attr.sql, "")};"""
          }
        }

      case g@Generate(generator, _, _, _, generatorOutput, _) =>
        val edgesForChildren = g.requiredChildOutput.zipWithIndex.flatMap { case (attr, i) =>
          inputAttrMap.get(attr).map { input => s"""$input -> "$currNodeName":$i;""" }
        }
        val edgeForGenerator = generator.references.flatMap(inputAttrMap.get).headOption
          .map { genInput =>
            generatorOutput.zipWithIndex.map { case (attr, i) =>
              s"""$genInput -> "$currNodeName":${g.requiredChildOutput.size + i}"""
            }
          }
        edgesForChildren ++ edgeForGenerator.seq.flatten

      case Expand(projections, _, _) =>
        projections.transpose.zipWithIndex.flatMap { case (projs, i) =>
          projs.flatMap(e => e.references.flatMap(inputAttrMap.get))
            .map { input => s"""$input -> "$currNodeName":$i;""" }
            .distinct
        }

      case _: Union =>
        inputAttrSeq.transpose.zipWithIndex.flatMap { case (attrs, i) =>
          attrs.map { case (_, input) => s"""$input -> "$currNodeName":$i""" }
        }

      case Join(_, _, joinType, condition) =>
        // To avoid ambiguous joins, we need this
        val Seq(left, right) = inputAttrSeq
        joinType match {
          case LeftExistence(_) =>
            val leftAttrSet = AttributeSet(left.map(_._1))
            val leftAttrIndexMap = AttributeMap(left.map(_._1).zipWithIndex)
            val predicateEdges = condition.map { c =>
              val referenceSeq = splitConjunctivePredicates(c).map(_.references)
              right.flatMap { case (attr, input) =>
                val leftAttrs = referenceSeq.flatMap { refs =>
                  if (refs.contains(attr)) {
                    refs.intersect(leftAttrSet).toSeq
                  } else {
                    Nil
                  }
                }
                leftAttrs.map { attr =>
                  s"""$input -> "$currNodeName":${leftAttrIndexMap(attr)};"""
                }
              }
            }
            val joinOutputEdges = left.map(_._2).zipWithIndex.map { case (input, i) =>
              s"""$input -> "$currNodeName":$i;"""
            }
            joinOutputEdges ++ predicateEdges.getOrElse(Nil)
          case _ =>
            (left ++ right).map(_._2).zipWithIndex.map { case (input, i) =>
              s"""$input -> "$currNodeName":$i;"""
            }
        }

      case _ =>
        outputAttrWithIndex.flatMap { case (attr, i) =>
          inputAttrMap.get(attr).map { input => s"""$input -> "$currNodeName":$i;""" }
        }
    }

    val edges2 = if (plan.expressions.exists(SubqueryExpression.hasSubquery)) collectEdgesInSubqueries(currNodeName, plan, nodeMap) else Nil

    edges1 ++ edges2
  }

  private def collectEdgesInSubqueries(
                                        nodeName: String,
                                        plan: LogicalPlan,
                                        nodeMap: mutable.Map[String, String])(implicit spark: SparkSession): Seq[String] = {
    val planOutputMap = AttributeMap(plan.output.zipWithIndex)

    def collectEdgesInExprs(ne: NamedExpression): Seq[String] = {
      val attr = ne.toAttribute
      val ss = ne.collectFirst { case ss: ScalarSubquery => ss }.get
      val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
      edges ++ ss.plan.output.indices.map { i =>
        if (planOutputMap.contains(attr)) {
          s""""$inputNodeId":$i -> "$nodeName":${planOutputMap(attr)}"""
        } else {
          s""""$inputNodeId":$i -> "$nodeName":nodeName"""
        }
      }
    }

    plan match {
      case Filter(SubqueryPredicate(subqueries), _) =>
        subqueries.flatMap { case (ss, attrs) =>
          val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
          edges ++ ss.plan.output.indices.flatMap { i =>
            val edgesInSubqueries = attrs.flatMap { attr =>
              if (planOutputMap.contains(attr)) {
                Some(s""""$inputNodeId":$i -> "$nodeName":${planOutputMap(attr)}""")
              } else {
                None
              }
            }

            if (edgesInSubqueries.nonEmpty) {
              edgesInSubqueries
            } else {
              s""""$inputNodeId":$i -> "$nodeName":nodeName""" :: Nil
            }
          }
        }

      case Project(projList, _) =>
        projList.filter(SubqueryExpression.hasSubquery).flatMap { ne =>
          collectEdgesInExprs(ne)
        }

      case Aggregate(_, aggregateExprs, _) =>
        aggregateExprs.filter(SubqueryExpression.hasSubquery).flatMap { ne =>
          collectEdgesInExprs(ne)
        }

      case _ =>
        val subqueries = plan.expressions.flatMap(_.collect { case ss: ScalarSubquery => ss })
        subqueries.flatMap { ss =>
          val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
          edges ++ ss.plan.output.indices.map { i =>
            s""""$inputNodeId":$i -> "$nodeName":nodeName"""
          }
        }
    }
  }

  def dot2Svg(dot: String, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph"): String = {
    val client = HttpClients.createDefault()
    val httpPost = new HttpPost(api)
    val entity = new StringEntity(dot, "UTF-8")
    httpPost.setEntity(entity)
    httpPost.setHeader("Content-type", "text/vnd.graphviz")
    httpPost.setHeader("Accept", "image/svg+xml")

    val httpResponse = try {
      client.execute(httpPost)
    } catch {
      case e: Exception => throw new RuntimeException(e.getMessage)
    }

    val svg = EntityUtils.toString(httpResponse.getEntity, "UTF-8")
    EntityUtils.consume(httpResponse.getEntity)

    client.close()
    svg
  }

  object SubqueryPredicate {
    def unapply(cond: Expression): Option[Seq[(ScalarSubquery, Seq[Attribute])]] = {
      val comps = cond.collect {
        case BinaryComparison(le, re) if SubqueryExpression.hasSubquery(re) =>
          val ss = re.collectFirst { case ss: ScalarSubquery => ss }.get
          (ss, le.references.toSeq)
      }
      if (comps.nonEmpty) {
        Some(comps)
      } else {
        None
      }
    }
  }

  /**
   *
   * @param plan
   * @param nodeMap
   * @param cached
   * @param isRoot
   * @param spark
   * @return (inputNodeName, edges)
   */
  protected def traversePlanRecursively(plan: LogicalPlan,
                                        nodeMap: mutable.Map[String, String],
                                        cached: Boolean = false,
                                        isRoot: Boolean = false)(implicit spark: SparkSession): (String, Seq[String])
}

case class CachedNode(cachedPlan: LogicalPlan) extends UnaryNode {
  override lazy val resolved: Boolean = true

  override def output: Seq[Attribute] = cachedPlan.output

  override def child: LogicalPlan = cachedPlan
}

case class TempViewNode(name: String, output: Seq[Attribute]) extends LeafNode {
  override lazy val resolved: Boolean = true
}


package org.apache.spark.sql

import java.io.FileWriter
import java.util.concurrent.atomic.AtomicInteger

import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.HttpClients
import org.apache.http.util.EntityUtils
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeSet, PredicateHelper}
import org.apache.spark.sql.catalyst.plans.LeftExistence
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}

import scala.collection.mutable

object DataLineage extends Logging with AbstractDataLineage {

  def tryGenerateDFImageFile(df: DataFrame, viewName: String, path: String, optimize: Boolean = false, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph")(implicit spark: SparkSession): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(dot2Svg(generateSpecificDataFrameDataLineageString(df, viewName, optimize), api))
    } finally {
      writer.close()
    }
  }

  def tryGenerateViewImageFile(viewName: String, path: String, optimize: Boolean = false, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph")(implicit spark: SparkSession): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(dot2Svg(generateTempViewDataLineageString(viewName, optimize), api))
    } finally {
      writer.close()
    }
  }

  /**
   *
   * @param plan
   * @param nodeMap
   * @param cached
   * @param isRoot
   * @param spark
   * @return (inputNodeName, edges)
   */
  override protected def traversePlanRecursively(plan: LogicalPlan, nodeMap: mutable.Map[String, String], cached: Boolean, isRoot: Boolean)(implicit spark: SparkSession): (String, Seq[String]) = plan match {
    case _: LeafNode =>
      val currNodeName = tryCreateNode(plan, nodeMap)
      (currNodeName, Nil)

    case CachedNode(cachedPlan) =>
      traversePlanRecursively(cachedPlan, nodeMap, cached = !isRoot)

    case _@Filter(condition, _) if condition.sql.contains("'__useless_") =>
      val currNodeName = tryCreateNode(plan, nodeMap)
      (currNodeName, Nil)

    case _ =>
      val edgesInChildren = plan.children.map(traversePlanRecursively(_, nodeMap))
      val currNodeName = tryCreateNode(plan, nodeMap, cached)
      if (plan.output.nonEmpty) {
        val inputNodeNames = edgesInChildren.map(_._1)
        val edges = getEdges(plan, currNodeName, inputNodeNames, nodeMap)

        (currNodeName, edges ++ edgesInChildren.flatMap(_._2))
      } else {
        (currNodeName, edgesInChildren.flatMap(_._2))
      }
  }
}cat: ./clientlifecycle-common-datalineage_2.11/src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./clientlifecycle-common-datalineage_2.11/src/site/resources: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./clientlifecycle-common-datalineage_2.11/src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>cat: ./clientlifecycle-common-datalineage_2.11/src/test: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala/com: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-datalineage_2.11/src/test/scala/com/hsbc/gbm/bd/clm: Is a directory
package com.hsbc.gbm.bd.clm

import java.io.File

import org.apache.spark.sql.{DataLineage, SparkSession}
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

import com.hsbc.gbm.bd.clm.utils._

@RunWith(classOf[JUnitRunner])
class DataLineageTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll {

  private val fileName = "target/df.svg"

  override def beforeAll(): Unit = {
    val script = new File(fileName)
    if (script.exists()) {
      script.delete()
    }
  }

  implicit def spark: SparkSession = SparkSession
    .builder()
    .master("local[*]")
    .config("spark.sql.crossJoin.enabled", "true")
    .enableHiveSupport()
    .getOrCreate()

  private val aTable = spark.createDataFrame(Seq(
    ("1", "a"),
    ("1", "b"),
    ("1", "c"),
    ("2", "e")
  )).toDF("id", "product")
    .cut("aTable")
    .as("a_table")

  private val bTable = spark.createDataFrame(Seq(
    ("1", "I"),
    ("2", "II")
  )).toDF("id", "brand")
    .cut("bTable")
    .as("b_table")

  test("test getSkewValues function") {
    val df = aTable.join(bTable, Seq("id"), "left")

    DataLineage.tryGenerateDFImageFile(df, "output", fileName, false)

    val script = new File(fileName)
    script should exist
  }
}
cat: ./clientlifecycle-common-datalineage_2.12: Is a directory
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>com.hsbc.gbm.bd.clm</groupId>
        <artifactId>clientlifecycle-common</artifactId>
        <version>2.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <artifactId>clientlifecycle-common-datalineage_2.12</artifactId>

    <dependencies>
        <dependency>
            <groupId>com.hsbc.gbm.bd.clm</groupId>
            <artifactId>clientlifecycle-common-core_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
        </dependency>

        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>
cat: ./clientlifecycle-common-datalineage_2.12/src: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org/apache: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org/apache/spark: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org/apache/spark/api: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org/apache/spark/api/python: Is a directory
package org.apache.spark.api.python

import java.io.FileWriter

import org.apache.spark.sql.DataLineage.generateTempViewDataLineageString

object DataLineageApi {

  def save(viewName: String, optimize: Boolean = false, path: String): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(generateTempViewDataLineageString(viewName, optimize))
    } finally {
      writer.close()
    }
  }

  def get(viewName: String, optimize: Boolean = false): String = {
    generateTempViewDataLineageString(viewName, optimize)
  }

  def get(viewName: String): String = {
    generateTempViewDataLineageString(viewName, false)
  }
}
cat: ./clientlifecycle-common-datalineage_2.12/src/main/scala/org/apache/spark/sql: Is a directory
package org.apache.spark.sql

import java.util.concurrent.atomic.AtomicInteger

import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.HttpClients
import org.apache.http.util.EntityUtils

import scala.collection.mutable
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.LeftExistence
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}


trait AbstractDataLineage extends PredicateHelper with Logging {

  private val nextNodeId = new AtomicInteger(0)

  def generateSpecificDataFrameDataLineageString(df: DataFrame, viewName: String, optimize: Boolean = false)(implicit spark: SparkSession): String = {
    df.createOrReplaceTempView(viewName)
    generateTempViewDataLineageString(viewName, optimize)
  }

  def generateTempViewDataLineageString(viewName: String, optimize: Boolean = false): String = {
    val nodeMap = mutable.Map[String, String]()

    implicit val spark: SparkSession = SparkSession.getActiveSession.getOrElse(throw new RuntimeException(s"Active SparkSession not found"))

    val catalog = spark.sessionState.catalog
    val executor = if (optimize) spark.sessionState.optimizer else spark.sessionState.analyzer

    //get all temp views
    val tempViews = catalog.listTables("default").filter(catalog.isTemporaryTable).filter(_.table == viewName.toLowerCase).map { x =>
      val analyzed = executor.execute(catalog.getTempView(x.table).get)
      (x.table, analyzed)
    }

    //get all temp views node information
    tempViews.foreach { case (viewName, analyzed) =>
      nodeMap(viewName) = generateNodeString(analyzed, viewName, {
        if (isCached(analyzed)) "lightblue" else "lightyellow"
      })
    }

    val edges = tempViews.map { case (tempView, analyzed) =>
      val analyzer = {
        val plan = analyzed.transformUp {
          case p if isCached(p) => CachedNode(p) // Because of cache node is a leaf node, so we have to transform it, CachedNode is a custom node and it no longer leaf node
        }.transformDown {
          case s@SubqueryAlias(identifier, _) if tempViews.map(_._1).contains(identifier.name) => TempViewNode(identifier.name, s.output)
        } match {
          case f@Filter(condition, _) if condition.sql.contains("'__useless_") => f.child
          case x => x
        }

        executor.execute(plan)
      }

      val (inputNodeId, edges) = traversePlanRecursively(analyzer, nodeMap, isRoot = true) //temp view node is certainly root node in the logical tree
      val inputNodeToTempView = if (inputNodeId != tempView) {
        analyzer.output.indices.map { i =>
          s""""$inputNodeId":$i -> "$tempView":$i;"""
        }
      } else {
        Nil
      }
      edges ++ inputNodeToTempView
    }

    generateGraphString(nodeMap.values.toSeq, edges.flatten)
  }

  private def isCached(plan: LogicalPlan)(implicit spark: SparkSession): Boolean = {
    spark.sharedState.cacheManager.lookupCachedData(plan).isDefined
  }

  private def isCached(name: String)(implicit spark: SparkSession): Boolean = {
    spark.sessionState.catalog.getTempView(name).exists { p =>
      val analyzed = spark.sessionState.analyzer.execute(p)
      spark.sharedState.cacheManager.lookupCachedData(analyzed).isDefined
    }
  }

  protected def getNodeName(p: LogicalPlan): String = p match {
    case TempViewNode(name, _) => name
    case LogicalRelation(fs: HadoopFsRelation, _, _, false) => fs.location.rootPaths.head.getName
    case LogicalRelation(_, _, Some(table), false) => table.qualifiedName
    case HiveTableRelation(table, _, _, _, _) => table.qualifiedName
    case j: Join => s"${p.nodeName}_${j.joinType}_${nextNodeId.getAndIncrement()}"
    case sa: SubqueryAlias => s"Alias:${sa.alias}"
    case _@Filter(condition, _) if condition.sql.contains("'__useless_") =>
      val pattern = ".*__useless_(.*)'.*".r
      val pattern(aliasName) = condition.sql
      aliasName
    case _ => s"${p.nodeName}_${nextNodeId.getAndIncrement()}"
  }

  private def getNodeColor(plan: LogicalPlan)(implicit spark: SparkSession): String = plan match {
    case TempViewNode(name, _) if isCached(name) => "lightblue"
    case _: View | _: TempViewNode => "lightyellow"
    case _: LeafNode => "lightpink"
    case _@Filter(condition, _) if condition.sql.contains("'__useless_") => "lightpink"
    case _ => "lightgray"
  }

  private def normalizeForHtml(str: String): String = {
    str.replaceAll("&", "&amp;")
      .replaceAll("<", "&lt;")
      .replaceAll(">", "&gt;")
  }

  protected def tryCreateNode(
                               plan: LogicalPlan,
                               nodeMap: mutable.Map[String, String],
                               cached: Boolean = false)(implicit spark: SparkSession): String = {
    val nodeName = getNodeName(plan)
    if (plan.output.nonEmpty) {
      val nodeColor = if (cached) "lightblue" else ""
      if (!nodeMap.contains(nodeName)) nodeMap.put(nodeName, generateNodeString(plan, nodeName, nodeColor))
    }
    nodeName
  }

  // extract  node extra info
  protected def getExtraInfos(plan: LogicalPlan): Option[String] = {
    plan match {
      case j: Join => Option(j.condition.get.sql)
      case f@Filter(condition, _) if (!condition.sql.contains("'__useless_")) && (condition.sql.split("trim").length < 3) => Option(f.condition.sql)
      case s: Project if (!s.validConstraints.foldLeft("")((s, x) => s + x.sql).contains("'__useless_"))
        && (s.validConstraints.foldLeft("")((s, x) => s + x.sql).split("trim").length < 3) => Option(s.validConstraints.map(_.sql.replaceAll("<=>", "to")).mkString(" :: "))
      case _ => None
    }
  }

  protected def generateGraphString(nodes: Seq[String], edges: Seq[String])(implicit spark: SparkSession): String = {
    if (nodes.nonEmpty) {
      s"""
         |digraph {
         |  graph [pad="0.5", nodesep="0.5", ranksep="2", fontname="Helvetica"];
         |  node [shape=plain]
         |  rankdir=LR;
         |
         |  ${nodes.sorted.mkString("\n")}
         |  ${edges.sorted.mkString("\n")}
         |}
       """.stripMargin
    } else {
      ""
    }
  }

  protected def generateNodeString(plan: LogicalPlan, nodeName: String, nodeColor: String = "")(implicit spark: SparkSession): String = {
    val outputAttrs = plan.output.zipWithIndex.map { case (attr, i) =>
      s"""<tr><td port="$i">${normalizeForHtml(attr.name)}</td></tr>"""
    }

    val extraInfos = getExtraInfos(plan)

    s"""
       |"$nodeName" [label=<
       |<table border="1" cellborder="0" cellspacing="0">
       |  <tr><td bgcolor="${if (nodeColor.isEmpty) getNodeColor(plan) else nodeColor}" port="nodeName"><i>$nodeName</i></td></tr>
       | ${
      if (extraInfos.nonEmpty && extraInfos.get.length != 0 && !extraInfos.get.contains(" to ")) {
        if (extraInfos.get.contains("::")) {
          extraInfos.get.split(" :: ").grouped(2).map(_.mkString(" :: ")).map(x => s"""<tr><td bgcolor="lightblue"><i>${x}</i></td></tr>""").mkString("\n")
        } else
          s"""<tr><td bgcolor="lightblue"><i>${extraInfos.get}</i></td></tr>"""
      } else ""
    }
       |  ${outputAttrs.mkString("\n")}
       |</table>>];
     """.stripMargin
  }

  protected def getEdges(
                          plan: LogicalPlan,
                          currNodeName: String,
                          inputNodeNames: Seq[String],
                          nodeMap: mutable.Map[String, String])(implicit spark: SparkSession): Seq[String] = {
    val inputAttrSeq = plan.children.map(_.output).zip(inputNodeNames).map { case (attrs, nodeName) =>
      attrs.zipWithIndex.map { case (a, i) =>
        a -> s""""$nodeName":$i"""
      }
    }
    val inputAttrMap = AttributeMap(inputAttrSeq.flatten)
    val outputAttrWithIndex = plan.output.zipWithIndex

    val edges1 = plan match {
      case Aggregate(_, aggExprs, _) =>
        aggExprs.zip(outputAttrWithIndex).flatMap { case (ne, (_, i)) =>
          ne.references.filter(inputAttrMap.contains).map { attr =>
            s"""${inputAttrMap(attr)} -> "$currNodeName":$i;"""
          }
        }

      case Project(projList, _) =>
        val extraInfos = getExtraInfos(plan)
        projList.zip(outputAttrWithIndex).flatMap { case (ne, (_, i)) =>
          val labelMap = if (extraInfos.nonEmpty && extraInfos.get.length != 0)
            extraInfos.get.replaceAll("[()]", "")
              .split(" :: ")
              .filter(_.contains(" to "))
              .map(x => x.trim.split(" to "))
              .map(x => x(0) -> x(1))
              .toMap
              .map(x => x._1 -> s"[label=<<SUB>${x._1} to ${x._2}</SUB>>]")
          else Map[String, String]()
          ne.references.filter(inputAttrMap.contains).map { attr =>
            s"""${inputAttrMap(attr)} -> "$currNodeName":$i ${labelMap.getOrElse(attr.sql, "")};"""
          }
        }

      case g@Generate(generator, _, _, _, generatorOutput, _) =>
        val edgesForChildren = g.requiredChildOutput.zipWithIndex.flatMap { case (attr, i) =>
          inputAttrMap.get(attr).map { input => s"""$input -> "$currNodeName":$i;""" }
        }
        val edgeForGenerator = generator.references.flatMap(inputAttrMap.get).headOption
          .map { genInput =>
            generatorOutput.zipWithIndex.map { case (attr, i) =>
              s"""$genInput -> "$currNodeName":${g.requiredChildOutput.size + i}"""
            }
          }
        edgesForChildren ++ edgeForGenerator.seq.flatten

      case Expand(projections, _, _) =>
        projections.transpose.zipWithIndex.flatMap { case (projs, i) =>
          projs.flatMap(e => e.references.flatMap(inputAttrMap.get))
            .map { input => s"""$input -> "$currNodeName":$i;""" }
            .distinct
        }

      case _: Union =>
        inputAttrSeq.transpose.zipWithIndex.flatMap { case (attrs, i) =>
          attrs.map { case (_, input) => s"""$input -> "$currNodeName":$i""" }
        }

      case Join(_, _, joinType, condition, _) =>
        // To avoid ambiguous joins, we need this
        val Seq(left, right) = inputAttrSeq
        joinType match {
          case LeftExistence(_) =>
            val leftAttrSet = AttributeSet(left.map(_._1))
            val leftAttrIndexMap = AttributeMap(left.map(_._1).zipWithIndex)
            val predicateEdges = condition.map { c =>
              val referenceSeq = splitConjunctivePredicates(c).map(_.references)
              right.flatMap { case (attr, input) =>
                val leftAttrs = referenceSeq.flatMap { refs =>
                  if (refs.contains(attr)) {
                    refs.intersect(leftAttrSet).toSeq
                  } else {
                    Nil
                  }
                }
                leftAttrs.map { attr =>
                  s"""$input -> "$currNodeName":${leftAttrIndexMap(attr)};"""
                }
              }
            }
            val joinOutputEdges = left.map(_._2).zipWithIndex.map { case (input, i) =>
              s"""$input -> "$currNodeName":$i;"""
            }
            joinOutputEdges ++ predicateEdges.getOrElse(Nil)
          case _ =>
            (left ++ right).map(_._2).zipWithIndex.map { case (input, i) =>
              s"""$input -> "$currNodeName":$i;"""
            }
        }

      case _ =>
        outputAttrWithIndex.flatMap { case (attr, i) =>
          inputAttrMap.get(attr).map { input => s"""$input -> "$currNodeName":$i;""" }
        }
    }

    val edges2 = if (plan.expressions.exists(SubqueryExpression.hasSubquery)) collectEdgesInSubqueries(currNodeName, plan, nodeMap) else Nil

    edges1 ++ edges2
  }

  private def collectEdgesInSubqueries(
                                        nodeName: String,
                                        plan: LogicalPlan,
                                        nodeMap: mutable.Map[String, String])(implicit spark: SparkSession): Seq[String] = {
    val planOutputMap = AttributeMap(plan.output.zipWithIndex)

    def collectEdgesInExprs(ne: NamedExpression): Seq[String] = {
      val attr = ne.toAttribute
      val ss = ne.collectFirst { case ss: ScalarSubquery => ss }.get
      val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
      edges ++ ss.plan.output.indices.map { i =>
        if (planOutputMap.contains(attr)) {
          s""""$inputNodeId":$i -> "$nodeName":${planOutputMap(attr)}"""
        } else {
          s""""$inputNodeId":$i -> "$nodeName":nodeName"""
        }
      }
    }

    plan match {
      case Filter(SubqueryPredicate(subqueries), _) =>
        subqueries.flatMap { case (ss, attrs) =>
          val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
          edges ++ ss.plan.output.indices.flatMap { i =>
            val edgesInSubqueries = attrs.flatMap { attr =>
              if (planOutputMap.contains(attr)) {
                Some(s""""$inputNodeId":$i -> "$nodeName":${planOutputMap(attr)}""")
              } else {
                None
              }
            }

            if (edgesInSubqueries.nonEmpty) {
              edgesInSubqueries
            } else {
              s""""$inputNodeId":$i -> "$nodeName":nodeName""" :: Nil
            }
          }
        }

      case Project(projList, _) =>
        projList.filter(SubqueryExpression.hasSubquery).flatMap { ne =>
          collectEdgesInExprs(ne)
        }

      case Aggregate(_, aggregateExprs, _) =>
        aggregateExprs.filter(SubqueryExpression.hasSubquery).flatMap { ne =>
          collectEdgesInExprs(ne)
        }

      case _ =>
        val subqueries = plan.expressions.flatMap(_.collect { case ss: ScalarSubquery => ss })
        subqueries.flatMap { ss =>
          val (inputNodeId, edges) = traversePlanRecursively(ss.plan, nodeMap)
          edges ++ ss.plan.output.indices.map { i =>
            s""""$inputNodeId":$i -> "$nodeName":nodeName"""
          }
        }
    }
  }

  def dot2Svg(dot: String, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph"): String = {
    val client = HttpClients.createDefault()
    val httpPost = new HttpPost(api)
    val entity = new StringEntity(dot, "UTF-8")
    httpPost.setEntity(entity)
    httpPost.setHeader("Content-type", "text/vnd.graphviz")
    httpPost.setHeader("Accept", "image/svg+xml")

    val httpResponse = try {
      client.execute(httpPost)
    } catch {
      case e: Exception => throw new RuntimeException(e.getMessage)
    }

    val svg = EntityUtils.toString(httpResponse.getEntity, "UTF-8")
    EntityUtils.consume(httpResponse.getEntity)

    client.close()
    svg
  }

  object SubqueryPredicate {
    def unapply(cond: Expression): Option[Seq[(ScalarSubquery, Seq[Attribute])]] = {
      val comps = cond.collect {
        case BinaryComparison(le, re) if SubqueryExpression.hasSubquery(re) =>
          val ss = re.collectFirst { case ss: ScalarSubquery => ss }.get
          (ss, le.references.toSeq)
      }
      if (comps.nonEmpty) {
        Some(comps)
      } else {
        None
      }
    }
  }

  /**
   *
   * @param plan
   * @param nodeMap
   * @param cached
   * @param isRoot
   * @param spark
   * @return (inputNodeName, edges)
   */
  protected def traversePlanRecursively(plan: LogicalPlan,
                                        nodeMap: mutable.Map[String, String],
                                        cached: Boolean = false,
                                        isRoot: Boolean = false)(implicit spark: SparkSession): (String, Seq[String])
}

case class CachedNode(cachedPlan: LogicalPlan) extends UnaryNode {
  override lazy val resolved: Boolean = true

  override def output: Seq[Attribute] = cachedPlan.output

  override def child: LogicalPlan = cachedPlan
}

case class TempViewNode(name: String, output: Seq[Attribute]) extends LeafNode {
  override lazy val resolved: Boolean = true
}


package org.apache.spark.sql

import java.io.FileWriter
import java.util.concurrent.atomic.AtomicInteger

import org.apache.http.client.methods.HttpPost
import org.apache.http.entity.StringEntity
import org.apache.http.impl.client.HttpClients
import org.apache.http.util.EntityUtils
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeMap, AttributeSet, PredicateHelper}
import org.apache.spark.sql.catalyst.plans.LeftExistence
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.execution.datasources.{HadoopFsRelation, LogicalRelation}

import scala.collection.mutable

object DataLineage extends Logging with AbstractDataLineage {

  def tryGenerateDFImageFile(df: DataFrame, viewName: String, path: String, optimize: Boolean = false, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph")(implicit spark: SparkSession): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(dot2Svg(generateSpecificDataFrameDataLineageString(df, viewName, optimize), api))
    } finally {
      writer.close()
    }
  }

  def tryGenerateViewImageFile(viewName: String, path: String, optimize: Boolean = false, api: String = "https://apprunner.hk.hsbc/graph/api/v1/graph")(implicit spark: SparkSession): Unit = {
    val writer = new FileWriter(path, false)
    try {
      writer.write(dot2Svg(generateTempViewDataLineageString(viewName, optimize), api))
    } finally {
      writer.close()
    }
  }

  /**
   *
   * @param plan
   * @param nodeMap
   * @param cached
   * @param isRoot
   * @param spark
   * @return (inputNodeName, edges)
   */
  override protected def traversePlanRecursively(plan: LogicalPlan, nodeMap: mutable.Map[String, String], cached: Boolean, isRoot: Boolean)(implicit spark: SparkSession): (String, Seq[String]) = plan match {
    case _: LeafNode =>
      val currNodeName = tryCreateNode(plan, nodeMap)
      (currNodeName, Nil)

    case CachedNode(cachedPlan) =>
      traversePlanRecursively(cachedPlan, nodeMap, cached = !isRoot)

    case _@Filter(condition, _) if condition.sql.contains("'__useless_") =>
      val currNodeName = tryCreateNode(plan, nodeMap)
      (currNodeName, Nil)

    case _ =>
      val edgesInChildren = plan.children.map(traversePlanRecursively(_, nodeMap))
      val currNodeName = tryCreateNode(plan, nodeMap, cached)
      if (plan.output.nonEmpty) {
        val inputNodeNames = edgesInChildren.map(_._1)
        val edges = getEdges(plan, currNodeName, inputNodeNames, nodeMap)

        (currNodeName, edges ++ edgesInChildren.flatMap(_._2))
      } else {
        (currNodeName, edgesInChildren.flatMap(_._2))
      }
  }
}cat: ./clientlifecycle-common-datalineage_2.12/src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./clientlifecycle-common-datalineage_2.12/src/site/resources: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./clientlifecycle-common-datalineage_2.12/src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>cat: ./clientlifecycle-common-datalineage_2.12/src/test: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala/com: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-datalineage_2.12/src/test/scala/com/hsbc/gbm/bd/clm: Is a directory
package com.hsbc.gbm.bd.clm

import java.io.File

import org.apache.spark.sql.{DataLineage, SparkSession}
import org.junit.runner.RunWith
import org.scalatest.junit.JUnitRunner
import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}

import com.hsbc.gbm.bd.clm.utils._

@RunWith(classOf[JUnitRunner])
class DataLineageTest extends FunSuite
  with Matchers
  with BeforeAndAfterAll {

  private val fileName = "target/df.svg"

  override def beforeAll(): Unit = {
    val script = new File(fileName)
    if (script.exists()) {
      script.delete()
    }
  }

  implicit def spark: SparkSession = SparkSession
    .builder()
    .master("local[*]")
    .config("spark.sql.crossJoin.enabled", "true")
    .enableHiveSupport()
    .getOrCreate()

  private val aTable = spark.createDataFrame(Seq(
    ("1", "a"),
    ("1", "b"),
    ("1", "c"),
    ("2", "e")
  )).toDF("id", "product")
    .cut("aTable")
    .as("a_table")

  private val bTable = spark.createDataFrame(Seq(
    ("1", "I"),
    ("2", "II")
  )).toDF("id", "brand")
    .cut("bTable")
    .as("b_table")

  test("test getSkewValues function") {
    val df = aTable.join(bTable, Seq("id"), "left")

    DataLineage.tryGenerateDFImageFile(df, "output", fileName, false)

    val script = new File(fileName)
    script should exist
  }
}
cat: ./clientlifecycle-common-hint_2.11: Is a directory
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>com.hsbc.gbm.bd.clm</groupId>
        <artifactId>clientlifecycle-common</artifactId>
        <version>2.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>clientlifecycle-common-hint_${scala.binary.version}</artifactId>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>org.scalaj</groupId>
            <artifactId>scalaj-http_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>
cat: ./clientlifecycle-common-hint_2.11/src: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org/apache: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org/apache/spark: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org/apache/spark/sql: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org/apache/spark/sql/catalyst: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/main/scala/org/apache/spark/sql/catalyst/analysis: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.catalyst.analysis

import java.util.Locale

import org.apache.commons.lang3.StringUtils
import org.apache.spark.sql.AnalysisException
import org.apache.spark.sql.catalyst.expressions.{In, Literal, Not}
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.catalyst.trees.CurrentOrigin
import org.apache.spark.sql.internal.SQLConf


/**
 * Collection of rules related to hints. The only hint currently available is broadcast join hint.
 *
 * Note that this is separately into two rules because in the future we might introduce new hint
 * rules that have different ordering requirements from broadcast.
 */
object ResolveHints {

  /**
   * For broadcast hint, we accept "BROADCAST", "BROADCASTJOIN", and "MAPJOIN", and a sequence of
   * relation aliases can be specified in the hint. A broadcast hint plan node will be inserted
   * on top of any relation (that is not aliased differently), subquery, or common table expression
   * that match the specified name.
   *
   * The hint resolution works by recursively traversing down the query plan to find a relation or
   * subquery that matches one of the specified broadcast aliases. The traversal does not go past
   * beyond any existing broadcast hints, subquery aliases.
   *
   * This rule must happen before common table expressions.
   */
  class ResolveBroadcastHints(conf: SQLConf) extends Rule[LogicalPlan] {
    private val BROADCAST_HINT_NAMES = Set("BROADCAST", "BROADCASTJOIN", "MAPJOIN")

    //SKEW_JOIN(filter_key(t1.id, t2.id), skew_values(0,1))
    private val SKEW_JOIN = "SKEW_JOIN"

    def resolver: Resolver = conf.resolver

    private def applyBroadcastHint(plan: LogicalPlan, toBroadcast: Set[String]): LogicalPlan = {
      // Whether to continue recursing down the tree
      var recurse = true

      val newNode = CurrentOrigin.withOrigin(plan.origin) {
        plan match {
          case u: UnresolvedRelation if toBroadcast.exists(resolver(_, u.tableIdentifier.table)) =>
            ResolvedHint(plan, HintInfo(broadcast = true))
          case r: SubqueryAlias if toBroadcast.exists(resolver(_, r.alias)) =>
            ResolvedHint(plan, HintInfo(broadcast = true))

          case _: ResolvedHint | _: View | _: With | _: SubqueryAlias =>
            // Don't traverse down these nodes.
            // For an existing broadcast hint, there is no point going down (if we do, we either
            // won't change the structure, or will introduce another broadcast hint that is useless.
            // The rest (view, with, subquery) indicates different scopes that we shouldn't traverse
            // down. Note that technically when this rule is executed, we haven't completed view
            // resolution yet and as a result the view part should be deadcode. I'm leaving it here
            // to be more future proof in case we change the view we do view resolution.
            recurse = false
            plan

          case _ =>
            plan
        }
      }

      if ((plan fastEquals newNode) && recurse) {
        newNode.mapChildren(child => applyBroadcastHint(child, toBroadcast))
      } else {
        newNode
      }
    }

    private def applySkewJointHint(plan: LogicalPlan, skewJoin: SkewJoin): LogicalPlan = {
      var recurse = true

      val newNode = CurrentOrigin.withOrigin(plan.origin) {
        plan match {
          case Join(left, right, joinType, condition) if condition.isDefined => {
            if (skewJoin.skewValues.isEmpty) return plan

            def getTableAliasName(lp: LogicalPlan): String = {
              val alias = lp.map(lp => lp).filter(_.isInstanceOf[SubqueryAlias]).map(_.asInstanceOf[SubqueryAlias].alias).toList.head
              if (!skewJoin.filterKey.keyMap.contains(alias)) throw new RuntimeException(s"Appear invalidated table alias")
              alias
            }

            val leftField = skewJoin.filterKey.keyMap(getTableAliasName(left))
            val rightField = skewJoin.filterKey.keyMap(getTableAliasName(right))
            val skewValues = skewJoin.skewValues.map(Literal(_))

            val left1 = Filter(Not(In(UnresolvedAttribute(leftField), skewValues)), left)
            val right1 = Filter(Not(In(UnresolvedAttribute(rightField), skewValues)), right)
            val left2 = Filter(In(UnresolvedAttribute(leftField), skewValues), left)
            val right2 = Filter(In(UnresolvedAttribute(rightField), skewValues), right)

            val join1 = Join(left1, right1, joinType, condition)
            val join2 = Join(ResolvedHint(left2, HintInfo(broadcast = true)), ResolvedHint(right2, HintInfo(broadcast = true)), joinType, condition)

            Union(join1, join2)
          }
          case _: ResolvedHint | _: View | _: With | _: SubqueryAlias =>
            recurse = false
            plan
          case _ =>
            plan
        }
      }

      if ((plan fastEquals newNode) && recurse) {
        newNode.mapChildren(child => applySkewJointHint(child, skewJoin))
      } else {
        newNode
      }
    }

    def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {
      case h: UnresolvedHint if BROADCAST_HINT_NAMES.contains(h.name.toUpperCase(Locale.ROOT)) =>
        if (h.parameters.isEmpty) {
          // If there is no table alias specified, turn the entire subtree into a BroadcastHint.
          ResolvedHint(h.child, HintInfo(broadcast = true))
        } else {
          // Otherwise, find within the subtree query plans that should be broadcasted.
          applyBroadcastHint(h.child, h.parameters.map {
            case tableName: String => tableName
            case tableId: UnresolvedAttribute => tableId.name
            case unsupported => throw new AnalysisException("Broadcast hint parameter should be " +
              s"an identifier or string but was $unsupported (${unsupported.getClass}")
          }.toSet)
        }

      case h: UnresolvedHint if SKEW_JOIN == h.name.toUpperCase(Locale.ROOT) => {
        if (h.parameters.isEmpty) {
          throw new RuntimeException("Feel free to fill parameter, filter_key(t1.id, t2.id) and skew_values(a,b) as well")
        } else {
          val paramMap = h.parameters.map {
            case UnresolvedFunction(funId, children, _) =>
              (
                funId.funcName.toLowerCase(),
                children.map {
                  case ua: UnresolvedAttribute => ua.name
                  case other => other.toString()
                }
              )
            case str: String => {
              val pattern = "(.*)[(](.*)[)]".r
              if (pattern.findAllIn(str).isEmpty) throw new RuntimeException(s"Skew join hint parameter should be filter_key(t1.id, t2.id) and skew_values(a,b) but was $str")
              val pattern(funId, param) = str
              (funId.toLowerCase, param.split(",").toList.map(_.trim).filter(StringUtils.isNotBlank(_)))
            }
            case unsupported => throw new RuntimeException(s"Skew join hint parameter should be filter_key(t1.id, t2.id) and skew_values(a,b) but was $unsupported (${unsupported.getClass}")
          }.toMap

          val filterKey = paramMap("filter_key")
          val skewValues = paramMap("skew_values")

          if (filterKey.length != 2) throw new RuntimeException("filter_key paramter lenght not equal 2")

          val skewJoin = SkewJoin(FilterKey(filterKey.head.trim, filterKey(1).trim), skewValues)
          applySkewJointHint(h.child, skewJoin)
        }
      }
    }
  }

  /**
   * Removes all the hints, used to remove invalid hints provided by the user.
   * This must be executed after all the other hint rules are executed.
   */
  object RemoveAllHints extends Rule[LogicalPlan] {
    def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {
      case h: UnresolvedHint => h.child
    }
  }

  case class FilterKey(leftPart: String, rightPart: String) {
    private val pattern = "(.*)[.].*".r
    private val pattern(tableName1) = leftPart
    private val pattern(tableName2) = rightPart

    val keyMap = Map(tableName1 -> leftPart, tableName2 -> rightPart)
  }

  case class SkewJoin(filterKey: FilterKey, skewValues: Seq[String])

}
cat: ./clientlifecycle-common-hint_2.11/src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./clientlifecycle-common-hint_2.11/src/site/resources: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./clientlifecycle-common-hint_2.11/src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>cat: ./clientlifecycle-common-hint_2.11/src/test: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala/com: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-hint_2.11/src/test/scala/com/hsbc/gbm/bd/clm: Is a directory
package com.hsbc.gbm.bd.clm;

import static org.junit.Assert.assertTrue;

import org.junit.Test;

/**
 * Unit test for simple App.
 */
public class AppTest 
{
    /**
     * Rigorous Test :-)
     */
    @Test
    public void shouldAnswerWithTrue()
    {
        assertTrue( true );
    }
}
cat: ./clientlifecycle-common-measure: Is a directory
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.CLM
Copyright 2017-2018 HSBC

This product includes software developed at
The Apache Software Foundation (http://www.apache.org/).<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>com.hsbc.gbm.bd.clm</groupId>
        <artifactId>clientlifecycle-common</artifactId>
        <version>2.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <artifactId>clientlifecycle-common-measure_${scala.binary.version}</artifactId>

    <dependencies>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>org.scalaj</groupId>
            <artifactId>scalaj-http_${scala.binary.version}</artifactId>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>
cat: ./clientlifecycle-common-measure/src: Is a directory
cat: ./clientlifecycle-common-measure/src/main: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/configuration: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/configuration/dqdefinition: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition

import com.fasterxml.jackson.annotation.{JsonInclude, JsonProperty}
import com.fasterxml.jackson.annotation.JsonInclude.Include
import org.apache.commons.lang.StringUtils
import com.hsbc.gbm.bd.clm.measure.configuration.enums._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DqType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DslType.{DslType, GriffinDsl}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.{DefaultFlattenType, FlattenType}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType.{OutputType, UnknownOutputType}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.SinkType.SinkType


/**
 * dq param
 *
 * @param name         name of dq measurement (must)
 * @param timestamp    default timestamp of measure in batch mode (optional)
 * @param procType     batch mode or streaming mode (must)
 * @param dataSources  data sources (must)
 * @param evaluateRule dq measurement (must)
 * @param sinks        sink types (optional, by default will be elasticsearch)
 */
case class DQConfig(
                     private val name: String,
                     private val timestamp: Long,
                     private val procType: String,
                     private val dataSources: Seq[DataSourceParam],
                     private val evaluateRule: EvaluateRuleParam,
                     private val sinks: Seq[String] = Nil)
  extends Param {
  def getName: String = name

  def getTimestampOpt: Option[Long] = if (timestamp != 0) Some(timestamp) else None

  def getProcType: String = procType

  def getDataSources: Seq[DataSourceParam] = {
    dataSources
      .foldLeft((Nil: Seq[DataSourceParam], Set[String]())) { (ret, ds) =>
        val (seq, names) = ret
        if (!names.contains(ds.getName)) {
          (seq :+ ds, names + ds.getName)
        } else ret
      }
      ._1
  }

  def getEvaluateRule: EvaluateRuleParam = evaluateRule

  def getSinkNames: Seq[String] = sinks

  def getValidSinkTypes: Seq[SinkType] = SinkType.validSinkTypes(sinks)

  def validate(): Unit = {
    assert(StringUtils.isNotBlank(name), "dq config name should not be blank")
    assert(StringUtils.isNotBlank(procType), "process.type should not be blank")
    assert(dataSources != null, "data.sources should not be null")
    assert(evaluateRule != null, "evaluate.rule should not be null")
    getDataSources.foreach(_.validate())
    evaluateRule.validate()
  }
}

/**
 * data source param
 *
 * @param name       data source name (must)
 * @param baseline   data source is baseline or not, false by default (optional)
 * @param connector  data connector (optional)
 * @param checkpoint data source checkpoint configuration (must in streaming mode with streaming connectors)
 */
case class DataSourceParam(
                            private val name: String,
                            private val connector: DataConnectorParam,
                            private val baseline: Boolean = false,
                            private val checkpoint: Map[String, Any] = null)
  extends Param {
  def getName: String = name

  def isBaseline: Boolean = if (Option(baseline).isDefined) baseline else false

  def getConnector: Option[DataConnectorParam] = Option(connector)

  def getCheckpointOpt: Option[Map[String, Any]] = Option(checkpoint)

  def validate(): Unit = {
    assert(StringUtils.isNotBlank(name), "data source name should not be empty")
    assert(getConnector.isDefined, "Connector is undefined or invalid")
    getConnector.foreach(_.validate())
  }
}

/**
 * data connector param
 *
 * @param conType       data connector type, e.g.: hive, avro, kafka (must)
 * @param dataFrameName data connector dataframe name, for pre-process input usage (optional)
 * @param config        detail configuration of data connector (must)
 * @param preProc       pre-process rules after load data (optional)
 */
case class DataConnectorParam(
                               private val conType: String,
                               private val dataFrameName: String,
                               private val config: Map[String, Any],
                               private val preProc: List[RuleParam])
  extends Param {
  def getType: String = conType

  def getDataFrameName(defName: String): String =
    if (dataFrameName != null) dataFrameName else defName

  def getConfig: Map[String, Any] = if (config != null) config else Map[String, Any]()

  def getPreProcRules: Seq[RuleParam] = if (preProc != null) preProc else Nil

  def validate(): Unit = {
    assert(StringUtils.isNotBlank(conType), "data connector type should not be empty")
    getPreProcRules.foreach(_.validate())
  }
}

/**
 * evaluate rule param
 *
 * @param rules rules to define dq measurement (optional)
 */
@JsonInclude(Include.NON_NULL)
case class EvaluateRuleParam(@JsonProperty("rules") private val rules: Seq[RuleParam])
  extends Param {
  def getRules: Seq[RuleParam] = if (rules != null) rules else Nil

  def validate(): Unit = {
    getRules.foreach(_.validate())
  }
}

/**
 * rule param
 *
 * @param dslType    dsl type of this rule (must)
 * @param dqType     dq type of this rule (must if dsl type is "griffin-dsl")
 * @param inDfName   name of input dataframe of this rule, by default will be the previous rule output dataframe name
 * @param outDfName  name of output dataframe of this rule, by default will be generated
 *                   as data connector dataframe name with index suffix
 * @param rule       rule to define dq step calculation (must)
 * @param details    detail config of rule (optional)
 * @param cache      cache the result for multiple usage (optional, valid for "spark-sql" and "df-ops" mode)
 * @param outputs    output ways configuration (optional)
 * @param errorConfs error configuration (valid for 'COMPLETENESS' mode)
 */
@JsonInclude(Include.NON_NULL)
case class RuleParam(
                      @JsonProperty("dsl.type") private val dslType: String,
                      @JsonProperty("dq.type") private val dqType: String,
                      @JsonProperty("in.dataframe.name") private val inDfName: String = null,
                      @JsonProperty("out.dataframe.name") private val outDfName: String = null,
                      @JsonProperty("rule") private val rule: String = null,
                      @JsonProperty("details") private val details: Map[String, Any] = null,
                      @JsonProperty("cache") private val cache: Boolean = false,
                      @JsonProperty("out") private val outputs: List[RuleOutputParam] = null,
                      @JsonProperty("error.confs") private val errorConfs: List[RuleErrorConfParam] = null)
  extends Param {
  def getDslType: DslType =
    if (dslType != null) DslType.withNameWithDefault(dslType) else GriffinDsl

  def getDqType: DqType = if (dqType != null) DqType.withNameWithDefault(dqType) else Unknown

  def getCache: Boolean = if (cache) cache else false

  def getInDfName(defName: String = ""): String = if (inDfName != null) inDfName else defName

  def getOutDfName(defName: String = ""): String = if (outDfName != null) outDfName else defName

  def getRule: String = if (rule != null) rule else ""

  def getDetails: Map[String, Any] = if (details != null) details else Map[String, Any]()

  def getOutputs: Seq[RuleOutputParam] = if (outputs != null) outputs else Nil

  def getOutputOpt(tp: OutputType): Option[RuleOutputParam] =
    getOutputs.find(_.getOutputType == tp)

  def getErrorConfs: Seq[RuleErrorConfParam] = if (errorConfs != null) errorConfs else Nil

  def replaceInDfName(newName: String): RuleParam = {
    if (StringUtils.equals(newName, inDfName)) this
    else RuleParam(dslType, dqType, newName, outDfName, rule, details, cache, outputs)
  }

  def replaceOutDfName(newName: String): RuleParam = {
    if (StringUtils.equals(newName, outDfName)) this
    else RuleParam(dslType, dqType, inDfName, newName, rule, details, cache, outputs)
  }

  def replaceInOutDfName(in: String, out: String): RuleParam = {
    if (StringUtils.equals(inDfName, in) && StringUtils.equals(outDfName, out)) this
    else RuleParam(dslType, dqType, in, out, rule, details, cache, outputs)
  }

  def replaceRule(newRule: String): RuleParam = {
    if (StringUtils.equals(newRule, rule)) this
    else RuleParam(dslType, dqType, inDfName, outDfName, newRule, details, cache, outputs)
  }

  def validate(): Unit = {
    assert(
      !(getDslType.equals(GriffinDsl) && getDqType.equals(Unknown)),
      "unknown dq type for griffin dsl")

    getOutputs.foreach(_.validate())
    getErrorConfs.foreach(_.validate())
  }
}

/**
 * out param of rule
 *
 * @param outputType output type (must)
 * @param name       output name (optional)
 * @param flatten    flatten type of output metric (optional, available in output metric type)
 */
@JsonInclude(Include.NON_NULL)
case class RuleOutputParam(
                            @JsonProperty("type") private val outputType: String,
                            @JsonProperty("name") private val name: String,
                            @JsonProperty("flatten") private val flatten: String)
  extends Param {
  def getOutputType: OutputType = {
    if (outputType != null) OutputType.withNameWithDefault(outputType)
    else UnknownOutputType
  }

  def getNameOpt: Option[String] = Some(name).filter(StringUtils.isNotBlank)

  def getFlatten: FlattenType = {
    if (StringUtils.isNotBlank(flatten)) FlattenType.withNameWithDefault(flatten)
    else DefaultFlattenType
  }

  def validate(): Unit = {}
}

/**
 * error configuration parameter
 *
 * @param columnName the name of the column
 * @param errorType  the way to match error, regex or enumeration
 * @param values     error value list
 */
@JsonInclude(Include.NON_NULL)
case class RuleErrorConfParam(
                               @JsonProperty("column.name") private val columnName: String,
                               @JsonProperty("type") private val errorType: String,
                               @JsonProperty("values") private val values: List[String])
  extends Param {
  def getColumnName: Option[String] = Some(columnName).filter(StringUtils.isNotBlank)

  def getErrorType: Option[String] = Some(errorType).filter(StringUtils.isNotBlank)

  def getValues: Seq[String] = if (values != null) values else Nil

  def validate(): Unit = {
    assert(
      "regex".equalsIgnoreCase(getErrorType.get) ||
        "enumeration".equalsIgnoreCase(getErrorType.get),
      "error error.conf type")
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition

import com.fasterxml.jackson.annotation.{JsonInclude, JsonProperty}
import com.fasterxml.jackson.annotation.JsonInclude.Include
import org.apache.commons.lang.StringUtils

import com.hsbc.gbm.bd.clm.measure.configuration.enums.SinkType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.SinkType.SinkType

/**
 * Model for Environment Config.
 *
 * @param sparkParam       Job specific Spark Configs to override the Defaults set on the cluster
 * @param sinkParams       A [[Seq]] of sink definitions where records and metrics can be persisted
 * @param checkpointParams Config of checkpoint locations (required in streaming mode)
 */
@JsonInclude(Include.NON_NULL)
case class EnvConfig(
                      @JsonProperty("spark") private val sparkParam: SparkParam,
                      @JsonProperty("sinks") private val sinkParams: List[SinkParam],
                      @JsonProperty("griffin.checkpoint") private val checkpointParams: List[CheckpointParam])
  extends Param {
  def getSparkParam: SparkParam = sparkParam

  def getSinkParams: Seq[SinkParam] = if (sinkParams != null) sinkParams else Nil

  def getCheckpointParams: Seq[CheckpointParam] =
    if (checkpointParams != null) checkpointParams else Nil

  def validate(): Unit = {
    assert(sparkParam != null, "spark param should not be null")
    sparkParam.validate()
    getSinkParams.foreach(_.validate())
    val repeatedSinks = sinkParams
      .map(_.getName)
      .groupBy(x => x)
      .mapValues(_.size)
      .filter(_._2 > 1)
      .keys
    assert(
      repeatedSinks.isEmpty,
      s"sink names must be unique. duplicate sink names ['${repeatedSinks.mkString("', '")}'] were found.")
    getCheckpointParams.foreach(_.validate())
  }
}

/**
 * spark param
 *
 * @param logLevel        log level of spark application (optional)
 * @param cpDir           checkpoint directory for spark streaming (required in streaming mode)
 * @param batchInterval   batch interval for spark streaming (required in streaming mode)
 * @param processInterval process interval for streaming dq calculation (required in streaming mode)
 * @param config          extra config for spark environment (optional)
 * @param initClear       clear checkpoint directory or not when initial (optional)
 */
@JsonInclude(Include.NON_NULL)
case class SparkParam(
                       @JsonProperty("log.level") private val logLevel: String,
                       @JsonProperty("checkpoint.dir") private val cpDir: String,
                       @JsonProperty("batch.interval") private val batchInterval: String,
                       @JsonProperty("process.interval") private val processInterval: String,
                       @JsonProperty("config") private val config: Map[String, String],
                       @JsonProperty("init.clear") private val initClear: Boolean)
  extends Param {
  def getLogLevel: String = if (logLevel != null) logLevel else "WARN"

  def getCpDir: String = if (cpDir != null) cpDir else ""

  def getBatchInterval: String = if (batchInterval != null) batchInterval else ""

  def getProcessInterval: String = if (processInterval != null) processInterval else ""

  def getConfig: Map[String, String] = if (config != null) config else Map[String, String]()

  def needInitClear: Boolean = if (initClear) initClear else false

  def validate(): Unit = {
    //    assert(StringUtils.isNotBlank(cpDir), "checkpoint.dir should not be empty")
    //    assert(TimeUtil.milliseconds(getBatchInterval).nonEmpty, "batch.interval should be valid time string")
    //    assert(TimeUtil.milliseconds(getProcessInterval).nonEmpty, "process.interval should be valid time string")
  }
}

/**
 * sink param
 *
 * @param sinkType sink type, e.g.: log, hdfs, http, mongo (must)
 * @param config   config of sink way (must)
 */
case class SinkParam(
                      private val name: String,
                      private val sinkType: String,
                      private val config: Map[String, Any] = Map.empty)
  extends Param {
  def getName: String = name

  def getType: SinkType = SinkType.withNameWithDefault(sinkType)

  def getConfig: Map[String, Any] = config

  def validate(): Unit = {
    assert(name != null, "sink name should must be defined")
    assert(StringUtils.isNotBlank(sinkType), "sink type should not be empty")
  }
}

/**
 * checkpoint param
 *
 * @param cpType checkpoint location type, e.g.: zookeeper (must)
 * @param config config of checkpoint location
 */
@JsonInclude(Include.NON_NULL)
case class CheckpointParam(
                            @JsonProperty("type") private val cpType: String,
                            @JsonProperty("config") private val config: Map[String, Any])
  extends Param {
  def getType: String = cpType

  def getConfig: Map[String, Any] = if (config != null) config else Map[String, Any]()

  def validate(): Unit = {
    assert(StringUtils.isNotBlank(cpType), "griffin checkpoint type should not be empty")
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition

import com.fasterxml.jackson.annotation.{JsonInclude, JsonProperty}
import com.fasterxml.jackson.annotation.JsonInclude.Include

/**
 * full set of griffin configuration
 * @param envConfig   environment configuration (must)
 * @param dqConfig    dq measurement configuration (must)
 */
@JsonInclude(Include.NON_NULL)
case class GriffinConfig(
    @JsonProperty("env") private val envConfig: EnvConfig,
    @JsonProperty("dq") private val dqConfig: DQConfig)
    extends Param {
  def getEnvConfig: EnvConfig = envConfig
  def getDqConfig: DQConfig = dqConfig

  def validate(): Unit = {
    assert(envConfig != null, "environment config should not be null")
    assert(dqConfig != null, "dq config should not be null")
    envConfig.validate()
    dqConfig.validate()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition

trait Param extends Serializable {

  /**
   * validate param internally
   */
  def validate(): Unit

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/configuration/dqdefinition/reader: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.reader

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.Param
import com.hsbc.gbm.bd.clm.measure.utils.{HdfsUtil, JsonUtil}

import scala.reflect.ClassTag
import scala.util.Try

/**
 * read params from config file path
 *
 * @param filePath:  hdfs path ("hdfs://cluster-name/path")
 *                   local file path ("file:///path")
 *                   relative file path ("relative/path")
 */
case class ParamFileReader(filePath: String) extends ParamReader {

  def readConfig[T <: Param](implicit m: ClassTag[T]): Try[T] = {
    Try {
      val source = HdfsUtil.openFile(filePath)
      val param = JsonUtil.fromJson[T](source)
      source.close()
      validate(param)
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.reader

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.Param
import com.hsbc.gbm.bd.clm.measure.utils.JsonUtil

import scala.reflect.ClassTag
import scala.util.Try


/**
 * read params from json string directly
 *
 * @param jsonString
 */
case class ParamJsonReader(jsonString: String) extends ParamReader {

  def readConfig[T <: Param](implicit m: ClassTag[T]): Try[T] = {
    Try {
      val param = JsonUtil.fromJson[T](jsonString)
      validate(param)
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.reader

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.Param

import scala.reflect.ClassTag
import scala.util.Try


trait ParamReader extends Loggable with Serializable {

  /**
   * read config param
   *
   * @tparam T     param type expected
   * @return       parsed param
   */
  def readConfig[T <: Param](implicit m: ClassTag[T]): Try[T]

  /**
   * validate config param
   *
   * @param param  param to be validated
   * @return       param itself
   */
  protected def validate[T <: Param](param: T): T = {
    param.validate()
    param
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.reader

import com.hsbc.gbm.bd.clm.measure.utils.JsonUtil


object ParamReaderFactory {

  val json = "json"
  val file = "file"

  /**
   * parse string content to get param reader
   * @param pathOrJson
   * @return
   */
  def getParamReader(pathOrJson: String): ParamReader = {
    val strType = paramStrType(pathOrJson)
    if (json.equals(strType)) ParamJsonReader(pathOrJson)
    else ParamFileReader(pathOrJson)
  }

  private def paramStrType(str: String): String = {
    try {
      JsonUtil.toAnyMap(str)
      json
    } catch {
      case _: Throwable => file
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/configuration/enums: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

import com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * effective when dsl type is "griffin-dsl",
 * indicates the dq type of griffin pre-defined measurements
 * <li> - The match percentage of items between source and target
 *                         count(source items matched with the ones from target) / count(source)
 *                         e.g.: source [1, 2, 3, 4, 5], target: [1, 2, 3, 4]
 *                         metric will be: { "total": 5, "miss": 1, "matched": 4 } accuracy is 80%.</li>
 * <li> - The statistic data of data source
 *                          e.g.: max, min, average, group by count, ...</li>
 * <li> - The uniqueness of data source comparing with itself
 *                           count(unique items in source) / count(source)
 *                           e.g.: [1, 2, 3, 3] -> { "unique": 2, "total": 4, "dup-arr": [ "dup": 1, "num": 1 ] }
 *                           uniqueness indicates the items without any replica of data</li>
 * <li> - The distinctness of data source comparing with itself
 *                             count(distinct items in source) / count(source)
 *                             e.g.: [1, 2, 3, 3] -> { "dist": 3, "total": 4, "dup-arr": [ "dup": 1, "num": 1 ] }
 *                             distinctness indicates the valid information of data
 *                             comparing with uniqueness, distinctness is more meaningful</li>
 * <li> - The latency of data source with timestamp information
 *                           e.g.: (receive_time - send_time)
 *                           timeliness can get the statistic metric of latency, like average, max, min,
 *                            percentile-value,
 *                           even more, it can record the items with latency above threshold you configured</li>
 * <li> - The completeness of data source
 *                             the columns you measure is incomplete if it is null</li>
 */
object DqType extends GriffinEnum {

  type DqType = Value

  val Accuracy, Profiling, Uniqueness, Duplicate, Distinct, Timeliness, Completeness = Value

  override def withNameWithDefault(name: String): enums.DqType.Value = {
    val dqType = super.withNameWithDefault(name)
    dqType match {
      case Uniqueness | Duplicate => Uniqueness
      case _ => dqType
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

import com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * dsl type indicates the language type of rule param
 * <li> - spark-sql: rule defined in "SPARK-SQL" directly</li>
 * <li> - df-ops|df-opr|: data frame operations rule, support some pre-defined data frame ops()</li>
 * <li> - griffin dsl rule, to define dq measurements easier</li>
 */
object DslType extends GriffinEnum {
  type DslType = Value

  val SparkSql, DfOps, DfOpr, DfOperations, GriffinDsl, DataFrameOpsType = Value

  /**
   *
   * @param name Dsltype from config file
   * @return Enum value corresponding to string
   */
  def withNameWithDslType(name: String): Value =
    values
      .find(_.toString.toLowerCase == name.replace("-", "").toLowerCase())
      .getOrElse(GriffinDsl)

  override def withNameWithDefault(name: String): enums.DslType.Value = {
    val dslType = withNameWithDslType(name)
    dslType match {
      case DfOps | DfOpr | DfOperations => DataFrameOpsType
      case _ => dslType
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * the strategy to flatten metric
 *  <li> -  default flatten strategy
 *                                     metrics contains 1 row -> flatten metric json map
 *                                     metrics contains n > 1 rows -> flatten metric json array
 *                                     n = 0: { }
 *                                     n = 1: { "col1": "value1", "col2": "value2", ... }
 *                                     n > 1: { "arr-name": [ { "col1": "value1", "col2": "value2", ... }, ... ] }
 *                                     all rows
 *  </li>
 *  <li> - metrics contains n rows -> flatten metric json map
 *                                    n = 0: { }
 *                                    n >= 1: { "col1": "value1", "col2": "value2", ... }
 *                                    the first row only
 *  </li>
 *  <li> -   metrics contains n rows -> flatten metric json array
 *                                    n = 0: { "arr-name": [ ] }
 *                                    n >= 1: { "arr-name": [ { "col1": "value1", "col2": "value2", ... }, ... ] }
 *                                    all rows
 *  </li>
 *  <li> - metrics contains n rows -> flatten metric json wrapped map
 *                                n = 0: { "map-name": { } }
 *                                n >= 1: { "map-name": { "col1": "value1", "col2": "value2", ... } }
 *                                the first row only
 *  </li>
 */
object FlattenType extends GriffinEnum {
  type FlattenType = Value

  val DefaultFlattenType, EntriesFlattenType, ArrayFlattenType, MapFlattenType =
    Value

  val List, Array, Entries, Map, Default = Value

  override def withNameWithDefault(name: String): Value = {
    val flattenType = super.withNameWithDefault(name)
    flattenType match {
      case Array | List => ArrayFlattenType
      case Map => MapFlattenType
      case Entries => EntriesFlattenType
      case _ => DefaultFlattenType
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

trait GriffinEnum extends Enumeration {
  type GriffinEnum = Value

  val Unknown: Value = Value

  /**
   *
   * @param name Constant value in String
   * @return Enum constant value
   */
  def withNameWithDefault(name: String): Value =
    values
      .find(_.toString.toLowerCase == name.replace("-", "").toLowerCase())
      .getOrElse(Unknown)

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * the strategy to output metric
 *  <li> - output the rule step result as metric</li>
 *  <li> - output the rule step result as records</li>
 *  <li> - output the rule step result to update data source cache</li>
 *  <li> - will not output the result </li>
 */
object OutputType extends GriffinEnum {
  type OutputType = Value

  val MetricOutputType, RecordOutputType, DscUpdateOutputType, UnknownOutputType = Value

  val Metric, Record, Records, DscUpdate = Value

  override def withNameWithDefault(name: String): Value = {
    val flattenType = super.withNameWithDefault(name)
    flattenType match {
      case Metric => MetricOutputType
      case Record | Records => RecordOutputType
      case DscUpdate => DscUpdateOutputType
      case _ => UnknownOutputType
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

import com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * process type enum
 *  <li> - Process in batch mode </li>
 *  <li> - Process in streaming mode</li>
 */
object ProcessType extends GriffinEnum {
  type ProcessType = Value

  val BatchProcessType: enums.ProcessType.Value = Value("Batch")
  val StreamingProcessType: enums.ProcessType.Value = Value("Streaming")
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

import com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * Supported Sink types
 *  <li>{@link #Console #Log} -  console sink, will sink metric in console (alias log)</li>
 *  <li>{@link #Hdfs} - hdfs sink, will sink metric and record in hdfs</li>
 *  <li>{@link #Es #Elasticsearch #Http} - elasticsearch sink, will sink metric
 *  in elasticsearch (alias Es and Http)</li>
 *  <li>{@link #Mongo #MongoDB} - mongo sink, will sink metric in mongo db (alias MongoDb)</li>
 *  <li>{@link #Custom} - custom sink (needs using extra jar-file-extension)</li>
 *  <li>{@link #Unknown} - </li>
 */
object SinkType extends GriffinEnum {
  type SinkType = Value

  val Console, Log, Hdfs, Es, Http, ElasticSearch, MongoDB, Mongo, Custom =
    Value

  def validSinkTypes(sinkTypeSeq: Seq[String]): Seq[SinkType] = {
    sinkTypeSeq
      .map(s => SinkType.withNameWithDefault(s))
      .filter(_ != SinkType.Unknown)
      .distinct
  }

  override def withNameWithDefault(name: String): enums.SinkType.Value = {
    val sinkType = super.withNameWithDefault(name)
    sinkType match {
      case Console | Log => Console
      case Es | ElasticSearch | Http => ElasticSearch
      case MongoDB | Mongo => MongoDB
      case _ => sinkType
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.configuration.enums

/**
 * write mode when write metrics and records
 */
sealed trait WriteMode {}

object WriteMode {
  def defaultMode(procType: ProcessType.ProcessType): WriteMode = {
    procType match {
      case ProcessType.BatchProcessType => SimpleMode
      case ProcessType.StreamingProcessType => TimestampMode
    }
  }
}

/**
 * simple mode: write metrics and records directly
 */
case object SimpleMode extends WriteMode {}

/**
 * timestamp mode: write metrics and records with timestamp information
 */
case object TimestampMode extends WriteMode {}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/context: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

/**
 * context id, unique by different timestamp and tag
 */
case class ContextId(timestamp: Long, tag: String = "") extends Serializable {
  def id: String = {
    if (tag.nonEmpty) s"${tag}_$timestamp" else s"$timestamp"
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.collection.mutable
import org.apache.spark.sql.DataFrame

/**
 * cache and unpersist dataframes
 */
case class DataFrameCache() extends Loggable {

  val dataFrames: mutable.Map[String, DataFrame] = mutable.Map()
  val trashDataFrames: mutable.MutableList[DataFrame] = mutable.MutableList()

  private def trashDataFrame(df: DataFrame): Unit = {
    trashDataFrames += df
  }
  private def trashDataFrames(dfs: Seq[DataFrame]): Unit = {
    trashDataFrames ++= dfs
  }

  def cacheDataFrame(name: String, df: DataFrame): Unit = {
    info(s"try to cache data frame $name")
    dataFrames.get(name) match {
      case Some(odf) =>
        trashDataFrame(odf)
        dataFrames += (name -> df)
        df.cache
        info("cache after replace old df")
      case _ =>
        dataFrames += (name -> df)
        df.cache
        info("cache after replace no old df")
    }
  }

  def uncacheDataFrame(name: String): Unit = {
    dataFrames.get(name).foreach(df => trashDataFrame(df))
    dataFrames -= name
  }
  def uncacheAllDataFrames(): Unit = {
    trashDataFrames(dataFrames.values.toSeq)
    dataFrames.clear
  }

  def clearAllTrashDataFrames(): Unit = {
    trashDataFrames.foreach(_.unpersist)
    trashDataFrames.clear
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.SinkParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType.ProcessType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType.BatchProcessType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.WriteMode
import com.hsbc.gbm.bd.clm.measure.datasource.DataSource
import com.hsbc.gbm.bd.clm.measure.sink.{Sink, SinkFactory}
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}


/**
 * dq context: the context of each calculation
 * unique context id in each calculation
 * access the same spark session this app created
 */
case class DQContext(
                      contextId: ContextId,
                      name: String,
                      dataSources: Seq[DataSource],
                      sinkParams: Seq[SinkParam],
                      procType: ProcessType)(@transient implicit val sparkSession: SparkSession) extends Loggable {

  val compileTableRegister: CompileTableRegister = CompileTableRegister()
  val runTimeTableRegister: RunTimeTableRegister = RunTimeTableRegister(sparkSession)

  val dataFrameCache: DataFrameCache = DataFrameCache()

  val metricWrapper: MetricWrapper = MetricWrapper(name, sparkSession.sparkContext.applicationId)
  val writeMode: WriteMode = WriteMode.defaultMode(procType)

  val dataSourceNames: Seq[String] = {
    // sort data source names, put baseline data source name to the head
    val (blOpt, others) = dataSources.foldLeft((None: Option[String], Nil: Seq[String])) {
      (ret, ds) =>
        val (opt, seq) = ret
        if (opt.isEmpty && ds.isBaseline) (Some(ds.name), seq) else (opt, seq :+ ds.name)
    }
    blOpt match {
      case Some(bl) => bl +: others
      case _ => others
    }
  }
  dataSourceNames.foreach(name => compileTableRegister.registerTable(name))

  def getDataSourceName(index: Int): String = {
    if (dataSourceNames.size > index) dataSourceNames(index) else ""
  }

  implicit val encoder: Encoder[String] = Encoders.STRING
  val functionNames: Seq[String] = sparkSession.catalog.listFunctions.map(_.name).collect.toSeq

  val dataSourceTimeRanges: Map[String, TimeRange] = loadDataSources()

  def loadDataSources(): Map[String, TimeRange] = {
    dataSources.map { ds =>
      (ds.name, ds.loadData(this))
    }.toMap
  }

  printTimeRanges()

  private val sinkFactory = SinkFactory(sinkParams, name)
  private val defaultSinks: Seq[Sink] = createSinks(contextId.timestamp)

  def getSinks(timestamp: Long): Seq[Sink] = {
    if (timestamp == contextId.timestamp) getSinks
    else createSinks(timestamp)
  }

  def getSinks: Seq[Sink] = defaultSinks

  private def createSinks(t: Long): Seq[Sink] = {
    procType match {
      case BatchProcessType => sinkFactory.getSinks(t, block = true)
    }
  }

  def cloneDQContext(newContextId: ContextId): DQContext = {
    DQContext(newContextId, name, dataSources, sinkParams, procType)(sparkSession)
  }

  def clean(): Unit = {
    compileTableRegister.unregisterAllTables()
    runTimeTableRegister.unregisterAllTables()

    dataFrameCache.uncacheAllDataFrames()
    dataFrameCache.clearAllTrashDataFrames()
  }

  private def printTimeRanges(): Unit = {
    if (dataSourceTimeRanges.nonEmpty) {
      val timeRangesStr = dataSourceTimeRanges
        .map { pair =>
          val (name, timeRange) = pair
          s"$name -> (${timeRange.begin}, ${timeRange.end}]"
        }
        .mkString("\r\n")
      info(s"data source timeRanges: \r\n$timeRangesStr")
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/context/metric: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context.metric

/**
 * accuracy metric
 * @param miss     miss count
 * @param total    total count
 */
case class AccuracyMetric(miss: Long, total: Long) extends Metric {

  type T = AccuracyMetric

  override def isLegal: Boolean = getTotal > 0

  def update(delta: T): T = {
    if (delta.miss < miss) AccuracyMetric(delta.miss, total) else this
  }

  def initial(): Boolean = {
    getMatch <= 0
  }

  def eventual(): Boolean = {
    this.miss <= 0
  }

  def differsFrom(other: T): Boolean = {
    (this.miss != other.miss) || (this.total != other.total)
  }

  def getMiss: Long = miss

  def getTotal: Long = total

  def getMatch: Long = total - miss

  def matchFraction: Double = if (getTotal <= 0) 1 else getMatch.toDouble / getTotal

  def matchPercentage: Double = matchFraction * 100

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context.metric

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.collection.mutable.{Map => MutableMap}

/**
 * in streaming mode, some metrics may update,
 * the old metrics are cached here
 */
object CacheResults extends Loggable {

  case class CacheResult(timeStamp: Long, updateTime: Long, result: Metric) {
    def olderThan(ut: Long): Boolean = updateTime < ut
    def update[A <: result.T: Manifest](ut: Long, r: Metric): Option[Metric] = {
      r match {
        case m: A if olderThan(ut) =>
          val ur = result.update(m)
          Some(ur).filter(result.differsFrom)
        case _ => None
      }
    }
  }

  private val cacheGroup: MutableMap[Long, CacheResult] = MutableMap()

  private def update(r: CacheResult): Unit = {
    cacheGroup += (r.timeStamp -> r)
  }

  /**
   * input new metric results, output the updated metric results.
   */
  def update(cacheResults: Iterable[CacheResult]): Iterable[CacheResult] = {
    val updatedCacheResults = cacheResults.flatMap { cacheResult =>
      val CacheResult(t, ut, r) = cacheResult
      (cacheGroup.get(t) match {
        case Some(cr) => cr.update(ut, r)
        case _ => Some(r)
      }).map(m => CacheResult(t, ut, m))
    }
    updatedCacheResults.foreach(r => update(r))
    updatedCacheResults
  }

  /**
   * clean the out-time cached results, to avoid memory leak
   */
  def refresh(overtime: Long): Unit = {
    val curCacheGroup = cacheGroup.toMap
    val deadCache = curCacheGroup.filter { pr =>
      val (_, cr) = pr
      cr.timeStamp < overtime || cr.result.eventual()
    }
    info(s"=== dead cache group count: ${deadCache.size} ===")
    deadCache.keySet.foreach(cacheGroup -= _)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context.metric

trait Metric extends Serializable {

  type T <: Metric

  def isLegal: Boolean = true

  def update(delta: T): T

  def initial(): Boolean

  def eventual(): Boolean

  def differsFrom(other: T): Boolean

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

import scala.collection.mutable.{Map => MutableMap}

/**
 * wrap metrics into one, each calculation produces one metric map
 */
case class MetricWrapper(name: String, applicationId: String) extends Serializable {

  val _Name = "name"
  val _Timestamp = "tmst"
  val _Value = "value"
  val _Metadata = "metadata"

  val metrics: MutableMap[Long, Map[String, Any]] = MutableMap()

  def insertMetric(timestamp: Long, value: Map[String, Any]): Unit = {
    val newValue = metrics.get(timestamp) match {
      case Some(v) => v ++ value
      case _ => value
    }
    metrics += (timestamp -> newValue)
  }

  def flush: Map[Long, Map[String, Any]] = {
    metrics.toMap.map { pair =>
      val (timestamp, value) = pair
      (
        timestamp,
        Map[String, Any](
          _Name -> name,
          _Timestamp -> timestamp,
          _Value -> value,
          _Metadata -> Map("applicationId" -> applicationId)))
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.collection.mutable.{Set => MutableSet}
import org.apache.spark.sql._

/**
 * register table name
 */
trait TableRegister extends Loggable with Serializable {

  protected val tables: MutableSet[String] = MutableSet()

  def registerTable(name: String): Unit = {
    tables += name
  }

  def existsTable(name: String): Boolean = {
    tables.exists(_.equals(name))
  }

  def unregisterTable(name: String): Unit = {
    if (existsTable(name)) tables -= name
  }
  def unregisterAllTables(): Unit = {
    tables.clear
  }

  def getTables: Set[String] = {
    tables.toSet
  }

}

/**
 * register table name when building dq job
 */
case class CompileTableRegister() extends TableRegister {}

/**
 * register table name and create temp view during calculation
 */
case class RunTimeTableRegister(@transient sparkSession: SparkSession) extends TableRegister {

  def registerTable(name: String, df: DataFrame): Unit = {
    registerTable(name)
    df.createOrReplaceTempView(name)
  }

  override def unregisterTable(name: String): Unit = {
    if (existsTable(name)) {
      sparkSession.catalog.dropTempView(name)
      tables -= name
    }
  }
  override def unregisterAllTables(): Unit = {
    val uts = getTables
    uts.foreach(t => sparkSession.catalog.dropTempView(t))
    tables.clear
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.context

import scala.math.{max, min}

case class TimeRange(begin: Long, end: Long, tmsts: Set[Long]) extends Serializable {
  def merge(tr: TimeRange): TimeRange = {
    TimeRange(min(begin, tr.begin), max(end, tr.end), tmsts ++ tr.tmsts)
  }
  def minTmstOpt: Option[Long] = {
    try {
      if (tmsts.nonEmpty) Some(tmsts.min) else None
    } catch {
      case _: Throwable => None
    }
  }
}

object TimeRange {
  val emptyTimeRange: TimeRange = TimeRange(0, 0, Set[Long]())
  def apply(range: (Long, Long), tmsts: Set[Long]): TimeRange =
    TimeRange(range._1, range._2, tmsts)
  def apply(ts: Long, tmsts: Set[Long]): TimeRange = TimeRange(ts, ts, tmsts)
  def apply(ts: Long): TimeRange = TimeRange(ts, ts, Set[Long](ts))
  def apply(tmsts: Set[Long]): TimeRange = {
    try {
      TimeRange(tmsts.min, tmsts.max, tmsts)
    } catch {
      case _: Throwable => emptyTimeRange
    }
  }
}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/datasource: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/datasource/connector: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/datasource/connector/batch: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector.batch

import com.hsbc.gbm.bd.clm.measure.datasource.connector.DataConnector

trait BatchDataConnector extends DataConnector {

  def init(): Unit = {}

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector.batch


import scala.collection.mutable.{Map => MutableMap}
import scala.util._
import org.apache.spark.sql.{DataFrame, SparkSession}
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.context.TimeRange
import com.hsbc.gbm.bd.clm.measure.datasource.TimestampStorage
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * A batch data connector for ElasticSearch source with read support for multiple indices.
 *
 * Supported Configurations:
 *  - filterExprs : [[Seq]] of string expressions that act as where conditions (row filters)
 *  - selectionExprs : [[Seq]] of string expressions that act as selection conditions (column filters)
 *  - options : [[Map]] of elasticsearch options. Refer to [[ConfigurationOptions]] for options
 *  - paths : [[Seq]] of elasticsearch paths (indexes) to read from
 *
 * Some defaults assumed by this connector (if not set) are as follows:
 *  - `es.nodes` in options is 'localhost',
 *  - `es.port` in options is 9200
 *  - filterExprs is empty list
 *  - selectionExprs is empty list
 *
 * Note:
 *  - When reading from multiple indices, the schemas are merged.
 *  - Selection expressions are applied first, then the filter expressions.
 *  - filterExprs/selectionExprs may be left empty if no filters are to be applied.
 */
case class ElasticSearchDataConnector(
    @transient sparkSession: SparkSession,
    dcParam: DataConnectorParam,
    timestampStorage: TimestampStorage)
    extends BatchDataConnector {

  final val ElasticSearchFormat: String = "es"
  final val Options: String = "options"

  final val Paths: String = "paths"
  final val FilterExprs: String = "filterExprs"
  final val SelectionExprs: String = "selectionExprs"

  val config: Map[String, Any] = dcParam.getConfig

  final val filterExprs: Seq[String] = config.getStringArr(FilterExprs)
  final val selectionExprs: Seq[String] = config.getStringArr(SelectionExprs)
  final val options: MutableMap[String, String] =
    MutableMap(config.getParamStringMap(Options, Map.empty).toSeq: _*)
  final val paths: String = config.getStringArr(Paths).map(_.trim).mkString(",") match {
    case s: String if s.isEmpty =>
      griffinLogger.error(s"Mandatory configuration '$Paths' is either empty or not defined.")
      throw new IllegalArgumentException()
    case s: String => s
  }

  override def data(ms: Long): (Option[DataFrame], TimeRange) = {
    val dfOpt = {
      val dfOpt = Try {
        val indexesDF = sparkSession.read
          .options(options)
          .format(ElasticSearchFormat)
          .load(paths)

        val df = {
          if (selectionExprs.nonEmpty) indexesDF.selectExpr(selectionExprs: _*)
          else indexesDF
        }

        filterExprs.foldLeft(df)((currentDf, expr) => currentDf.where(expr))
      }

      dfOpt match {
        case Success(_) =>
        case Failure(exception) =>
          griffinLogger.error("Error occurred while reading data set.", exception)
      }

      val preDfOpt = preProcess(dfOpt.toOption, ms)
      preDfOpt
    }

    (dfOpt, TimeRange(ms, readTmst(ms)))
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector.batch

import scala.collection.mutable.{Map => MutableMap}
import scala.util._
import org.apache.spark.sql.{DataFrame, DataFrameReader, SparkSession}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.types.StructType
import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.context.TimeRange
import com.hsbc.gbm.bd.clm.measure.datasource.TimestampStorage
import com.hsbc.gbm.bd.clm.measure.utils.HdfsUtil
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * A batch data connector for file based sources which allows support various
 * file based data sources like Parquet, CSV, TSV, ORC etc.
 * Local files can also be read by prepending `file://` namespace.
 *
 * Currently supported formats like Parquet, ORC, AVRO, Text and Delimited types like CSV, TSV etc.
 *
 * Supported Configurations:
 *  - format : [[String]] specifying the type of file source (parquet, orc, etc.).
 *  - paths : [[Seq]] specifying the paths to be read
 *  - options : [[Map]] of format specific options
 *  - skipOnError : [[Boolean]] specifying whether to continue execution if one or more paths are invalid.
 *  - schema : [[Seq]] of {colName, colType and isNullable} given as key value pairs. If provided, this can
 * help skip the schema inference step for some underlying data sources.
 *
 * Some defaults assumed by this connector (if not set) are as follows:
 *  - `delimiter` is \t for TSV format,
 *  - `schema` is None,
 *  - `header` is false,
 *  - `format` is parquet
 */
case class FileBasedDataConnector(
    @transient sparkSession: SparkSession,
    dcParam: DataConnectorParam,
    timestampStorage: TimestampStorage)
    extends BatchDataConnector {

  import FileBasedDataConnector._

  val config: Map[String, Any] = dcParam.getConfig
  val options: MutableMap[String, String] = MutableMap(
    config.getParamStringMap(Options, Map.empty).toSeq: _*)

  var format: String = config.getString(Format, DefaultFormat).toLowerCase
  val paths: Seq[String] = config.getStringArr(Paths, Nil)
  val schemaSeq: Seq[Map[String, String]] =
    config.getAnyRef[Seq[Map[String, String]]](Schema, Nil)
  val skipErrorPaths: Boolean = config.getBoolean(SkipErrorPaths, defValue = false)

  val currentSchema: Option[StructType] = Try(getUserDefinedSchema) match {
    case Success(structType) if structType.fields.nonEmpty => Some(structType)
    case _ => None
  }

  assert(
    SupportedFormats.contains(format),
    s"Invalid format '$format' specified. Must be one of ${SupportedFormats.mkString("['", "', '", "']")}")

  if (format == "csv") validateCSVOptions()
  if (format == "tsv") {
    format = "csv"
    options.getOrElseUpdate(Delimiter, TabDelimiter)
  }

  /**
   * Builds a [[StructType]] from the given schema string provided as `Schema` config.
   *
   * @example
   * {"schema":[{"name":"user_id","type":"string","nullable":"true"},{"name":"age","type":"int","nullable":"false"}]}
   * {"schema":[{"name":"user_id","type":"decimal(5,2)","nullable":"true"}]}
   * {"schema":[{"name":"my_struct","type":"struct<f1:int,f2:string>","nullable":"true"}]}
   * @return
   */
  private def getUserDefinedSchema: StructType = {
    schemaSeq.foldLeft(new StructType())((currentStruct, fieldMap) => {
      val colName = fieldMap(ColName).toLowerCase
      val colType = fieldMap(ColType).toLowerCase
      val isNullable = Try(fieldMap(IsNullable).toLowerCase.toBoolean).getOrElse(true)

      currentStruct.add(colName, colType, isNullable)
    })
  }

  /**
   * Ensures the presence of schema either via `header` or `schema` options.
   *
   *  - If both are present, the preference will be given to `schema`. First row will be omitted
   * if `header` is set to true, else will be included.
   *  - If `schema` is defined, it must be valid.
   *  - If neither is set, a fatal exception is thrown.
   */
  private def validateCSVOptions(): Unit = {
    if (options.contains(Header) && config.contains(Schema)) {
      griffinLogger.warn(
        s"Both $Options.$Header and $Schema were provided. Defaulting to provided $Schema")
    }

    if (!options.contains(Header) && !config.contains(Schema)) {
      throw new IllegalArgumentException(
        s"Either '$Header' must be set in '$Options' or '$Schema' must be set.")
    }

    if (config.contains(Schema) && (schemaSeq.isEmpty || currentSchema.isEmpty)) {
      throw new IllegalStateException("Unable to create schema from specification")

    }
  }

  def data(ms: Long): (Option[DataFrame], TimeRange) = {
    val validPaths = getValidPaths(paths, skipErrorPaths)

    val dfOpt = {
      val dfOpt = Try(
        sparkSession.read
          .options(options)
          .format(format)
          .withSchemaIfAny(currentSchema)
          .load(validPaths: _*))

      dfOpt match {
        case Success(_) =>
        case Failure(exception) =>
          griffinLogger.error("Error occurred while reading data set.", exception)
      }

      val preDfOpt = preProcess(dfOpt.toOption, ms)
      preDfOpt
    }

    (dfOpt, TimeRange(ms, readTmst(ms)))
  }
}

object FileBasedDataConnector extends Loggable {
  private val Format: String = "format"
  private val Paths: String = "paths"
  private val Options: String = "options"
  private val SkipErrorPaths: String = "skipErrorPaths"
  private val Schema: String = "schema"
  private val Header: String = "header"
  private val Delimiter: String = "delimiter"

  private val ColName: String = "name"
  private val ColType: String = "type"
  private val IsNullable: String = "nullable"
  private val TabDelimiter: String = "\t"

  private val DefaultFormat: String = SQLConf.DEFAULT_DATA_SOURCE_NAME.defaultValueString
  private val SupportedFormats: Seq[String] = Seq("parquet", "orc", "avro", "text", "csv", "tsv")

  /**
   * Validates the existence of paths in a given sequence.
   * Set option `skipOnError` to true to avoid fatal errors if any erroneous paths are encountered.
   *
   * @param paths       given sequence of paths
   * @param skipOnError flag to skip erroneous paths if any
   * @return
   */
  private def getValidPaths(paths: Seq[String], skipOnError: Boolean): Seq[String] = {
    val validPaths = paths.filter(
      path =>
        if (HdfsUtil.existPath(path)) true
        else {
          val msg = s"Path '$path' does not exist!"
          if (skipOnError) griffinLogger.error(msg)
          else throw new IllegalArgumentException(msg)

          false
      })

    assert(validPaths.nonEmpty, "No paths were given for the data source.")
    validPaths
  }

  /**
   * Adds methods implicitly to [[DataFrameReader]]
   *
   * @param dfr an instance of [[DataFrameReader]]
   */
  implicit class Implicits(dfr: DataFrameReader) {

    /**
     * Applies a schema to this [[DataFrameReader]] if any.
     *
     * @param schemaOpt an optional Schema
     * @return
     */
    def withSchemaIfAny(schemaOpt: Option[StructType]): DataFrameReader = {
      schemaOpt match {
        case Some(structType) => dfr.schema(structType)
        case None => dfr
      }
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector.batch

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.context.TimeRange
import com.hsbc.gbm.bd.clm.measure.datasource.TimestampStorage
import org.apache.spark.sql.{DataFrame, SparkSession}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._


/**
 * batch data connector for hive table
 */
case class HiveBatchDataConnector(
    @transient sparkSession: SparkSession,
    dcParam: DataConnectorParam,
    timestampStorage: TimestampStorage)
    extends BatchDataConnector {

  val config: Map[String, Any] = dcParam.getConfig

  val Database = "database"
  val TableName = "table.name"
  val Where = "where"

  val database: String = config.getString(Database, "default")
  val tableName: String = config.getString(TableName, "")
  val whereString: String = config.getString(Where, "")

  val concreteTableName = s"$database.$tableName"
  val wheres: Array[String] = whereString.split(",").map(_.trim).filter(_.nonEmpty)

  def data(ms: Long): (Option[DataFrame], TimeRange) = {
    val dfOpt = {
      val dtSql = dataSql()
      info(dtSql)
      val df = sparkSession.sql(dtSql)
      val dfOpt = Some(df)
      val preDfOpt = preProcess(dfOpt, ms)
      preDfOpt
    }
    val tmsts = readTmst(ms)
    (dfOpt, TimeRange(ms, tmsts))
  }

  private def dataSql(): String = {
    val tableClause = s"SELECT * FROM $concreteTableName"
    if (wheres.length > 0) {
      val clauses = wheres.map { w =>
        s"$tableClause WHERE $w"
      }
      clauses.mkString(" UNION ALL ")
    } else tableClause
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector

import java.util.concurrent.atomic.AtomicLong

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType.BatchProcessType
import com.hsbc.gbm.bd.clm.measure.context.{ContextId, DQContext, TimeRange}
import com.hsbc.gbm.bd.clm.measure.datasource.TimestampStorage
import com.hsbc.gbm.bd.clm.measure.job.builder.DQJobBuilder
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.preproc.PreProcParamMaker

import scala.collection.mutable

trait DataConnector extends Loggable with Serializable {

  val sparkSession: SparkSession

  val dcParam: DataConnectorParam

  val id: String = DataConnectorIdGenerator.genId

  val timestampStorage: TimestampStorage
  protected def saveTmst(t: Long): mutable.SortedSet[Long] = timestampStorage.insert(t)
  protected def readTmst(t: Long): Set[Long] = timestampStorage.fromUntil(t, t + 1)

  def init(): Unit

  // get data frame in batch mode
  def data(ms: Long): (Option[DataFrame], TimeRange)

  private def createContext(t: Long): DQContext = {
    DQContext(ContextId(t, id), id, Nil, Nil, BatchProcessType)(sparkSession)
  }

  def preProcess(dfOpt: Option[DataFrame], ms: Long): Option[DataFrame] = {
    // new context
    val context = createContext(ms)

    val timestamp = context.contextId.timestamp
    val suffix = context.contextId.id
    val dcDfName = dcParam.getDataFrameName("this")

    try {
      saveTmst(timestamp) // save timestamp

      dfOpt.flatMap { df =>
        val (preProcRules, thisTable) =
          PreProcParamMaker.makePreProcRules(dcParam.getPreProcRules, suffix, dcDfName)

        // init data
        context.compileTableRegister.registerTable(thisTable)
        context.runTimeTableRegister.registerTable(thisTable, df)

        // build job
        val preprocJob = DQJobBuilder.buildDQJob(context, preProcRules)

        // job execute
        preprocJob.execute(context)

        // out data
        val outDf = context.sparkSession.table(s"`$thisTable`")

        // add tmst column
        val withTmstDf = outDf.withColumn(ConstantColumns.tmst, lit(timestamp))

        // clean context
        context.clean()

        Some(withTmstDf)
      }

    } catch {
      case e: Throwable =>
        error(s"pre-process of data connector [$id] error: ${e.getMessage}", e)
        None
    }
  }
}

object DataConnectorIdGenerator {
  private val counter: AtomicLong = new AtomicLong(0L)
  private val head: String = "dc"

  def genId: String = {
    s"$head$increment"
  }

  private def increment: Long = {
    counter.incrementAndGet()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource.connector

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.datasource.TimestampStorage
import com.hsbc.gbm.bd.clm.measure.datasource.connector.batch._
import org.apache.spark.sql.SparkSession

import scala.util.Try
import scala.util.matching.Regex

object DataConnectorFactory extends Loggable {

  @deprecated val AvroRegex: Regex = """^(?i)avro$""".r
  @deprecated val TextDirRegex: Regex = """^(?i)text-dir$""".r

  val HiveRegex: Regex = """^(?i)hive$""".r
  val FileRegex: Regex = """^(?i)file$""".r
  val KafkaRegex: Regex = """^(?i)kafka$""".r
  val JDBCRegex: Regex = """^(?i)jdbc$""".r
  val CustomRegex: Regex = """^(?i)custom$""".r
  val ElasticSearchRegex: Regex = """^(?i)elasticsearch$""".r

  /**
   * create data connector
   *
   * @param sparkSession spark env
   * @param dcParam      data connector param
   * @param tmstCache    same tmst cache in one data source
   * @return data connector
   */
  def getDataConnector(
                        sparkSession: SparkSession,
                        dcParam: DataConnectorParam,
                        tmstCache: TimestampStorage): Try[DataConnector] = {
    val conType = dcParam.getType
    Try {
      conType match {
        case HiveRegex() => HiveBatchDataConnector(sparkSession, dcParam, tmstCache)
        case FileRegex() => FileBasedDataConnector(sparkSession, dcParam, tmstCache)
        case ElasticSearchRegex() => ElasticSearchDataConnector(sparkSession, dcParam, tmstCache)
        case CustomRegex() =>
          getCustomConnector(sparkSession, dcParam, tmstCache)
        case _ => throw new Exception("connector creation error!")
      }
    }
  }

  private def getCustomConnector(
                                  sparkSession: SparkSession,
                                  dcParam: DataConnectorParam,
                                  timestampStorage: TimestampStorage): DataConnector = {
    val className = dcParam.getConfig("class").asInstanceOf[String]
    val cls = Class.forName(className)
    if (classOf[BatchDataConnector].isAssignableFrom(cls)) {
      val method = cls.getDeclaredMethod(
        "apply",
        classOf[SparkSession],
        classOf[DataConnectorParam],
        classOf[TimestampStorage])
      method
        .invoke(null, sparkSession, dcParam, timestampStorage)
        .asInstanceOf[BatchDataConnector]
    } else {
      throw new ClassCastException(
        s"$className should extend BatchDataConnector or StreamingDataConnector")
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataSourceParam
import com.hsbc.gbm.bd.clm.measure.context.{DQContext, TimeRange}
import com.hsbc.gbm.bd.clm.measure.datasource.connector.DataConnector
import org.apache.spark.sql._
import com.hsbc.gbm.bd.clm.measure.utils.DataFrameUtil._

/**
 * data source
 *
 * @param name          name of data source
 * @param dsParam       param of this data source
 * @param dataConnector data connector
 */
case class DataSource(
                       name: String,
                       dsParam: DataSourceParam,
                       dataConnector: Option[DataConnector])
  extends Loggable
    with Serializable {

  val isBaseline: Boolean = dsParam.isBaseline

  def init(): Unit = {
    dataConnector.foreach(_.init())
  }

  def loadData(context: DQContext): TimeRange = {
    info(s"load data [$name]")
    try {
      val timestamp = context.contextId.timestamp
      val (dfOpt, timeRange) = data(timestamp)
      dfOpt match {
        case Some(df) =>
          context.runTimeTableRegister.registerTable(name, df)
        case None =>
          throw new RuntimeException(s"Data source [$name] is null!")
      }
      timeRange
    } catch {
      case e: Throwable =>
        error(s"load data source [$name] fails")
        throw e
    }
  }

  private def data(timestamp: Long): (Option[DataFrame], TimeRange) = {
    val batches = dataConnector.flatMap { dc =>
      val (dfOpt, timeRange) = dc.data(timestamp)
      dfOpt match {
        case Some(_) => Some((dfOpt, timeRange))
        case _ => None
      }
    }

    if (batches.nonEmpty) {
      batches.reduce { (a, b) =>
        (unionDfOpts(a._1, b._1), a._2.merge(b._2))
      }
    } else {
      (None, TimeRange.emptyTimeRange)
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataSourceParam
import com.hsbc.gbm.bd.clm.measure.datasource.connector.DataConnectorFactory
import org.apache.spark.sql.SparkSession

import scala.util.Success

object DataSourceFactory extends Loggable {

  def getDataSources(sparkSession: SparkSession, dataSources: Seq[DataSourceParam]): Seq[DataSource] = {
    dataSources.zipWithIndex.flatMap { pair =>
      val (param, _) = pair
      getDataSource(sparkSession, param)
    }
  }

  private def getDataSource(sparkSession: SparkSession, dataSourceParam: DataSourceParam): Option[DataSource] = {
    val name = dataSourceParam.getName
    val timestampStorage = TimestampStorage()

    val connectorParamsOpt = dataSourceParam.getConnector

    connectorParamsOpt match {
      case Some(connectorParam) =>
        val dataConnectors = DataConnectorFactory.getDataConnector(
          sparkSession,
          connectorParam,
          timestampStorage) match {
          case Success(connector) => Some(connector)
          case _ => None
        }

        Some(DataSource(name, dataSourceParam, dataConnectors))
      case None => None
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.datasource

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.collection.mutable.{SortedSet => MutableSortedSet}


/**
 * tmst cache, CRUD of timestamps
 */
case class TimestampStorage() extends Loggable {

  private val tmstGroup: MutableSortedSet[Long] = MutableSortedSet.empty[Long]

  // -- insert tmst into tmst group --
  def insert(tmst: Long): MutableSortedSet[Long] = tmstGroup += tmst
  def insert(tmsts: Iterable[Long]): MutableSortedSet[Long] = tmstGroup ++= tmsts

  // -- remove tmst from tmst group --
  def remove(tmst: Long): MutableSortedSet[Long] = tmstGroup -= tmst
  def remove(tmsts: Iterable[Long]): MutableSortedSet[Long] = tmstGroup --= tmsts

  // -- get subset of tmst group --
  def fromUntil(from: Long, until: Long): Set[Long] = tmstGroup.range(from, until).toSet
  def afterTil(after: Long, til: Long): Set[Long] = tmstGroup.range(after + 1, til + 1).toSet
  def until(until: Long): Set[Long] = tmstGroup.until(until).toSet
  def from(from: Long): Set[Long] = tmstGroup.from(from).toSet
  def all: Set[Long] = tmstGroup.toSet

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/job: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/job/builder: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.job.builder

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.job._
import com.hsbc.gbm.bd.clm.measure.step.builder.DQStepBuilder
import com.hsbc.gbm.bd.clm.measure.step.write.MetricFlushStep

/**
 * build dq job based on configuration
 */
object DQJobBuilder {

  /**
   * build dq job with rule param
   * @param context              dq context
   * @param evaluateRuleParam    evaluate rule param
   * @return       dq job
   */
  def buildDQJob(context: DQContext, evaluateRuleParam: EvaluateRuleParam): DQJob = {
    val ruleParams = evaluateRuleParam.getRules
    buildDQJob(context, ruleParams)
  }

  /**
   * build dq job with rules in evaluate rule param or pre-proc param
   * @param context          dq context
   * @param ruleParams       rule params
   * @return       dq job
   */
  def buildDQJob(context: DQContext, ruleParams: Seq[RuleParam]): DQJob = {
    // build steps by datasources
    val dsSteps = context.dataSources.flatMap { dataSource =>
      DQStepBuilder.buildStepOptByDataSourceParam(context, dataSource.dsParam)
    }
    // build steps by rules
    val ruleSteps = ruleParams.flatMap { ruleParam =>
      DQStepBuilder.buildStepOptByRuleParam(context, ruleParam)
    }
    // metric flush step
    val metricFlushStep = MetricFlushStep()

    DQJob(dsSteps ++ ruleSteps :+ metricFlushStep)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.job

import scala.util.{Failure, Success, Try}
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep

case class DQJob(dqSteps: Seq[DQStep]) extends Serializable {

  def execute(context: DQContext): Try[Boolean] = {
    dqSteps
      .map(_.execute(context))
      .foldLeft(Try(true)) { (ret, stepResult) =>
        (ret, stepResult) match {
          case (Success(_), nextResult) => nextResult
          case (Failure(_), _) => ret
        }
      }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/launch: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/launch/batch: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.launch.batch

import java.util.concurrent.TimeUnit

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DQConfig, SinkParam}
import com.hsbc.gbm.bd.clm.measure.context.{ContextId, DQContext}

import scala.util.Try
import org.apache.spark.sql.SparkSession
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType.BatchProcessType
import com.hsbc.gbm.bd.clm.measure.datasource.DataSourceFactory
import com.hsbc.gbm.bd.clm.measure.job.builder.DQJobBuilder
import com.hsbc.gbm.bd.clm.measure.launch.DQApp
import com.hsbc.gbm.bd.clm.measure.step.builder.udf.GriffinUDFAgent
import com.hsbc.gbm.bd.clm.measure.utils.CommonUtils

case class BatchDQApp(dqParam: DQConfig, sinkParams: Seq[SinkParam]) extends DQApp {

  def retryable: Boolean = false

  def run(implicit spark: SparkSession): Try[Boolean] = {
    CommonUtils.timeThis({
      val logLevel = getGriffinLogLevel
      griffinLogger.setLevel(logLevel)
      // register udf
      GriffinUDFAgent.register(spark)

      val measureTime = getMeasureTime
      val contextId = ContextId(measureTime)

      // get data sources
      val dataSources =
        DataSourceFactory.getDataSources(spark, dqParam.getDataSources)
      dataSources.foreach(_.init())

      // create dq context
      val dqContext = DQContext(contextId, dqParam.getName, dataSources, sinkParams, BatchProcessType)(spark)

      // start id
      val applicationId = spark.sparkContext.applicationId
      dqContext.getSinks.foreach(_.open(applicationId))

      // build job
      val dqJob = DQJobBuilder.buildDQJob(dqContext, dqParam.getEvaluateRule)

      // dq job execute
      val executeStatuCode = dqJob.execute(dqContext)

      // clean context
      dqContext.clean()

      // finish
      dqContext.getSinks.foreach(_.close())

      executeStatuCode
    }, TimeUnit.MILLISECONDS)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.launch

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.SinkParam

import scala.util.Try
import org.apache.spark.metrics.sink.Sink
import org.apache.spark.sql.SparkSession
import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DQConfig, EnvConfig, SinkParam}

/**
 * dq application process
 */
trait DQApp extends Loggable with Serializable {

  def sinkParams: Seq[SinkParam]

  def dqParam: DQConfig

  /**
   * @return execution success
   */
  def run(implicit spark: SparkSession): Try[Boolean]

  /**
   * application will exit if it fails in run phase.
   * if retryable is true, the exception will be threw to spark env,
   * and enable retry strategy of spark application
   */
  def retryable: Boolean

  /**
   * timestamp as a key for metrics
   */
  protected def getMeasureTime: Long = {
    dqParam.getTimestampOpt match {
      case Some(t) if t > 0 => t
      case _ => System.currentTimeMillis
    }
  }

  /**
   * Gets a valid [[Sink]] definition from the Env Config for each [[Sink]] defined in Job Config.
   *
   * @throws AssertionError if Env Config does not contain definition for a sink defined in Job Config
   * @return [[Seq]] of [[Sink]] definitions
   */
  protected def getSinkParams: Seq[SinkParam] = {
    val sinkParams = dqParam.getSinkNames
      .map(_.toLowerCase())
      .map { sinkName =>
        (sinkName, this.sinkParams.find(_.getName.toLowerCase().matches(sinkName)))
      }

    val missingSinks = sinkParams.filter(_._2.isEmpty).map(_._1)

    assert(
      missingSinks.isEmpty,
      s"Sink(s) ['${missingSinks.mkString("', '")}'] not defined in env config.")

    sinkParams.flatMap(_._2)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure

import org.apache.log4j.Level
import org.apache.log4j.Logger

trait Loggable {

  @transient private lazy val logger = Logger.getLogger(getClass)

  @transient protected lazy val griffinLogger: Logger = Logger.getLogger("com.hsbc.gbm.bd.clm")

  def getGriffinLogLevel: Level = {
    var logger = griffinLogger
    while (logger != null && logger.getLevel == null) {
      logger = logger.getParent.asInstanceOf[Logger]
    }
    logger.getLevel
  }

  protected def info(msg: => String): Unit = {
    logger.info(msg)
  }

  protected def debug(msg: => String): Unit = {
    logger.debug(msg)
  }

  protected def warn(msg: => String): Unit = {
    logger.warn(msg)
  }

  protected def warn(msg: => String, e: Throwable): Unit = {
    logger.warn(msg, e)
  }

  protected def error(msg: => String): Unit = {
    logger.error(msg)
  }

  protected def error(msg: => String, e: Throwable): Unit = {
    logger.error(msg, e)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.reader.ParamReaderFactory
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DQConfig, DataConnectorParam, DataSourceParam, EvaluateRuleParam, Param, RuleParam, SinkParam}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType.BatchProcessType
import com.hsbc.gbm.bd.clm.measure.launch.DQApp
import com.hsbc.gbm.bd.clm.measure.launch.batch.BatchDQApp
import com.hsbc.gbm.bd.clm.measure.utils.HdfsUtil
import org.apache.spark.sql.SparkSession

import scala.reflect.ClassTag
import scala.util.{Failure, Success, Try}

trait MeasureDQ extends Loggable {

  def measureDirPath: String

  def dsParams: Seq[DataSourceParam]

  def sinkParams: Seq[SinkParam]

  def measurementName = "CLM_Measure"

  implicit def spark: SparkSession

  def measure(): Unit = {
    val evaluateRuleParam: EvaluateRuleParam = HdfsUtil.listSubPathsByType(measureDirPath, "file", fullPath = true).map(x => {
      readFile[EvaluateRuleParam](x) match {
        case Success(p) => p
        case Failure(ex) =>
          error(ex.getMessage, ex)
          sys.exit(-1)
      }
    }).foldLeft(EvaluateRuleParam(List[RuleParam]()))((x, xs) => EvaluateRuleParam(x.getRules ++ xs.getRules))

    val dqParam: DQConfig = DQConfig(measurementName, 0L, "batch", dsParams, evaluateRuleParam, sinkParams.map(_.getName))

    // choose process
    val procType: ProcessType.Value = ProcessType.withNameWithDefault(dqParam.getProcType)
    val dqApp: DQApp = procType match {
      case BatchProcessType => BatchDQApp(dqParam, sinkParams)
      case _ =>
        error(s"$procType is unsupported process type!")
        sys.exit(-2)
    }

    // dq app run
    val success: Boolean = dqApp.run match {
      case Success(result) =>
        info("process run result: " + (if (result) "success" else "failed"))
        result

      case Failure(ex) =>
        error(s"process run error: ${ex.getMessage}", ex)

        if (dqApp.retryable) {
          throw ex
        } else {
          sys.exit(-3)
        }
    }

    if (!success) {
      sys.exit(-3)
    }
  }

  private def readFile[T <: Param](file: String)(implicit m: ClassTag[T]): Try[T] = {
    val paramReader = ParamReaderFactory.getParamReader(file)
    paramReader.readConfig[T]
  }
}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/sink: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame
import com.hsbc.gbm.bd.clm.measure.utils.JsonUtil
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * Console Sink for Records and Metrics.
 * Records are shown in a tabular structure and Metrics are logged as JSON string.
 *
 * Supported Configurations:
 *  - truncate : [[Boolean]] Whether truncate long strings. If true, strings more than 20 characters
 * will be truncated and all cells will be aligned right. Default is true.
 *  - numRows : [[Int]] Number of rows to show. Default is 20.
 */
case class ConsoleSink(config: Map[String, Any], jobName: String, timeStamp: Long) extends Sink {

  val block: Boolean = true

  val Truncate: String = "truncate"
  val truncateRecords: Boolean = config.getBoolean(Truncate, defValue = true)

  val NumberOfRows: String = "numRows"
  val numRows: Int = config.getInt(NumberOfRows, 20)

  def validate(): Boolean = true

  override def open(applicationId: String): Unit = {
    griffinLogger.info(
      s"Opened ConsoleSink for job with name '$jobName', " +
        s"timestamp '$timeStamp' and applicationId '$applicationId'")
  }

  override def close(): Unit = {
    griffinLogger.info(
      s"Closed ConsoleSink for job with name '$jobName' and timestamp '$timeStamp'")
  }

  override def sinkRecords(records: RDD[String], name: String): Unit = {}

  override def sinkRecords(records: Iterable[String], name: String): Unit = {}

  override def sinkMetrics(metrics: Map[String, Any])(context: DQContext): Unit = {
    import context.sparkSession.implicits._
    Seq(JsonUtil.toJson(metrics)).toDF(s"${jobName.replaceAll("[ |-]", "_")}_Metrics").show(false)
  }

  override def sinkBatchRecords(dataset: DataFrame, key: Option[String] = None): Unit = {
    println(key.getOrElse(""))
    dataset.show(numRows, truncateRecords)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import com.hsbc.gbm.bd.clm.measure.context.DQContext

import scala.concurrent.Future
import org.apache.spark.sql.DataFrame
import com.hsbc.gbm.bd.clm.measure.utils.{HttpUtil, JsonUtil, TimeUtil}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * sink metric and record through http request
 */
case class ElasticSearchSink(
    config: Map[String, Any],
    jobName: String,
    timeStamp: Long,
    block: Boolean)
    extends Sink {

  val Api = "api"
  val Method = "method"
  val ConnectionTimeout = "connection.timeout"
  val Retry = "retry"

  val api: String = config.getString(Api, "")
  val method: String = config.getString(Method, "post")

  val connectionTimeout: Long =
    TimeUtil.milliseconds(config.getString(ConnectionTimeout, "")).getOrElse(-1L)

  val retry: Int = config.getInt(Retry, 10)

  val _Value = "value"

  def validate(): Boolean = {
    api.nonEmpty
  }

  private def httpResult(dataMap: Map[String, Any]): Unit = {
    try {
      val data = JsonUtil.toJson(dataMap)
      // http request
      val params = Map[String, Object]()
      val header = Map[String, Object](("Content-Type", "application/json"))

      def func(): (Long, Future[Boolean]) = {
        import scala.concurrent.ExecutionContext.Implicits.global
        (timeStamp, Future(HttpUtil.doHttpRequest(api, method, params, header, data)))
      }
      if (block) SinkTaskRunner.addBlockTask(func _, retry, connectionTimeout)
      else SinkTaskRunner.addNonBlockTask(func _, retry)
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }

  }

  override def sinkMetrics(metrics: Map[String, Any])(context: DQContext): Unit = {
    httpResult(metrics)
  }

  override def sinkBatchRecords(dataset: DataFrame, key: Option[String] = None): Unit = {}
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame
import com.hsbc.gbm.bd.clm.measure.utils.{HdfsUtil, JsonUtil}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * sink metric and record to hdfs
 */
case class HdfsSink(config: Map[String, Any], jobName: String, timeStamp: Long) extends Sink {

  val block: Boolean = true

  val PathKey = "path"
  val MaxPersistLines = "max.persist.lines"
  val MaxLinesPerFile = "max.lines.per.file"

  val parentPath: String = config.getOrElse(PathKey, "").toString
  val maxPersistLines: Int = config.getInt(MaxPersistLines, -1)
  val maxLinesPerFile: Int = math.min(config.getInt(MaxLinesPerFile, 10000), 1000000)

  val StartFile: String = filePath("_START")
  val FinishFile: String = filePath("_FINISH")
  val MetricsFile: String = filePath("_METRICS")

  val LogFile: String = filePath("_LOG")

  var _init = true

  def validate(): Boolean = {
    parentPath.nonEmpty
  }

//  private def logHead: String = {
//    if (_init) {
//      _init = false
//      val dt = new Date(timeStamp)
//      s"================ log of $dt ================\n"
//    } else ""
//  }
//
//  private def timeHead(rt: Long): String = {
//    val dt = new Date(rt)
//    s"--- $dt ---\n"
//  }
//
//  private def logWrap(rt: Long, msg: String): String = {
//    logHead + timeHead(rt) + s"$msg\n\n"
//  }

  protected def filePath(file: String): String = {
    HdfsUtil.getHdfsFilePath(parentPath, s"$jobName/$timeStamp/$file")
  }

  protected def withSuffix(path: String, suffix: String): String = {
    s"$path.$suffix"
  }

  override def open(applicationId: String): Unit = {
    try {
      HdfsUtil.writeContent(StartFile, applicationId)
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

  override def close(): Unit = {
    try {
      HdfsUtil.createEmptyFile(FinishFile)
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

//  def log(rt: Long, msg: String): Unit = {
//    try {
//      val logStr = logWrap(rt, msg)
//      HdfsUtil.withHdfsFile(LogFile) { out =>
//        out.write(logStr.getBytes("utf-8"))
//      }
//    } catch {
//      case e: Throwable => error(e.getMessage, e)
//    }
//  }

  private def getHdfsPath(path: String, groupId: Int): String = {
    HdfsUtil.getHdfsFilePath(path, s"$groupId")
  }

  private def clearOldRecords(path: String): Unit = {
    HdfsUtil.deleteHdfsPath(path)
  }

  override def sinkRecords(records: RDD[String], name: String): Unit = {
    val path = filePath(name)
    clearOldRecords(path)
    try {
      val recordCount = records.count

      val count =
        if (maxPersistLines < 0) recordCount else scala.math.min(maxPersistLines, recordCount)

      if (count > 0) {
        val groupCount = ((count - 1) / maxLinesPerFile + 1).toInt
        if (groupCount <= 1) {
          val recs = records.take(count.toInt)
          sinkRecords2Hdfs(path, recs)
        } else {
          val groupedRecords: RDD[(Long, Iterable[String])] =
            records.zipWithIndex
              .flatMap { r =>
                val gid = r._2 / maxLinesPerFile
                if (gid < groupCount) Some((gid, r._1)) else None
              }
              .groupByKey()
          groupedRecords.foreach { group =>
            val (gid, recs) = group
            val hdfsPath = if (gid == 0) path else withSuffix(path, gid.toString)
            sinkRecords2Hdfs(hdfsPath, recs)
          }
        }
      }
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

  override def sinkRecords(records: Iterable[String], name: String): Unit = {
    val path = filePath(name)
    clearOldRecords(path)
    try {
      val recordCount = records.size

      val count =
        if (maxPersistLines < 0) recordCount else scala.math.min(maxPersistLines, recordCount)

      if (count > 0) {
        val groupCount = (count - 1) / maxLinesPerFile + 1
        if (groupCount <= 1) {
          val recs = records.take(count.toInt)
          sinkRecords2Hdfs(path, recs)
        } else {
          val groupedRecords = records.grouped(maxLinesPerFile).zipWithIndex
          groupedRecords.take(groupCount).foreach { group =>
            val (recs, gid) = group
            val hdfsPath = getHdfsPath(path, gid)
            sinkRecords2Hdfs(hdfsPath, recs)
          }
        }
      }
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

  override def sinkMetrics(metrics: Map[String, Any])(context: DQContext): Unit = {
    try {
      val json = JsonUtil.toJson(metrics)
      sinkRecords2Hdfs(MetricsFile, json :: Nil)
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

  private def sinkRecords2Hdfs(hdfsPath: String, records: Iterable[String]): Unit = {
    try {
      HdfsUtil.withHdfsFile(hdfsPath, appendIfExists = false) { out =>
        records.map { record =>
          out.write((record + "\n").getBytes("utf-8"))
        }
      }
    } catch {
      case e: Throwable => error(e.getMessage, e)
    }
  }

  override def sinkBatchRecords(dataset: DataFrame, key: Option[String] = None): Unit = {
    sinkRecords(dataset.toJSON.rdd, key.getOrElse(""))
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame

/**
 * Base trait for batch and Streaming Sinks.
 * To implement custom sinks, extend your classes with this trait.
 */
trait Sink extends Loggable with Serializable {

  val jobName: String
  val timeStamp: Long

  val config: Map[String, Any]

  val block: Boolean

  /**
   * Ensures that the pre-requisites (if any) of the Sink are met before opening it.
   */
  def validate(): Boolean

  /**
   * Allows initialization of the connection to the sink (if required).
   *
   * @param applicationId Spark Application ID
   */
  def open(applicationId: String): Unit = {}

  /**
   * Allows clean up for the sink (if required).
   */
  def close(): Unit = {}

  /**
   * Implementation of persisting records for streaming pipelines.
   */
  def sinkRecords(records: RDD[String], name: String): Unit = {}

  /**
   * Implementation of persisting records for streaming pipelines.
   */
  def sinkRecords(records: Iterable[String], name: String): Unit = {}

  /**
   * Implementation of persisting metrics.
   */
  def sinkMetrics(metrics: Map[String, Any])(context: DQContext): Unit = {}

  /**
   * Implementation of persisting records for batch pipelines.
   */
  def sinkBatchRecords(dataset: DataFrame, key: Option[String] = None): Unit = {}
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import scala.util.{Failure, Success, Try}

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.SinkParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.SinkType._
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * SinkFactory, responsible for creation of Batch and Streaming Sinks based on the definition
 * provided in Env Config.
 *
 * @param sinkParamIter [[Seq]] of sink definitions as [[SinkParam]]
 * @param jobName name of the current Griffin Job
 */
case class SinkFactory(sinkParamIter: Seq[SinkParam], jobName: String)
    extends Loggable
    with Serializable {

  /**
   * Creates all the sinks defined in the Env Config.
   *
   * @param timeStamp epoch timestamp
   * @param block persist in blocking or non-blocking way
   * @return a [[Seq]] of [[Sink]] that were created successfully
   */
  def getSinks(timeStamp: Long, block: Boolean): Seq[Sink] = {
    sinkParamIter.flatMap(param => getSink(timeStamp, param, block))
  }

  /**
   * Creates a [[Sink]] from the definition provided in the Env Config.
   * Supported [[Sink]] are defined in [[SinkType]].
   *
   * @param timeStamp epoch timestamp
   * @param sinkParam sink definition
   * @param block persist in blocking or non-blocking way
   * @return [[Some]](sink) if successfully created sink else [[None]]
   */
  private def getSink(timeStamp: Long, sinkParam: SinkParam, block: Boolean): Option[Sink] = {
    val config = sinkParam.getConfig
    val sinkType = sinkParam.getType
    val sinkTry = sinkType match {
      case Console => Try(ConsoleSink(config, jobName, timeStamp))
      case Hdfs => Try(HdfsSink(config, jobName, timeStamp))
      case ElasticSearch => Try(ElasticSearchSink(config, jobName, timeStamp, block))
      case Custom => Try(getCustomSink(config, timeStamp, block))
      case _ => throw new Exception(s"sink type $sinkType is not supported!")
    }
    sinkTry match {
      case Success(sink) if sink.validate() => Some(sink)
      case Failure(ex) =>
        error("Failed to get sink", ex)
        None
    }
  }

  /**
   * Creates a custom [[Sink]] using reflection for a provided class name.
   * Refer to measure configuration guide for more information regarding Custom sinks.
   *
   * @throws ClassCastException when the provided class name does not extend [[Sink]]
   * @param config values defined in Env Config for the custom sink
   * @param timeStamp epoch timestamp
   * @param block persist in blocking or non-blocking way
   * @return [[Sink]] if created successfully
   *
   */
  private def getCustomSink(config: Map[String, Any], timeStamp: Long, block: Boolean): Sink = {
    val className = config.getString("class", "")
    val cls = Class.forName(className)
    if (classOf[Sink].isAssignableFrom(cls)) {
      val method = cls.getDeclaredMethod(
        "apply",
        classOf[Map[String, Any]],
        classOf[String],
        classOf[Long],
        classOf[Boolean])
      method
        .invoke(
          null,
          config,
          jobName.asInstanceOf[Object],
          timeStamp.asInstanceOf[Object],
          block.asInstanceOf[Object])
        .asInstanceOf[Sink]
    } else {
      throw new ClassCastException(s"$className should extend Sink")
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.sink

import java.util.Date
import java.util.concurrent.TimeUnit

import scala.concurrent._
import scala.concurrent.duration._
import scala.util.{Failure, Success}

import com.hsbc.gbm.bd.clm.measure.Loggable

/**
 * sink task runner, to sink metrics in block or non-block mode
 */
object SinkTaskRunner extends Loggable {

  import scala.concurrent.ExecutionContext.Implicits.global

  val MAX_RETRY = 100

  def addNonBlockTask(func: () => (Long, Future[_]), retry: Int): Unit = {
    val r = validRetryNum(retry)
    nonBlockExecute(func, r)
  }

  def addBlockTask(func: () => (Long, Future[_]), retry: Int, wait: Long): Unit = {
    val r = validRetryNum(retry)
    val duration = if (wait >= 0) Duration(wait, TimeUnit.MILLISECONDS) else Duration.Inf
    blockExecute(func, r, duration)
  }

  private def nonBlockExecute(func: () => (Long, Future[_]), retry: Int): Unit = {
    val nextRetry = nextRetryCount(retry)
    val st = new Date().getTime
    val (t, res) = func()
    res.onComplete {
      case Success(value) =>
        val et = new Date().getTime
        info(s"task $t success with ($value) [ using time ${et - st} ms ]")

      case Failure(e) =>
        val et = new Date().getTime
        warn(s"task $t fails [ using time ${et - st} ms ] : ${e.getMessage}")
        if (nextRetry >= 0) {
          info(s"task $t retry [ rest retry count: $nextRetry ]")
          nonBlockExecute(func, nextRetry)
        } else {
          error(s"task fails: task $t retry ends but fails", e)
        }
    }
  }

  @scala.annotation.tailrec
  private def blockExecute(
      func: () => (Long, Future[_]),
      retry: Int,
      waitDuration: Duration): Unit = {
    val nextRetry = nextRetryCount(retry)
    val st = new Date().getTime
    val (t, res) = func()
    try {
      val value = Await.result(res, waitDuration)
      val et = new Date().getTime
      info(s"task $t success with ($value) [ using time ${et - st} ms ]")
    } catch {
      case e: Throwable =>
        val et = new Date().getTime
        warn(s"task $t fails [ using time ${et - st} ms ] : ${e.getMessage}")
        if (nextRetry >= 0) {
          info(s"task $t retry [ rest retry count: $nextRetry ]")
          blockExecute(func, nextRetry, waitDuration)
        } else {
          error(s"task fails: task $t retry ends but fails", e)
        }
    }
  }

  private def validRetryNum(retry: Int): Int = {
    if (retry > MAX_RETRY) MAX_RETRY else retry
  }
  private def nextRetryCount(retry: Int): Int = {
    if (retry >= 0) retry - 1 else -1
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.DataConnectorParam
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.read.ReadStep

case class BatchDataSourceStepBuilder() extends DataSourceParamStepBuilder {

  def buildReadSteps(context: DQContext, dcParam: DataConnectorParam): Option[ReadStep] = {
    None
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

/**
 * for griffin dsl rules, the constant columns might be used during calculation,
 */
object ConstantColumns {
  val tmst = "__tmst"
  val metric = "__metric"
  val record = "__record"
  val empty = "__empty"

  val beginTs = "__begin_ts"
  val endTs = "__end_ts"

  val distinct = "__distinct"

  val rowNumber = "__rn"

  val columns: List[String] =
    List[String](tmst, metric, record, empty, beginTs, endTs, distinct, rowNumber)
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.transform.DataFrameOpsTransformStep

case class DataFrameOpsDQStepBuilder() extends RuleParamStepBuilder {

  def buildSteps(context: DQContext, ruleParam: RuleParam): Seq[DQStep] = {
    val name = getStepName(ruleParam.getOutDfName())
    val inputDfName = getStepName(ruleParam.getInDfName())
    val transformStep = DataFrameOpsTransformStep(
      name,
      inputDfName,
      ruleParam.getRule,
      ruleParam.getDetails,
      None,
      ruleParam.getCache)
    transformStep +: buildDirectWriteSteps(ruleParam)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.step.read.{ReadStep, UnionReadStep}
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DataConnectorParam, DataSourceParam}
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep

/**
 * build dq step by data source param
 */
trait DataSourceParamStepBuilder extends DQStepBuilder {

  type ParamType = DataSourceParam

  def buildDQStep(context: DQContext, param: ParamType): Option[DQStep] = {
    val name = getStepName(param.getName)

    param.getConnector match {
      case Some(dc) =>
        val steps = buildReadSteps(context, dc)
        if (steps.isDefined) Some(UnionReadStep(name, Seq(steps.get)))
        else None
      case _ => None
    }
  }

  protected def buildReadSteps(context: DQContext, dcParam: DataConnectorParam): Option[ReadStep]

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import org.apache.commons.lang.StringUtils

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DataSourceParam, Param, RuleParam}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DslType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step._

/**
 * build dq step by param
 */
trait DQStepBuilder extends Loggable with Serializable {

  type ParamType <: Param

  def buildDQStep(context: DQContext, param: ParamType): Option[DQStep]

  protected def getStepName(name: String): String = {
    if (StringUtils.isNotBlank(name)) name
    else DQStepNameGenerator.genName
  }

}

object DQStepBuilder {

  def buildStepOptByDataSourceParam(
      context: DQContext,
      dsParam: DataSourceParam): Option[DQStep] = {
    getDataSourceParamStepBuilder(context.procType)
      .flatMap(_.buildDQStep(context, dsParam))
  }

  private def getDataSourceParamStepBuilder(
      procType: ProcessType): Option[DataSourceParamStepBuilder] = {
    procType match {
      case BatchProcessType => Some(BatchDataSourceStepBuilder())
      case _ => None
    }
  }

  def buildStepOptByRuleParam(context: DQContext, ruleParam: RuleParam): Option[DQStep] = {
    val dslType = ruleParam.getDslType
    val dsNames = context.dataSourceNames
    val funcNames = context.functionNames
    val dqStepOpt = getRuleParamStepBuilder(dslType, dsNames, funcNames)
      .flatMap(_.buildDQStep(context, ruleParam))
    dqStepOpt.toSeq
      .flatMap(_.getNames)
      .foreach(name => context.compileTableRegister.registerTable(name))
    dqStepOpt
  }

  private def getRuleParamStepBuilder(
      dslType: DslType,
      dsNames: Seq[String],
      funcNames: Seq[String]): Option[RuleParamStepBuilder] = {
    dslType match {
      case SparkSql => Some(SparkSqlDQStepBuilder())
      case DataFrameOpsType => Some(DataFrameOpsDQStepBuilder())
      case GriffinDsl => Some(GriffinDslDQStepBuilder(dsNames, funcNames))
      case _ => None
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import java.util.concurrent.atomic.AtomicLong

object DQStepNameGenerator {
  private val counter: AtomicLong = new AtomicLong(0L)
  private val head: String = "step"

  def genName: String = {
    s"$head$increment"
  }

  private def increment: Long = {
    counter.incrementAndGet()
  }
}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/dsl: Is a directory
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/dsl/expr: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait AliasableExpr extends Expr {

  def alias: Option[String]

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait ClauseExpression extends Expr {}

case class SelectClause(exprs: Seq[Expr], extraConditionOpt: Option[ExtraConditionExpr])
    extends ClauseExpression {

  addChildren(exprs)

  def desc: String = {
    extraConditionOpt match {
      case Some(cdtn) => s"${cdtn.desc} ${exprs.map(_.desc).mkString(", ")}"
      case _ => s"${exprs.map(_.desc).mkString(", ")}"
    }
  }
  def coalesceDesc: String = desc

  override def map(func: Expr => Expr): SelectClause = {
    SelectClause(
      exprs.map(func(_)),
      extraConditionOpt.map(func(_).asInstanceOf[ExtraConditionExpr]))
  }

}

case class FromClause(dataSource: String) extends ClauseExpression {

  def desc: String = s"FROM `$dataSource`"
  def coalesceDesc: String = desc

}

case class WhereClause(expr: Expr) extends ClauseExpression {

  addChild(expr)

  def desc: String = s"WHERE ${expr.desc}"
  def coalesceDesc: String = s"WHERE ${expr.coalesceDesc}"

  override def map(func: Expr => Expr): WhereClause = {
    WhereClause(func(expr))
  }

}

case class GroupbyClause(exprs: Seq[Expr], havingClauseOpt: Option[Expr])
    extends ClauseExpression {

  addChildren(exprs ++ havingClauseOpt.toSeq)

  def desc: String = {
    val gbs = exprs.map(_.desc).mkString(", ")
    havingClauseOpt match {
      case Some(having) => s"GROUP BY $gbs HAVING ${having.desc}"
      case _ => s"GROUP BY $gbs"
    }
  }
  def coalesceDesc: String = {
    val gbs = exprs.map(_.desc).mkString(", ")
    havingClauseOpt match {
      case Some(having) => s"GROUP BY $gbs HAVING ${having.coalesceDesc}"
      case _ => s"GROUP BY $gbs"
    }
  }

  def merge(other: GroupbyClause): GroupbyClause = {
    val newHavingClauseOpt = (havingClauseOpt, other.havingClauseOpt) match {
      case (Some(hc), Some(ohc)) =>
        val logical1 = LogicalFactorExpr(hc, withBracket = false, None)
        val logical2 = LogicalFactorExpr(ohc, withBracket = false, None)
        Some(BinaryLogicalExpr(logical1, ("AND", logical2) :: Nil))
      case (a @ Some(_), _) => a
      case (_, b @ Some(_)) => b
      case (_, _) => None
    }
    GroupbyClause(exprs ++ other.exprs, newHavingClauseOpt)
  }

  override def map(func: Expr => Expr): GroupbyClause = {
    GroupbyClause(exprs.map(func(_)), havingClauseOpt.map(func(_)))
  }

}

case class OrderItem(expr: Expr, orderOpt: Option[String]) extends Expr {
  addChild(expr)
  def desc: String = {
    orderOpt match {
      case Some(os) => s"${expr.desc} ${os.toUpperCase}"
      case _ => s"${expr.desc}"
    }
  }
  def coalesceDesc: String = desc

  override def map(func: Expr => Expr): OrderItem = {
    OrderItem(func(expr), orderOpt)
  }
}

case class OrderbyClause(items: Seq[OrderItem]) extends ClauseExpression {

  addChildren(items.map(_.expr))

  def desc: String = {
    val obs = items.map(_.desc).mkString(", ")
    s"ORDER BY $obs"
  }
  def coalesceDesc: String = {
    val obs = items.map(_.desc).mkString(", ")
    s"ORDER BY $obs"
  }

  override def map(func: Expr => Expr): OrderbyClause = {
    OrderbyClause(items.map(func(_).asInstanceOf[OrderItem]))
  }
}

case class SortbyClause(items: Seq[OrderItem]) extends ClauseExpression {

  addChildren(items.map(_.expr))

  def desc: String = {
    val obs = items.map(_.desc).mkString(", ")
    s"SORT BY $obs"
  }
  def coalesceDesc: String = {
    val obs = items.map(_.desc).mkString(", ")
    s"SORT BY $obs"
  }

  override def map(func: Expr => Expr): SortbyClause = {
    SortbyClause(items.map(func(_).asInstanceOf[OrderItem]))
  }
}

case class LimitClause(expr: Expr) extends ClauseExpression {

  addChild(expr)

  def desc: String = s"LIMIT ${expr.desc}"
  def coalesceDesc: String = s"LIMIT ${expr.coalesceDesc}"

  override def map(func: Expr => Expr): LimitClause = {
    LimitClause(func(expr))
  }
}

case class CombinedClause(
    selectClause: SelectClause,
    fromClauseOpt: Option[FromClause],
    tails: Seq[ClauseExpression])
    extends ClauseExpression {

  addChildren({
    val headClauses: Seq[ClauseExpression] = selectClause +: fromClauseOpt.toSeq
    headClauses ++ tails
  })

  def desc: String = {
    val selectDesc = s"SELECT ${selectClause.desc}"
    val fromDesc = fromClauseOpt.map(_.desc).mkString(" ")
    val headDesc = s"$selectDesc $fromDesc"
    tails.foldLeft(headDesc) { (head, tail) =>
      s"$head ${tail.desc}"
    }
  }
  def coalesceDesc: String = {
    val selectDesc = s"SELECT ${selectClause.coalesceDesc}"
    val fromDesc = fromClauseOpt.map(_.coalesceDesc).mkString(" ")
    val headDesc = s"$selectDesc $fromDesc"
    tails.foldLeft(headDesc) { (head, tail) =>
      s"$head ${tail.coalesceDesc}"
    }
  }

  override def map(func: Expr => Expr): CombinedClause = {
    CombinedClause(
      func(selectClause).asInstanceOf[SelectClause],
      fromClauseOpt.map(func(_).asInstanceOf[FromClause]),
      tails.map(func(_).asInstanceOf[ClauseExpression]))
  }
}

case class ProfilingClause(
    selectClause: SelectClause,
    fromClauseOpt: Option[FromClause],
    groupbyClauseOpt: Option[GroupbyClause],
    preGroupbyClauses: Seq[ClauseExpression],
    postGroupbyClauses: Seq[ClauseExpression])
    extends ClauseExpression {
  addChildren({
    val headClauses: Seq[ClauseExpression] = selectClause +: fromClauseOpt.toSeq
    groupbyClauseOpt match {
      case Some(gc) => (headClauses ++ preGroupbyClauses) ++ (gc +: postGroupbyClauses)
      case _ => (headClauses ++ preGroupbyClauses) ++ postGroupbyClauses
    }
  })

  def desc: String = {
    val selectDesc = selectClause.desc
    val fromDesc = fromClauseOpt.map(_.desc).mkString(" ")
    val groupbyDesc = groupbyClauseOpt.map(_.desc).mkString(" ")
    val preDesc = preGroupbyClauses.map(_.desc).mkString(" ")
    val postDesc = postGroupbyClauses.map(_.desc).mkString(" ")
    s"$selectDesc $fromDesc $preDesc $groupbyDesc $postDesc"
  }
  def coalesceDesc: String = {
    val selectDesc = selectClause.coalesceDesc
    val fromDesc = fromClauseOpt.map(_.coalesceDesc).mkString(" ")
    val groupbyDesc = groupbyClauseOpt.map(_.coalesceDesc).mkString(" ")
    val preDesc = preGroupbyClauses.map(_.coalesceDesc).mkString(" ")
    val postDesc = postGroupbyClauses.map(_.coalesceDesc).mkString(" ")
    s"$selectDesc $fromDesc $preDesc $groupbyDesc $postDesc"
  }

  override def map(func: Expr => Expr): ProfilingClause = {
    ProfilingClause(
      func(selectClause).asInstanceOf[SelectClause],
      fromClauseOpt.map(func(_).asInstanceOf[FromClause]),
      groupbyClauseOpt.map(func(_).asInstanceOf[GroupbyClause]),
      preGroupbyClauses.map(func(_).asInstanceOf[ClauseExpression]),
      postGroupbyClauses.map(func(_).asInstanceOf[ClauseExpression]))
  }
}

case class UniquenessClause(exprs: Seq[Expr]) extends ClauseExpression {
  addChildren(exprs)

  def desc: String = exprs.map(_.desc).mkString(", ")
  def coalesceDesc: String = exprs.map(_.coalesceDesc).mkString(", ")
  override def map(func: Expr => Expr): UniquenessClause = UniquenessClause(exprs.map(func(_)))
}

case class DistinctnessClause(exprs: Seq[Expr]) extends ClauseExpression {
  addChildren(exprs)

  def desc: String = exprs.map(_.desc).mkString(", ")
  def coalesceDesc: String = exprs.map(_.coalesceDesc).mkString(", ")
  override def map(func: Expr => Expr): DistinctnessClause =
    DistinctnessClause(exprs.map(func(_)))
}

case class TimelinessClause(exprs: Seq[Expr]) extends ClauseExpression {
  addChildren(exprs)

  def desc: String = exprs.map(_.desc).mkString(", ")
  def coalesceDesc: String = exprs.map(_.coalesceDesc).mkString(", ")
  override def map(func: Expr => Expr): TimelinessClause = TimelinessClause(exprs.map(func(_)))
}

case class CompletenessClause(exprs: Seq[Expr]) extends ClauseExpression {
  addChildren(exprs)

  def desc: String = exprs.map(_.desc).mkString(", ")
  def coalesceDesc: String = exprs.map(_.coalesceDesc).mkString(", ")
  override def map(func: Expr => Expr): CompletenessClause =
    CompletenessClause(exprs.map(func(_)))
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

/**
 * expr parsed by griffin dsl
 */
trait Expr extends TreeNode with ExprTag with Serializable {

  def desc: String

  def coalesceDesc: String

  def extractSelf: Expr = this

  // execution
  def map(func: Expr => Expr): Expr = func(this)

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait ExprTag { this: Expr =>
  var tag: String = ""
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

case class ExtraConditionExpr(cdtn: String) extends Expr {

  def desc: String = cdtn.toUpperCase

  def coalesceDesc: String = desc

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

case class FunctionExpr(
    functionName: String,
    args: Seq[Expr],
    extraConditionOpt: Option[ExtraConditionExpr],
    aliasOpt: Option[String])
    extends Expr
    with AliasableExpr {

  addChildren(args)

  def desc: String = {
    extraConditionOpt match {
      case Some(cdtn) => s"$functionName(${cdtn.desc} ${args.map(_.desc).mkString(", ")})"
      case _ => s"$functionName(${args.map(_.desc).mkString(", ")})"
    }
  }
  def coalesceDesc: String = desc
  def alias: Option[String] = {
    if (aliasOpt.isEmpty) {
      Some(functionName)
    } else aliasOpt
  }

  override def map(func: Expr => Expr): FunctionExpr = {
    FunctionExpr(
      functionName,
      args.map(func(_)),
      extraConditionOpt.map(func(_).asInstanceOf[ExtraConditionExpr]),
      aliasOpt)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

import com.hsbc.gbm.bd.clm.measure.utils.TimeUtil

trait LiteralExpr extends Expr {
  def coalesceDesc: String = desc
}

case class LiteralNullExpr(str: String) extends LiteralExpr {
  def desc: String = "NULL"
}

case class LiteralNanExpr(str: String) extends LiteralExpr {
  def desc: String = "NaN"
}

case class LiteralStringExpr(str: String) extends LiteralExpr {
  def desc: String = str
}

case class LiteralNumberExpr(str: String) extends LiteralExpr {
  def desc: String = {
    try {
      if (str.contains(".")) {
        str.toDouble.toString
      } else {
        str.toLong.toString
      }
    } catch {
      case _: Throwable => throw new Exception(s"$str is invalid number")
    }
  }
}

case class LiteralTimeExpr(str: String) extends LiteralExpr {
  def desc: String = {
    TimeUtil.milliseconds(str) match {
      case Some(t) => t.toString
      case _ => throw new Exception(s"$str is invalid time")
    }
  }
}

case class LiteralBooleanExpr(str: String) extends LiteralExpr {
  final val TrueRegex = """(?i)true""".r
  final val FalseRegex = """(?i)false""".r
  def desc: String = {
    str match {
      case TrueRegex() => true.toString
      case FalseRegex() => false.toString
      case _ => throw new Exception(s"$str is invalid boolean")
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait LogicalExpr extends Expr {}

case class InExpr(head: Expr, is: Boolean, range: Seq[Expr]) extends LogicalExpr {

  addChildren(head +: range)

  def desc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.desc}$notStr IN (${range.map(_.desc).mkString(", ")})"
  }
  def coalesceDesc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.coalesceDesc}$notStr IN (${range.map(_.coalesceDesc).mkString(", ")})"
  }

  override def map(func: Expr => Expr): InExpr = {
    InExpr(func(head), is, range.map(func(_)))
  }
}

case class BetweenExpr(head: Expr, is: Boolean, range: Seq[Expr]) extends LogicalExpr {

  range match {
    case first :: second :: _ => addChildren(head :: first :: second :: Nil)
    case _ => throw new Exception("between expression exception: range less than 2")
  }

  def desc: String = {
    val notStr = if (is) "" else " NOT"
    val rangeStr = range match {
      case first :: second :: _ => s"${first.desc} AND ${second.desc}"
      case _ => throw new Exception("between expression exception: range less than 2")
    }
    s"${head.desc}$notStr BETWEEN $rangeStr"
  }
  def coalesceDesc: String = {
    val notStr = if (is) "" else " NOT"
    val rangeStr = range match {
      case first :: second :: _ => s"${first.coalesceDesc} AND ${second.coalesceDesc}"
      case _ => throw new Exception("between expression exception: range less than 2")
    }
    s"${head.coalesceDesc}$notStr BETWEEN $rangeStr"
  }

  override def map(func: Expr => Expr): BetweenExpr = {
    BetweenExpr(func(head), is, range.map(func(_)))
  }
}

case class LikeExpr(head: Expr, is: Boolean, value: Expr) extends LogicalExpr {

  addChildren(head :: value :: Nil)

  def desc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.desc}$notStr LIKE ${value.desc}"
  }
  def coalesceDesc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.coalesceDesc}$notStr LIKE ${value.coalesceDesc}"
  }

  override def map(func: Expr => Expr): LikeExpr = {
    LikeExpr(func(head), is, func(value))
  }
}

case class RLikeExpr(head: Expr, is: Boolean, value: Expr) extends LogicalExpr {

  addChildren(head :: value :: Nil)

  def desc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.desc}$notStr RLIKE ${value.desc}"
  }
  def coalesceDesc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.coalesceDesc}$notStr RLIKE ${value.coalesceDesc}"
  }

  override def map(func: Expr => Expr): RLikeExpr = {
    RLikeExpr(func(head), is, func(value))
  }
}

case class IsNullExpr(head: Expr, is: Boolean) extends LogicalExpr {

  addChild(head)

  def desc: String = {
    val notStr = if (is) "" else " NOT"
    s"${head.desc} IS$notStr NULL"
  }
  def coalesceDesc: String = desc

  override def map(func: Expr => Expr): IsNullExpr = {
    IsNullExpr(func(head), is)
  }
}

case class IsNanExpr(head: Expr, is: Boolean) extends LogicalExpr {

  addChild(head)

  def desc: String = {
    val notStr = if (is) "" else "NOT "
    s"${notStr}isnan(${head.desc})"
  }
  def coalesceDesc: String = desc

  override def map(func: Expr => Expr): IsNanExpr = {
    IsNanExpr(func(head), is)
  }
}

// -----------

case class LogicalFactorExpr(factor: Expr, withBracket: Boolean, aliasOpt: Option[String])
    extends LogicalExpr
    with AliasableExpr {

  addChild(factor)

  def desc: String = if (withBracket) s"(${factor.desc})" else factor.desc
  def coalesceDesc: String = factor.coalesceDesc
  def alias: Option[String] = aliasOpt
  override def extractSelf: Expr = {
    if (aliasOpt.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): LogicalFactorExpr = {
    LogicalFactorExpr(func(factor), withBracket, aliasOpt)
  }
}

case class UnaryLogicalExpr(oprs: Seq[String], factor: LogicalExpr) extends LogicalExpr {

  addChild(factor)

  def desc: String = {
    oprs.foldRight(factor.desc) { (opr, fac) =>
      s"(${trans(opr)} $fac)"
    }
  }
  def coalesceDesc: String = {
    oprs.foldRight(factor.coalesceDesc) { (opr, fac) =>
      s"(${trans(opr)} $fac)"
    }
  }
  private def trans(s: String): String = {
    s match {
      case "!" => "NOT"
      case _ => s.toUpperCase
    }
  }
  override def extractSelf: Expr = {
    if (oprs.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): UnaryLogicalExpr = {
    UnaryLogicalExpr(oprs, func(factor).asInstanceOf[LogicalExpr])
  }
}

case class BinaryLogicalExpr(factor: LogicalExpr, tails: Seq[(String, LogicalExpr)])
    extends LogicalExpr {

  addChildren(factor +: tails.map(_._2))

  def desc: String = {
    val res = tails.foldLeft(factor.desc) { (fac, tail) =>
      val (opr, expr) = tail
      s"$fac ${trans(opr)} ${expr.desc}"
    }
    if (tails.size <= 0) res else s"$res"
  }
  def coalesceDesc: String = {
    val res = tails.foldLeft(factor.coalesceDesc) { (fac, tail) =>
      val (opr, expr) = tail
      s"$fac ${trans(opr)} ${expr.coalesceDesc}"
    }
    if (tails.size <= 0) res else s"$res"
  }
  private def trans(s: String): String = {
    s match {
      case "&&" => "AND"
      case "||" => "OR"
      case _ => s.trim.toUpperCase
    }
  }
  override def extractSelf: Expr = {
    if (tails.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): BinaryLogicalExpr = {
    BinaryLogicalExpr(func(factor).asInstanceOf[LogicalExpr], tails.map { pair =>
      (pair._1, func(pair._2).asInstanceOf[LogicalExpr])
    })
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait MathExpr extends Expr {}

case class MathFactorExpr(factor: Expr, withBracket: Boolean, aliasOpt: Option[String])
    extends MathExpr
    with AliasableExpr {

  addChild(factor)

  def desc: String = if (withBracket) s"(${factor.desc})" else factor.desc
  def coalesceDesc: String = factor.coalesceDesc
  def alias: Option[String] = aliasOpt
  override def extractSelf: Expr = {
    if (aliasOpt.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): MathFactorExpr = {
    MathFactorExpr(func(factor), withBracket, aliasOpt)
  }
}

case class UnaryMathExpr(oprs: Seq[String], factor: MathExpr) extends MathExpr {

  addChild(factor)

  def desc: String = {
    oprs.foldRight(factor.desc) { (opr, fac) =>
      s"($opr$fac)"
    }
  }
  def coalesceDesc: String = {
    oprs.foldRight(factor.coalesceDesc) { (opr, fac) =>
      s"($opr$fac)"
    }
  }
  override def extractSelf: Expr = {
    if (oprs.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): UnaryMathExpr = {
    UnaryMathExpr(oprs, func(factor).asInstanceOf[MathExpr])
  }
}

case class BinaryMathExpr(factor: MathExpr, tails: Seq[(String, MathExpr)]) extends MathExpr {

  addChildren(factor +: tails.map(_._2))

  def desc: String = {
    val res = tails.foldLeft(factor.desc) { (fac, tail) =>
      val (opr, expr) = tail
      s"$fac $opr ${expr.desc}"
    }
    if (tails.size <= 0) res else s"$res"
  }
  def coalesceDesc: String = {
    val res = tails.foldLeft(factor.coalesceDesc) { (fac, tail) =>
      val (opr, expr) = tail
      s"$fac $opr ${expr.coalesceDesc}"
    }
    if (tails.size <= 0) res else s"$res"
  }
  override def extractSelf: Expr = {
    if (tails.nonEmpty) this
    else factor.extractSelf
  }

  override def map(func: Expr => Expr): BinaryMathExpr = {
    BinaryMathExpr(func(factor).asInstanceOf[MathExpr], tails.map { pair =>
      (pair._1, func(pair._2).asInstanceOf[MathExpr])
    })
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

trait HeadExpr extends Expr with AliasableExpr {
  def alias: Option[String] = None
}

case class DataSourceHeadExpr(name: String) extends HeadExpr {
  def desc: String = s"`$name`"
  def coalesceDesc: String = desc
}

case class FieldNameHeadExpr(field: String) extends HeadExpr {
  def desc: String = s"`$field`"
  def coalesceDesc: String = desc
  override def alias: Option[String] = Some(field)
}

case class AllSelectHeadExpr() extends HeadExpr {
  def desc: String = "*"
  def coalesceDesc: String = desc
}

case class OtherHeadExpr(expr: Expr) extends HeadExpr {

  addChild(expr)

  def desc: String = expr.desc
  def coalesceDesc: String = expr.coalesceDesc
  override def alias: Option[String] = Some(expr.desc)

  override def map(func: Expr => Expr): OtherHeadExpr = {
    OtherHeadExpr(func(expr))
  }
}

// -------------

trait SelectExpr extends Expr with AliasableExpr {}

case class AllFieldsSelectExpr() extends SelectExpr {
  def desc: String = ".*"
  def coalesceDesc: String = desc
  def alias: Option[String] = None
}

case class FieldSelectExpr(field: String) extends SelectExpr {
  def desc: String = s".`$field`"
  def coalesceDesc: String = desc
  override def alias: Option[String] = Some(field)
}

case class IndexSelectExpr(index: Expr) extends SelectExpr {

  addChild(index)

  def desc: String = s"[${index.desc}]"
  def coalesceDesc: String = desc
  def alias: Option[String] = Some(index.desc)

  override def map(func: Expr => Expr): IndexSelectExpr = {
    IndexSelectExpr(func(index))
  }
}

case class FunctionSelectExpr(functionName: String, args: Seq[Expr]) extends SelectExpr {

  addChildren(args)

  def desc: String = ""
  def coalesceDesc: String = desc
  def alias: Option[String] = Some(functionName)

  override def map(func: Expr => Expr): FunctionSelectExpr = {
    FunctionSelectExpr(functionName, args.map(func(_)))
  }
}

// -------------

case class SelectionExpr(head: HeadExpr, selectors: Seq[SelectExpr], aliasOpt: Option[String])
    extends SelectExpr {

  addChildren(head +: selectors)

  def desc: String = {
    selectors.foldLeft(head.desc) { (hd, sel) =>
      sel match {
        case FunctionSelectExpr(funcName, args) =>
          val nargs = hd +: args.map(_.desc)
          s"$funcName(${nargs.mkString(", ")})"
        case _ => s"$hd${sel.desc}"
      }
    }
  }
  def coalesceDesc: String = {
    selectors.lastOption match {
      case None => desc
      case Some(_: FunctionSelectExpr) => desc
      case _ => s"coalesce($desc, '')"
    }
  }
  def alias: Option[String] = {
    if (aliasOpt.isEmpty) {
      val aliasSeq = (head +: selectors).flatMap(_.alias)
      if (aliasSeq.nonEmpty) Some(aliasSeq.mkString("_")) else None
    } else aliasOpt
  }

  override def map(func: Expr => Expr): SelectionExpr = {
    SelectionExpr(
      func(head).asInstanceOf[HeadExpr],
      selectors.map(func(_).asInstanceOf[SelectExpr]),
      aliasOpt)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr

import scala.reflect.ClassTag

trait TreeNode extends Serializable {

  var children: Seq[TreeNode] = Seq[TreeNode]()

  def addChild(expr: TreeNode): Unit = { children :+= expr }
  def addChildren(exprs: Seq[TreeNode]): Unit = { children ++= exprs }

  def preOrderTraverseDepthFirst[T, A <: TreeNode](z: T)(seqOp: (A, T) => T, combOp: (T, T) => T)(
      implicit tag: ClassTag[A]): T = {

    val clazz = tag.runtimeClass
    if (clazz.isAssignableFrom(this.getClass)) {
      val tv = seqOp(this.asInstanceOf[A], z)
      children.foldLeft(combOp(z, tv)) { (ov, tn) =>
        combOp(ov, tn.preOrderTraverseDepthFirst(z)(seqOp, combOp))
      }
    } else {
      z
    }

  }
  def postOrderTraverseDepthFirst[T, A <: TreeNode](
      z: T)(seqOp: (A, T) => T, combOp: (T, T) => T)(implicit tag: ClassTag[A]): T = {

    val clazz = tag.runtimeClass
    if (clazz.isAssignableFrom(this.getClass)) {
      val cv = children.foldLeft(z) { (ov, tn) =>
        combOp(ov, tn.postOrderTraverseDepthFirst(z)(seqOp, combOp))
      }
      combOp(z, seqOp(this.asInstanceOf[A], cv))
    } else {
      z
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/dsl/parser: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.parser

import scala.util.parsing.combinator.JavaTokenParsers

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

/**
 * basic parser for sql like syntax
 */
trait BasicParser extends JavaTokenParsers with Serializable {

  val dataSourceNames: Seq[String]
  val functionNames: Seq[String]

  private def trim(str: String): String = {
    val regex = """`(.*)`""".r
    str match {
      case regex(s) => s
      case _ => str
    }
  }

  // scalastyle:off
  /**
   * BNF for basic parser
   *
   * -- literal --
   * <literal> ::= <literal-string> | <literal-number> | <literal-time> | <literal-boolean> | <literal-null> | <literal-nan>
   * <literal-string> ::= <any-string>
   * <literal-number> ::= <integer> | <double>
   * <literal-time> ::= <integer> ("d"|"h"|"m"|"s"|"ms")
   * <literal-boolean> ::= true | false
   * <literal-null> ::= null
   * <literal-nan> ::= nan
   *
   * -- selection --
   * <selection> ::= <selection-head> [ <field-sel> | <index-sel> | <function-sel> ]* [<as-alias>]?
   * <selection-head> ::= ("data source name registered") | <function> | <field-name> | <all-selection>
   * <field-sel> ::= "." <field-name> | "[" <quote-field-name> "]"
   * <index-sel> ::= "[" <arg> "]"
   * <function-sel> ::= "." <function-name> "(" [<arg>]? [, <arg>]* ")"
   * <arg> ::= <math-expr>
   *
   * -- as alias --
   * <as-alias> ::= <as> <field-name>
   *
   * -- math expr --
   * <math-factor> ::= <literal> | <function> | <selection> | "(" <math-expr> ")" [<as-alias>]?
   * <unary-math-expr> ::= [<unary-opr>]* <math-factor>
   * <binary-math-expr> ::= <unary-math-expr> [<binary-opr> <unary-math-expr>]+
   * <math-expr> ::= <binary-math-expr>
   *
   * -- logical expr --
   * <in-expr> ::= <math-expr> [<not>]? <in> <range-expr>
   * <between-expr> ::= <math-expr> [<not>]? <between> (<math-expr> <and> <math-expr> | <range-expr>)
   * <range-expr> ::= "(" [<math-expr>]? [, <math-expr>]+ ")"
   * <like-expr> ::= <math-expr> [<not>]? <like> <math-expr>
   * <rlike-expr> ::= <math-expr> [<not>]? <rlike> <math-expr>
   * <is-null-expr> ::= <math-expr> <is> [<not>]? <null>
   * <is-nan-expr> ::= <math-expr> <is> [<not>]? <nan>
   *
   * <logical-factor> ::= <math-expr> | <in-expr> | <between-expr> | <like-expr> | <is-null-expr> | <is-nan-expr> | "(" <logical-expr> ")" [<as-alias>]?
   * <unary-logical-expr> ::= [<unary-logical-opr>]* <logical-factor>
   * <binary-logical-expr> ::= <unary-logical-expr> [<binary-logical-opr> <unary-logical-expr>]+
   * <logical-expr> ::= <binary-logical-expr>
   *
   * -- expression --
   * <expr> = <math-expr> | <logical-expr>
   *
   * -- function expr --
   * <function> ::= <function-name> "(" [<arg>] [, <arg>]+ ")" [<as-alias>]?
   * <function-name> ::= ("function name registered")
   * <arg> ::= <expr>
   *
   * -- clauses --
   * <select-clause> = <expr> [, <expr>]*
   * <where-clause> = <where> <expr>
   * <from-clause> = <from> ("data source name registered")
   * <having-clause> = <having> <expr>
   * <groupby-clause> = <group> <by> <expr> [ <having-clause> ]?
   * <orderby-item> = <expr> [ <DESC> ]?
   * <orderby-clause> = <order> <by> <orderby-item> [ , <orderby-item> ]*
   * <limit-clause> = <limit> <expr>
   *
   * -- combined clauses --
   * <combined-clauses> = <select-clause> [ <from-clause> ]+ [ <where-clause> ]+ [ <groupby-clause> ]+ [ <orderby-clause> ]+ [ <limit-clause> ]+
   */
  protected def genDataSourceNamesParser(names: Seq[String]): Parser[String] = {
    names.reverse
      .map { fn =>
        s"""(?i)`$fn`|$fn""".r: Parser[String]
      }
      .reduce(_ | _)
  }
  protected def genFunctionNamesParser(names: Seq[String]): Parser[String] = {
    names.reverse
      .map { fn =>
        s"""(?i)$fn""".r: Parser[String]
      }
      .reduce(_ | _)
  }

  object Literal {
    val NULL: Parser[String] = """(?i)null""".r
    val NAN: Parser[String] = """(?i)nan""".r
  }
  import Literal._

  object Operator {
    val MATH_UNARY: Parser[String] = "+" | "-"
    val MATH_BINARIES: Seq[Parser[String]] = Seq("*" | "/" | "%", "+" | "-")

    val NOT: Parser[String] = """(?i)not\s""".r | "!"
    val AND: Parser[String] = """(?i)and\s""".r | "&&"
    val OR: Parser[String] = """(?i)or\s""".r | "||"
    val IN: Parser[String] = """(?i)in\s""".r
    val BETWEEN: Parser[String] = """(?i)between\s""".r
    val AND_ONLY: Parser[String] = """(?i)and\s""".r
    val IS: Parser[String] = """(?i)is\s""".r
    val LIKE: Parser[String] = """(?i)like\s""".r
    val RLIKE: Parser[String] = """(?i)rlike\s""".r
    val COMPARE: Parser[String] = "=" | "!=" | "<>" | "<=" | ">=" | "<" | ">"
    val LOGICAL_UNARY: Parser[String] = NOT
    val LOGICAL_BINARIES: Seq[Parser[String]] = Seq(COMPARE, AND, OR)

    val LSQBR: Parser[String] = "["
    val RSQBR: Parser[String] = "]"
    val LBR: Parser[String] = "("
    val RBR: Parser[String] = ")"

    val DOT: Parser[String] = "."
    val ALLSL: Parser[String] = "*"
    val SQUOTE: Parser[String] = "'"
    val DQUOTE: Parser[String] = "\""
    val UQUOTE: Parser[String] = "`"
    val COMMA: Parser[String] = ","

    val SELECT: Parser[String] = """(?i)select\s""".r
    val DISTINCT: Parser[String] = """(?i)distinct""".r
//    val ALL: Parser[String] = """(?i)all""".r
    val FROM: Parser[String] = """(?i)from\s""".r
    val AS: Parser[String] = """(?i)as\s""".r
    val WHERE: Parser[String] = """(?i)where\s""".r
    val GROUP: Parser[String] = """(?i)group\s""".r
    val ORDER: Parser[String] = """(?i)order\s""".r
    val SORT: Parser[String] = """(?i)sort\s""".r
    val BY: Parser[String] = """(?i)by\s""".r
    val DESC: Parser[String] = """(?i)desc""".r
    val ASC: Parser[String] = """(?i)asc""".r
    val HAVING: Parser[String] = """(?i)having\s""".r
    val LIMIT: Parser[String] = """(?i)limit\s""".r
  }
  import Operator._

  object Strings {
    def AnyString: Parser[String] = """"(?:"|[^"])*"""".r | """'(?:'|[^'])*'""".r
    def SimpleTableFieldName: Parser[String] = """[a-zA-Z_]\w*""".r
    def UnQuoteTableFieldName: Parser[String] = """`(?:[\\][`]|[^`])*`""".r
//    def FieldName: Parser[String] = UnQuoteTableFieldName | SimpleTableFieldName
    def DataSourceName: Parser[String] = genDataSourceNamesParser(dataSourceNames)
    def FunctionName: Parser[String] = genFunctionNamesParser(functionNames)

    def IntegerNumber: Parser[String] = """[+\-]?\d+""".r
    def DoubleNumber: Parser[String] = """[+\-]?(\.\d+|\d+\.\d*)""".r
    def IndexNumber: Parser[String] = IntegerNumber

    def TimeString: Parser[String] = """([+\-]?\d+)(d|h|m|s|ms)""".r
    def BooleanString: Parser[String] = """(?i)true|false""".r
  }
  import Strings._

  /**
   * -- literal --
   * <literal> ::= <literal-string> | <literal-number> | <literal-time> | <literal-boolean> | <literal-null> | <literal-nan>
   * <literal-string> ::= <any-string>
   * <literal-number> ::= <integer> | <double>
   * <literal-time> ::= <integer> ("d"|"h"|"m"|"s"|"ms")
   * <literal-boolean> ::= true | false
   * <literal-null> ::= null
   * <literal-nan> ::= nan
   */
  def literal: Parser[LiteralExpr] =
    literalNull | literalNan | literalBoolean | literalString | literalTime | literalNumber
  def literalNull: Parser[LiteralNullExpr] = NULL ^^ { LiteralNullExpr }
  def literalNan: Parser[LiteralNanExpr] = NAN ^^ { LiteralNanExpr }
  def literalString: Parser[LiteralStringExpr] = AnyString ^^ { LiteralStringExpr }
  def literalNumber: Parser[LiteralNumberExpr] = (DoubleNumber | IntegerNumber) ^^ {
    LiteralNumberExpr
  }
  def literalTime: Parser[LiteralTimeExpr] = TimeString ^^ { LiteralTimeExpr }
  def literalBoolean: Parser[LiteralBooleanExpr] = BooleanString ^^ { LiteralBooleanExpr }

  /**
   * -- selection --
   * <selection> ::= <selection-head> [ <field-sel> | <index-sel> | <function-sel> ]* [<as-alias>]?
   * <selection-head> ::= ("data source name registered") | <function> | <field-name> | <all-selection>
   * <field-sel> ::= "." <field-name> | "[" <quote-field-name> "]"
   * <index-sel> ::= "[" <arg> "]"
   * <function-sel> ::= "." <function-name> "(" [<arg>]? [, <arg>]* ")"
   * <arg> ::= <math-expr>
   */
  def selection: Parser[SelectionExpr] = selectionHead ~ rep(selector) ~ opt(asAlias) ^^ {
    case head ~ sels ~ aliasOpt => SelectionExpr(head, sels, aliasOpt)
  }
  def selectionHead: Parser[HeadExpr] =
    DataSourceName ^^ { ds =>
      DataSourceHeadExpr(trim(ds))
    } | function ^^ {
      OtherHeadExpr(_)
    } | SimpleTableFieldName ^^ {
      FieldNameHeadExpr
    } | UnQuoteTableFieldName ^^ { s =>
      FieldNameHeadExpr(trim(s))
    } | ALLSL ^^ { _ =>
      AllSelectHeadExpr()
    }
  def selector: Parser[SelectExpr] = functionSelect | allFieldsSelect | fieldSelect | indexSelect
  def allFieldsSelect: Parser[AllFieldsSelectExpr] = DOT ~> ALLSL ^^ { _ =>
    AllFieldsSelectExpr()
  }
  def fieldSelect: Parser[FieldSelectExpr] =
    DOT ~> (SimpleTableFieldName ^^ {
      FieldSelectExpr
    } | UnQuoteTableFieldName ^^ { s =>
      FieldSelectExpr(trim(s))
    })
  def indexSelect: Parser[IndexSelectExpr] = LSQBR ~> argument <~ RSQBR ^^ { IndexSelectExpr }
  def functionSelect: Parser[FunctionSelectExpr] =
    DOT ~ FunctionName ~ LBR ~ repsep(argument, COMMA) ~ RBR ^^ {
      case _ ~ name ~ _ ~ args ~ _ => FunctionSelectExpr(name, args)
    }

  /**
   * -- as alias --
   * <as-alias> ::= <as> <field-name>
   */
  def asAlias: Parser[String] = AS ~> (SimpleTableFieldName | UnQuoteTableFieldName ^^ { trim })

  /**
   * -- math expr --
   * <math-factor> ::= <literal> | <function> | <selection> | "(" <math-expr> ")" [<as-alias>]?
   * <unary-math-expr> ::= [<unary-opr>]* <math-factor>
   * <binary-math-expr> ::= <unary-math-expr> [<binary-opr> <unary-math-expr>]+
   * <math-expr> ::= <binary-math-expr>
   */
  def mathFactor: Parser[MathExpr] =
    (literal | function | selection) ^^ {
      MathFactorExpr(_, withBracket = false, None)
    } | LBR ~ mathExpression ~ RBR ~ opt(asAlias) ^^ {
      case _ ~ expr ~ _ ~ aliasOpt => MathFactorExpr(expr, withBracket = true, aliasOpt)
    }
  def unaryMathExpression: Parser[MathExpr] = rep(MATH_UNARY) ~ mathFactor ^^ {
    case Nil ~ a => a
    case list ~ a => UnaryMathExpr(list, a)
  }
  def binaryMathExpressions: Seq[Parser[MathExpr]] =
    MATH_BINARIES.foldLeft(List[Parser[MathExpr]](unaryMathExpression)) {
      (parsers, binaryParser) =>
        val pre = parsers.headOption.orNull
        val cur = pre ~ rep(binaryParser ~ pre) ^^ {
          case a ~ Nil => a
          case a ~ list => BinaryMathExpr(a, list.map(c => (c._1, c._2)))
        }
        cur :: parsers
    }
  def mathExpression: Parser[MathExpr] = binaryMathExpressions.headOption.orNull

  /**
   * -- logical expr --
   * <in-expr> ::= <math-expr> [<not>]? <in> <range-expr>
   * <between-expr> ::= <math-expr> [<not>]? <between> (<math-expr> <and> <math-expr> | <range-expr>)
   * <range-expr> ::= "(" [<math-expr>]? [, <math-expr>]+ ")"
   * <like-expr> ::= <math-expr> [<not>]? <like> <math-expr>
   * <rlike-expr> ::= <math-expr> [<not>]? <rlike> <math-expr>
   * <is-null-expr> ::= <math-expr> <is> [<not>]? <null>
   * <is-nan-expr> ::= <math-expr> <is> [<not>]? <nan>
   *
   * <logical-factor> ::= <math-expr> | <in-expr> | <between-expr> | <like-expr> | <is-null-expr> | <is-nan-expr> | "(" <logical-expr> ")" [<as-alias>]?
   * <unary-logical-expr> ::= [<unary-logical-opr>]* <logical-factor>
   * <binary-logical-expr> ::= <unary-logical-expr> [<binary-logical-opr> <unary-logical-expr>]+
   * <logical-expr> ::= <binary-logical-expr>
   */
  def inExpr: Parser[LogicalExpr] =
    mathExpression ~ opt(NOT) ~ IN ~ LBR ~ repsep(mathExpression, COMMA) ~ RBR ^^ {
      case head ~ notOpt ~ _ ~ _ ~ list ~ _ => InExpr(head, notOpt.isEmpty, list)
    }
  def betweenExpr: Parser[LogicalExpr] =
    mathExpression ~ opt(NOT) ~ BETWEEN ~ LBR ~ repsep(mathExpression, COMMA) ~ RBR ^^ {
      case head ~ notOpt ~ _ ~ _ ~ list ~ _ => BetweenExpr(head, notOpt.isEmpty, list)
    } | mathExpression ~ opt(NOT) ~ BETWEEN ~ mathExpression ~ AND_ONLY ~ mathExpression ^^ {
      case head ~ notOpt ~ _ ~ first ~ _ ~ second =>
        BetweenExpr(head, notOpt.isEmpty, Seq(first, second))
    }
  def likeExpr: Parser[LogicalExpr] = mathExpression ~ opt(NOT) ~ LIKE ~ mathExpression ^^ {
    case head ~ notOpt ~ _ ~ value => LikeExpr(head, notOpt.isEmpty, value)
  }
  def rlikeExpr: Parser[LogicalExpr] = mathExpression ~ opt(NOT) ~ RLIKE ~ mathExpression ^^ {
    case head ~ notOpt ~ _ ~ value => RLikeExpr(head, notOpt.isEmpty, value)
  }
  def isNullExpr: Parser[LogicalExpr] = mathExpression ~ IS ~ opt(NOT) ~ NULL ^^ {
    case head ~ _ ~ notOpt ~ _ => IsNullExpr(head, notOpt.isEmpty)
  }
  def isNanExpr: Parser[LogicalExpr] = mathExpression ~ IS ~ opt(NOT) ~ NAN ^^ {
    case head ~ _ ~ notOpt ~ _ => IsNanExpr(head, notOpt.isEmpty)
  }

  def logicalFactor: Parser[LogicalExpr] =
    (inExpr | betweenExpr | likeExpr | rlikeExpr | isNullExpr | isNanExpr | mathExpression) ^^ {
      LogicalFactorExpr(_, withBracket = false, None)
    } | LBR ~ logicalExpression ~ RBR ~ opt(asAlias) ^^ {
      case _ ~ expr ~ _ ~ aliasOpt => LogicalFactorExpr(expr, withBracket = true, aliasOpt)
    }
  def unaryLogicalExpression: Parser[LogicalExpr] = rep(LOGICAL_UNARY) ~ logicalFactor ^^ {
    case Nil ~ a => a
    case list ~ a => UnaryLogicalExpr(list, a)
  }
  def binaryLogicalExpressions: Seq[Parser[LogicalExpr]] =
    LOGICAL_BINARIES.foldLeft(List[Parser[LogicalExpr]](unaryLogicalExpression)) {
      (parsers, binaryParser) =>
        val pre = parsers.headOption.orNull
        val cur = pre ~ rep(binaryParser ~ pre) ^^ {
          case a ~ Nil => a
          case a ~ list => BinaryLogicalExpr(a, list.map(c => (c._1, c._2)))
        }
        cur :: parsers
    }
  def logicalExpression: Parser[LogicalExpr] = binaryLogicalExpressions.headOption.orNull

  /**
   * -- expression --
   * <expr> = <math-expr> | <logical-expr>
   */
  def expression: Parser[Expr] = logicalExpression | mathExpression

  /**
   * -- function expr --
   * <function> ::= <function-name> "(" [<arg>] [, <arg>]+ ")" [<as-alias>]?
   * <function-name> ::= ("function name registered")
   * <arg> ::= <expr>
   */
  def function: Parser[FunctionExpr] =
    FunctionName ~ LBR ~ opt(DISTINCT) ~ repsep(argument, COMMA) ~ RBR ~ opt(asAlias) ^^ {
      case name ~ _ ~ extraCdtnOpt ~ args ~ _ ~ aliasOpt =>
        FunctionExpr(name, args, extraCdtnOpt.map(ExtraConditionExpr), aliasOpt)
    }
  def argument: Parser[Expr] = expression

  /**
   * -- clauses --
   * <select-clause> = <expr> [, <expr>]*
   * <where-clause> = <where> <expr>
   * <from-clause> = <from> ("data source name registered")
   * <having-clause> = <having> <expr>
   * <groupby-clause> = <group> <by> <expr> [ <having-clause> ]?
   * <orderby-item> = <expr> [ <DESC> ]?
   * <orderby-clause> = <order> <by> <orderby-item> [ , <orderby-item> ]*
   * <limit-clause> = <limit> <expr>
   */
  def selectClause: Parser[SelectClause] =
    opt(SELECT) ~> opt(DISTINCT) ~ rep1sep(expression, COMMA) ^^ {
      case extraCdtnOpt ~ exprs => SelectClause(exprs, extraCdtnOpt.map(ExtraConditionExpr))
    }
  def fromClause: Parser[FromClause] = FROM ~> DataSourceName ^^ { ds =>
    FromClause(trim(ds))
  }
  def whereClause: Parser[WhereClause] = WHERE ~> expression ^^ { WhereClause }
  def havingClause: Parser[Expr] = HAVING ~> expression
  def groupbyClause: Parser[GroupbyClause] =
    GROUP ~ BY ~ rep1sep(expression, COMMA) ~ opt(havingClause) ^^ {
      case _ ~ _ ~ cols ~ havingOpt => GroupbyClause(cols, havingOpt)
    }
  def orderItem: Parser[OrderItem] = expression ~ opt(DESC | ASC) ^^ {
    case expr ~ orderOpt => OrderItem(expr, orderOpt)
  }
  def orderbyClause: Parser[OrderbyClause] = ORDER ~ BY ~ rep1sep(orderItem, COMMA) ^^ {
    case _ ~ _ ~ cols => OrderbyClause(cols)
  }
  def sortbyClause: Parser[SortbyClause] = SORT ~ BY ~ rep1sep(orderItem, COMMA) ^^ {
    case _ ~ _ ~ cols => SortbyClause(cols)
  }
  def limitClause: Parser[LimitClause] = LIMIT ~> expression ^^ { LimitClause }

  /**
   * -- combined clauses --
   * <combined-clauses> = <select-clause> [ <from-clause> ]+ [ <where-clause> ]+ [ <groupby-clause> ]+ [ <orderby-clause> ]+ [ <limit-clause> ]+
   */
  def combinedClause: Parser[CombinedClause] =
    selectClause ~ opt(fromClause) ~ opt(whereClause) ~
      opt(groupbyClause) ~ opt(orderbyClause) ~ opt(limitClause) ^^ {
      case sel ~ fromOpt ~ whereOpt ~ groupbyOpt ~ orderbyOpt ~ limitOpt =>
        val tails = Seq(whereOpt, groupbyOpt, orderbyOpt, limitOpt).flatten
        CombinedClause(sel, fromOpt, tails)
    }
  // scalastyle:on
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.parser

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.{ProfilingClause, UniquenessClause}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DqType._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

/**
 * parser for griffin dsl rule
 */
case class GriffinDslParser(dataSourceNames: Seq[String], functionNames: Seq[String])
    extends BasicParser {

  import Operator._

  /**
   * -- profiling clauses --
   * <profiling-clauses> = <select-clause> [ <from-clause> ]+ [ <where-clause> ]+
   *  [ <groupby-clause> ]+ [ <orderby-clause> ]+ [ <limit-clause> ]+
   */
  def profilingClause: Parser[ProfilingClause] =
    selectClause ~ opt(fromClause) ~ opt(whereClause) ~
      opt(groupbyClause) ~ opt(orderbyClause) ~ opt(limitClause) ^^ {
      case sel ~ fromOpt ~ whereOpt ~ groupbyOpt ~ orderbyOpt ~ limitOpt =>
        val preClauses = Seq(whereOpt).flatten
        val postClauses = Seq(orderbyOpt, limitOpt).flatten
        ProfilingClause(sel, fromOpt, groupbyOpt, preClauses, postClauses)
    }

  /**
   * -- uniqueness clauses --
   * <uniqueness-clauses> = <expr> [, <expr>]+
   */
  def uniquenessClause: Parser[UniquenessClause] =
    rep1sep(expression, Operator.COMMA) ^^ (exprs => UniquenessClause(exprs))

  /**
   * -- distinctness clauses --
   * <sqbr-expr> = "[" <expr> "]"
   * <dist-expr> = <sqbr-expr> | <expr>
   * <distinctness-clauses> = <distExpr> [, <distExpr>]+
   */
  def sqbrExpr: Parser[Expr] = LSQBR ~> expression <~ RSQBR ^^ { expr =>
    expr.tag = "[]"; expr
  }
  def distExpr: Parser[Expr] = expression | sqbrExpr
  def distinctnessClause: Parser[DistinctnessClause] =
    rep1sep(distExpr, Operator.COMMA) ^^ (exprs => DistinctnessClause(exprs))

  /**
   * -- timeliness clauses --
   * <timeliness-clauses> = <expr> [, <expr>]+
   */
  def timelinessClause: Parser[TimelinessClause] =
    rep1sep(expression, Operator.COMMA) ^^ (exprs => TimelinessClause(exprs))

  /**
   * -- completeness clauses --
   * <completeness-clauses> = <expr> [, <expr>]+
   */
  def completenessClause: Parser[CompletenessClause] =
    rep1sep(expression, Operator.COMMA) ^^ (exprs => CompletenessClause(exprs))

  def parseRule(rule: String, dqType: DqType): ParseResult[Expr] = {
    val rootExpr = dqType match {
      case Accuracy => logicalExpression
      case Profiling => profilingClause
      case Uniqueness => uniquenessClause
      case Distinct => distinctnessClause
      case Timeliness => timelinessClause
      case Completeness => completenessClause
      case _ => expression
    }
    parseAll(rootExpr, rule)
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/dsl/transform: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.DefaultFlattenType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.AccuracyAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.{DataFrameOps, DataFrameOpsTransformStep, SparkSqlTransformStep}
import com.hsbc.gbm.bd.clm.measure.step.transform.DataFrameOps.AccuracyOprKeys
import com.hsbc.gbm.bd.clm.measure.step.write.{DataSourceUpdateWriteStep, MetricWriteStep, RecordWriteStep}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * generate accuracy dq steps
 */
case class AccuracyExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
    extends Expr2DQSteps {

  private object AccuracyKeys {
    val _source = "source"
    val _target = "target"
    val _miss = "miss"
    val _total = "total"
    val _matched = "matched"
    val _matchedFraction = "matchedFraction"
  }
  import AccuracyKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val accuracyExpr = expr.asInstanceOf[LogicalExpr]

    val sourceName = details.getString(_source, context.getDataSourceName(0))
    val targetName = details.getString(_target, context.getDataSourceName(1))
    val analyzer = AccuracyAnalyzer(accuracyExpr, sourceName, targetName)

    val procType = context.procType
    val timestamp = context.contextId.timestamp

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else {
      // 1. miss record
      val missRecordsTableName = "__missRecords"
      val selClause = s"`$sourceName`.*"
      val missRecordsSql =
        if (!context.runTimeTableRegister.existsTable(targetName)) {
          warn(s"[$timestamp] data source $targetName not exists")
          s"SELECT $selClause FROM `$sourceName`"
        } else {
          val onClause = expr.coalesceDesc
          val sourceIsNull = analyzer.sourceSelectionExprs
            .map { sel =>
              s"${sel.desc} IS NULL"
            }
            .mkString(" AND ")
          val targetIsNull = analyzer.targetSelectionExprs
            .map { sel =>
              s"${sel.desc} IS NULL"
            }
            .mkString(" AND ")
          val whereClause = s"(NOT ($sourceIsNull)) AND ($targetIsNull)"
          s"SELECT $selClause FROM `$sourceName` " +
            s"LEFT JOIN `$targetName` ON $onClause WHERE $whereClause"
        }

      val missRecordsWriteSteps = procType match {
        case BatchProcessType =>
          RecordWriteStep(ruleParam.getOutDfName(), missRecordsTableName)
        case StreamingProcessType =>
          val dsName =
            ruleParam
              .getOutputOpt(DscUpdateOutputType)
              .flatMap(_.getNameOpt)
              .getOrElse(sourceName)
          DataSourceUpdateWriteStep(dsName, missRecordsTableName)
      }

      val missRecordsTransStep =
        SparkSqlTransformStep(
          missRecordsTableName,
          missRecordsSql,
          emptyMap,
          Some(missRecordsWriteSteps),
          cache = true)

      // 2. miss count
      val missCountTableName = "__missCount"
      val missColName = details.getStringOrKey(_miss)
      val missCountSql = procType match {
        case BatchProcessType =>
          s"SELECT COUNT(*) AS `$missColName` FROM `$missRecordsTableName`"
        case StreamingProcessType =>
          s"SELECT `${ConstantColumns.tmst}`,COUNT(*) AS `$missColName` " +
            s"FROM `$missRecordsTableName` GROUP BY `${ConstantColumns.tmst}`"
      }
      val missCountTransStep =
        SparkSqlTransformStep(missCountTableName, missCountSql, emptyMap)
      missCountTransStep.parentSteps += missRecordsTransStep

      // 3. total count
      val totalCountTableName = "__totalCount"
      val totalColName = details.getStringOrKey(_total)
      val totalCountSql = procType match {
        case BatchProcessType =>
          s"SELECT COUNT(*) AS `$totalColName` FROM `$sourceName`"
        case StreamingProcessType =>
          s"SELECT `${ConstantColumns.tmst}`, COUNT(*) AS `$totalColName` " +
            s"FROM `$sourceName` GROUP BY `${ConstantColumns.tmst}`"
      }
      val totalCountTransStep =
        SparkSqlTransformStep(totalCountTableName, totalCountSql, emptyMap)

      // 4. accuracy metric
      val accuracyTableName = ruleParam.getOutDfName()
      val matchedColName = details.getStringOrKey(_matched)
      val matchedFractionColName = details.getStringOrKey(_matchedFraction)
      val accuracyMetricSql = procType match {
        case BatchProcessType =>
          s"""
             SELECT A.total AS `$totalColName`,
                    A.miss AS `$missColName`,
                    (A.total - A.miss) AS `$matchedColName`,
                    coalesce( (A.total - A.miss) / A.total, 1.0) AS `$matchedFractionColName`
             FROM (
               SELECT `$totalCountTableName`.`$totalColName` AS total,
                      coalesce(`$missCountTableName`.`$missColName`, 0) AS miss
               FROM `$totalCountTableName` LEFT JOIN `$missCountTableName`
             ) AS A
         """
        case StreamingProcessType =>
          // scalastyle:off
          s"""
             |SELECT `$totalCountTableName`.`${ConstantColumns.tmst}` AS `${ConstantColumns.tmst}`,
             |`$totalCountTableName`.`$totalColName` AS `$totalColName`,
             |coalesce(`$missCountTableName`.`$missColName`, 0) AS `$missColName`,
             |(`$totalCountTableName`.`$totalColName` - coalesce(`$missCountTableName`.`$missColName`, 0)) AS `$matchedColName`
             |FROM `$totalCountTableName` LEFT JOIN `$missCountTableName`
             |ON `$totalCountTableName`.`${ConstantColumns.tmst}` = `$missCountTableName`.`${ConstantColumns.tmst}`
         """.stripMargin
        // scalastyle:on
      }

      val accuracyMetricWriteStep = procType match {
        case BatchProcessType =>
          val metricOpt = ruleParam.getOutputOpt(MetricOutputType)
          val mwName =
            metricOpt.flatMap(_.getNameOpt).getOrElse(ruleParam.getOutDfName())
          val flattenType = metricOpt
            .map(_.getFlatten)
            .getOrElse(DefaultFlattenType)
          Some(MetricWriteStep(mwName, accuracyTableName, flattenType))
        case StreamingProcessType => None
      }

      val accuracyTransStep =
        SparkSqlTransformStep(
          accuracyTableName,
          accuracyMetricSql,
          emptyMap,
          accuracyMetricWriteStep)
      accuracyTransStep.parentSteps += missCountTransStep
      accuracyTransStep.parentSteps += totalCountTransStep

      procType match {
        case BatchProcessType => accuracyTransStep :: Nil
        // streaming extra steps
        case StreamingProcessType =>
          // 5. accuracy metric merge
          val accuracyMetricTableName = "__accuracy"
          val accuracyMetricRule = DataFrameOps._accuracy
          val accuracyMetricDetails: Map[String, Any] = Map(
            (AccuracyOprKeys._miss, missColName),
            (AccuracyOprKeys._total, totalColName),
            (AccuracyOprKeys._matched, matchedColName))
          val accuracyMetricWriteStep = {
            val metricOpt = ruleParam.getOutputOpt(MetricOutputType)
            val mwName = metricOpt
              .flatMap(_.getNameOpt)
              .getOrElse(ruleParam.getOutDfName())
            val flattenType = metricOpt
              .map(_.getFlatten)
              .getOrElse(DefaultFlattenType)
            MetricWriteStep(mwName, accuracyMetricTableName, flattenType)
          }
          val accuracyMetricTransStep = DataFrameOpsTransformStep(
            accuracyMetricTableName,
            accuracyTableName,
            accuracyMetricRule,
            accuracyMetricDetails,
            Some(accuracyMetricWriteStep))
          accuracyMetricTransStep.parentSteps += accuracyTransStep

          // 6. collect accuracy records
          val accuracyRecordTableName = "__accuracyRecords"
          val accuracyRecordSql = {
            s"""
               |SELECT `${ConstantColumns.tmst}`, `${ConstantColumns.empty}`
               |FROM `$accuracyMetricTableName` WHERE `${ConstantColumns.record}`
             """.stripMargin
          }

          val accuracyRecordWriteStep = {
            val rwName =
              ruleParam
                .getOutputOpt(RecordOutputType)
                .flatMap(_.getNameOpt)
                .getOrElse(missRecordsTableName)

            RecordWriteStep(rwName, missRecordsTableName, Some(accuracyRecordTableName))
          }
          val accuracyRecordTransStep = SparkSqlTransformStep(
            accuracyRecordTableName,
            accuracyRecordSql,
            emptyMap,
            Some(accuracyRecordWriteStep))
          accuracyRecordTransStep.parentSteps += accuracyMetricTransStep

          accuracyRecordTransStep :: Nil
      }
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/dsl/transform/analyzer: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class AccuracyAnalyzer(expr: LogicalExpr, sourceName: String, targetName: String)
    extends BasicAnalyzer {

  val dataSourceNames: Set[String] =
    expr.preOrderTraverseDepthFirst(Set[String]())(seqDataSourceNames, combDataSourceNames)

  val sourceSelectionExprs: Seq[SelectionExpr] = {
    val seq = seqSelectionExprs(sourceName)
    expr.preOrderTraverseDepthFirst(Seq[SelectionExpr]())(seq, combSelectionExprs)
  }
  val targetSelectionExprs: Seq[SelectionExpr] = {
    val seq = seqSelectionExprs(targetName)
    expr.preOrderTraverseDepthFirst(Seq[SelectionExpr]())(seq, combSelectionExprs)
  }

  val selectionExprs: Seq[AliasableExpr] = sourceSelectionExprs ++ {
    expr.preOrderTraverseDepthFirst(Seq[AliasableExpr]())(seqWithAliasExprs, combWithAliasExprs)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.{DataSourceHeadExpr, SelectionExpr}
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

/**
 * analyzer of expr, to help generate dq steps by expr
 */
trait BasicAnalyzer extends Serializable {

  val expr: Expr

  val seqDataSourceNames: (Expr, Set[String]) => Set[String] = (expr: Expr, v: Set[String]) => {
    expr match {
      case DataSourceHeadExpr(name) => v + name
      case _ => v
    }
  }
  val combDataSourceNames: (Set[String], Set[String]) => Set[String] =
    (a: Set[String], b: Set[String]) => a ++ b

  val seqSelectionExprs: String => (Expr, Seq[SelectionExpr]) => Seq[SelectionExpr] =
    (dsName: String) =>
      (expr: Expr, v: Seq[SelectionExpr]) => {
        expr match {
          case se @ SelectionExpr(head: DataSourceHeadExpr, _, _) if head.name == dsName =>
            v :+ se
          case _ => v
        }
    }
  val combSelectionExprs: (Seq[SelectionExpr], Seq[SelectionExpr]) => Seq[SelectionExpr] =
    (a: Seq[SelectionExpr], b: Seq[SelectionExpr]) => a ++ b

  val seqWithAliasExprs: (Expr, Seq[AliasableExpr]) => Seq[AliasableExpr] =
    (expr: Expr, v: Seq[AliasableExpr]) => {
      expr match {
        case _: SelectExpr => v
        case a: AliasableExpr if a.alias.nonEmpty => v :+ a
        case _ => v
      }
    }
  val combWithAliasExprs: (Seq[AliasableExpr], Seq[AliasableExpr]) => Seq[AliasableExpr] =
    (a: Seq[AliasableExpr], b: Seq[AliasableExpr]) => a ++ b

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.CompletenessClause
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class CompletenessAnalyzer(expr: CompletenessClause, sourceName: String)
    extends BasicAnalyzer {

  val seqAlias: (Expr, Seq[String]) => Seq[String] = (expr: Expr, v: Seq[String]) => {
    expr match {
      case apr: AliasableExpr => v ++ apr.alias
      case _ => v
    }
  }
  val combAlias: (Seq[String], Seq[String]) => Seq[String] = (a: Seq[String], b: Seq[String]) =>
    a ++ b

  private val exprs = expr.exprs
  private def genAlias(idx: Int): String = s"alias_$idx"
  val selectionPairs: Seq[(Expr, String)] = exprs.zipWithIndex.map { pair =>
    val (pr, idx) = pair
    val res = pr.preOrderTraverseDepthFirst(Seq[String]())(seqAlias, combAlias)
    (pr, res.headOption.getOrElse(genAlias(idx)))
  }

  if (selectionPairs.isEmpty) {
    throw new Exception("completeness analyzer error: empty selection")
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class DistinctnessAnalyzer(expr: DistinctnessClause, sourceName: String)
    extends BasicAnalyzer {

  val seqAlias: (Expr, Seq[String]) => Seq[String] = (expr: Expr, v: Seq[String]) => {
    expr match {
      case apr: AliasableExpr => v ++ apr.alias
      case _ => v
    }
  }
  val combAlias: (Seq[String], Seq[String]) => Seq[String] = (a: Seq[String], b: Seq[String]) =>
    a ++ b

  private val exprs = expr.exprs
  private def genAlias(idx: Int): String = s"alias_$idx"
  val selectionPairs: Seq[(Expr, String, Boolean)] = exprs.zipWithIndex.map { pair =>
    val (pr, idx) = pair
    val res = pr.preOrderTraverseDepthFirst(Seq[String]())(seqAlias, combAlias)
    (pr, res.headOption.getOrElse(genAlias(idx)), pr.tag.isEmpty)
  }

  if (selectionPairs.isEmpty) {
    throw new Exception("uniqueness analyzer error: empty selection")
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.ProfilingClause
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class ProfilingAnalyzer(expr: ProfilingClause, sourceName: String) extends BasicAnalyzer {

  val dataSourceNames: Set[String] =
    expr.preOrderTraverseDepthFirst(Set[String]())(seqDataSourceNames, combDataSourceNames)

  val selectionExprs: Seq[Expr] = {
    expr.selectClause.exprs.map(_.extractSelf).flatMap { expr =>
      expr match {
        case e: SelectionExpr => Some(e)
        case e: FunctionExpr => Some(e)
        case _ => None
      }
    }
  }

  val groupbyExprOpt: Option[GroupbyClause] = expr.groupbyClauseOpt
  val preGroupbyExprs: Seq[Expr] = expr.preGroupbyClauses.map(_.extractSelf)
  val postGroupbyExprs: Seq[Expr] = expr.postGroupbyClauses.map(_.extractSelf)

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class TimelinessAnalyzer(expr: TimelinessClause, sourceName: String) extends BasicAnalyzer {

//  val tsExpr = expr.desc

//  val seqAlias = (expr: Expr, v: Seq[String]) => {
//    expr match {
//      case apr: AliasableExpr => v ++ apr.alias
//      case _ => v
//    }
//  }
//  val combAlias = (a: Seq[String], b: Seq[String]) => a ++ b
//
//  private val exprs = expr.exprs.toList
//  val selectionPairs = exprs.map { pr =>
//    val res = pr.preOrderTraverseDepthFirst(Seq[String]())(seqAlias, combAlias)
//    println(res)
//    println(pr)
//    (pr, res.headOption)
//  }
//
//  val (tsExprPair, endTsPairOpt) = selectionPairs match {
//    case Nil => throw new Exception(s"timeliness analyzer error: ts column not set")
//    case tsPair :: Nil => (tsPair, None)
//    case tsPair :: endTsPair :: _ => (tsPair, Some(endTsPair))
//  }
//
//  def getSelAlias(pair: (Expr, Option[String]), defAlias: String): (String, String) = {
//    val (pr, aliasOpt) = pair
//    val alias = aliasOpt.getOrElse(defAlias)
//    (pr.desc, alias)
//  }

  private val exprs = expr.exprs.map(_.desc).toList

  val (btsExpr, etsExprOpt) = exprs match {
    case Nil => throw new Exception("timeliness analyzer error: ts column not set")
    case btsExpr :: Nil => (btsExpr, None)
    case btsExpr :: etsExpr :: _ => (btsExpr, Some(etsExpr))
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer

import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.UniquenessClause
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._

case class UniquenessAnalyzer(expr: UniquenessClause, sourceName: String, targetName: String)
    extends BasicAnalyzer {

  val seqAlias: (Expr, Seq[String]) => Seq[String] = (expr: Expr, v: Seq[String]) => {
    expr match {
      case apr: AliasableExpr => v ++ apr.alias
      case _ => v
    }
  }
  val combAlias: (Seq[String], Seq[String]) => Seq[String] = (a: Seq[String], b: Seq[String]) =>
    a ++ b

  private val exprs = expr.exprs
  private def genAlias(idx: Int): String = s"alias_$idx"
  val selectionPairs: Seq[(Expr, String)] = exprs.zipWithIndex.map { pair =>
    val (pr, idx) = pair
    val res = pr.preOrderTraverseDepthFirst(Seq[String]())(seqAlias, combAlias)
    (pr, res.headOption.getOrElse(genAlias(idx)))
  }

  if (selectionPairs.isEmpty) {
    throw new Exception("uniqueness analyzer error: empty selection")
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import org.apache.commons.lang.StringUtils
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{RuleErrorConfParam, RuleParam}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.DefaultFlattenType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.CompletenessAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep
import com.hsbc.gbm.bd.clm.measure.step.write.{MetricWriteStep, RecordWriteStep}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * generate completeness dq steps
 */
case class CompletenessExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
  extends Expr2DQSteps {

  private object CompletenessKeys {
    val _source = "source"
    val _total = "total"
    val _complete = "complete"
    val _incomplete = "incomplete"
  }

  import CompletenessKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val completenessExpr = expr.asInstanceOf[CompletenessClause]

    val sourceName = details.getString(_source, context.getDataSourceName(0))

    val procType = context.procType
    val timestamp = context.contextId.timestamp

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else {
      val analyzer = CompletenessAnalyzer(completenessExpr, sourceName)

      val selItemsClause = analyzer.selectionPairs
        .map { pair =>
          val (expr, alias) = pair
          s"${expr.desc} AS `$alias`"
        }
        .mkString(", ")
      val aliases = analyzer.selectionPairs.map(_._2)

      val selClause = procType match {
        case BatchProcessType => selItemsClause
        case StreamingProcessType => s"`${ConstantColumns.tmst}`, $selItemsClause"
      }

      // 1. source alias
      val sourceAliasTableName = "__sourceAlias"
      val sourceAliasSql = {
        s"SELECT * FROM `$sourceName`"
      }
      val sourceAliasTransStep =
        SparkSqlTransformStep(sourceAliasTableName, sourceAliasSql, emptyMap, None, cache = true)

      // 2. incomplete record
      val incompleteRecordsTableName = "__incompleteRecords"
      val errorConfs: Seq[RuleErrorConfParam] = ruleParam.getErrorConfs
      var incompleteWhereClause: String = ""
      if (errorConfs.isEmpty) {
        // without errorConfs
        val completeWhereClause = aliases.map(a => s"`$a` IS NOT NULL").mkString(" AND ")
        incompleteWhereClause = s"NOT ($completeWhereClause)"
      } else {
        // with errorConfs
        incompleteWhereClause = this.getErrorConfCompleteWhereClause(errorConfs)
      }

      val incompleteRecordsSql =
        s"SELECT * FROM `$sourceAliasTableName` WHERE $incompleteWhereClause"

      val incompleteRecordWriteStep = RecordWriteStep(ruleParam.getOutDfName(), incompleteRecordsTableName)

      val incompleteRecordTransStep =
        SparkSqlTransformStep(
          incompleteRecordsTableName,
          incompleteRecordsSql,
          emptyMap,
          Some(incompleteRecordWriteStep),
          cache = true)
      incompleteRecordTransStep.parentSteps += sourceAliasTransStep

      // 3. incomplete count
      val incompleteCountTableName = "__incompleteCount"
      val incompleteColName = details.getStringOrKey(_incomplete)
      val incompleteCountSql = procType match {
        case BatchProcessType =>
          s"SELECT COUNT(*) AS `$incompleteColName` FROM `$incompleteRecordsTableName`"
        case StreamingProcessType =>
          s"SELECT `${ConstantColumns.tmst}`, COUNT(*) AS `$incompleteColName` " +
            s"FROM `$incompleteRecordsTableName` GROUP BY `${ConstantColumns.tmst}`"
      }
      val incompleteCountTransStep =
        SparkSqlTransformStep(incompleteCountTableName, incompleteCountSql, emptyMap)
      incompleteCountTransStep.parentSteps += incompleteRecordTransStep

      // 4. total count
      val totalCountTableName = "__totalCount"
      val totalColName = details.getStringOrKey(_total)
      val totalCountSql = procType match {
        case BatchProcessType =>
          s"SELECT COUNT(*) AS `$totalColName` FROM `$sourceAliasTableName`"
        case StreamingProcessType =>
          s"SELECT `${ConstantColumns.tmst}`, COUNT(*) AS `$totalColName` " +
            s"FROM `$sourceAliasTableName` GROUP BY `${ConstantColumns.tmst}`"
      }
      val totalCountTransStep =
        SparkSqlTransformStep(totalCountTableName, totalCountSql, emptyMap)
      totalCountTransStep.parentSteps += sourceAliasTransStep

      // 5. complete metric
      val completeTableName = ruleParam.getOutDfName()
      val completeColName = details.getStringOrKey(_complete)
      // scalastyle:off
      val completeMetricSql = procType match {
        case BatchProcessType =>
          s"""
             |SELECT `$totalCountTableName`.`$totalColName` AS `$totalColName`,
             |coalesce(`$incompleteCountTableName`.`$incompleteColName`, 0) AS `$incompleteColName`,
             |(`$totalCountTableName`.`$totalColName` - coalesce(`$incompleteCountTableName`.`$incompleteColName`, 0)) AS `$completeColName`
             |FROM `$totalCountTableName` LEFT JOIN `$incompleteCountTableName`
         """.stripMargin
        case StreamingProcessType =>
          s"""
             |SELECT `$totalCountTableName`.`${ConstantColumns.tmst}` AS `${ConstantColumns.tmst}`,
             |`$totalCountTableName`.`$totalColName` AS `$totalColName`,
             |coalesce(`$incompleteCountTableName`.`$incompleteColName`, 0) AS `$incompleteColName`,
             |(`$totalCountTableName`.`$totalColName` - coalesce(`$incompleteCountTableName`.`$incompleteColName`, 0)) AS `$completeColName`
             |FROM `$totalCountTableName` LEFT JOIN `$incompleteCountTableName`
             |ON `$totalCountTableName`.`${ConstantColumns.tmst}` = `$incompleteCountTableName`.`${ConstantColumns.tmst}`
         """.stripMargin
      }
      // scalastyle:on
      val completeWriteStep = {
        val metricOpt = ruleParam.getOutputOpt(MetricOutputType)
        val mwName = metricOpt.flatMap(_.getNameOpt).getOrElse(completeTableName)
        val flattenType = metricOpt.map(_.getFlatten).getOrElse(DefaultFlattenType)
        MetricWriteStep(mwName, completeTableName, flattenType)
      }
      val completeTransStep =
        SparkSqlTransformStep(
          completeTableName,
          completeMetricSql,
          emptyMap,
          Some(completeWriteStep))
      completeTransStep.parentSteps += incompleteCountTransStep
      completeTransStep.parentSteps += totalCountTransStep

      val transSteps = completeTransStep :: Nil
      transSteps
    }
  }

  /**
   * get 'error' where clause
   *
   * @param errorConfs error configuraion list
   * @return 'error' where clause
   */
  def getErrorConfCompleteWhereClause(errorConfs: Seq[RuleErrorConfParam]): String = {
    errorConfs.map(errorConf => this.getEachErrorWhereClause(errorConf)).mkString(" OR ")
  }

  /**
   * get error sql for each column
   *
   * @param errorConf error configuration
   * @return 'error' sql for each column
   */
  def getEachErrorWhereClause(errorConf: RuleErrorConfParam): String = {
    val errorType: Option[String] = errorConf.getErrorType
    val columnName: String = errorConf.getColumnName.get
    if ("regex".equalsIgnoreCase(errorType.get)) {
      // only have one regular expression
      val regexValue: String = errorConf.getValues.head
      val afterReplace: String = regexValue.replaceAll("""\\""", """\\\\""")
      return s"(`$columnName` REGEXP '$afterReplace')"
    } else if ("enumeration".equalsIgnoreCase(errorType.get)) {
      val values: Seq[String] = errorConf.getValues
      var inResult = ""
      var nullResult = ""
      if (values.contains("hive_none")) {
        // hive_none means NULL
        nullResult = s"`$columnName` IS NULL"
      }

      val valueWithQuote: String = values
        .filter(value => !"hive_none".equals(value))
        .map(value => s"'$value'")
        .mkString(", ")

      if (!StringUtils.isEmpty(valueWithQuote)) {
        inResult = s"`$columnName` IN ($valueWithQuote)"
      }

      var result = ""
      if (!StringUtils.isEmpty(inResult) && !StringUtils.isEmpty(nullResult)) {
        result = s"($inResult OR $nullResult)"
      } else if (!StringUtils.isEmpty(inResult)) {
        result = s"($inResult)"
      } else {
        result = s"($nullResult)"
      }

      return result
    }
    throw new IllegalArgumentException(
      "type in error.confs only supports regex and enumeration way")
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.{DistinctnessClause, _}
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.DistinctnessAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep
import com.hsbc.gbm.bd.clm.measure.step.write.{DataSourceUpdateWriteStep, MetricWriteStep, RecordWriteStep}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * generate distinctness dq steps
 */
case class DistinctnessExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
  extends Expr2DQSteps {

  private object DistinctnessKeys {
    val _source = "source"
    val _target = "target"
    val _distinct = "distinct"
    val _total = "total"
    val _dup = "dup"
    val _accu_dup = "accu_dup"
    val _num = "num"

    val _duplicationArray = "duplication.array"
    val _withAccumulate = "with.accumulate"

    val _recordEnable = "record.enable"
  }

  import DistinctnessKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val distinctnessExpr = expr.asInstanceOf[DistinctnessClause]

    val sourceName = details.getString(_source, context.getDataSourceName(0))
    val targetName = details.getString(_target, context.getDataSourceName(1))
    val analyzer = DistinctnessAnalyzer(distinctnessExpr, sourceName)

    val procType = context.procType
    val timestamp = context.contextId.timestamp
    val dsTimeRanges = context.dataSourceTimeRanges

    val beginTmst = dsTimeRanges.get(sourceName).map(_.begin) match {
      case Some(t) => t
      case _ => throw new Exception(s"empty begin tmst from $sourceName")
    }
    val endTmst = dsTimeRanges.get(sourceName).map(_.end) match {
      case Some(t) => t
      case _ => throw new Exception(s"empty end tmst from $sourceName")
    }

    val writeTimestampOpt = Some(endTmst)

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else {
      val withOlderTable = {
        details.getBoolean(_withAccumulate, defValue = true) &&
          context.runTimeTableRegister.existsTable(targetName)
      }

      val selClause = analyzer.selectionPairs
        .map { pair =>
          val (expr, alias, _) = pair
          s"${expr.desc} AS `$alias`"
        }
        .mkString(", ")
      val distAliases = analyzer.selectionPairs.filter(_._3).map(_._2)
      val distAliasesClause = distAliases.map(a => s"`$a`").mkString(", ")
      val allAliases = analyzer.selectionPairs.map(_._2)
      val allAliasesClause = allAliases.map(a => s"`$a`").mkString(", ")
      val groupAliases = analyzer.selectionPairs.filter(!_._3).map(_._2)
      val groupAliasesClause = groupAliases.map(a => s"`$a`").mkString(", ")
      val dupColName = details.getStringOrKey(_dup)

      // 1. source alias
      val sourceAliasTableName = "__sourceAlias"
      val sourceAliasSql = {
        s"""
           |SELECT *,
           |(count(*) over(partition by $distAliasesClause) - 1) as $dupColName
           |FROM `$sourceName`
           |""".stripMargin
      }
      val sourceAliasTransStep =
        SparkSqlTransformStep(sourceAliasTableName, sourceAliasSql, emptyMap, None, cache = true)

      // 2. total metric
      val totalTableName = "__totalMetric"
      val totalColName = details.getStringOrKey(_total)
      val totalSql = {
        s"SELECT COUNT(*) AS `$totalColName` FROM `$sourceAliasTableName`"
      }
      val totalMetricWriteStep = {
        MetricWriteStep(s"${ruleParam.getOutDfName()}%%$totalColName", totalTableName, MapFlattenType, writeTimestampOpt)
      }
      val totalTransStep =
        SparkSqlTransformStep(totalTableName, totalSql, emptyMap, Some(totalMetricWriteStep))
      totalTransStep.parentSteps += sourceAliasTransStep

      // 3. group by self
      val selfGroupTableName = "__selfGroup"
      val accuDupColName = details.getStringOrKey(_accu_dup)
      val selfGroupSql = {
        s"""
           |SELECT $distAliasesClause, (COUNT(*) - 1) AS `$dupColName`,
           |TRUE AS `${ConstantColumns.distinct}`
           |FROM `$sourceAliasTableName` GROUP BY $distAliasesClause
          """.stripMargin
      }
      val selfGroupTransStep =
        SparkSqlTransformStep(selfGroupTableName, selfGroupSql, emptyMap, None, cache = true)
      selfGroupTransStep.parentSteps += sourceAliasTransStep

      val transSteps1 = totalTransStep :: selfGroupTransStep :: Nil

      val (transSteps2, dupCountTableName) = procType match {
        case StreamingProcessType if withOlderTable =>
          // 4.0 update old data
          val targetDsUpdateWriteStep = DataSourceUpdateWriteStep(targetName, targetName)

          // 4. older alias
          val olderAliasTableName = "__older"
          val olderAliasSql = {
            s"SELECT $selClause FROM `$targetName` WHERE `${ConstantColumns.tmst}` <= $beginTmst"
          }
          val olderAliasTransStep =
            SparkSqlTransformStep(olderAliasTableName, olderAliasSql, emptyMap)

          // 5. join with older data
          val joinedTableName = "__joined"
          val selfSelClause = (distAliases :+ dupColName)
            .map { alias =>
              s"`$selfGroupTableName`.`$alias`"
            }
            .mkString(", ")
          val onClause = distAliases
            .map { alias =>
              s"coalesce(`$selfGroupTableName`.`$alias`, '') = coalesce(`$olderAliasTableName`.`$alias`, '')"
            }
            .mkString(" AND ")
          val olderIsNull = distAliases
            .map { alias =>
              s"`$olderAliasTableName`.`$alias` IS NULL"
            }
            .mkString(" AND ")
          val joinedSql = {
            s"""
               |SELECT $selfSelClause, ($olderIsNull) AS `${ConstantColumns.distinct}`
               |FROM `$olderAliasTableName` RIGHT JOIN `$selfGroupTableName`
               |ON $onClause
            """.stripMargin
          }
          val joinedTransStep = SparkSqlTransformStep(joinedTableName, joinedSql, emptyMap)
          joinedTransStep.parentSteps += selfGroupTransStep
          joinedTransStep.parentSteps += olderAliasTransStep

          // 6. group by joined data
          val groupTableName = "__group"
          val moreDupColName = "_more_dup"
          val groupSql = {
            s"""
               |SELECT $distAliasesClause, `$dupColName`, `${ConstantColumns.distinct}`,
               |COUNT(*) AS `$moreDupColName`
               |FROM `$joinedTableName`
               |GROUP BY $distAliasesClause, `$dupColName`, `${ConstantColumns.distinct}`
             """.stripMargin
          }
          val groupTransStep = SparkSqlTransformStep(groupTableName, groupSql, emptyMap)
          groupTransStep.parentSteps += joinedTransStep

          // 7. final duplicate count
          val finalDupCountTableName = "__finalDupCount"

          /**
           * dupColName:      the duplicate count of duplicated items only occurs in new data,
           * which means the distinct one in new data is also duplicate
           * accuDupColName:  the count of duplicated items accumulated in new data and old data,
           * which means the accumulated distinct count in all data
           * e.g.:  new data [A, A, B, B, C, D], old data [A, A, B, C]
           * selfGroupTable will be (A, 1, F), (B, 1, F), (C, 0, T), (D, 0, T)
           * joinedTable will be (A, 1, F), (A, 1, F), (B, 1, F), (C, 0, F), (D, 0, T)
           * groupTable will be (A, 1, F, 2), (B, 1, F, 1), (C, 0, F, 1), (D, 0, T, 1)
           * finalDupCountTable will be (A, F, 2, 3), (B, F, 2, 2), (C, F, 1, 1), (D, T, 0, 0)
           * The distinct result of new data only should be: (A, 2), (B, 2), (C, 1), (D, 0),
           * which means in new data [A, A, B, B, C, D], [A, A, B, B, C] are all duplicated,
           * only [D] is distinct
           */
          val finalDupCountSql = {
            s"""
               |SELECT $distAliasesClause, `${ConstantColumns.distinct}`,
               |CASE WHEN `${ConstantColumns.distinct}` THEN `$dupColName`
               |ELSE (`$dupColName` + 1) END AS `$dupColName`,
               |CASE WHEN `${ConstantColumns.distinct}` THEN `$dupColName`
               |ELSE (`$dupColName` + `$moreDupColName`) END AS `$accuDupColName`
               |FROM `$groupTableName`
             """.stripMargin
          }
          val finalDupCountTransStep =
            SparkSqlTransformStep(
              finalDupCountTableName,
              finalDupCountSql,
              emptyMap,
              None,
              cache = true)
          finalDupCountTransStep.parentSteps += groupTransStep

          (finalDupCountTransStep :: targetDsUpdateWriteStep :: Nil, finalDupCountTableName)
        case _ =>
          (selfGroupTransStep :: Nil, selfGroupTableName)
      }

      // 8. distinct metric
      val distTableName = "__distMetric"
      val distColName = details.getStringOrKey(_distinct)
      val distSql = {
        s"""
           |SELECT COUNT(*) AS `$distColName`
           |FROM `$dupCountTableName` WHERE `${ConstantColumns.distinct}`
         """.stripMargin
      }
      val distMetricWriteStep = {
        MetricWriteStep(s"${ruleParam.getOutDfName()}%%$distColName", distTableName, MapFlattenType, writeTimestampOpt)
      }
      val distTransStep =
        SparkSqlTransformStep(distTableName, distSql, emptyMap, Some(distMetricWriteStep))

      val transSteps3 = distTransStep :: Nil

      val duplicationArrayName = details.getString(_duplicationArray, "")
      val transSteps4 = if (duplicationArrayName.nonEmpty) {
        val recordEnable = details.getBoolean(_recordEnable, defValue = true)
        if (groupAliases.nonEmpty) {
          // with some group by requirement
          // 9. origin data join with distinct information
          val informedTableName = "__informed"
          val onClause = distAliases
            .map { alias =>
              s"coalesce(`$sourceAliasTableName`.`$alias`, '') = coalesce(`$dupCountTableName`.`$alias`, '')"
            }
            .mkString(" AND ")
          val informedSql = {
            s"""
               |SELECT `$sourceAliasTableName`.*,
               |`$dupCountTableName`.`$dupColName` AS `$dupColName`,
               |`$dupCountTableName`.`${ConstantColumns.distinct}` AS `${ConstantColumns.distinct}`
               |FROM `$sourceAliasTableName` LEFT JOIN `$dupCountTableName`
               |ON $onClause
               """.stripMargin
          }
          val informedTransStep = SparkSqlTransformStep(informedTableName, informedSql, emptyMap)

          // 10. add row number
          val rnTableName = "__rowNumber"
          val rnDistClause = distAliasesClause
          val rnSortClause = s"SORT BY `${ConstantColumns.distinct}`"
          val rnSql = {
            s"""
               |SELECT *,
               |ROW_NUMBER() OVER (DISTRIBUTE BY $rnDistClause $rnSortClause) `${ConstantColumns.rowNumber}`
               |FROM `$informedTableName`
               """.stripMargin
          }
          val rnTransStep = SparkSqlTransformStep(rnTableName, rnSql, emptyMap)
          rnTransStep.parentSteps += informedTransStep

          // 11. recognize duplicate items
          val dupItemsTableName = "__dupItems"
          val dupItemsSql = {
            s"""
               |SELECT $allAliasesClause, `$dupColName` FROM `$rnTableName`
               |WHERE NOT `${ConstantColumns.distinct}` OR `${ConstantColumns.rowNumber}` > 1
               """.stripMargin
          }
          val dupItemsWriteStep = {
            val rwName = ruleParam
              .getOutputOpt(RecordOutputType)
              .flatMap(_.getNameOpt)
              .getOrElse(dupItemsTableName)
            RecordWriteStep(rwName, dupItemsTableName, None, writeTimestampOpt)
          }
          val dupItemsTransStep = {
            if (recordEnable) {
              SparkSqlTransformStep(
                dupItemsTableName,
                dupItemsSql,
                emptyMap,
                Some(dupItemsWriteStep))
            } else {
              SparkSqlTransformStep(dupItemsTableName, dupItemsSql, emptyMap)
            }
          }
          dupItemsTransStep.parentSteps += rnTransStep

          // 12. group by dup Record metric
          val groupDupMetricTableName = "__groupDupMetric"
          val numColName = details.getStringOrKey(_num)
          val groupSelClause = groupAliasesClause
          val groupDupMetricSql = {
            s"""
               |SELECT $groupSelClause, `$dupColName`, COUNT(*) AS `$numColName`
               |FROM `$dupItemsTableName` GROUP BY $groupSelClause, `$dupColName`
             """.stripMargin
          }
          val groupDupMetricWriteStep = {
            MetricWriteStep(
              s"${ruleParam.getOutDfName()}%%$duplicationArrayName",
              groupDupMetricTableName,
              ArrayFlattenType,
              writeTimestampOpt)
          }
          val groupDupMetricTransStep =
            SparkSqlTransformStep(
              groupDupMetricTableName,
              groupDupMetricSql,
              emptyMap,
              Some(groupDupMetricWriteStep))
          groupDupMetricTransStep.parentSteps += dupItemsTransStep

          groupDupMetricTransStep :: Nil
        } else {
          // no group by requirement
          // 9. duplicate record
          val dupRecordTableName = "__dupRecords"
          val dupRecordSelClause = procType match {
            case StreamingProcessType if withOlderTable =>
              s"$distAliasesClause, `$dupColName`, `$accuDupColName`"

            case _ => s"$distAliasesClause"
          }
          val dupRecordSql = {
            s"""
               |SELECT distinct *
               |FROM `$sourceAliasTableName`
               |where $dupColName >= 1
               |""".stripMargin
          }
          val dupRecordWriteStep = RecordWriteStep(ruleParam.getOutDfName(), dupRecordTableName, None, writeTimestampOpt)
          val dupRecordTransStep = {
            if (recordEnable) {
              SparkSqlTransformStep(
                dupRecordTableName,
                dupRecordSql,
                emptyMap,
                Some(dupRecordWriteStep),
                cache = true)
            } else {
              SparkSqlTransformStep(
                dupRecordTableName,
                dupRecordSql,
                emptyMap,
                None,
                cache = true)
            }
          }

          // 10. duplicate metric
          val dupMetricTableName = "__dupMetric"
          val numColName = details.getStringOrKey(_num)
          val dupMetricSql = {
            s"""
               |SELECT $dupColName, COUNT(*) AS `$numColName`
               |FROM `$dupRecordTableName` GROUP BY `$dupColName`
              """.stripMargin
          }
          val dupMetricWriteStep = {
            MetricWriteStep(
              s"${ruleParam.getOutDfName()}%%$duplicationArrayName",
              dupMetricTableName,
              ArrayFlattenType,
              writeTimestampOpt)
          }
          val dupMetricTransStep =
            SparkSqlTransformStep(
              dupMetricTableName,
              dupMetricSql,
              emptyMap,
              Some(dupMetricWriteStep))
          dupMetricTransStep.parentSteps += dupRecordTransStep

          dupMetricTransStep :: Nil
        }
      } else Nil

      // full steps
      transSteps1 ++ transSteps2 ++ transSteps3 ++ transSteps4
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DqType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr.Expr

trait Expr2DQSteps extends Loggable with Serializable {

  protected val emtptDQSteps: Seq[DQStep] = Seq[DQStep]()
  protected val emptyMap: Map[String, Any] = Map[String, Any]()

  def getDQSteps: Seq[DQStep]
}

/**
 * get dq steps generator for griffin dsl rule
 */
object Expr2DQSteps {
  private val emtptExpr2DQSteps: Expr2DQSteps = new Expr2DQSteps {
    def getDQSteps: Seq[DQStep] = emtptDQSteps
  }

  def apply(context: DQContext, expr: Expr, ruleParam: RuleParam): Expr2DQSteps = {
    ruleParam.getDqType match {
      case Accuracy => AccuracyExpr2DQSteps(context, expr, ruleParam)
      case Profiling => ProfilingExpr2DQSteps(context, expr, ruleParam)
      case Uniqueness => UniquenessExpr2DQSteps(context, expr, ruleParam)
      case Distinct => DistinctnessExpr2DQSteps(context, expr, ruleParam)
      case Timeliness => TimelinessExpr2DQSteps(context, expr, ruleParam)
      case Completeness => CompletenessExpr2DQSteps(context, expr, ruleParam)
      case _ => emtptExpr2DQSteps
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import org.apache.commons.lang.StringUtils
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.DefaultFlattenType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.ProfilingAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep
import com.hsbc.gbm.bd.clm.measure.step.write.MetricWriteStep
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * generate profiling dq steps
 */
case class ProfilingExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
    extends Expr2DQSteps {

  private object ProfilingKeys {
    val _source = "source"
  }
  import ProfilingKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val profilingExpr = expr.asInstanceOf[ProfilingClause]

    val sourceName = profilingExpr.fromClauseOpt match {
      case Some(fc) => fc.dataSource
      case _ => details.getString(_source, context.getDataSourceName(0))
    }
    val fromClause = profilingExpr.fromClauseOpt.getOrElse(FromClause(sourceName)).desc

    val procType = context.procType
    val timestamp = context.contextId.timestamp

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else {
      val analyzer = ProfilingAnalyzer(profilingExpr, sourceName)
      val selExprDescs = analyzer.selectionExprs.map { sel =>
        val alias = sel match {
          case s: AliasableExpr =>
            s.alias.filter(StringUtils.isNotEmpty).map(a => s" AS `$a`").getOrElse("")

          case _ => ""
        }
        s"${sel.desc}$alias"
      }
      val selCondition = profilingExpr.selectClause.extraConditionOpt.map(_.desc).mkString
      val selClause = procType match {
        case BatchProcessType => selExprDescs.mkString(", ")
        case StreamingProcessType => (s"`${ConstantColumns.tmst}`" +: selExprDescs).mkString(", ")
      }
      val groupByClauseOpt = analyzer.groupbyExprOpt
      val groupbyClause = procType match {
        case BatchProcessType => groupByClauseOpt.map(_.desc).getOrElse("")
        case StreamingProcessType =>
          val tmstGroupbyClause =
            GroupbyClause(LiteralStringExpr(s"`${ConstantColumns.tmst}`") :: Nil, None)
          val mergedGroubbyClause = tmstGroupbyClause.merge(groupByClauseOpt match {
            case Some(gbc) => gbc
            case _ => GroupbyClause(Nil, None)
          })
          mergedGroubbyClause.desc
      }
      val preGroupbyClause = analyzer.preGroupbyExprs.map(_.desc).mkString(" ")
      val postGroupbyClause = analyzer.postGroupbyExprs.map(_.desc).mkString(" ")

      // 1. select statement
      val profilingSql = {
        s"SELECT $selCondition $selClause " +
          s"$fromClause $preGroupbyClause $groupbyClause $postGroupbyClause"
      }
      val profilingName = ruleParam.getOutDfName()
      val profilingMetricWriteStep = {
        val metricOpt = ruleParam.getOutputOpt(MetricOutputType)
        val mwName = metricOpt.flatMap(_.getNameOpt).getOrElse(ruleParam.getOutDfName())
        val flattenType = metricOpt.map(_.getFlatten).getOrElse(DefaultFlattenType)
        MetricWriteStep(mwName, profilingName, flattenType)
      }
      val profilingTransStep =
        SparkSqlTransformStep(
          profilingName,
          profilingSql,
          details,
          Some(profilingMetricWriteStep))
      profilingTransStep :: Nil
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.{ArrayFlattenType, DefaultFlattenType}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.TimelinessAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep
import com.hsbc.gbm.bd.clm.measure.step.write.{MetricWriteStep, RecordWriteStep}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._
import com.hsbc.gbm.bd.clm.measure.utils.TimeUtil

/**
 * generate timeliness dq steps
 */
case class TimelinessExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
  extends Expr2DQSteps {

  private object TimelinessKeys {
    val _source = "source"
    val _latency = "latency"
    val _total = "total"
    val _avg = "avg"
    val _threshold = "threshold"
    val _step = "step"
    val _count = "count"
    val _stepSize = "step.size"
    val _percentileColPrefix = "percentile"
    val _percentileValues = "percentile.values"
  }

  import TimelinessKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val timelinessExpr = expr.asInstanceOf[TimelinessClause]

    val sourceName = details.getString(_source, context.getDataSourceName(0))

    val procType = context.procType
    val timestamp = context.contextId.timestamp

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else {
      val analyzer = TimelinessAnalyzer(timelinessExpr, sourceName)
      val btsSel = analyzer.btsExpr
      val etsSelOpt = analyzer.etsExprOpt

      // 1. in time
      val inTimeTableName = "__inTime"
      val inTimeSql = etsSelOpt match {
        case Some(etsSel) =>
          s"""
             |SELECT *, ($btsSel) AS `${ConstantColumns.beginTs}`,
             |($etsSel) AS `${ConstantColumns.endTs}`
             |FROM $sourceName WHERE ($btsSel) IS NOT NULL AND ($etsSel) IS NOT NULL
           """.stripMargin
        case _ =>
          s"""
             |SELECT *, ($btsSel) AS `${ConstantColumns.beginTs}`
             |FROM $sourceName WHERE ($btsSel) IS NOT NULL
           """.stripMargin
      }
      val inTimeTransStep = SparkSqlTransformStep(inTimeTableName, inTimeSql, emptyMap)

      // 2. latency
      val latencyTableName = "__lat"
      val latencyColName = details.getStringOrKey(_latency)
      val etsColName = etsSelOpt match {
        case Some(_) => ConstantColumns.endTs
        case _ => ConstantColumns.tmst
      }
      val latencySql = {
        s"SELECT *, (`$etsColName` - `${ConstantColumns.beginTs}`) AS `$latencyColName` " +
          s"FROM `$inTimeTableName`"
      }
      val latencyTransStep =
        SparkSqlTransformStep(latencyTableName, latencySql, emptyMap, None, cache = true)
      latencyTransStep.parentSteps += inTimeTransStep

      // 3. timeliness metric
      val metricTableName = ruleParam.getOutDfName()
      val totalColName = details.getStringOrKey(_total)
      val avgColName = details.getStringOrKey(_avg)
      val metricSql = procType match {

        case BatchProcessType =>
          s"""
             |SELECT COUNT(*) AS `$totalColName`,
             |CAST(AVG(`$latencyColName`) AS BIGINT) AS `$avgColName`
             |FROM `$latencyTableName`
           """.stripMargin

        case StreamingProcessType =>
          s"""
             |SELECT `${ConstantColumns.tmst}`,
             |COUNT(*) AS `$totalColName`,
             |CAST(AVG(`$latencyColName`) AS BIGINT) AS `$avgColName`
             |FROM `$latencyTableName`
             |GROUP BY `${ConstantColumns.tmst}`
           """.stripMargin
      }
      val metricWriteStep = {
        val metricOpt = ruleParam.getOutputOpt(MetricOutputType)
        val mwName = metricOpt.flatMap(_.getNameOpt).getOrElse(ruleParam.getOutDfName())
        val flattenType = metricOpt.map(_.getFlatten).getOrElse(DefaultFlattenType)
        MetricWriteStep(mwName, metricTableName, flattenType)
      }
      val metricTransStep =
        SparkSqlTransformStep(metricTableName, metricSql, emptyMap, Some(metricWriteStep))
      metricTransStep.parentSteps += latencyTransStep

      // current steps
      val transSteps1 = metricTransStep :: Nil

      // 4. timeliness record
      val transSteps2 = TimeUtil.milliseconds(details.getString(_threshold, "")) match {
        case Some(tsh) =>
          val recordTableName = "__lateRecords"
          val recordSql = {
            s"SELECT * FROM `$latencyTableName` WHERE `$latencyColName` > $tsh"
          }
          val recordWriteStep = RecordWriteStep(ruleParam.getOutDfName(), recordTableName, None)

          val recordTransStep =
            SparkSqlTransformStep(recordTableName, recordSql, emptyMap, Some(recordWriteStep))
          recordTransStep.parentSteps += latencyTransStep

          recordTransStep :: Nil
        case _ => Nil
      }

      // 5. ranges
      val transSteps3 = TimeUtil.milliseconds(details.getString(_stepSize, "")) match {
        case Some(stepSize) =>
          // 5.1 range
          val rangeTableName = "__range"
          val stepColName = details.getStringOrKey(_step)
          val rangeSql = {
            s"""
               |SELECT *, CAST((`$latencyColName` / $stepSize) AS BIGINT) AS `$stepColName`
               |FROM `$latencyTableName`
             """.stripMargin
          }
          val rangeTransStep = SparkSqlTransformStep(rangeTableName, rangeSql, emptyMap)
          rangeTransStep.parentSteps += latencyTransStep

          // 5.2 range metric
          val rangeMetricTableName = "__rangeMetric"
          val countColName = details.getStringOrKey(_count)
          val rangeMetricSql = procType match {
            case BatchProcessType =>
              s"""
                 |SELECT `$stepColName`, COUNT(*) AS `$countColName`
                 |FROM `$rangeTableName` GROUP BY `$stepColName`
                """.stripMargin
            case StreamingProcessType =>
              s"""
                 |SELECT `${ConstantColumns.tmst}`, `$stepColName`, COUNT(*) AS `$countColName`
                 |FROM `$rangeTableName` GROUP BY `${ConstantColumns.tmst}`, `$stepColName`
                """.stripMargin
          }
          val rangeMetricWriteStep = {
            MetricWriteStep(stepColName, rangeMetricTableName, ArrayFlattenType)
          }
          val rangeMetricTransStep =
            SparkSqlTransformStep(
              rangeMetricTableName,
              rangeMetricSql,
              emptyMap,
              Some(rangeMetricWriteStep))
          rangeMetricTransStep.parentSteps += rangeTransStep

          rangeMetricTransStep :: Nil
        case _ => Nil
      }

      // 6. percentiles
      val percentiles = getPercentiles(details)
      val transSteps4 = if (percentiles.nonEmpty) {
        val percentileTableName = "__percentile"
        val percentileColName = details.getStringOrKey(_percentileColPrefix)
        val percentileCols = percentiles
          .map { pct =>
            val pctName = (pct * 100).toInt.toString
            s"floor(percentile_approx($latencyColName, $pct)) " +
              s"AS `${percentileColName}_$pctName`"
          }
          .mkString(", ")
        val percentileSql = {
          s"""
             |SELECT $percentileCols
             |FROM `$latencyTableName`
            """.stripMargin
        }
        val percentileWriteStep = {
          MetricWriteStep(percentileTableName, percentileTableName, DefaultFlattenType)
        }
        val percentileTransStep =
          SparkSqlTransformStep(
            percentileTableName,
            percentileSql,
            emptyMap,
            Some(percentileWriteStep))
        percentileTransStep.parentSteps += latencyTransStep

        percentileTransStep :: Nil
      } else Nil

      // full steps
      transSteps1 ++ transSteps2 ++ transSteps3 ++ transSteps4
    }
  }

  private def getPercentiles(details: Map[String, Any]): Seq[Double] = {
    details.getDoubleArr(_percentileValues).filter(d => d >= 0 && d <= 1)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform

import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.{ArrayFlattenType, EntriesFlattenType}
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.configuration.enums.ProcessType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.expr._
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.analyzer.UniquenessAnalyzer
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep
import com.hsbc.gbm.bd.clm.measure.step.write.{MetricWriteStep, RecordWriteStep}
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * generate uniqueness dq steps
 */
case class UniquenessExpr2DQSteps(context: DQContext, expr: Expr, ruleParam: RuleParam)
  extends Expr2DQSteps {

  private object UniquenessKeys {
    val _source = "source"
    val _target = "target"
    val _unique = "unique"
    val _total = "total"
    val _dup = "dup"
    val _num = "num"

    val _duplicationArray = "duplication.array"
  }

  import UniquenessKeys._

  def getDQSteps: Seq[DQStep] = {
    val details = ruleParam.getDetails
    val uniquenessExpr = expr.asInstanceOf[UniquenessClause]

    val sourceName = details.getString(_source, context.getDataSourceName(0))
    val targetName = details.getString(_target, context.getDataSourceName(1))
    val analyzer = UniquenessAnalyzer(uniquenessExpr, sourceName, targetName)

    val procType = context.procType
    val timestamp = context.contextId.timestamp

    if (!context.runTimeTableRegister.existsTable(sourceName)) {
      warn(s"[$timestamp] data source $sourceName not exists")
      Nil
    } else if (!context.runTimeTableRegister.existsTable(targetName)) {
      warn(s"[$timestamp] data source $targetName not exists")
      Nil
    } else {
      val selItemsClause = analyzer.selectionPairs
        .map { pair =>
          val (expr, alias) = pair
          s"${expr.desc} AS `$alias`"
        }
        .mkString(", ")
      val aliases = analyzer.selectionPairs.map(_._2)

      val selClause = procType match {
        case BatchProcessType => selItemsClause
        case StreamingProcessType => s"`${ConstantColumns.tmst}`, $selItemsClause"
      }
      val selAliases = procType match {
        case BatchProcessType => aliases
        case StreamingProcessType => ConstantColumns.tmst +: aliases
      }

      // 1. source distinct mapping
      val sourceTableName = "__source"
      val sourceSql = s"SELECT DISTINCT $selClause FROM $sourceName"
      val sourceTransStep = SparkSqlTransformStep(sourceTableName, sourceSql, emptyMap)

      // 2. target mapping
      val targetTableName = "__target"
      val targetSql = s"SELECT $selClause FROM $targetName"
      val targetTransStep = SparkSqlTransformStep(targetTableName, targetSql, emptyMap)

      // 3. joined
      val joinedTableName = "__joined"
      val joinedSelClause = selAliases
        .map { alias =>
          s"`$sourceTableName`.`$alias` AS `$alias`"
        }
        .mkString(", ")
      val onClause = aliases
        .map { alias =>
          s"coalesce(`$sourceTableName`.`$alias`, '') = coalesce(`$targetTableName`.`$alias`, '')"
        }
        .mkString(" AND ")
      val joinedSql = {
        s"SELECT $joinedSelClause FROM `$targetTableName` RIGHT JOIN `$sourceTableName` ON $onClause"
      }
      val joinedTransStep = SparkSqlTransformStep(joinedTableName, joinedSql, emptyMap)
      joinedTransStep.parentSteps += sourceTransStep
      joinedTransStep.parentSteps += targetTransStep

      // 4. group
      val groupTableName = "__group"
      val groupSelClause = selAliases
        .map { alias =>
          s"`$alias`"
        }
        .mkString(", ")
      val dupColName = details.getStringOrKey(_dup)
      val groupSql = {
        s"SELECT $groupSelClause, (COUNT(*) - 1) AS `$dupColName` " +
          s"FROM `$joinedTableName` GROUP BY $groupSelClause"
      }
      val groupTransStep =
        SparkSqlTransformStep(groupTableName, groupSql, emptyMap, None, cache = true)
      groupTransStep.parentSteps += joinedTransStep

      // 5. total metric
      val totalTableName = "__totalMetric"
      val totalColName = details.getStringOrKey(_total)
      val totalSql = procType match {
        case BatchProcessType => s"SELECT COUNT(*) AS `$totalColName` FROM `$sourceName`"
        case StreamingProcessType =>
          s"""
             |SELECT `${ConstantColumns.tmst}`, COUNT(*) AS `$totalColName`
             |FROM `$sourceName` GROUP BY `${ConstantColumns.tmst}`
           """.stripMargin
      }
      val totalMetricWriteStep = MetricWriteStep(totalColName, totalTableName, EntriesFlattenType)
      val totalTransStep =
        SparkSqlTransformStep(totalTableName, totalSql, emptyMap, Some(totalMetricWriteStep))

      // 6. unique record
      val uniqueRecordTableName = "__uniqueRecord"
      val uniqueRecordSql = {
        s"SELECT * FROM `$groupTableName` WHERE `$dupColName` = 0"
      }
      val uniqueRecordTransStep =
        SparkSqlTransformStep(uniqueRecordTableName, uniqueRecordSql, emptyMap)
      uniqueRecordTransStep.parentSteps += groupTransStep

      // 7. unique metric
      val uniqueTableName = "__uniqueMetric"
      val uniqueColName = details.getStringOrKey(_unique)
      val uniqueSql = procType match {
        case BatchProcessType =>
          s"SELECT COUNT(*) AS `$uniqueColName` FROM `$uniqueRecordTableName`"
        case StreamingProcessType =>
          s"""
             |SELECT `${ConstantColumns.tmst}`, COUNT(*) AS `$uniqueColName`
             |FROM `$uniqueRecordTableName` GROUP BY `${ConstantColumns.tmst}`
           """.stripMargin
      }
      val uniqueMetricWriteStep =
        MetricWriteStep(uniqueColName, uniqueTableName, EntriesFlattenType)
      val uniqueTransStep =
        SparkSqlTransformStep(uniqueTableName, uniqueSql, emptyMap, Some(uniqueMetricWriteStep))
      uniqueTransStep.parentSteps += uniqueRecordTransStep

      val transSteps1 = totalTransStep :: uniqueTransStep :: Nil

      val duplicationArrayName = details.getString(_duplicationArray, "")
      val transSteps2 = if (duplicationArrayName.nonEmpty) {
        // 8. duplicate record
        val dupRecordTableName = "__dupRecords"
        val dupRecordSql = {
          s"SELECT * FROM `$groupTableName` WHERE `$dupColName` > 0"
        }

        val dupRecordWriteStep = RecordWriteStep(ruleParam.getOutDfName(), dupRecordTableName)
        val dupRecordTransStep =
          SparkSqlTransformStep(
            dupRecordTableName,
            dupRecordSql,
            emptyMap,
            Some(dupRecordWriteStep),
            cache = true)

        // 9. duplicate metric
        val dupMetricTableName = "__dupMetric"
        val numColName = details.getStringOrKey(_num)
        val dupMetricSelClause = procType match {
          case BatchProcessType => s"`$dupColName`, COUNT(*) AS `$numColName`"

          case StreamingProcessType =>
            s"`${ConstantColumns.tmst}`, `$dupColName`, COUNT(*) AS `$numColName`"
        }
        val dupMetricGroupbyClause = procType match {
          case BatchProcessType => s"`$dupColName`"
          case StreamingProcessType => s"`${ConstantColumns.tmst}`, `$dupColName`"
        }
        val dupMetricSql = {
          s"""
             |SELECT $dupMetricSelClause FROM `$dupRecordTableName`
             |GROUP BY $dupMetricGroupbyClause
          """.stripMargin
        }
        val dupMetricWriteStep = {
          MetricWriteStep(duplicationArrayName, dupMetricTableName, ArrayFlattenType)
        }
        val dupMetricTransStep =
          SparkSqlTransformStep(
            dupMetricTableName,
            dupMetricSql,
            emptyMap,
            Some(dupMetricWriteStep))
        dupMetricTransStep.parentSteps += dupRecordTransStep

        dupMetricTransStep :: Nil
      } else Nil

      // full steps
      transSteps1 ++ transSteps2
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.parser.GriffinDslParser
import com.hsbc.gbm.bd.clm.measure.step.builder.dsl.transform.Expr2DQSteps

case class GriffinDslDQStepBuilder(dataSourceNames: Seq[String], functionNames: Seq[String])
    extends RuleParamStepBuilder {

  val filteredFunctionNames: Seq[String] = functionNames.filter { fn =>
    fn.matches("""^[a-zA-Z_]\w*$""")
  }
  val parser: GriffinDslParser = GriffinDslParser(dataSourceNames, filteredFunctionNames)

  def buildSteps(context: DQContext, ruleParam: RuleParam): Seq[DQStep] = {
    val name = getStepName(ruleParam.getOutDfName())
    val rule = ruleParam.getRule
    val dqType = ruleParam.getDqType
    try {
      val result = parser.parseRule(rule, dqType)
      if (result.successful) {
        val expr = result.get
        val expr2DQSteps = Expr2DQSteps(context, expr, ruleParam.replaceOutDfName(name))
        expr2DQSteps.getDQSteps
      } else {
        warn(s"parse rule [ $rule ] fails: \n$result")
        Nil
      }
    } catch {
      case e: Throwable =>
        error(s"generate rule plan $name fails: ${e.getMessage}", e)
        Nil
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/preproc: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.preproc

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.DslType._

/**
 * generate each entity pre-proc params by template defined in pre-proc param
 */
object PreProcParamMaker {

  case class StringAnyMap(values: Map[String, Any])

  def makePreProcRules(
      rules: Seq[RuleParam],
      suffix: String,
      dfName: String): (Seq[RuleParam], String) = {
    val len = rules.size
    val (newRules, _) = rules.zipWithIndex.foldLeft((Nil: Seq[RuleParam], dfName)) {
      (ret, pair) =>
        val (rls, prevOutDfName) = ret
        val (rule, i) = pair
        val inName = rule.getInDfName(prevOutDfName)
        val outName = if (i == len - 1) dfName else rule.getOutDfName(genNameWithIndex(dfName, i))
        val ruleWithNames = rule.replaceInOutDfName(inName, outName)
        (rls :+ makeNewPreProcRule(ruleWithNames, suffix), outName)
    }
    (newRules, withSuffix(dfName, suffix))
  }

  private def makeNewPreProcRule(rule: RuleParam, suffix: String): RuleParam = {
    val newInDfName = withSuffix(rule.getInDfName(), suffix)
    val newOutDfName = withSuffix(rule.getOutDfName(), suffix)
    val rpRule = rule.replaceInOutDfName(newInDfName, newOutDfName)
    rule.getDslType match {
      case DataFrameOpsType => rpRule
      case _ =>
        val newRule = replaceDfNameSuffix(rule.getRule, rule.getInDfName(), suffix)
        rpRule.replaceRule(newRule)
    }
  }

  private def genNameWithIndex(name: String, i: Int): String = s"$name$i"

  private def replaceDfNameSuffix(str: String, dfName: String, suffix: String): String = {
    val regexStr = s"(?i)$dfName"
    val replaceDfName = withSuffix(dfName, suffix)
    str.replaceAll(regexStr, replaceDfName)
  }

  def withSuffix(str: String, suffix: String): String = s"${str}_$suffix"

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.step.{DQStep, SeqDQStep}
import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.configuration.enums.OutputType._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.SeqDQStep
import com.hsbc.gbm.bd.clm.measure.step.write.{DataSourceUpdateWriteStep, MetricWriteStep, RecordWriteStep}

/**
 * build dq step by rule param
 */
trait RuleParamStepBuilder extends DQStepBuilder {

  type ParamType = RuleParam

  def buildDQStep(context: DQContext, param: ParamType): Option[DQStep] = {
    val steps = buildSteps(context, param)
    if (steps.size > 1) Some(SeqDQStep(steps))
    else if (steps.size == 1) steps.headOption
    else None
  }

  def buildSteps(context: DQContext, ruleParam: RuleParam): Seq[DQStep]

  protected def buildDirectWriteSteps(ruleParam: RuleParam): Seq[DQStep] = {
    val name = getStepName(ruleParam.getOutDfName())
    // metric writer
    val metricSteps = ruleParam
      .getOutputOpt(MetricOutputType)
      .map { metric =>
        MetricWriteStep(metric.getNameOpt.getOrElse(name), name, metric.getFlatten)
      }
      .toSeq
    // record writer
    val recordSteps = ruleParam
      .getOutputOpt(RecordOutputType)
      .map { record =>
        RecordWriteStep(record.getNameOpt.getOrElse(name), name)
      }
      .toSeq
    // update writer
    val dsCacheUpdateSteps = ruleParam
      .getOutputOpt(DscUpdateOutputType)
      .map { dsCacheUpdate =>
        DataSourceUpdateWriteStep(dsCacheUpdate.getNameOpt.getOrElse(""), name)
      }
      .toSeq

    metricSteps ++ recordSteps ++ dsCacheUpdateSteps
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.RuleParam
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.transform.SparkSqlTransformStep

case class SparkSqlDQStepBuilder() extends RuleParamStepBuilder {

  def buildSteps(context: DQContext, ruleParam: RuleParam): Seq[DQStep] = {
    val name = getStepName(ruleParam.getOutDfName())
    val transformStep = SparkSqlTransformStep(
      name,
      ruleParam.getRule,
      ruleParam.getDetails,
      None,
      ruleParam.getCache)
    transformStep +: buildDirectWriteSteps(ruleParam)
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/builder/udf: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.builder.udf

import org.apache.spark.sql.SparkSession

object GriffinUDFAgent {
  def register(sparkSession: SparkSession): Unit = {
    GriffinUDFs.register(sparkSession)
    GriffinUDAggFs.register(sparkSession)
  }
}

/**
 * user defined functions extension
 */
object GriffinUDFs {

  def register(sparkSession: SparkSession): Unit = {
    sparkSession.udf.register("index_of", indexOf _)
    sparkSession.udf.register("matches", matches _)
    sparkSession.udf.register("reg_replace", regReplace _)
  }

  private def indexOf(arr: Seq[String], v: String) = {
    arr.indexOf(v)
  }

  private def matches(s: String, regex: String) = {
    s.matches(regex)
  }

  private def regReplace(s: String, regex: String, replacement: String) = {
    s.replaceAll(regex, replacement)
  }

}

/**
 * aggregation functions extension
 */
object GriffinUDAggFs {

  def register(sparkSession: SparkSession): Unit = {}

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step

import scala.util.Try

import com.hsbc.gbm.bd.clm.measure.Loggable
import com.hsbc.gbm.bd.clm.measure.context.DQContext

trait DQStep extends Loggable {

  val name: String

  /**
   * @return execution success
   */
  def execute(context: DQContext): Try[Boolean]

  def getNames: Seq[String] = name :: Nil

}

object DQStepStatus extends Enumeration {
  val PENDING: DQStepStatus.Value = Value
  val RUNNING: DQStepStatus.Value = Value
  val COMPLETE: DQStepStatus.Value = Value
  val FAILED: DQStepStatus.Value = Value
}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/read: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.read

import org.apache.spark.sql._

import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep

trait ReadStep extends DQStep {

  val config: Map[String, Any]

  val cache: Boolean

  def execute(context: DQContext): Try[Boolean] = Try {
    info(s"read data source [$name]")
    read(context) match {
      case Some(df) =>
//        if (needCache) context.dataFrameCache.cacheDataFrame(name, df)
        context.runTimeTableRegister.registerTable(name, df)
        true
      case _ =>
        warn(s"read data source [$name] fails")
        false
    }
  }

  def read(context: DQContext): Option[DataFrame]

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.read

import org.apache.spark.sql._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.utils.DataFrameUtil._

case class UnionReadStep(name: String, readSteps: Seq[ReadStep]) extends ReadStep {

  val config: Map[String, Any] = Map()
  val cache: Boolean = false

  def read(context: DQContext): Option[DataFrame] = {
    val dfOpts = readSteps.map { readStep =>
      readStep.read(context)
    }
    if (dfOpts.nonEmpty) {
      dfOpts.reduce((a, b) => unionDfOpts(a, b))
    } else None
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step

import com.hsbc.gbm.bd.clm.measure.context.DQContext

import scala.util.{Failure, Success, Try}
import com.hsbc.gbm.bd.clm.measure.context.DQContext

/**
 * sequence of dq steps
 */
case class SeqDQStep(dqSteps: Seq[DQStep]) extends DQStep {

  val name: String = ""
  val rule: String = ""
  val details: Map[String, Any] = Map()

  /**
   * @return execution success
   */
  def execute(context: DQContext): Try[Boolean] = {
    dqSteps
      .map(_.execute(context))
      .foldLeft(Try(true))((ret, stepResult) => {
        (ret, stepResult) match {
          case (Success(_), nextResult) => nextResult
          case (Failure(_), _) => ret
        }
      })
  }

  override def getNames: Seq[String] = {
    dqSteps.foldLeft(Nil: Seq[String]) { (ret, dqStep) =>
      ret ++ dqStep.getNames
    }
  }

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/transform: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.transform

import java.util.Date

import org.apache.spark.sql.{Encoders, Row, _}
import org.apache.spark.sql.types._
import com.hsbc.gbm.bd.clm.measure.context.ContextId
import com.hsbc.gbm.bd.clm.measure.context.metric.{AccuracyMetric, CacheResults}
import com.hsbc.gbm.bd.clm.measure.context.metric.CacheResults.CacheResult
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * pre-defined data frame operations
 */
object DataFrameOps {

  final val _fromJson = "from_json"
  final val _accuracy = "accuracy"
  final val _clear = "clear"

  object AccuracyOprKeys {
    val _dfName = "df.name"
    val _miss = "miss"
    val _total = "total"
    val _matched = "matched"
    val _matchedFraction = "matchedFraction"
  }

  def fromJson(
      sparkSession: SparkSession,
      inputDfName: String,
      details: Map[String, Any]): DataFrame = {
    val _colName = "col.name"
    val colNameOpt = details.get(_colName).map(_.toString)

    implicit val encoder: Encoder[String] = Encoders.STRING

    val df: DataFrame = sparkSession.table(s"`$inputDfName`")
    val rdd = colNameOpt match {
      case Some(colName: String) => df.map(r => r.getAs[String](colName))
      case _ => df.map(_.getAs[String](0))
    }
    sparkSession.read.json(rdd) // slow process
  }

  def accuracy(
      sparkSession: SparkSession,
      inputDfName: String,
      contextId: ContextId,
      details: Map[String, Any]): DataFrame = {
    import AccuracyOprKeys._

    val miss = details.getStringOrKey(_miss)
    val total = details.getStringOrKey(_total)
    val matched = details.getStringOrKey(_matched)
    val matchedFraction = details.getStringOrKey(_matchedFraction)

    val updateTime = new Date().getTime

    def getLong(r: Row, k: String): Option[Long] = {
      try {
        Some(r.getAs[Long](k))
      } catch {
        case _: Throwable => None
      }
    }

    val df = sparkSession.table(s"`$inputDfName`")

    val results = df.rdd.flatMap { row =>
      try {
        val tmst = getLong(row, ConstantColumns.tmst).getOrElse(contextId.timestamp)
        val missCount = getLong(row, miss).getOrElse(0L)
        val totalCount = getLong(row, total).getOrElse(0L)
        val ar = AccuracyMetric(missCount, totalCount)
        if (ar.isLegal) Some((tmst, ar)) else None
      } catch {
        case _: Throwable => None
      }
    }.collect

    // cache and update results
    val updatedResults = CacheResults.update(results.map { pair =>
      val (t, r) = pair
      CacheResult(t, updateTime, r)
    })

    // generate metrics
    val schema = StructType(
      Array(
        StructField(ConstantColumns.tmst, LongType),
        StructField(miss, LongType),
        StructField(total, LongType),
        StructField(matched, LongType),
        StructField(matchedFraction, DoubleType),
        StructField(ConstantColumns.record, BooleanType),
        StructField(ConstantColumns.empty, BooleanType)))
    val rows = updatedResults.map { r =>
      val ar = r.result.asInstanceOf[AccuracyMetric]
      Row(
        r.timeStamp,
        ar.miss,
        ar.total,
        ar.getMatch,
        ar.matchFraction,
        !ar.initial,
        ar.eventual())
    }.toArray
    val rowRdd = sparkSession.sparkContext.parallelize(rows)
    val retDf = sparkSession.createDataFrame(rowRdd, schema)

    retDf
  }

  def clear(
      sparkSession: SparkSession,
      inputDfName: String,
      details: Map[String, Any]): DataFrame = {
    val df = sparkSession.table(s"`$inputDfName`")
    val emptyRdd = sparkSession.sparkContext.emptyRDD[Row]
    sparkSession.createDataFrame(emptyRdd, df.schema)
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.transform


import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.write.WriteStep

/**
 * data frame ops transform step
 */
case class DataFrameOpsTransformStep[T <: WriteStep](
    name: String,
    inputDfName: String,
    rule: String,
    details: Map[String, Any],
    writeStepOpt: Option[T] = None,
    cache: Boolean = false)
    extends TransformStep {

  def doExecute(context: DQContext): Try[Boolean] =
    Try {
      val sparkSession = context.sparkSession
      val df = rule match {
        case DataFrameOps._fromJson => DataFrameOps.fromJson(sparkSession, inputDfName, details)
        case DataFrameOps._accuracy =>
          DataFrameOps.accuracy(sparkSession, inputDfName, context.contextId, details)
        case DataFrameOps._clear => DataFrameOps.clear(sparkSession, inputDfName, details)
        case _ => throw new Exception(s"df opr [ $rule ] not supported")
      }
      if (cache) context.dataFrameCache.cacheDataFrame(name, df)
      context.runTimeTableRegister.registerTable(name, df)
      writeStepOpt match {
        case Some(writeStep) => writeStep.execute(context)
        case None => Try(true)
      }
    }.flatten

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.transform


import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.write.WriteStep

/**
 * spark sql transform step
 */
case class SparkSqlTransformStep[T <: WriteStep](
                                                  name: String,
                                                  rule: String,
                                                  details: Map[String, Any],
                                                  writeStepOpt: Option[T] = None,
                                                  cache: Boolean = false)
  extends TransformStep {

  def doExecute(context: DQContext): Try[Boolean] =
    Try {
      val sparkSession = context.sparkSession
      val df = sparkSession.sql(rule)
      if (cache) context.dataFrameCache.cacheDataFrame(name, df)
      context.runTimeTableRegister.registerTable(name, df)
      writeStepOpt match {
        case Some(writeStep) => writeStep.execute(context)
        case None => Try(true)
      }
    }.flatten

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.transform

import com.hsbc.gbm.bd.clm.measure.step.{DQStep, DQStepStatus}

import scala.collection.mutable
import scala.concurrent.ExecutionContext
import scala.concurrent.Future
import scala.concurrent.duration.Duration
import scala.util.{Failure, Success, Try}
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.DQStep
import com.hsbc.gbm.bd.clm.measure.step.DQStepStatus._
import com.hsbc.gbm.bd.clm.measure.utils.ThreadUtils

trait TransformStep extends DQStep {

  val rule: String

  val details: Map[String, Any]

  val cache: Boolean

  var status: DQStepStatus.Value = PENDING

  val parentSteps = new mutable.HashSet[TransformStep]

  def doExecute(context: DQContext): Try[Boolean]

  override def execute(context: DQContext): Try[Boolean] = {

    val threadName = Thread.currentThread().getName
    info(threadName + " begin transform step : \n" + debugString())

    // Submit parents Steps
    val parentStepFutures = parentSteps.filter(checkAndUpdateStatus).map { parentStep =>
      Future {
        val result = parentStep.execute(context)
        parentStep.synchronized {
          result match {
            case Success(_) => parentStep.status = COMPLETE
            case Failure(_) => parentStep.status = FAILED
          }
        }
        result
      }(TransformStep.transformStepContext)
    }

    val parentsResultSet = ThreadUtils.awaitResult(
      Future.sequence(parentStepFutures)(implicitly, TransformStep.transformStepContext),
      Duration.Inf)

    val parentsResult = parentsResultSet.foldLeft(Try(true)) { (ret, step) =>
      (ret, step) match {
        case (Success(_), nextResult) => nextResult
        case (Failure(_), _) => ret
      }
    }

    parentSteps.foreach(step => {
      while (step.status == RUNNING) {
        Thread.sleep(1000L)
      }
    })

    parentsResult match {
      case Success(_) =>
        info(threadName + " end transform step : \n" + debugString())
        doExecute(context)
      case Failure(_) =>
        error("Parent transform step failed!")
        parentsResult
    }
  }

  def checkAndUpdateStatus(step: TransformStep): Boolean = {
    step.synchronized {
      if (step.status == PENDING) {
        step.status = RUNNING
        true
      } else {
        false
      }
    }
  }

  def debugString(level: Int = 0): String = {
    val stringBuffer = new StringBuilder
    if (level > 0) {
      for (_ <- 0 until level) {
        stringBuffer.append("|   ")
      }
      stringBuffer.append("|---")
    }
    stringBuffer.append(name + "\n")
    parentSteps.foreach(parentStep => stringBuffer.append(parentStep.debugString(level + 1)))
    stringBuffer.toString()
  }
}

object TransformStep {
  private[transform] val transformStepContext =
    ExecutionContext.fromExecutorService(ThreadUtils.newDaemonCachedThreadPool("transform-step"))
}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/step/write: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import org.apache.commons.lang.StringUtils
import org.apache.spark.sql.DataFrame

import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.context.DQContext

/**
 * update data source streaming cache
 */
case class DataSourceUpdateWriteStep(dsName: String, inputName: String) extends WriteStep {

  val name: String = ""
  val writeTimestampOpt: Option[Long] = None

  def execute(context: DQContext): Try[Boolean] = Try {
    getDataSourceCacheUpdateDf(context) match {
      case Some(_) =>
        context.dataSources
          .find(ds => StringUtils.equals(ds.name, dsName))
      case _ =>
        warn(s"update $dsName from $inputName fails")
    }
    true
  }

  private def getDataFrame(context: DQContext, name: String): Option[DataFrame] = {
    try {
      val df = context.sparkSession.table(s"`$name`")
      Some(df)
    } catch {
      case e: Throwable =>
        error(s"get data frame $name fails", e)
        None
    }
  }

  private def getDataSourceCacheUpdateDf(context: DQContext): Option[DataFrame] =
    getDataFrame(context, inputName)

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import com.hsbc.gbm.bd.clm.measure.context.DQContext

import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.context.DQContext

/**
 * flush final metric map in context and write
 */
case class MetricFlushStep() extends WriteStep {

  val name: String = ""
  val inputName: String = ""
  val writeTimestampOpt: Option[Long] = None

  def execute(context: DQContext): Try[Boolean] = Try {
    context.metricWrapper.flush.foldLeft(true) { (ret, pair) =>
      val (t, metric) = pair
      val pr = try {
        context.getSinks(t).foreach { sink =>
          try {
            sink.sinkMetrics(metric.filterKeys(_ != "tmst").flatMap(ele => {
              if (ele._1 == "value") {
                Map(ele._1 -> ele._2.asInstanceOf[Map[String, Any]].foldLeft(Map[String, Any]())(
                  (x, xs) => {
                    val (key, subKey) = if (xs._1.contains("%%")) {
                      val pattern = "(.*)[%][%](.*)".r
                      val pattern(key, subKey) = xs._1
                      (key, subKey)
                    } else {
                      (xs._1, xs._2)
                    }

                    x + (key -> (xs._2 match {
                      case m: Map[String, _] => m.asInstanceOf[Map[String, Any]] ++ x.getOrElse(key, Map[String, Any]()).asInstanceOf[Map[String, Any]]
                      case a: Seq[Map[String, Any]] => Map(subKey -> a) ++ x.getOrElse(key, Map[String, Any]()).asInstanceOf[Map[String, Any]]
                      case _ => xs._2
                    }))
                  }
                ))
              } else Map(ele._1 -> ele._2)
            }))(context)
          } catch {
            case e: Throwable => error(s"sink metrics error: ${e.getMessage}", e)
          }
        }
        true
      } catch {
        case e: Throwable =>
          error(s"flush metrics error: ${e.getMessage}", e)
          false
      }
      ret && pr
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.FlattenType
import com.hsbc.gbm.bd.clm.measure.configuration.enums.{SimpleMode, TimestampMode}
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns

import scala.util.Try
import com.hsbc.gbm.bd.clm.measure.configuration.enums.SimpleMode
import com.hsbc.gbm.bd.clm.measure.configuration.enums.FlattenType.{ArrayFlattenType, EntriesFlattenType, FlattenType, MapFlattenType}
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.utils.JsonUtil
import com.hsbc.gbm.bd.clm.measure.utils.ParamUtil._

/**
 * write metrics into context metric wrapper
 */
case class MetricWriteStep(
    name: String,
    inputName: String,
    flattenType: FlattenType,
    writeTimestampOpt: Option[Long] = None)
    extends WriteStep {

  val emptyMetricMap: Map[Long, Map[String, Any]] = Map[Long, Map[String, Any]]()
  val emptyMap: Map[String, Any] = Map[String, Any]()

  def execute(context: DQContext): Try[Boolean] = Try {
    val timestamp = writeTimestampOpt.getOrElse(context.contextId.timestamp)

    // get metric list from data frame
    val metricMaps: Seq[Map[String, Any]] = getMetricMaps(context)

    // get timestamp and normalize metric
    val writeMode = writeTimestampOpt.map(_ => SimpleMode).getOrElse(context.writeMode)
    val timestampMetricMap: Map[Long, Map[String, Any]] = writeMode match {

      case SimpleMode =>
        val metrics: Map[String, Any] = flattenMetric(metricMaps, name, flattenType)
        emptyMetricMap + (timestamp -> metrics)

      case TimestampMode =>
        val tmstMetrics = metricMaps.map { metric =>
          val tmst = metric.getLong(ConstantColumns.tmst, timestamp)
          val pureMetric = metric.removeKeys(ConstantColumns.columns)
          (tmst, pureMetric)
        }
        tmstMetrics.groupBy(_._1).map { pair =>
          val (k, v) = pair
          val maps = v.map(_._2)
          val mtc = flattenMetric(maps, name, flattenType)
          (k, mtc)
        }
    }

    // write to metric wrapper
    timestampMetricMap.foreach { pair =>
      val (timestamp, v) = pair
      context.metricWrapper.insertMetric(timestamp, v)
    }

    true
  }

  private def getMetricMaps(context: DQContext): Seq[Map[String, Any]] = {
    try {
      val pdf = context.sparkSession.table(s"`$inputName`")
      val rows = pdf.collect()
      val columns = pdf.columns
      if (rows.size > 0) {
        rows.map(_.getValuesMap(columns))
      } else Nil
    } catch {
      case e: Throwable =>
        error(s"get metric $name fails", e)
        Nil
    }
  }

  private def flattenMetric(
      metrics: Seq[Map[String, Any]],
      name: String,
      flattenType: FlattenType): Map[String, Any] = {
    flattenType match {
      case EntriesFlattenType => metrics.headOption.getOrElse(emptyMap)
      case ArrayFlattenType => Map[String, Any](name -> metrics)
      case MapFlattenType =>
        val v = metrics.headOption.getOrElse(emptyMap)
        Map[String, Any](name -> v)
      case _ =>
        if (metrics.size > 1) Map[String, Any](name -> metrics)
        else metrics.headOption.getOrElse(emptyMap)
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import com.hsbc.gbm.bd.clm.measure.configuration.enums.{SimpleMode, TimestampMode}
import com.hsbc.gbm.bd.clm.measure.context.DQContext

import scala.util.Try
import org.apache.spark.rdd.RDD
import org.apache.spark.sql._
import com.hsbc.gbm.bd.clm.measure.configuration.enums._
import com.hsbc.gbm.bd.clm.measure.context.DQContext
import com.hsbc.gbm.bd.clm.measure.step.builder.ConstantColumns
import com.hsbc.gbm.bd.clm.measure.utils.JsonUtil

/**
 * write records needs to be sink
 */
case class RecordWriteStep(
                            name: String,
                            inputName: String,
                            filterTableNameOpt: Option[String] = None,
                            writeTimestampOpt: Option[Long] = None)
  extends WriteStep {

  def execute(context: DQContext): Try[Boolean] = Try {
    val timestamp = writeTimestampOpt.getOrElse(context.contextId.timestamp)

    val writeMode = writeTimestampOpt.map(_ => SimpleMode).getOrElse(context.writeMode)
    writeMode match {
      case SimpleMode =>
        // batch records
        val recordsOpt = getBatchRecords(context)
        // write records
        recordsOpt match {
          case Some(records) =>
            context.getSinks(timestamp).foreach { sink =>
              try {
                sink.sinkBatchRecords(records.drop("__tmst"), Option(this.name))
              } catch {
                case e: Throwable => error(s"sink records error: ${e.getMessage}", e)
              }
            }
          case _ =>
        }
      case TimestampMode =>
        // streaming records
        val (recordsOpt, emptyTimestamps) = getStreamingRecords(context)
        // write records
        recordsOpt.foreach { records =>
          records.foreach { pair =>
            val (t, strRecords) = pair
            context.getSinks(t).foreach { sink =>
              try {
                sink.sinkRecords(strRecords, name)
              } catch {
                case e: Throwable => error(s"sink records error: ${e.getMessage}", e)
              }
            }
          }
        }
        emptyTimestamps.foreach { t =>
          context.getSinks(t).foreach { sink =>
            try {
              sink.sinkRecords(Nil, name)
            } catch {
              case e: Throwable => error(s"sink records error: ${e.getMessage}", e)
            }
          }
        }
    }
    true
  }

  private def getTmst(row: Row, defTmst: Long): Long = {
    try {
      row.getAs[Long](ConstantColumns.tmst)
    } catch {
      case _: Throwable => defTmst
    }
  }

  private def getDataFrame(context: DQContext, name: String): Option[DataFrame] = {
    try {
      val df = context.sparkSession.table(s"`$name`")
      Some(df)
    } catch {
      case e: Throwable =>
        error(s"get data frame $name fails", e)
        None
    }
  }

  private def getFilterTableDataFrame(context: DQContext): Option[DataFrame] =
    filterTableNameOpt.flatMap(getDataFrame(context, _))

  private def getBatchRecords(context: DQContext): Option[DataFrame] = {
    getDataFrame(context, inputName)
  }

  private def getStreamingRecords(
                                   context: DQContext): (Option[RDD[(Long, Iterable[String])]], Set[Long]) = {
    implicit val encoder: Encoder[(Long, String)] =
      Encoders.tuple(Encoders.scalaLong, Encoders.STRING)
    val defTimestamp = context.contextId.timestamp
    getDataFrame(context, inputName) match {
      case Some(df) =>
        val (filterFuncOpt, emptyTimestamps) = getFilterTableDataFrame(context) match {
          case Some(filterDf) =>
            // timestamps with empty flag
            val tmsts: Array[(Long, Boolean)] = filterDf.collect.flatMap { row =>
              try {
                val tmst = getTmst(row, defTimestamp)
                val empty = row.getAs[Boolean](ConstantColumns.empty)
                Some((tmst, empty))
              } catch {
                case _: Throwable => None
              }
            }
            val emptyTmsts = tmsts.filter(_._2).map(_._1).toSet
            val recordTmsts = tmsts.filter(!_._2).map(_._1).toSet
            val filterFuncOpt: Option[Long => Boolean] = if (recordTmsts.nonEmpty) {
              Some((t: Long) => recordTmsts.contains(t))
            } else None

            (filterFuncOpt, emptyTmsts)
          case _ => (Some((_: Long) => true), Set[Long]())
        }

        // filter timestamps need to record
        filterFuncOpt match {
          case Some(filterFunc) =>
            val records = df.flatMap { row =>
              val tmst = getTmst(row, defTimestamp)
              if (filterFunc(tmst)) {
                try {
                  val map = SparkRowFormatter.formatRow(row)
                  val str = JsonUtil.toJson(map)
                  Some((tmst, str))
                } catch {
                  case _: Throwable => None
                }
              } else None
            }
            (Some(records.rdd.groupByKey), emptyTimestamps)
          case _ => (None, emptyTimestamps)
        }
      case _ => (None, Set[Long]())
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import scala.collection.mutable.ArrayBuffer

import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{ArrayType, DataType, StructField, StructType}

/**
 * spark row formatter
 */
object SparkRowFormatter {

  def formatRow(row: Row): Map[String, Any] = {
    formatRowWithSchema(row, row.schema)
  }

  private def formatRowWithSchema(row: Row, schema: StructType): Map[String, Any] = {
    formatStruct(schema.fields, row)
  }

  private def formatStruct(schema: Seq[StructField], r: Row) = {
    val paired = schema.zip(r.toSeq)
    paired.foldLeft(Map[String, Any]())((s, p) => s ++ formatItem(p))
  }

  private def formatItem(p: (StructField, Any)): Map[String, Any] = {
    p match {
      case (sf, a) =>
        sf.dataType match {
          case ArrayType(et, _) =>
            Map(
              sf.name ->
                (if (a == null) a else formatArray(et, a.asInstanceOf[ArrayBuffer[Any]])))
          case StructType(s) =>
            Map(sf.name -> (if (a == null) a else formatStruct(s, a.asInstanceOf[Row])))
          case _ => Map(sf.name -> a)
        }
    }
  }

  private def formatArray(et: DataType, arr: ArrayBuffer[Any]): Seq[Any] = {
    et match {
      case StructType(s) => arr.map(e => formatStruct(s, e.asInstanceOf[Row]))
      case ArrayType(t, _) =>
        arr.map(e => formatArray(t, e.asInstanceOf[ArrayBuffer[Any]]))
      case _ => arr
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.step.write

import com.hsbc.gbm.bd.clm.measure.step.DQStep

trait WriteStep extends DQStep {

  val inputName: String

  val writeTimestampOpt: Option[Long]

  override def getNames: Seq[String] = Nil

}
cat: ./clientlifecycle-common-measure/src/main/scala/com/hsbc/gbm/bd/clm/measure/utils: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import java.util.concurrent.TimeUnit

import com.hsbc.gbm.bd.clm.measure.Loggable


object CommonUtils extends Loggable {

  /**
   * Executes a given code block and logs the time taken for its execution.
   *
   * @param f Arbitrary code block
   * @param timeUnit required for time conversion to desired unit. Default: [[TimeUnit.SECONDS]]
   * @tparam T resultant type parameter
   * @return result of type T
   */
  def timeThis[T](f: => T, timeUnit: TimeUnit = TimeUnit.SECONDS): T = {
    val startNanos = System.nanoTime()
    val result = f
    val endNanos = System.nanoTime()

    griffinLogger.info(s"Time taken: ${timeUnit
      .convert(endNanos - startNanos, TimeUnit.NANOSECONDS)} ${timeUnit.name().toLowerCase}")

    result
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

object DataFrameUtil {

  def unionDfOpts(dfOpt1: Option[DataFrame], dfOpt2: Option[DataFrame]): Option[DataFrame] = {
    (dfOpt1, dfOpt2) match {
      case (Some(df1), Some(df2)) => Some(unionByName(df1, df2))
      case (Some(_), _) => dfOpt1
      case (_, Some(_)) => dfOpt2
      case _ => None
    }
  }

  def unionByName(a: DataFrame, b: DataFrame): DataFrame = {
    val columns = a.columns.toSet.intersect(b.columns.toSet).map(col).toSeq
    a.select(columns: _*).union(b.select(columns: _*))
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import java.io.File
import java.net.URI

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.collection.mutable.{Map => MutableMap}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem


object FSUtil extends Loggable {

  private val fsMap: MutableMap[String, FileSystem] = MutableMap()
  private val defaultFS: FileSystem = FileSystem.get(getConfiguration)

  def getFileSystem(path: String): FileSystem = {
    getUriOpt(path) match {
      case Some(uri) =>
        fsMap.get(uri.getScheme) match {
          case Some(fs) => fs
          case _ =>
            val fs = try {
              FileSystem.get(uri, getConfiguration)
            } catch {
              case e: Throwable =>
                error(s"get file system error: ${e.getMessage}", e)
                throw e
            }
            fsMap += (uri.getScheme -> fs)
            fs
        }
      case _ => defaultFS
    }
  }

  private def getConfiguration: Configuration = {
    val conf = new Configuration()
    conf.setBoolean("dfs.support.append", true)
//    conf.set("fs.defaultFS", "hdfs://localhost")    // debug in hdfs localhost env
    conf
  }

  private def getUriOpt(path: String): Option[URI] = {
    val uriOpt = try {
      Some(new URI(path))
    } catch {
      case _: Throwable => None
    }
    uriOpt.flatMap { uri =>
      if (uri.getScheme == null) {
        try {
          Some(new File(path).toURI)
        } catch {
          case _: Throwable => None
        }
      } else Some(uri)
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import com.hsbc.gbm.bd.clm.measure.Loggable
import org.apache.hadoop.fs.{FSDataInputStream, FSDataOutputStream, Path}


object HdfsUtil extends Loggable {

  private val seprator = "/"

  private def getFS(implicit path: Path) = FSUtil.getFileSystem(path.toString)

  def existPath(filePath: String): Boolean = {
    try {
      implicit val path: Path = new Path(filePath)
      getFS.exists(path)
    } catch {
      case _: Throwable => false
    }
  }

  def existFileInDir(dirPath: String, fileName: String): Boolean = {
    val filePath = getHdfsFilePath(dirPath, fileName)
    existPath(filePath)
  }

  def createFile(filePath: String): FSDataOutputStream = {
    implicit val path: Path = new Path(filePath)
    if (getFS.exists(path)) getFS.delete(path, true)
    getFS.create(path)
  }

  def appendOrCreateFile(filePath: String): FSDataOutputStream = {
    implicit val path: Path = new Path(filePath)
    if (getFS.getConf.getBoolean("dfs.support.append", false) && getFS.exists(path)) {
      getFS.append(path)
    } else createFile(filePath)
  }

  def openFile(filePath: String): FSDataInputStream = {
    implicit val path: Path = new Path(filePath)
    getFS.open(path)
  }

  def writeContent(filePath: String, message: String): Unit = {
    val out = createFile(filePath)
    out.write(message.getBytes("utf-8"))
    out.close()
  }

  def withHdfsFile(filePath: String, appendIfExists: Boolean = true)(
      f: FSDataOutputStream => Unit): Unit = {
    val out =
      if (appendIfExists) {
        appendOrCreateFile(filePath)
      } else {
        createFile(filePath)
      }

    f(out)
    out.close()
  }

  def createEmptyFile(filePath: String): Unit = {
    val out = createFile(filePath)
    out.close()
  }

  def getHdfsFilePath(parentPath: String, fileName: String): String = {
    if (parentPath.endsWith(seprator)) parentPath + fileName else parentPath + seprator + fileName
  }

  def deleteHdfsPath(dirPath: String): Unit = {
    try {
      implicit val path: Path = new Path(dirPath)
      if (getFS.exists(path)) getFS.delete(path, true)
    } catch {
      case e: Throwable => error(s"delete path [$dirPath] error: ${e.getMessage}", e)
    }
  }

  def listSubPathsByType(
      dirPath: String,
      subType: String,
      fullPath: Boolean = false): Iterable[String] = {
    if (existPath(dirPath)) {
      try {
        implicit val path: Path = new Path(dirPath)
        val fileStatusArray = getFS.listStatus(path)
        fileStatusArray
          .filter { fileStatus =>
            subType match {
              case "dir" => fileStatus.isDirectory
              case "file" => fileStatus.isFile
              case _ => true
            }
          }
          .map { fileStatus =>
            val fname = fileStatus.getPath.getName
            if (fullPath) getHdfsFilePath(dirPath, fname) else fname
          }
      } catch {
        case e: Throwable =>
          warn(s"list path [$dirPath] warn: ${e.getMessage}", e)
          Nil
      }
    } else Nil
  }

  def listSubPathsByTypes(
      dirPath: String,
      subTypes: Iterable[String],
      fullPath: Boolean = false): Iterable[String] = {
    subTypes.flatMap { subType =>
      listSubPathsByType(dirPath, subType, fullPath)
    }
  }

  def fileNameFromPath(filePath: String): String = {
    val path = new Path(filePath)
    path.getName
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import scala.util.matching.Regex

import org.apache.http.client.methods.{HttpGet, HttpPost}
import org.apache.http.entity.{ContentType, StringEntity}
import org.apache.http.impl.client.HttpClientBuilder
import scalaj.http._

object HttpUtil {

  val GET_REGEX: Regex = """^(?i)get$""".r
  val POST_REGEX: Regex = """^(?i)post$""".r
  val PUT_REGEX: Regex = """^(?i)put$""".r
  val DELETE_REGEX: Regex = """^(?i)delete$""".r

  def postData(
      url: String,
      params: Map[String, Object],
      headers: Map[String, Object],
      data: String): Boolean = {
    val response = Http(url)
      .params(convertObjMap2StrMap(params))
      .headers(convertObjMap2StrMap(headers))
      .postData(data)
      .asString

    response.isSuccess
  }

  def doHttpRequest(
      url: String,
      method: String,
      params: Map[String, Object],
      headers: Map[String, Object],
      data: String): Boolean = {
    val client = HttpClientBuilder.create.build
    method match {
      case POST_REGEX() =>
        val post = new HttpPost(url)
        convertObjMap2StrMap(headers) foreach (header => post.addHeader(header._1, header._2))
        post.setEntity(new StringEntity(data, ContentType.APPLICATION_JSON))

        // send the post request
        val response = client.execute(post)
        val code = response.getStatusLine.getStatusCode
        code >= 200 && code < 300
      case PUT_REGEX() =>
        val get = new HttpGet(url)
        convertObjMap2StrMap(headers) foreach (header => get.addHeader(header._1, header._2))
        val response = client.execute(get)
        val code = response.getStatusLine.getStatusCode
        code >= 200 && code < 300
      case _ => false
    }
  }

  def httpRequest(
      url: String,
      method: String,
      params: Map[String, Object],
      headers: Map[String, Object],
      data: String): Boolean = {
    val httpReq = Http(url)
      .params(convertObjMap2StrMap(params))
      .headers(convertObjMap2StrMap(headers))
    method match {
      case POST_REGEX() =>
        val res = httpReq.postData(data).asString
        res.isSuccess
      case PUT_REGEX() =>
        val res = httpReq.put(data).asString
        res.isSuccess
      case _ => false
    }
  }

  private def convertObjMap2StrMap(map: Map[String, Object]): Map[String, String] = {
    map.map(pair => pair._1 -> pair._2.toString)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import java.io.InputStream

import scala.reflect._

import com.fasterxml.jackson.databind.{DeserializationFeature, ObjectMapper}
import com.fasterxml.jackson.module.scala.DefaultScalaModule

object JsonUtil {
  val mapper = new ObjectMapper()
  mapper.registerModule(DefaultScalaModule)
  mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)

  def toJson(value: Map[Symbol, Any]): String = {
    toJson(value map { case (k, v) => k.name -> v })
  }

  def toJson(value: Any): String = {
    mapper.writeValueAsString(value)
  }

  def fromJson[T: ClassTag](json: String): T = {
    mapper.readValue[T](json, classTag[T].runtimeClass.asInstanceOf[Class[T]])
  }

  def fromJson[T: ClassTag](is: InputStream): T = {
    mapper.readValue[T](is, classTag[T].runtimeClass.asInstanceOf[Class[T]])
  }

  def toAnyMap(json: String): Map[String, Any] = {
    mapper.readValue(json, classOf[Map[String, Any]])
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import scala.reflect.ClassTag
import scala.util.Try

object ParamUtil {

  object TransUtil {
    def toAny(value: Any): Option[Any] = Some(value)
    def toAnyRef[T: ClassTag](value: Any): Option[T] = {
      value match {
        case v: T => Some(v)
        case _ => None
      }
    }
    def toStringOpt(value: Any): Option[String] = {
      value match {
        case v: String => Some(v)
        case v => Some(v.toString)
      }
    }
    def toByte(value: Any): Option[Byte] = {
      try {
        value match {
          case v: String => Some(v.toByte)
          case v: Byte => Some(v.toByte)
          case v: Short => Some(v.toByte)
          case v: Int => Some(v.toByte)
          case v: Long => Some(v.toByte)
          case v: Float => Some(v.toByte)
          case v: Double => Some(v.toByte)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toShort(value: Any): Option[Short] = {
      try {
        value match {
          case v: String => Some(v.toShort)
          case v: Byte => Some(v.toShort)
          case v: Short => Some(v.toShort)
          case v: Int => Some(v.toShort)
          case v: Long => Some(v.toShort)
          case v: Float => Some(v.toShort)
          case v: Double => Some(v.toShort)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toInt(value: Any): Option[Int] = {
      try {
        value match {
          case v: String => Some(v.toInt)
          case v: Byte => Some(v.toInt)
          case v: Short => Some(v.toInt)
          case v: Int => Some(v.toInt)
          case v: Long => Some(v.toInt)
          case v: Float => Some(v.toInt)
          case v: Double => Some(v.toInt)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toLong(value: Any): Option[Long] = {
      try {
        value match {
          case v: String => Some(v.toLong)
          case v: Byte => Some(v.toLong)
          case v: Short => Some(v.toLong)
          case v: Int => Some(v.toLong)
          case v: Long => Some(v.toLong)
          case v: Float => Some(v.toLong)
          case v: Double => Some(v.toLong)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toFloat(value: Any): Option[Float] = {
      try {
        value match {
          case v: String => Some(v.toFloat)
          case v: Byte => Some(v.toFloat)
          case v: Short => Some(v.toFloat)
          case v: Int => Some(v.toFloat)
          case v: Long => Some(v.toFloat)
          case v: Float => Some(v.toFloat)
          case v: Double => Some(v.toFloat)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toDouble(value: Any): Option[Double] = {
      try {
        value match {
          case v: String => Some(v.toDouble)
          case v: Byte => Some(v.toDouble)
          case v: Short => Some(v.toDouble)
          case v: Int => Some(v.toDouble)
          case v: Long => Some(v.toDouble)
          case v: Float => Some(v.toDouble)
          case v: Double => Some(v.toDouble)
          case _ => None
        }
      } catch {
        case _: NumberFormatException => None
      }
    }
    def toBoolean(value: Any): Option[Boolean] = {
      try {
        value match {
          case v: String => Some(v.toBoolean)
          case v: Boolean => Some(v)
          case _ => None
        }
      } catch {
        case _: IllegalArgumentException => None
      }
    }
  }
  import TransUtil._

  implicit class ParamMap(params: Map[String, Any]) {
    def getAny(key: String, defValue: Any): Any = {
      params.get(key).flatMap(toAny).getOrElse(defValue)
    }

    def getAnyRef[T: ClassTag](key: String, defValue: T): T = {
      params.get(key).flatMap(toAnyRef[T]).getOrElse(defValue)
    }

    def getString(key: String, defValue: String): String = {
      params.get(key).flatMap(toStringOpt).getOrElse(defValue)
    }

    def getLazyString(key: String, defValue: () => String): String = {
      params.get(key).flatMap(toStringOpt).getOrElse(defValue())
    }

    def getStringOrKey(key: String): String = getString(key, key)

    def getByte(key: String, defValue: Byte): Byte = {
      params.get(key).flatMap(toByte).getOrElse(defValue)
    }

    def getShort(key: String, defValue: Short): Short = {
      params.get(key).flatMap(toShort).getOrElse(defValue)
    }

    def getInt(key: String, defValue: Int): Int = {
      params.get(key).flatMap(toInt).getOrElse(defValue)
    }

    def getLong(key: String, defValue: Long): Long = {
      params.get(key).flatMap(toLong).getOrElse(defValue)
    }

    def getFloat(key: String, defValue: Float): Float = {
      params.get(key).flatMap(toFloat).getOrElse(defValue)
    }

    def getDouble(key: String, defValue: Double): Double = {
      params.get(key).flatMap(toDouble).getOrElse(defValue)
    }

    def getBoolean(key: String, defValue: Boolean): Boolean = {
      params.get(key).flatMap(toBoolean).getOrElse(defValue)
    }

    def getParamAnyMap(
        key: String,
        defValue: Map[String, Any] = Map[String, Any]()): Map[String, Any] = {
      params.get(key) match {
        case Some(v: Map[_, _]) => v.map(pair => (pair._1.toString, pair._2))
        case _ => defValue
      }
    }

    def getParamStringMap(
        key: String,
        defValue: Map[String, String] = Map[String, String]()): Map[String, String] = {
      params.get(key) match {
        case Some(v: Map[_, _]) => v.map(pair => (pair._1.toString, pair._2.toString))
        case _ => defValue
      }
    }

    def getStringArr(key: String, defValue: Seq[String] = Nil): Seq[String] = {
      params.get(key) match {
        case Some(seq: Seq[_]) => seq.flatMap(toStringOpt)
        case _ => defValue
      }
    }

    def getDoubleArr(key: String, defValue: Seq[Double] = Nil): Seq[Double] = {
      params.get(key) match {
        case Some(seq: Seq[_]) => seq.flatMap(toDouble)
        case _ => defValue
      }
    }

    def addIfNotExist(key: String, value: Any): Map[String, Any] = {
      params.get(key) match {
        case None => params + (key -> value)
        case _ => params
      }
    }

    def removeKeys(keys: Iterable[String]): Map[String, Any] = {
      params -- keys
    }
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import java.util.concurrent._

import scala.concurrent.{Awaitable, ExecutionContext, ExecutionContextExecutor}
import scala.concurrent.duration.Duration
import scala.concurrent.forkjoin.{
  ForkJoinPool => SForkJoinPool,
  ForkJoinWorkerThread => SForkJoinWorkerThread
}
import scala.util.control.NonFatal

import com.google.common.util.concurrent.{MoreExecutors, ThreadFactoryBuilder}

object ThreadUtils {

  private val sameThreadExecutionContext =
    ExecutionContext.fromExecutorService(MoreExecutors.sameThreadExecutor())

  /**
   * An `ExecutionContextExecutor` that runs each task in the thread that invokes `execute/submit`.
   * The caller should make sure the tasks running in this `ExecutionContextExecutor` are short and
   * never block.
   */
  def sameThread: ExecutionContextExecutor = sameThreadExecutionContext

  /**
   * Create a thread factory that names threads with a prefix and also sets the threads to daemon.
   */
  def namedThreadFactory(prefix: String): ThreadFactory = {
    new ThreadFactoryBuilder().setDaemon(true).setNameFormat(prefix + "-%d").build()
  }

  /**
   * Wrapper over newCachedThreadPool. Thread names are formatted as prefix-ID, where ID is a
   * unique, sequentially assigned integer.
   */
  def newDaemonCachedThreadPool(prefix: String): ThreadPoolExecutor = {
    val threadFactory = namedThreadFactory(prefix)
    Executors.newCachedThreadPool(threadFactory).asInstanceOf[ThreadPoolExecutor]
  }

  /**
   * Create a cached thread pool whose max number of threads is `maxThreadNumber`. Thread names
   * are formatted as prefix-ID, where ID is a unique, sequentially assigned integer.
   */
  def newDaemonCachedThreadPool(
      prefix: String,
      maxThreadNumber: Int,
      keepAliveSeconds: Int = 60): ThreadPoolExecutor = {
    val threadFactory = namedThreadFactory(prefix)
    val threadPool = new ThreadPoolExecutor(
      maxThreadNumber, // corePoolSize: the max number of threads to create before queuing the tasks
      maxThreadNumber, // maximumPoolSize: because we use LinkedBlockingDeque, this one is not used
      keepAliveSeconds,
      TimeUnit.SECONDS,
      new LinkedBlockingQueue[Runnable],
      threadFactory)
    threadPool.allowCoreThreadTimeOut(true)
    threadPool
  }

  /**
   * Wrapper over newFixedThreadPool. Thread names are formatted as prefix-ID, where ID is a
   * unique, sequentially assigned integer.
   */
  def newDaemonFixedThreadPool(nThreads: Int, prefix: String): ThreadPoolExecutor = {
    val threadFactory = namedThreadFactory(prefix)
    Executors.newFixedThreadPool(nThreads, threadFactory).asInstanceOf[ThreadPoolExecutor]
  }

  /**
   * Wrapper over newSingleThreadExecutor.
   */
  def newDaemonSingleThreadExecutor(threadName: String): ExecutorService = {
    val threadFactory =
      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(threadName).build()
    Executors.newSingleThreadExecutor(threadFactory)
  }

  /**
   * Wrapper over ScheduledThreadPoolExecutor.
   */
  def newDaemonSingleThreadScheduledExecutor(threadName: String): ScheduledExecutorService = {
    val threadFactory =
      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(threadName).build()
    val executor = new ScheduledThreadPoolExecutor(1, threadFactory)
    // By default, a cancelled task is not automatically removed from the work queue until its delay
    // elapses. We have to enable it manually.
    executor.setRemoveOnCancelPolicy(true)
    executor
  }

  /**
   * Run a piece of code in a new thread and return the result. Exception in the new thread is
   * thrown in the caller thread with an adjusted stack trace that removes references to this
   * method for clarity. The exception stack traces will be like the following
   *
   * SomeException: exception-message
   *   at CallerClass.body-method (sourcefile.scala)
   *   at ... run in separate thread using org.apache.griffin.measure.utils.ThreadUtils ... ()
   *   at CallerClass.caller-method (sourcefile.scala)
   *   ...
   */
  def runInNewThread[T](threadName: String, isDaemon: Boolean = true)(body: => T): T = {
    @volatile var exception: Option[Throwable] = None
    @volatile var result: T = null.asInstanceOf[T]

    val thread = new Thread(threadName) {
      override def run(): Unit = {
        try {
          result = body
        } catch {
          case NonFatal(e) =>
            exception = Some(e)
        }
      }
    }
    thread.setDaemon(isDaemon)
    thread.start()
    thread.join()

    exception match {
      case Some(realException) =>
        // Remove the part of the stack that shows method calls into this helper method
        // This means drop everything from the top until the stack element
        // ThreadUtils.runInNewThread(), and then drop that as well (hence the `drop(1)`).
        val baseStackTrace = Thread
          .currentThread()
          .getStackTrace
          .dropWhile(!_.getClassName.contains(this.getClass.getSimpleName))
          .drop(1)

        // Remove the part of the new thread stack that shows methods call from this helper method
        val extraStackTrace = realException.getStackTrace.takeWhile(
          !_.getClassName.contains(this.getClass.getSimpleName))

        // Combine the two stack traces, with a place holder just specifying that there
        // was a helper method used, without any further details of the helper
        val placeHolderStackElem = new StackTraceElement(
          s"... run in separate thread using ${ThreadUtils.getClass.getName.stripSuffix("$")} ..",
          " ",
          "",
          -1)
        val finalStackTrace = extraStackTrace ++ Seq(placeHolderStackElem) ++ baseStackTrace

        // Update the stack trace and rethrow the exception in the caller thread
        realException.setStackTrace(finalStackTrace)
        throw realException
      case None =>
        result
    }
  }

  /**
   * Construct a new Scala ForkJoinPool with a specified max parallelism and name prefix.
   */
  def newForkJoinPool(prefix: String, maxThreadNumber: Int): SForkJoinPool = {
    // Custom factory to set thread names
    val factory = new SForkJoinPool.ForkJoinWorkerThreadFactory {
      override def newThread(pool: SForkJoinPool): SForkJoinWorkerThread =
        new SForkJoinWorkerThread(pool) {
          setName(prefix + "-" + super.getName)
        }
    }
    new SForkJoinPool(
      maxThreadNumber,
      factory,
      null, // handler
      false // asyncMode
    )
  }

  // scalastyle:off awaitresult
  /**
   * Preferred alternative to `Await.result()`.
   *
   * This method wraps and re-throws any exceptions thrown by the underlying `Await` call, ensuring
   * that this thread's stack trace appears in logs.
   *
   * In addition, it calls `Awaitable.result` directly to avoid using `ForkJoinPool`'s
   * `BlockingContext`. Codes running in the user's thread may be in a thread of Scala ForkJoinPool.
   * As concurrent executions in ForkJoinPool may see some [[ThreadLocal]] value unexpectedly, this
   * method basically prevents ForkJoinPool from running other tasks in the current waiting thread.
   * In general, we should use this method because it's hard to debug when [[ThreadLocal]]s leak
   * to other tasks.
   */
  @throws(classOf[Exception])
  def awaitResult[T](awaitable: Awaitable[T], atMost: Duration): T = {
    try {
      // `awaitPermission` is not actually used anywhere so it's safe to pass in null here.
      val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait]
      awaitable.result(atMost)(awaitPermission)
    } catch {
      // TimeoutException is thrown in the current thread, so not need to warp the exception.
      case NonFatal(t) if !t.isInstanceOf[TimeoutException] =>
        throw new Exception("Exception thrown in awaitResult: ", t)
    }
  }
  // scalastyle:on awaitresult

  // scalastyle:off awaitready
  /**
   * Preferred alternative to `Await.ready()`.
   *
   * @see [[awaitResult]]
   */
  @throws(classOf[Exception])
  def awaitReady[T](awaitable: Awaitable[T], atMost: Duration): awaitable.type = {
    try {
      // `awaitPermission` is not actually used anywhere so it's safe to pass in null here.
      val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait]
      awaitable.ready(atMost)(awaitPermission)
    } catch {
      // TimeoutException is thrown in the current thread, so not need to warp the exception.
      case NonFatal(t) if !t.isInstanceOf[TimeoutException] =>
        throw new Exception("Exception thrown in awaitResult: ", t)
    }
  }
  // scalastyle:on awaitready
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.hsbc.gbm.bd.clm.measure.utils

import com.hsbc.gbm.bd.clm.measure.Loggable

import scala.util.{Failure, Success, Try}
import scala.util.matching.Regex


object TimeUtil extends Loggable {

  private object Units {
    case class TimeUnit(name: String, shortName: String, ut: Long, regex: Regex) {
      def toMs(t: Long): Long = t * ut
      def fromMs(ms: Long): Long = ms / ut
      def fitUnit(ms: Long): Boolean = ms % ut == 0
    }

    val dayUnit: TimeUnit = TimeUnit("day", "d", 24 * 60 * 60 * 1000, """^(?i)d(?:ay)?$""".r)
    val hourUnit: TimeUnit = TimeUnit("hour", "h", 60 * 60 * 1000, """^(?i)h(?:our|r)?$""".r)
    val minUnit: TimeUnit = TimeUnit("minute", "m", 60 * 1000, """^(?i)m(?:in(?:ute)?)?$""".r)
    val secUnit: TimeUnit = TimeUnit("second", "s", 1000, """^(?i)s(?:ec(?:ond)?)?$""".r)
    val msUnit: TimeUnit =
      TimeUnit("millisecond", "ms", 1, """^(?i)m(?:illi)?s(?:ec(?:ond)?)?$""".r)

    val timeUnits: List[TimeUnit] = dayUnit :: hourUnit :: minUnit :: secUnit :: msUnit :: Nil
  }
  import Units._

//  final val TimeRegex = """^([+\-]?\d+)(ms|s|m|h|d)$""".r
  final val TimeRegex = """^([+\-]?\d+)([a-zA-Z]+)$""".r
  final val PureTimeRegex = """^([+\-]?\d+)$""".r

  def milliseconds(timeString: String): Option[Long] = {
    val value: Option[Long] = {
      Try {
        timeString match {
          case TimeRegex(time, unit) =>
            val t = time.toLong
            unit match {
              case dayUnit.regex() => dayUnit.toMs(t)
              case hourUnit.regex() => hourUnit.toMs(t)
              case minUnit.regex() => minUnit.toMs(t)
              case secUnit.regex() => secUnit.toMs(t)
              case msUnit.regex() => msUnit.toMs(t)
              case _ => throw new Exception(s"$timeString is invalid time format")
            }
          case PureTimeRegex(time) =>
            val t = time.toLong
            msUnit.toMs(t)
          case _ => throw new Exception(s"$timeString is invalid time format")
        }
      } match {
        case Success(v) => Some(v)
        case Failure(_) => None
      }
    }
    value
  }

  def timeToUnit(ms: Long, unit: String): Long = {
    unit match {
      case dayUnit.regex() => dayUnit.fromMs(ms)
      case hourUnit.regex() => hourUnit.fromMs(ms)
      case minUnit.regex() => minUnit.fromMs(ms)
      case secUnit.regex() => secUnit.fromMs(ms)
      case msUnit.regex() => msUnit.fromMs(ms)
      case _ => ms
    }
  }

  def timeFromUnit(t: Long, unit: String): Long = {
    unit match {
      case dayUnit.regex() => dayUnit.toMs(t)
      case hourUnit.regex() => hourUnit.toMs(t)
      case minUnit.regex() => minUnit.toMs(t)
      case secUnit.regex() => secUnit.toMs(t)
      case msUnit.regex() => msUnit.toMs(t)
      case _ => t
    }
  }

  def time2String(t: Long): String = {
    val matchedUnitOpt = timeUnits.foldLeft(None: Option[TimeUnit]) { (retOpt, unit) =>
      if (retOpt.isEmpty && unit.fitUnit(t)) Some(unit) else retOpt
    }
    val unit = matchedUnitOpt.getOrElse(msUnit)
    val unitTime = unit.fromMs(t)
    val unitStr = unit.shortName
    s"$unitTime$unitStr"
  }

}
cat: ./clientlifecycle-common-measure/src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./clientlifecycle-common-measure/src/site/resources: Is a directory
cat: ./clientlifecycle-common-measure/src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./clientlifecycle-common-measure/src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>cat: ./clientlifecycle-common-measure/src/test: Is a directory
cat: ./clientlifecycle-common-measure/src/test/resources: Is a directory
cat: ./clientlifecycle-common-measure/src/test/resources/data: Is a directory
tresataId_sub|booking_country|cin|source
|bk1|cin1|ccs
tid||cin2|ccs
||cin2|ccs
||cin2|ccs
tid3|bk3|cin3|ccs
tid3|bk3|cin3|ccs
tid3|bk3|cin3|ccs
tid3||cin3|ccs
tid4|bk4|cin4|isd
tid4|bk4|cin4|isd
tid4|bk4|cin4|isd
tid5|bk5|cin5|omstresataId_sub|booking_country_manual|cin|source
tid1|bk1|cin|ccs
tid2|bk2|cin|ccs
tid3|bk3|cin|ccs#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#


log4j.rootLogger=WARN, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss.SSS} %-5p [%c] - %m%ncat: ./clientlifecycle-common-measure/src/test/resources/measure: Is a directory
{
  "rules": [
    {
      "dsl.type": "griffin-dsl",
      "dq.type": "distinct",
      "out.dataframe.name": "cda_distinct",
      "rule": "tresataId_sub, booking_country",
      "details": {
        "source": "cda",
        "distinct": "distinct_cnt"
      }
    },
    {
      "dsl.type": "griffin-dsl",
      "dq.type": "completeness",
      "out.dataframe.name": "cda_completeness",
      "rule": "tresataId_sub, booking_country",
      "details": {
        "source": "cda",
        "complete": "complete",
        "incomplete": "incomplete"
      },
      "out": [
        {
          "type": "metric",
          "name": "cda",
          "flatten": "map"
        }
      ]
    }
  ]
}{
  "rules": [
    {
      "dsl.type": "griffin-dsl",
      "dq.type": "accuracy",
      "out.dataframe.name": "dsl_accuracy",
      "rule": "mda.tresataId_sub = cda.tresataId_sub AND upper(cda.booking_country) = upper(mda.booking_country_manual) ",
      "details": {
        "source": "mda",
        "target": "cda",
        "miss": "miss",
        "total": "count",
        "matched": "matched",
        "matchedFraction": "matchedFraction"
      },
      "out": [
        {
          "type": "metric",
          "name": "mda",
          "flatten": "map"
        }
      ]
    }
  ]
}cat: ./clientlifecycle-common-measure/src/test/scala: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com/hsbc: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com/hsbc/gbm: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com/hsbc/gbm/bd/clm: Is a directory
cat: ./clientlifecycle-common-measure/src/test/scala/com/hsbc/gbm/bd/clm/measure: Is a directory
package com.hsbc.gbm.bd.clm.measure

import com.hsbc.gbm.bd.clm.measure.configuration.dqdefinition.{DataConnectorParam, DataSourceParam, SinkParam}
import org.apache.spark.sql.SparkSession

object MeasureDQTest extends MeasureDQ {

  val pathMap: Map[String, String] = Map(
    "mda" -> "clientlifecycle-common-measure/src/test/resources/data/mda"
    , "cda" -> "clientlifecycle-common-measure/src/test/resources/data/cda"
  )

  override def measureDirPath: String = "clientlifecycle-common-measure/src/test/resources/measure"

  override def dsParams: Seq[DataSourceParam] = pathMap
    .map(x => (x._1.replaceAll("[-]", "_"), x._2))
    .map(x => {
      val dcParam = DataConnectorParam("file", x._1, Map(
        "format" -> "csv",
        "paths" -> List(x._2),
        "skipOnError" -> false,
        "options" -> Map("header" -> true, "delimiter" -> "|")
      ), null)
      DataSourceParam(x._1, dcParam)
    }).toList

  override def sinkParams: Seq[SinkParam] = List(
    SinkParam("consoleSink", "CONSOLE", Map("max.log.lines" -> 10))
  )

  override implicit def spark: SparkSession = SparkSession
    .builder()
    .master("local[*]")
    .config("spark.sql.crossJoin.enabled", "true")
    .enableHiveSupport()
    .getOrCreate()

  def main(args: Array[String]): Unit = {
    measure()
  }
}
#!/usr/bin/env groovy
// Declarative //

library identifier: "data-products-pipeline-library@1.6", changelog: false

def jiraId
def jiraSummary
def gitLog

def getArtifactAndVersion() {
    return pom.artifactId + "-" + pom.version
}

def getGitLogMessages() {
    return sh(script: "git log -10 --format=\"%s\"", returnStdout: true)
}

pipeline {
    agent {
        label 'cloud-slave'
    }
    tools {
        maven 'M3'
        jdk 'JDK1.8'
    }
    triggers {
        githubPush()
    }
    environment {
        scmVars = null
        service_account_creds = credentials("GB-GBM-BDNEXUS")
        defaultVersionUpdatePattern = "=+0"
        nexusBaseUrl = "https://dsnexus.uk.hibm.hsbc:8081"
    }
    options {
        disableConcurrentBuilds()
        buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '1'))
    }
    stages {
        stage('Prepare Env') {
            steps {
                echo "Preparing Environment"
                cleanWs()
                script {
                    scmVars = checkout scm
                    pom = readMavenPom file: "pom.xml"
                    gitLog = getGitLogMessages()
                    jiraId = pipelineApi.getLastCommitJiraId(gitLog)
                    println "Retrieved JIRA ID from GIT log: '${jiraId}'"
                    jiraSummary = pipelineApi.getJiraSummary(jiraId)
                    println "JIRA Summary: '${jiraSummary}'"
                }
            }
        }
        stage('Update POM Version') {
            steps {
                echo "Updating POM version..."
                script {
                    if (env.GIT_BRANCH == "master") {
                        echo "Skipping POM version update for master branch"
                        def gitLogVersionUpdatePattern = pipelineApi.getVersionUpdatePattern(gitLog)
                        def versionUpdatePattern = gitLogVersionUpdatePattern != null ? gitLogVersionUpdatePattern : defaultVersionUpdatePattern
                        newVersion = pipelineApi.getNextReleaseVersion(nexusBaseUrl, service_account_creds_usr, service_account_creds_psw, pom.groupId, pom.artifactId, versionUpdatePattern)
                    } else {
                        newVersion = jiraId.replaceAll("-", "") + "-SNAPSHOT"
                    }
                    println "Version to be used: ${newVersion}"
                }

                sh "mvn versions:set -DnewVersion=${newVersion} -Pspark2"
                sh "mvn versions:set -DnewVersion=${newVersion} -Pspark3"

                script {
                    pom = readMavenPom file: "pom.xml"
                    String buildName = "[${env.BUILD_NUMBER}] ${pom.version}"
                    currentBuild.displayName = buildName
                    currentBuild.description = jiraSummary
                }
            }
        }
        stage('Build and Test') {
            steps {
                echo "Running ${env.BUILD_ID} on ${env.JENKINS_URL}"
                sh "mvn -U clean integration-test -Pspark2"
                sh "mvn -U clean integration-test -Pspark3"
            }
        }
        stage('Tag Release') {
            when {
                branch 'master'
            }
            steps {
                echo "Creating a tag for version: ${getArtifactAndVersion()}"
                sh "git tag -a ${getArtifactAndVersion()} -m \"Tagging version ${getArtifactAndVersion()} \""
                echo "Pushing to remote repository..."
                sh "git push origin ${getArtifactAndVersion()}"
            }
        }
        stage('Upload Artifacts') {
            steps {
                sh "mvn clean deploy -DskipTests -Pspark2"
                //release version don't allow overwrite, so we need to remove parent module
                sh "mvn clean deploy -DskipTests -Pspark3 -pl ./clientlifecycle-common-core,./clientlifecycle-common-datalineage_2.12,./clientlifecycle-common-measure"
                script {
                    String testReportFileName = "${pom.artifactId}-${pom.version}-test-report.pdf"
                    byte[] testReportBinaryContent = readFile(file: "target/${testReportFileName}", encoding: "Base64").decodeBase64()
                    pipelineApi.uploadTestEvidence(service_account_creds_usr, service_account_creds_psw, jiraId, "DEV-${testReportFileName}", "application/pdf", testReportBinaryContent)
                    println "Uploaded test evidence"
                }
            }
        }
        stage('Clean-Up') {
            steps {
                cleanWs()
            }
        }
    }
}<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.hsbc.gbm.bd.clm</groupId>
    <artifactId>clientlifecycle-common</artifactId>
    <version>2.0.0</version>
    <packaging>pom</packaging>

    <modules>
        <module>clientlifecycle-common-core</module>
        <module>clientlifecycle-common-measure</module>
    </modules>

    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <jdk.version>1.8</jdk.version>
        <scoverage.minimum.coverage>1</scoverage.minimum.coverage>
        <scalatest.version>3.0.5</scalatest.version>
        <slf4j.version>1.7.25</slf4j.version>
        <log4j.version>1.2.16</log4j.version>
        <junit.version>4.12</junit.version>
        <common-lang3.version>3.10</common-lang3.version>
        <snakeyaml.version>1.25</snakeyaml.version>
        <jinjava.version>2.5.2</jinjava.version>
        <datafactory.api.version>1.0.5</datafactory.api.version>
        <classgraph.version>4.8.90</classgraph.version>
        <scalaj.version>2.3.0</scalaj.version>
        <http.version>4.4.12</http.version>
    </properties>

    <profiles>
        <profile>
            <id>spark2</id>
            <modules>
                <module>clientlifecycle-common-hint_2.11</module>
                <module>clientlifecycle-common-datalineage_2.11</module>
            </modules>
            <properties>
                <spark.version>2.3.0</spark.version>
                <scala.version>2.11.8</scala.version>
                <scala.binary.version>2.11</scala.binary.version>
                <hadoop.version>2.6.0</hadoop.version>
                <elasticsearch-spark.version>20</elasticsearch-spark.version>
                <elasticsearch.version>7.8.1</elasticsearch.version>
            </properties>
        </profile>
        <profile>
            <id>spark3</id>
            <properties>
                <spark.version>3.1.1</spark.version>
                <scala.version>2.12.10</scala.version>
                <scala.binary.version>2.12</scala.binary.version>
                <hadoop.version>2.7.3.2.6.5.0-292</hadoop.version>
                <elasticsearch-spark.version>30</elasticsearch-spark.version>
                <elasticsearch.version>7.12.0</elasticsearch.version>
            </properties>
            <modules>
                <module>clientlifecycle-common-datalineage_2.12</module>
            </modules>
        </profile>
    </profiles>

    <scm>
        <developerConnection>scm:git:git@alm-github.systems.uk.hsbc:wsdna-client-lifecycle-management/clm-common.git
        </developerConnection>
        <tag>HEAD</tag>
    </scm>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.apache.commons</groupId>
                <artifactId>commons-lang3</artifactId>
                <version>${common-lang3.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-client</artifactId>
                <version>${hadoop.version}</version>
            </dependency>

            <dependency>
                <groupId>org.scalaj</groupId>
                <artifactId>scalaj-http_${scala.binary.version}</artifactId>
                <version>${scalaj.version}</version>
            </dependency>

            <dependency>
                <groupId>io.github.classgraph</groupId>
                <artifactId>classgraph</artifactId>
                <version>${classgraph.version}</version>
            </dependency>

            <dependency>
                <groupId>com.hsbc.gbm.bd.datafactory</groupId>
                <artifactId>datafactory-api_${scala.binary.version}</artifactId>
                <version>${datafactory.api.version}</version>
            </dependency>

            <dependency>
                <groupId>org.elasticsearch</groupId>
                <artifactId>elasticsearch-spark-${elasticsearch-spark.version}_${scala.binary.version}</artifactId>
                <version>${elasticsearch.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.apache.spark</groupId>
                        <artifactId>spark-streaming_${scala.binary.version}</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.apache.spark</groupId>
                        <artifactId>spark-yarn_${scala.binary.version}</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>

            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-common</artifactId>
                <version>${hadoop.version}</version>
            </dependency>

            <dependency>
                <groupId>com.hubspot.jinjava</groupId>
                <artifactId>jinjava</artifactId>
                <version>${jinjava.version}</version>
            </dependency>

            <dependency>
                <groupId>org.yaml</groupId>
                <artifactId>snakeyaml</artifactId>
                <version>${snakeyaml.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-core_${scala.binary.version}</artifactId>
                <version>${spark.version}</version>
                <exclusions>
                    <exclusion>
                        <artifactId>slf4j-api</artifactId>
                        <groupId>org.slf4j</groupId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-hive_${scala.binary.version}</artifactId>
                <version>${spark.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-sql_${scala.binary.version}</artifactId>
                <version>${spark.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
                <version>${spark.version}</version>
            </dependency>

            <dependency>
                <groupId>junit</groupId>
                <artifactId>junit</artifactId>
                <version>${junit.version}</version>
            </dependency>

            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-log4j12</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>log4j</groupId>
                <artifactId>log4j</artifactId>
                <version>${log4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.scala-lang</groupId>
                <artifactId>scala-library</artifactId>
                <version>${scala.version}</version>
                <scope>provided</scope>
            </dependency>
            <dependency>
                <groupId>org.scala-lang</groupId>
                <artifactId>scala-reflect</artifactId>
                <version>${scala.version}</version>
                <scope>provided</scope>
            </dependency>
            <dependency>
                <groupId>org.scala-lang</groupId>
                <artifactId>scala-compiler</artifactId>
                <version>${scala.version}</version>
                <scope>provided</scope>
            </dependency>

            <dependency>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest_${scala.binary.version}</artifactId>
                <version>${scalatest.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.httpcomponents</groupId>
                <artifactId>httpcore</artifactId>
                <version>${http.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>


    <distributionManagement>
        <snapshotRepository>
            <id>dsnexus-snapshots</id>
            <uniqueVersion>true</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots</url>
        </snapshotRepository>
        <repository>
            <id>dsnexus-releases</id>
            <uniqueVersion>false</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases</url>
        </repository>
    </distributionManagement>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-clean-plugin</artifactId>
                <version>3.0.0</version>
                <configuration>
                    <filesets>
                        <fileset>
                            <directory>.</directory>
                            <includes>
                                <include>**/*.ser</include>
                            </includes>
                        </fileset>
                    </filesets>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.5.1</version>
                <configuration>
                    <encoding>UTF-8</encoding>
                    <source>1.8</source>
                    <target>1.8</target>
                    <fork>true</fork>
                    <compilerArgs>
                        <arg>-XDignore.symbol.file=true</arg>
                        <arg>-Werror</arg>
                    </compilerArgs>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <scalaVersion>${scala.version}</scalaVersion>
                    <args>
                        <arg>-target:jvm-1.8</arg>
                    </args>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-pdf-plugin</artifactId>
                <version>1.4</version>
                <executions>
                    <execution>
                        <id>pdf</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>pdf</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}</outputDirectory>
                            <includeReports>true</includeReports>
                        </configuration>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>3.2.0</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.17</version>
                <executions>
                    <execution>
                        <id>default-test</id>
                        <phase>test</phase>
                        <goals>
                            <goal>test</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.scoverage</groupId>
                <artifactId>scoverage-maven-plugin</artifactId>
                <version>1.4.1</version>
                <configuration>
                    <minimumCoverage>${scoverage.minimum.coverage}</minimumCoverage>
                    <failOnMinimumCoverage>true</failOnMinimumCoverage>
                    <highlighting>true</highlighting>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>report</goal>
                        </goals>
                        <phase>verify</phase>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>com.github.danielflower.mavenplugins</groupId>
                <artifactId>multi-module-maven-release-plugin</artifactId>
                <version>3.2.0</version>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-project-info-reports-plugin</artifactId>
                <version>2.1.2</version>
                <reportSets>
                    <reportSet>
                        <reports>
                            <report>summary</report>
                        </reports>
                    </reportSet>
                </reportSets>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-report-plugin</artifactId>
                <version>3.0.0-M4</version>
            </plugin>
        </plugins>
    </reporting>
</project>cat: ./README.assets: Is a directory
PNG

   
IHDR     F   Rv  	IDATx}LTw`w>sC],(jZup@%ib,zjWU	QC%F)Rzl<(BTF>;mLl7?3
J&sf~{3V4K|S       k}qzG#0tQ                                                                       (Z|>_        L0t       
:
}       So7hZK        c       SV@       I                                (:      `srFKO>4     0,CUZZ?\wtY^oeI      ?k@sGW^5kTWW t      `H>OK]-YJ+K.k})%%E2t      `H>OJ%<"fWn7nH2c       CwJ@DYN2        B                            &@n>,'U<&W|ZU)ns}HW{     L-G)k{P=y	SLy9x[Alm lUYXro+P|]      rDi6J--QgtbGHWI`|GX,xM;vns;Zu>7rh
N;B:{u>k_9V{
U]7\-noUVTgXM{^     LjB:x2I^#at(5S=cI:b~b,kwD {uau:Vt~;'Ll}Z4~1roVscuqro6Vci5m [UsovjoHl]L9hpZ*gUJ[WPt`e}rCdoUK]wKauW%:X6,O]lO+     L8me\c?rA
+cS[/!tFZ7MX$57>5CU&\f`HrDmTr};SV+tt9l&IJxE|+`9|*=AnSyTI<_]*?BgYIKdpZ|N8x^g#P />[IK/KI{?juZZ|Z4kimkB+Sc=3EoIZu#bWn:^+5_Qle^0%]K{Y_s      ]CR+v-%Zf}R}J
INt>
E6hG4ITmS4_R4Y=KVH~YzUZORjR5TR
Z8MfM
r{Q :_|68^39k=-
[f+;Lc=%lQk"8qBYYYZnrss#X
{*>l*e2i`J!]z$\By{]#LH<3nTRf+)_kVKdTm?SRtWZ0]JwEa      ?Zz4T}|3^37%k{gfjR9_]*\Pg}ZL~S:aC3 5
6^c_Ab`@ul*//vGu).ARV3P?SsYz<$k]K<$:7Y&zikt'nrN+?bU6dE7kSd%mpA      cOvwZeVR`Z:SwC>bz4

U#s|0
zxm?V3.<D4n#,)F-:WhnJ]]CmtJYK0"t-/;U)0g>Y{9t][k/xYt*_     ~fZW?h92#$__wZc,wg0vr1g$[=mTG([c,]~8iFEErrr3)ci<g g^+o67]kl3{u>72[@aqC6kB^>`dn>,3;oUu?8iMW67VfymuyhV]))C{K_     &"yR'N'*f:l,u^%-60?!3=8h+3>"i=!X'5OJ	bpP~0D%%%.&z+4r[/Rew]=rP6[[P
kgM+h*8wK;OZ+[tlg(|/m1ff]wKIMhm     $%6h]ef]4?Wj6$?tF-~
lV'IZa=e?w\l
n{^sJG!w #JK[#]5r7-t     b~zJ7D	$08^YA.9*f~MO~iI~z(}zG#0p=$mJwg      \&	y uFN;<V     `RKc6$a     `t                      |IIRk      9s$Iw_z%SK{nI9sc~      LZa_[nZ"]0;acz:     0$0~-_\~-Z{,eSO<_|Qud:     0$0d{"D Ru_w^Q      (I~|q
dPTTbbbn%d      WTT\.bbb<      `D3010                                               8Z.Cn/g{x       o>GGGI. p/ Y$       sOarp
K*       0[EK-        SH                @       pD       pD       pieg    IENDB`PNG

   
IHDR            IDATx{tTLVQ(8(KDQBj&?Z v%KKS*_DT@L%4z,p~!(0QRBU.{dy%yfb>gxI""b)t>)ED,H/"bA
RX_D"" )ED,H/"bA
RX_D"" uo'/|/amyP""rfQL,""3& (WG	W0~
~4v4L[?@/	1,)D#|}khr$sT<rUa{^H95?|(@@hl`ph.xa2Z%uO3DDbEg^FK`]W_>"lm#t33)x5(^|3}	26d0W\|
""1gW>@q?{3gL4B! I33 *&=|/!{3>F^o3oza9!,:5vPQ0)wy!~W-_M=hsI/'/EsZAD$vtma 1Y=?CdTv |!pC7W~\>09Ni/_ME_Hhw?G&{xLfm!q5Ls'kW/X4B_ 1
2/4-,gn\z
""1kWkz7<`90%e1vf]IAK20%Yws}aIyO67F|bdqGXa;phI}]7/"a>""/H_D"" )ED,H/"bA
RX_D"" )ED,H/"bA
RXP']=dsnWADD:YYgcNoz""tWD"" )ED,H/"bA
RX_D"" )ED,H/"bA
RX_D"" )ED,H/"bA
RX_D"" )ED,H/"bA
ooGJIm40]5s2,LO-4}cPJ\st
?qUC^Y'WD]3]9V%y|*-,]D{J\YO4bwHi>yeCs(Vtm,3Cg{T<6O-w;h rfdj|}$O[~m~GTM]RZ#"E=#u4\T$;3z0vNsH.gnD#~_Lnr
?$"3]TGkVs2D~yd?Va,h}T/AaM[GM8.f P][f)]+qGX5${SPZ4SYE\}-~.j)W)2-J#G5oL4D%B$"g\ iF;:\mvF9,gM
NH\17-#[]pxmTy 9c}>)$S@uPMg<R21vu&g*7~%SW")h8Vi@;8,<[~/;2N?M 38'3#I(5;w^NC`x#%3^zr|= sjp@4Dc<CONN#mv>1&'G Qu/,"UCe]_Gi;H7;/jz*-J'VU|Ke 8R:loIh3'|%31T'ToFgWUO3#9#TBW;[~PS;M"t}J\+eLLrr#2QKv/mZ1fj)CAqU0H_8=yZ;t59M$<.p25vdCv9TYN-m)yPQt7_kOi^<m2,Wd X^YiwKO""/v )ED,H/"bA
RX_D"" )ED,H/"bA
j\>t"}#"bA
RX_D"" )ED,H/"bA
RX_D GJI(9;Oc{
Ckxla,sg8""JmVO#^<>>R0>#X~DDym,S]^K4Wp&"YIU}&TQ*K$w{.vi6?E	Km3YKzQMCJD8HDDGJWvNf|I"C~LyLjHuCnQ0Zp_SH+wWD?;a'!I$5Q;oT34kwh?YKZ*t6^B<IV_Mmt_nRX_D""!~4u+O7Siu	^[9T)ECv)N%3(HO
V(_HLpZGJv_ZA+uTFjR&CoHkx&y,4G>ke/Y-<DDt+=Z-PHi#\Lz-0E@2Tp5w*2y,Z
E?,"J+Nu5OWO5TG
N'g;Se9nZ-07y=O|lUa6O&1H.~WNDr"'\ktcibllcHmGWc~>S]i/bu|bl;l]ch+5"e]}e	5n|6|;qr&)E1ob6oMqTf=_
%nl%ob.D]M
_;o|"=	lo6EoGcFQ(
p+P0O>u8j;ic<o~jj"=l#nE	
n_xu8%f(E1g^bcM_OMoE7@Y
3;466f?qR6x%H/]h41;o|(EbYc#6Olwi3!^2"uuVc?j3ic<|7Q?6Q6uV'Rz
H}%uWc8E
IJ
>J5P ;'L/}E	>p9e(E4|UW8)[v j"g[o|vOvCci{w"cf|CMpy8BH[;l|Y4/yn"]F/W`_wi3xt+Dz
u,%+?,_b_l{*1nQ.5_'m4F	ku+D}
lc{c
qs+n8V}([z{'k)OV7Vcv|iS[RKv?%@=6}zZ8/g|O~bHPKe;~{,lxm.%nVkiT
QzL'Op7KDV^rf*lwf"O7-9Zi+_z:#2
Y7zp
w}hl$Vd|wDb_z^vCq4[!c|W4^SK7El]q8tkq}kD]#8~cv;O=vNcu'Ut:	T<6O"sH|sN
]p;'tZF_{-bjN
]gKN}3Q; LF }bH|L0O#]'"?H`zWfZX]9ds't'|*`~#a=S1o9R0#5B|*}p4?r=
Xr,{] ,,Eg1p`YF^YsEJ`U:m>k2$;+>Iq*OIf*eS6cbrk"T)HGU>kjkle-32g11T'!cC mX9`	dN4KIHUs:Trkywv25iX
9s1f
iT*q0B yd1jWA+:YFv^%E'O,9\HwOIWjV&28DmDeBQzCEmT5WpNf|?"*)@IU}.MaBIHl?#5:|9!U	GI Si.wr6m5<Ldr~%b'|~$'BY	pC5}sL9g"J~fXr48H,RGIk"Q4 cHlL#(mZG=|u\k!c!1*v~w+NQxk; , wgNweLzv *55Bnfv{,j.8yRt+]TS-{=|> 
~.444(ot{=oSzB
RX_D'C]=m_<zgDw4GM\VJSY.1G'$oyOZTsIddMw2qEn92&.%t:RS7|#nOt]`.=^}{"/oT{uk[ne8I~I~n<g|R(Fx'
_@oy\ G+1CTw+yl8?&cr,5'O!'w5^{5kGu/
&	K_! l\S^4Q  H?f0np60*sel7Q97>BVx![3e)70
(yY;5f$}[1Q, fANM}	x
cF`3fg7
)/}	 `>FOTJO<z+>`^?)Ovbf{en KH[>{ nbVt;3`,`iVau$Wo Z&(00H=O
CJ2-#
sV.DHxx_'-S[UR2fo)wTx !_RK_!;(|	~<,:W+Y`+x!(Up
6ot; Q[)$?G y:2x4)){3g7%ackxZ]St;/2^x! ,t/
+=&KsKM,2F9	'-J">{'{yC'|!|v`rf! n"-.]i'(SfR2{)iCbBmtO7OhF`'Pqd}7#xos!6m
f*?s*oL7'|Z1CnY:%o9/Nnh 6])8v+} M3P74]0SHvs?9=V*HWkO$lOHw4ot+Xu<">],V>"Vff"" l;x4"76Gs(c	]@~U>GWEA~U>`M	7_M3{#"/#swl6{!c9yr=E?	)8^zlG3oWG9z('ObYgqyqq9<A3gOw(E:ff3{">>?(cF\\v^z)Er?X8
=_D[/ds[vtatm^*`;xzYR{.t2J7s3=l{xy#lE8t93q[:7dcci[0`"Vl2yhM1+aENaMo\{jov`G7!6[+2`}>mx`M+lo
Vh(K6>H a,Cl0
2GE>ngKgs n]-48x1GXzcUwI%eD*){;mH3QnbqC<Hcws6H|c_$0t9/3u?_x=}O9'}=5#7
"|  IDAT>K*f1d.;1!A[Ze#Ryd8Hi`{O:X.he2*{(JnG/0(;zI.a1C_;GFEE2Kx=w&YA{Hewc4Ba_U`<oiU3:`S/IFo1.cI>? ^$i}	|{g{2MsfrQxiJ33mPg~X<VFt!H)o},>9sfIa[2Am=Y&_{sd~,OO%#Ry|Mj]2efA	G)_!dCBj4Uf)W?{X?2q$3,6Kf\-%#5kkg)w5oTD6#8H	\2%G3=\LK9+Vb8Gq_<<08pRwdGsgcl=5^Hq}K;h\kH	F3}R4>p?Lt>}.o1f""i/"bA
RX_D?(5*    IENDB`PNG

   
IHDR        
a    IDATxLft0I9=F]`IF+%9kFEvY3 `nzck;}W*53lUc.F"	mM5V?o~W~>$o}|7owd?_
      HF_i?\Oa
xo_z~c=Gyd_WSee***      
=#_7z~G=,G):$.'h      JEE*++Y~>~_?)UWN?*r"     TTTmw_F;;;w `_XP[|3g>O@     x{GOZOc=	SQQ++u}W      GyDNRee%3p (T>[     gG-p4:        N&h       @Y@               P       @R       ny=Uc\oc;       8&vvGS@WTTT~SORh@      IUTTS8U*NE       'X0]4       `7v[       P 4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (].h~~_;JFW{      NA6^z7s/\zE}yY31*       ~: 5$}k/_K%#?kg%I?w.Io}9k_KFK$IW.m&++}Oz$}y]K>y^gg/f~OH^yomI/871	       nI_N	-_{CEdLw<.>gS?_{A$BM"50wm!YxZyC5+Pw0x_@g3/%Il}WO2ko/[|"D7c
d3?o>W      A8OwP99Of?.]/J;Q? }YzJByO_OU=}o}"m'7lsr^ l=P
      Pz@r*;zBOIlz>l[fjt6e5:wuV      QtF/_~Aw;O+^k'{oD\"ki8       C@,RI$',.0[Z_3Phw}1mwAwm/sl}nvzU4       q@/K
0Mwu/BPO^+
 K/<w^J?5gogv~2UEn7       +}omK/w^N_~A}E5#gX}_K>'z[fhm>K?K;/uM]k/g^Et$y]}2.K]'tI>.WsX       ;;;;z?h|Xc;~\iq7W       ^-h>JUTT8Q2[>u      CPZpZ}      aF Wgz       Zp                P               4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (h       @Y@               Pu>H9uJ8!       };0; Nc>       #}H       !8 G       CC       eA 
       (h       @Y@      1=@      1a%&      c =t>!44       qB>\SkrM|)5vuzh:       8+:P[3i/>      8mtU"]=3=       (#@oiGc%5z7M	SgM._6{?k/7?*k       mhL=}\QK&}Len'P>E^$&0YX3WI6z}q+q8zaf;%56jU(*       gGzE7RKdq
=zYBgKm5Hd+F5tq?      \jPCbD2
lX/1SGtN9       dw9|F{G4UZZ:7O\JD$NI{OqV-H       '
i<<Pj%?z`srMyjO@SC5?S:_Kzn       C'_H[nT@SlI>Ene-26ico       xx zmR`>51zz-;gvkPu6       c'*Vs"\U]ta       p:        N&n9[rh<ZS    wfV:7[AmZ1:g'F>xk{q;
<V4XI'WcvbQ^#r]r^FiM9~O'25w[;#r{rk<Sd#     NVhvV-\kJScKKW=L>UC]>5/VMZ9SvUjij{5Zs8Sd`E3^q;*=    s[}p&BJRfjTzhbny&W d#tU#sr$
jj:%`-e#rM%&hbXi[=hR.OKZ)e_"5UsMR/Z~kaO\!RrY2Y?k^Ea\38q,OwWyf#T{9EOs!-]6)rbv1w)<tnA
u'C)u^{O(~r.v=9/<     @G	a00IZ
I>ExW>My5'm^_<-za\,Z_?<*-58^5v*eb<`(AKrRi)8565N!IfZ6ijCq@%m+aHO^E$Z1B{N5^5f$3whZz4ScSkYC#
Py?~'Z(YEF89vcEE~>Yrv"M=%jpI}Q>Ev3=\55VV|jLiN98crs)4{GP\R<g)yNDh
s\wmR!m/rC9=H73$k<nYK(4     '+z0G6KV `m: ?2u&9TTS+
3Ff6]Vl(OK`lA|Mx<1wJ$[$y5R!IkfE^jv>K>0Z&}{j_:)_VWzij#95\C
2>R%v#i<g4-utI>`TmJcS~.v:s    QvQmW(,eL$>t[Z4UfeD r3qd COk(@Ry=Lb/N%D|Ef{bs_n'f"K_,~2llRR[NX[jIC
    N FX))RtaJmJ.g--]I
ug7aN3c
,6%{Z|O%xPCn9ufU9sr\)fyfxmTsVJQj,4*%iy#%74=DPg)7&Z42E%PAel-g[36`~OJgs
Yrk<Cl}Bu='-u;3+eVn?li[}?     s*k'+?vkLmr|b e?{J=r<^vvkLruky/Uuey{Q[Z{R	Emioe5'O)sjuL{AZ>C80u^1&|3 (Y*y-<[n^4H(QW/i-YW$5zBX*p<+d?VN"<jno5xrrCV;o;!wSa1~HNV^?Y^d     Vq=x@_o4^V}}]CS{=?U)Ia>58kq+:'2wlR    p~=IK>*w
YcZ7REmwZp4:eOiyP/~E%Ii=G%>oex=Z\    Dq`o=K3Uc-80{>,UU]EBO>1BO>9zGe8E9$?~}{CEj     D       /:'9r9TIwIF      @Y@Mb[>g{>hY}`x_&"Y
7         QEtm)2Pa      8NV sPruHR[[LW~sYwU%cM	/:E?-1d/kx\SkumA
uRMlyNmMyUyoj{^e7@x
      (:-8BkqXWN"a"mZ-/A

)i3y{	{
x86_f{zVU]a"{Zu~AOzuNnSHAmkKKW={fk|\m4[6vf9kE      p|
hIR$obO67yKjl&W|biFrV`\WU%)5M2jlZIK[Z
Ti:
;kKs.wssFE"      p zc{^S9
0+znW8m{lQH5ht      8N`,gU#Y1^G_R*[A-XU:=Ql0[HYUIz*pAO:dSHBcGl}WTz;      I}{I]>#xuj-O[c^fo=Hy_Rh(ArmQ+m4#R+n=Gff;^79eXT%Z\z7O3nMZ=q      ;;;;z?hc#):'
#G997vhr=O6      p{oF*w
YcZ7REmp8!yZrN@[>       #*             h%1       :[AmzdGlu~+      `qHxT5S$K!yx!      #a`_WMg$V]M	$WRMI2B]o/5.:'`(BC%IcnyRHUIjlW{>u!}z4_       
hzaZz[1YD&mm-_ N-F<s<Y}llhyVCdvHJ-u4O       PN'*IU\R,IYxf\:tUKoiYmrzl$5/3k<IAUt'Tc]       DV@gZL;
5 )^
hU=B       8DI m0ZTliZIz=j=V6A-0mG3mVq9[[Zu`      xH&=_)}R6wk|M
x\{5nTik-$s9gcfNUAWz)#      Qq=x@_o4^V}}9p`       W$}m?ohQ&    IDATry1S>*++UQQQv'$z!&      @O>_>6zI5a       PZp       B        
        4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (h       @Y@               P               4       ,       eA 
       (h       @Y@               P               4lu:a      @9 :P[3Q)6sr%jIW.b[Z}n/$kSz#:g[ZDKam<1h5=*1LnU$<%)9       P@;K<@f>E^VF	j!$--]5S$l^MI\SkMz4>E%5V#}62mKKWwz_n 9w395qv~{qd       N=uL99C
A
uk9FfjLY^yInVlg[]a: ?TX$$5I$}V%@:}]K
%UZXz@+ZJnhYrdE}gj*r=972~Gj21Xt'Nl{q$5*2|I^cO}ko{Kl]s       M)b-54JI<{v[Zg)Ez	?mmW4=	{N5^i28%m\ZufZ6ijCq1jopAkj#HVNil 5*,)^V=mFa{1IU]iUSPC^8}3sza8".{jhpC}p&
)%6"       poiYmXzh\W`A_IR$M@zd`OX9,Nj,Wr6A
MIvi!iMu|2kuQekE3^5RJs.kj%cnw4(fLcSE
      <] :2{EkF-{{Euj$Z8c4zDQAkOm
#$b}ntIJt+mYX|?1\	wu9Sb`cI<T@      E8vtQA-Vj5gjnSX+cV5lj}VhtN5&|EMipHv+M;jQHPZ~s1430ug>x5YmhiVny/       dWq=x@_o4^V}=#Y\/W_`V!44hn^c=.3j':M*Y[ml!!YkKV_a^7aq=a~Yle-=2ZIU>r>yl/>      vW$}m?ory1S>*++UQQQv2q#      pcebj<rNe
      #hx Xi:      qcU3g{         8"g[3	     +iukhqt-J}9=V99[G~5TP6G99[KyAm&euJ1~'ZYl8l@l2Kt.9Gko     X#iR_D[Y~|T.[ZgyQW_A%Tu*pC;'I#kd2qV55S$l^5F     vBt\$My#tA
uk9ufjL~O3VUdm$#r$k:Q!r[g5UX&BUVNX)k;M@(X=97U3[,UWs0|JynZYQ_y_k6wg\~._2>kSk"B&M@x?1e9%6]n6{s]-o7m$IO%<     (8CCqyvF|J3lu"
jg
y{,	Wpqy+jEh&94i<:'ZjnXn#o6C9+za8"u= 64.IU$\qFIHVuXdSK\WIfFbyl-dH+%^3^iE6%TcVNiltVsX)\Dle{_p&&|1[v5oiy-\"L7/<j,xrcG-$GmcyZ7R~     WG4%KLX${+@3-9Wk\-E^0*'uR`CtsUr]1+*K!s1
W4cxgTqm1jSVq]1QEe>uubk7#\3*G0CJf 9wGZl54%tpVRHP\X$$5R[lXd,xlxm
U:}t;     rDh[SB%b:n$3[Hr ??ae(}fk^9,d{>J_E>wUK
-^`Gs	     ;G!Vi^^[0xSKY%h{Eqk?dbN=SiJ	1Hs!eWJvaq=	I;%5!IF%s_hRfHyntIz6]#_/Z.EfYdfZ |     'CwvvvwO}xy_XJ&6i@y>R9&b2:}Nju,w3 \R[+$+m3eVfbr;7wGZWN:Hs-9<sy6yd{@sr B      {{GNJblA}QUVVE 
2Clu%A/      I
t</~E%Ih$po     <dq~}'H|>
\W\VVx?/o?V*n/     G       8BhC=Gluk&z#ju9r#     U <99[G]$q[ZVyP`dn!<"I
fVnrd~<'v_&i+^*>I~%e     8Nn jhIQ0*b}]a?ZjMnWU>!<@z{q$#U$S_s;[ZkH5jStN)iOBq?yAfl-ZbGO=j	x97c1w@S:38y-7Y2?9 I\Wr7d     8ZN ":'`Hja{-Iu^RV6;@H5 y]lXt}T6B$d$o/OnnW%5oc#Ak
S?<I=:w!mv9x7F)q9(:C\3xPCT)kJ_6z^cS
)MkbOuM=~'&i6kR~rrV/QW/6=hRO6U56zSf@j?o
-KZq\SKlXok1V^m\>GlAe\,/Bc~Gkz1&;     p
h3lnM8fu0S$-a"mRMf;4Vh(^[~zZDU1--]5MN!JZe<cjSB$XadaySz;[
1UHkyh(uL
^<SIZ_iJCH~y3*rs:Wni9<Tu&OzE6k=d6+sW*qi)Um5Iw1{4mT&-,R2-\j6%UV4
S#l;GVNIwmo'zXs\
%GI2xc~?X=Qy{Vp/J;O!-o     8NTUg(d{>FtkV5o'CbusUr]):8M"z\$G2z7,G_F\
issKjui X#9z	~cy(sg-:
2;i##l+As"k5sGz?s^}[y\I+{FU/Id7^4+hU^wtu^Cs8Y[|l54%IBkTsg#v	7d}+zkj-kr^?[v)wTVqvIc$~_a      kT99xD;V e 9m9Af(#')mOLE(QIR\?h
ldyf-OF-
/YU;YZA&HmUS')07tOQ_89FjkdUX;Ij7d)=ujl_~?AwKYZ73     pt'y]f;Jg`[V[\&?_n"sY)}%_`kT]/ohUI:?frEaLOy1<,/xidL9]Agj^"'y6:n7sX"y>C.vl>;)`tO~~W~=juLmZZ
7     h}<x{7/>/'pUYf>H/[t}d0WZf[:F}gV&`@)Kl<Q=b94j<cg5sp#V5~6fmxm
Q6k0z=n/`'Rmy]
kL=L
Z$?.uu?|e`m m^q\~?D\f^dcC{KCl-O>R>?Y     8	~rJ%x1S>*++UQQQv'7>`6d=#9t     1u#SSE>
s?Zn}lu,>\?tKv?v9G$IovpTTc5|hww2     (C@O>1BO>)w}AqO/w,>\<b_7w?O~zp=lN            !        *Mvjhusrh)^d*ylt9|/:'gFlScR`d_{}      pd :^9&xPC!&|}\VZ+%>OvUyOoiZH|N>,Gr4      =9:w^KjTph6)$uzhua^c#P,Pzo
WV'XJl-
z'96-y;<II]|Kje7%ZUSr+O4U\Srud?y2(6'RKT%k~
sn6uO}$	O}.okl^nR8}I/(mWNek]}	cw?      e{SlojpTcI<n}`3%4 pKKW[G(!iH`x^-}f=D+kV,gRlS5
`}Mj ~y}pWCji\H&guG
387w6f	AuH3fpkC<2Xc}RK-yP5HuA
u{56N\Su:fguk]H5E~E`_Gc?{^ni>1My      uhjt7]K3FPanDA6m-d	Uue
=zYe>6>+@=8wInx[	sjes;}QkUJjCnfVZTL5zf&EBRC,Dq5\sj!P+ZuLW
UVKIcdr9     9Q=Fh}ol
isrz<L)pnziu^7Z#;|FEl_2	[?VO6I!yZbE3VzbGO=c]    IDATj5)rB|A
ZsF<9z      8'*dmZp8OHh9jRakUlRUMn?4*m5\@jTSS]V4jU:3jnrIZ^(O];*i!R--`Z\Z*2d!:'h0TP@1      ` ZJT(Ll3G%fm\>G^VV4uNBI}V +h1m?s'	iyq1DZ"d@<DE8=9q%y
4*1Tu
h\6(:Nv%`2l"b{-^26<|%>h6!Bc      }<x{7/>D`oE5    ?{V~	H/._e0Y`fjbC
f{g/BlX"XvtA-cz&%st_Q^0t|>h.^z}DDrW_5
O?k\z+W: Z^\7qC_~!^_/?^JWkbcVDDDDDDDV=|iT/_5}lOus~S?<aW;|7Zp\"j!r,6WDDDDDDDDD^*OTeY[QSu'_v{{MMrmku.P1x.N7{.sg
x[:>^yN_M[zzta-NHbI.>j8w2P-2K?N{p.xg`op2c
_I2 !pk^dQq30&,q0/t*aIs	C	e4x:4Wmbl<U >&\3Q\'HO[- /d~2=W=d_ }]e^n
;YZ3Ge:;q16@t $Th #MHCLS1+~H|mV=2#8`i
V lS1s<![S"	+1_MaZ8~O<#>$\un^CL
Jf8j31#tO
DIXzozf7	v}j={ces-OrSZgtZc0ADDDDDDDDtUUZY
F@$*68#>@p63%j5i	{y=E+?pc279C)GAo.mr3$~}o5Dx,+Qn+DSqsGc49C.iT =qaie8dEMIEN_7}\=?tBWU@[UWlM
+8p2cm>3'f8mT-uzQO@{x*/DDDDDDDDDWWw@eHjfy{O".'e>bove7B~ ]Ef{eU!7zc	v%W1X?kF,T?}4+= LoV~5Y=x=1,4u3x)+iu> qm1>,#muk
_ojJo5?Xs*{x`=]b=3f;@m=X?V/3}>|{`<ffc9W?Wx=cO=a~uj6';wo_z_1W_5
O?k\z+W: ZN=Ns[EDDDDDDDD.[@c,|Kbp&v\_a[L-8DDDDDDDDDD.i
s!v99v6'yZ`On5wuX><g$|c96?Fglo/^s>yP~X9qcu{4TCZU}`]?3iWT_ADDDDDDDmta-Z)Pzh*F:!bK@	gbS15A<#d|/>$Zv]<Q)!%`> :YNHoO+G{!Ip
cM-1Ysq2JI1/L!9c}/q4F	{y?N2Ol>@F gK!qiqI##{E5g{QNBE,2TU-72ADDDDDDD[tg ;E
polF<YH'MXvEC6BrlU&[m>g,<]:4o9Y.qvW}(GI>ehO_=pV6
G>d)a'p3:`~{_qs';_1NU$9@]N
OXk_ngb|wZy$m"cc8SDDDDDDDDLMI"uO],($`<#]V}zh*.sc[d[5eU1cU[S"	+U]fsc,tCU{p>LEqq=/gbdEESVU)A>[hqwCxn?&Aj
c<5*x|d[ZzksV?tMz{px ~jfF3@m<sX?ZY79:zNYuYA^{=7N7+5>wr""""""""*>YkOm}3
'#h*r7cA)~1+Ud3P]k0V1-)8j;Ob>ko7l#r^Y!l?gchhm:ZYWG9 r}u[4UqJG
LpGmq4|raK{4^R},m7EDDDDDDDKuU 3HzP'jSb$=kD
Fl[
/3lgj;bhelyJFS".;K=-w6Z	=K|iFk'"F~YZjC>f{l
4Tyjb87uT}/DDDDDDDDSWga'$"}1wttLl^!3,@:j-h+|6+N;hh[:Y|^jJ~<0p};$?-Gz<`l<\ !jsUQ1edvK6V*K14v^DDDDDDDD`@CBSFfn	I,}?|SV07|1o<lu'i>6o+~KUVjyFK
{jeh_
xdCLAugfj9b,M^?U=([;A{w4uW< =Yc};5FADDDDDDD\y+^<{?w=?qGrm';wo_ """""""oqU}S]WrJK{s^=Y_aE[.k5j0gtiby?~;kkFh^_31=a~o;kk=lN-8DDDDDDDDDD.nj!V{
Bld[QSu'ong[$W1f7+5RqLsUy1slkZy-Z~ADDDDDDDD 4Aa]:WsSTt*B$b8K@
E"1c<QS'3ku1JBgz_y0 """"""""uCxg`op2=~H$ "gq;-`{:,27E5A
Fo93"n?l:j0~9q3-5~	cg:rl$Nntxk GA45q2PxI|lCN3DW(^?Fb{>c8Y O0$ k:*3-pI FNbWRed=F4pXE>"*]k bVK[u9%bV b8lxcU~iQ!hU-.JCL
"]zip&h8^yW"px vm H&W7~[
n>EIDI`Hr4%2:`~1\6Cghgc# uONoX 8VI
ST&
7\?9AO9*Pzw`|ths!q3*w(Uh7d}!c!_qdd&5
O8n ss/BhjGG`3nw#1Fa1DDDDDDDDD_WU@.^=rbXpmX^n
;!n)'oc	!z V!`6wLa9<na"""""""""hWd:WEaiw`>C}R!z${f[\:	g	2emp|l,DmPz<`k)AFoKJ)<s`MU-Y:=&ADDDDDDDDt_ 
^_~2'Ek=\W	{sle&[x J!foOdl16vN:v+VLU!-q!u\3 s
+zr?fU|5c2&j^^F~S[?AG>
14t"""""""""+^xgxGOwx."|b+-gS
1e=;1jS~~kz*W\isY];?u Y_ot!RWoE FPek/eUDDDDDDDDN-8DDDDDDDDDD.i
s!v99v6'yZ`On5wub}6|ovo\K'e36pbn6V<W?elh2x=;;ADDDDDDDD{2o:ulFxol"gNE(e<+~2kcvdw2ne!C'""""""""yp.xg`op2=~H$ "gq;-`{Gy
mqMwY3!Yk S'HsMJX6/k>qvt
HxM	'+}\3^g$B$';_A',{^T3dOLv)S~-{hZ'>W}l?q>x3/ z5di-s1$""""""""]*3-I FhN`$6+5{Y
4VQlqHJ?=#Kb8n[HYc91WbFpunV(U1e78F8aF8=83N$i[l/+IV[8< k; dv{pAeg'O~o lmg JhOW;Y6~fCzFJpEs`j$""""""""]*Ov*BHSgb8U{I(<a/hc37XAiNxL?<~H<@1
5EMa8Y5^hH~%vas	<c[|8QZ`]|V4 ef5kx\rI~=Iz	c]j#""""""""o
{FImp[
NZ`ya
7Ueh=Qva ar m#'`VA;l}L+]8j.e2U7	"W:xoZXu$""""""""Zp'8
m1DSC$3[Ipi{9fk	<XU}KNw~pD~')xed	dg5MXtl5007jlule&[x ^CLY/J]kf86vNnPuWWC}VCx8kXc]`)yfWlk1j9[ob^B/$lk#'cpiT{f%JW?Wx=cO=a~ujey,}2ADDDDDDDm=;1jS~~kz*W\isY];?u Y_o~Nu_k_~!^_/??v~3ngtp^sV=|i/_]x/    IDAT:{?>/O|_g?Y_N;gy;%"""""""""""MUtn5.'}.sd7xT8Tk{lvm<g$DJ3gCvf^KhyADDDDDDDD Ya]:`sp%F:e-7P-%0KLOHoOIDCy<*	_0hc@Uu\1F^ Evl lmNqpC"I !=3qm4 `a-2	n0t,q0/t*aIs	C	e4x:47n{o?"HIWy;nrK}5^lCN3DW(^?Fb[ydgZ?@xDDDDDDDDD^6>t'IbDn*F:Uf0^CkJKcUVasl2B<1[>DVbS-96?,cyUqx*F|If!FPEv)sXU>S~"=I,hb*e$|*uJgca#6ZM.m<ntjTa,VfW7IfKADDDDDDDDtUQz@p%z?6R$T!l(b&
OC)*S]|x.	g(f0>Hz9=CLbLVn^B!#h[
)us!c&q4
O8n ss/Bh`3nw#1Fi9
E+~dw:P1D,VnOZ{z6nE5'o2|lc-2:C2]|	f{ """"""""rQ*pxO#p1r;S|\YC;DO_?dlK',A>FpP0Ky?TZ=uYOuzuTkU}98-_%%KD1\Lo{|9Y62-<`1e0{Xc6~L9s~-mC]l+oX#2ns;:Jd~5c2&j^^F~c3i;2ADDDDDDDD5W7^z/x	{~VC'Z,g3~ """"""""Nme>)]\rugtwwxg}'^h[_EDDDDDDD[?~/_A$gbyW;DCvk"""""""""*Odd7xT8Tk{lV^7sl{{MMrMNsP<w5.}^v)]^oa|?{""""""""Zto ]ecK0i:}57!spx*F:cyK@5oH"9`9V';YNH9g!6{{E5ggyc+?P\1H&$",;l":qQ$@%7bilqT~`alxVY<Tr!	&\ l^|J'(}C8~w2]?PW#D dYK/_RjVVC@xxQq2Ixg7>5k{cl>kak5W~Kkz?cFc~PQ1@pmu_Vy=I:#R12T_Xr@VjW!pV	o}N$U-96?,cygx*F|If!Fhao1b/O ?(Y$]
12Jj5YQ[
9	<buck)|1fTY$Nt*T[m6f. UCf-Z%u?lOoM|>hJ#1[UdK6'ePu~IU=)""""""""U';j{?6R$T!l(b&
OC)=~S
]3x]$=fHiVCB|3TjT_^<78nrK-KcaO4j+CxzggT:NJv}0t\r]+-~p1XJ?ji2v;"""""""""gtlg@[
bEOWG~
z6gN=^n
;!`	$;cn#k3,hcIkxn?[!uZeMW7, v9iv(A~GGqJ6DCo)T0s5{8x8;}6|a~zn \8c'3 >{xn0E~E5?7a,EWdTw`>4l{cch{RDDDDDDDDU-8GO&Cf9u
_o1x`pCVxgy~<'okkgS9h1:fy)v8:{3z@/Uod=)y!ssd ^"7a;^FY2i007f(>oj{#q3
'u
` 3*xk]_ F~35 `CC """"""""roz/^3;~;8=^$yg1zbTY4~--U0=;1jS~~kz*W\iswwxg}:{Lc/?}8K4ACkg]l[\.oE Fek/T5	S5e/3]WsxG7]Fj!"""""""""rHoU [
y/F>M{0^|r!MP&9xPov~3lj~dg?SZ8l:tv1 Yao^uj$ :kHg%`> ~=,Mk0+U[v,2t}cyp.xg`op2c
_I2 !pk^dQq.sc[d\`TiY3!a+^<Tr!	&\Oi\}?3eauh6'7o9-qB{?"HI36=5?sP[6dky>ss&w$<>cV~?x,Q>Vc>kkXwsN+c}e|Vu(;2wcc8YDDDDDDDD:* 3-ofn4bST&?$6${Y
4VQl`i+6OoO9cDV)8;gsc,*OO;=$hj(V1e78+kns31#PuO.<JEw%	UA=xf^*BWBRyZB$1?s}plV3Oy!n>@||bv4`_+M	{FPuO1> dgR.YF@
g+>bnCr7Sx^SpM=n s'ds~)GF03X PN"lunwKMm?K'mXx/JglPf)Y3[8^?x5\^)e
Z?r~Rp=YKwc}sU-8zFIm
qd3-Lgn,0zP{UN
lx[-*d8 3b3B'I>6|fT}OvJR%lgPafm91Do+mfvedVI6p
h3 ,VzhK}~fw~o*&8nrc	6F{C-CAf;M5X^_ZPjUa7Fc}4abW'x[}!AWMVz,sle.Fv^CLY/OlrsmD[k:4`]6xUE>{%e!{|Ked=^~HDFO$a_eO:`!0ym+}<2vWy=Uz=/`{fRH1H]y+^<{?w=?qGr![E:dg.DDDDDDDD.Nme>)]\rugtwwxg}:]8Z_.ngtpyV=|i/_]x:]^f^0_Ls`v	WO-8DDDDDDDDDD.i
s!v99v6'yZ`On5wu-*|{MMrMNsPv8qrCSADDDDDDDD7.2w@c:}/27$#v^a d~;F:!KK+<Ybx6`	2#=c.2he1';C.{F]
/DDDDDDDDDyp.@e5 [}[A"#}6|~H$ "gq;-`F@Yenlkah
\M[
O04> n=l^|J'(}C8~-GG5^3^$B&ac}+v>d)=dgA ="b9sgz5di-Q3h?1VCIk|"""""""""t_zf'IbDn5T_UFl4pXE>"@fem1E"{+HYccU~ferf!FPbop&VZW0vYoJAYJb*e$\Uan!^4
4#2*lFTe>{ezFIh1IkU'oV=7H+\

LlIm31gC*=QVV*V
d3nsS
b9Hz9]C[2cd9hjXWb|*~iFp_jdkj&Z= ~:9Kb*k2UH
hb3>P1.VnO@7U6]o1\hA)3xU{K<5V6>62ADDDDDDDDu]@F8
BR0xd>bOvYCfdlbTQ"=G_Ko?}MvV7{tD5hc
)L,';_ZrBu+NFciEPPTT~2'E[B	{sle.F >k$b
}axZ5f3rsO;'D[Pm{qOoPA ^	Ps],gU1e^`zv53:c`3Fw)6bJm?3chF """"""""+^xgxGOwx."[v,u25cW_5
O?k\z+W: Z^\5/?/|s;m!6(oE Fek/[S]zO	S_DDDDDDDDlj!"""""""""rHoU [
y/F>M{0^|r!M';x}!o\q1f7+5>7s\:}[+FV>hlka.5~k|dNv""""""""F Ya3\|~voHt/}u$HT[|#|-=o
_x R<^~es2>pqNvj]' x,PrADDDDDDDD.w/z "w6	k  '8(D@ Bzf7bilqTq793"n?l:j0~9q+D-5~	cg:rl$Nn_S>@/-!3 {rDaDaqdJuy5q2Q9`]k urW_WcM{#wiZ,\;8^i8W{`>=i\0sDhw
< Iy y_4\n@gX'I&dMXnG,2ed=FjXWlVV G ?<gV$p-2    IDAT#m_ddy7I-O`gCrPfsb*h7#]S
0_28cI)gJEq/L*\[!)cqI)q7ZK;YN
pEp>T**"Znx?UoK=66pL{,6\~]@[UF<2,5GJbttU'!xk q81!aFI69gtp1WtEF
3L6l6P^0mU[9~5bE~IEN_wxxgi!""""""""SW]$'>P1^5	X=,/<>p ~Z+86Um`d8mU68sf7G:#8!lz$	hw4$\7>6_RADDDDDDDD]U

'ViVai*j=::7<*I+=XC?d/;r$& NW]z\^hgNj*A6Xz>0'
U@Sku`TRy'jU~{.kgch4^w*
eO
O&BxkVoV(<aVlqoMBTI<Z/OlrsO;'D[m{)2gUKWn mnJvkf5H1e^`z{?LIDJUvV3,#F+	[x3fUb#?fuXSLW5^F!jL_Y9R+Jm5|""""""""oz!e<W^={~o;zv(R;*i""""""""]{VwQn;_sr5^+WZ\wV@Wxyg}w[z-ii^VC|'C> }BBcwPDDDDDDD3W>[ {/P^_3kNx	S>/9}&{"""""""">Zp\*Zp|nV{
Bld[QSu'_dgoZ
^cx]eskYGak|du""""""""7D1h)"vP!Ip
cSQ]IVp~?Gk#>/aS1?Y{Z?$voZ\1E\d7&!aa{
Ad~{>G?$d 3CKc<]&,q0/y[
O04> n=l^|J'(}C8~N/Nk[>%73  9Svf(Mq*[
Nc~{{8_-.'|;lO'd0xd$' cM5?c;5bU{`>=Z<58 3e5"|'Hx>S""""""""rit_yf'IbDn*F:Uf0^CkKcUVas-8@xoIXNEb8lxc
jCr!ika[m1e78+cZa
O9*3XY3HzOf-\}63!>MiN
;B#,-#+^[ON 2Oj\y!H( Y9@~yMcdkD(\>	wY """"""""IWU@,\;(U3Qa$*68#>@p63%r; \|x.	g	#*e"&pONM?=9P"JxBATt aMGT7X-f
SwdfY*[zA"a(0a!Hxhx0hPCttNIw'WsP3~=QZEU
wYleTP*?w	o;ime@7uaO
IK-1?MZwDm}CE-]l]7kkyj-]p5c=      _mou}JqSW{@PWvy=?ITapG}VV|PK(jwM_u yZf\m]t
? 9x{iWa|Xo;&%.     ....%7#mA/}{hy	[[F~OZqW+|KHyS4~bg.\g<GRs:7v1n#T:3f:Uh]tMw(@jvtX@g;_W6K]z,L7T\^x&;.z.ty      @Ty[f9RP WvgV#l<GoV>T4pT-9zoR!Nf'[fc-(
a&Zlg]39m39_9Zu AusgZ_);X+m[:R^}?Z<dNAw[wz_|JI<<0'_Myc30<n[{rkC~&._CgfzH]      f5441jllK4z=u~^0ipbZBH>1Ue/Ud-j      N|rGp$4=RIR~C*>>^ev@1;+K/I</=</\?F$uW4?1]?W_~g9HF4~     L"nz?$|~K.hz|/j:>kS}u5cY,     @1       #8        Qgt	AT ]6_OPe9Z_{*_Uv
bp?Ukp+Vo-~Nz"_ }9kvn;p]|5:9g>>w~I
      <^yH;jqXD[w,;,	<[5z3<o~`toN[[a^L
B:}*#9_WLY
      K.%HWXR~$mT^FI|s/ybKvLU6?\v=QS
T&jV/WQd_*T)a}rU>]AU%ux6K/<1X[Z5iJ@KO8ON6Zs&kB-.bvC}]>vgwP?gFw]      TRvB5
Zwgmo8uK[a1zMG?R%@yyydr_[=m+lQ;Q/l=g_iq5FQH*:>AE;hd{KU}ERt'{o#mAsz:W`s*YuN|
;,ZzOW^P|nw"xBvw]o|JLGg      6_u@Flv.R%'>d&P|$.F:V#Rc=}0@?JQuTtK|>Ssm'uV}H{	!xN%tq*!mJ6OW*x^J_wWj
z*yh&yvv0:XRNg;5u!      h&_kw-
gcqudsK7V^_Y'or3E*?%zjwh6)/*.g/j      @Y@ }-xC7t&n['vS9>}N-x-:bI'H*{kNf;czGGt{'Cv?F{^_'C?
;ydIJ-y=s       J,E n5|22\r@^ABh9Ze~2f=,iB7)7xpgf{o*Z<rymtsa_gu-k=+vk-NwwCgseg	^-zkK	ws!
<;ZFYHz`O)VaNJy~
      ePhb544566K_{:?R       #U):teYVX       00@       \A 
       p4               Wt  <].tj~27%G2    @@ ijUyH2Y{8$55c|j~    4h  Oe)\}KtQq    hD 
  :di	rYu\hg    Q   Nr{2Yg>)eedf!s	    	  d_{.?	ey9f    v @Y{e<$558qL=vsV[$r`    @L# ?dX=X}WZj{tL      |  'HjS.R>Y2%3u3Vc=i     u/8{
{xLL{d
sP    @C 
 @w<YTv(58k(    x  D+Wd8(O3$/]'3k=VcV8    g 6>;Kdq&3+C2Sn8      D1O*-2Y{K_}ZGTezeK    @ 
 @Xd.sj]{uLV|Y2D+     _JLVdedfFtR     z4  n|E
Y{YH\?L&8    D 
 @ohjuK+e56!2mLMx    >@ 
 @O#	Yd+ORxkdfMIHp^     "  0Y2y.tcnq1]JJ      tYZj	{{oWb     D'h  ,k{=e|IefyeMX     @ &Yd)8W[{3Z&q      m@ O/]&~6ge)
F     O?{Je]{ri{KqR     h @E<vLgw7gf!3    # ww:z,#F{{u    0 @ b(8:xHjjoqe#9<e[0     4  |>:&n'pxi4wM\0     # DcS5qf8dRjd&]     D
4  .\irYoLi=2sN     - W_/@XO_;jde$vN     pU rEeW#Rs&a8     V>\ye9*eJd\	kIHfv8     (h $[T:qZ,}W_sLV:OO~
      @ '/`~Lqg=R3fgdLU    hG 
 YS~O$Z_~)ow8+T1qefHG^u      @ e]Tq~)c[Y+[jT}d6L!3VCq      4 @*\g\<S.8(!,3n9+smR    0@ mB;I\2^{w$$r      @ a56V[zL$)_t)qh/W     h .\PO_3FLLL2gH#FX      ><AI/%LX6     W  5a    U 13;j4QjY'e}rR>!/M=hV
     #h 23|}YSuT}R''d<-55)n    @@ 
 CdRoI=fSY'Ns3     4 8enY&     &        W@       \A 
       pE6)       6"]BDm mz>ht1  ` 2h__t   4|M\#81:p6_V9g   D1F2   0544n171"*;}>)R}   #D   DS{T}>,[&(...e-:1|:g       I1{$/TtB7MGh       W\Y                               
h       +E  &upeyu 7YtE   D6Tr'_GFD @ ZnG
   D?&|]zz/Tr6"% b  A5U5ZXH^pM,zQ^qRKGrp'uY>J!Jx-T@4T'8N+*Z   ~W
x\VkKO  IDAT'joUN>x^.-WQ D': *8.QOC<U2|xt.|CxO	%)i-/g  @l$}u4R
%Z>d$>K7 kJIM87&q\SpNI
j;\m%sq tu=_V]X:  \jIWl=UVP*]zxQ}ZgJi=SkW4  @@{::#=N%yq>,Q*NT'lS/)<8>gxx8ITs   ;wA'J]AG*/Hj8#>B	| b4 DP3Yeq	e=Q=\
  0Rc'J:Zg0YCD-*Z E 
 Q+Q/	KERE.~[AMByx-<op&JjlByFp   `ow	BR_gY5R.hn;A 4 DPR9(qz=UQ*%=U\kx.NQ^,IN6CIF5WJ   ?wi4Z<[%]z}s&J!}u=>;=6w }k Jj$O}<6k.xUM=U!3/*n	*|6p%)   
_+{;0gI/l-
<*z^Iz, AVCC	|F]tIk~SiA/tO   einTffIO-pE   ^^VW||,?RIR~Cvt@       \A 
       p4               W@       \53,       "!p%aNl$YVDk      HE}P*I~W}"]@(e)7Ug   .   ]3eXne(xO__Y[,       \u44>E'B%(!!AokgdONh   9K#]    ]K 'j*eYO      `Og)Jh5D       z[~Q@               W@       \A 
       p4               W@       \A 
       p4               W@       \A 
       p4               W@       \A 
       p4               W@       \A 
       p4               W@       \A 
       p4               W
       t@O#\W        zc     IENDB`PNG

   
IHDR           IDATxoh\OmmXE&Q$#tHBLu! d'L@Ckz[-#a+nZf#lO5Qp oF![kg{D"~Su@hF       VDca       */=               Bp       p!8              \       .        c       1              Bp       p!8              \       .        c       1              Bp       p!8       |=0WOp|[G^R=s|(-k.?zlSOkI&~k;rQmGOW:r       58<~0KyQGz5tg0S:3.H5fw{bAG>|Jg9      Z~#Ky
u:\Ouut^f1mG<^>j_g[oW>T]IlUb[kJ=/'fnFC1      gM+'Tu&{BcIm`#\vM}m.yPj1pJCGz5tIg?iBf3P.8P}^8R@3      yCvq%w-$E>?{EQM].\koMwj<-?ZZ</j_zhz{H@K      =Ys]Yt&J(	o~9/;C_n`yAk;}vjr3k2R/3V]:!REjAWU8      8."5c!2T=Yq| YzrJR~^BjQK:3#Sy/i      `sY4>V\]!-.op>?K}|S+?!Z?dLZOvZ]Uo;7sz\zjqBc      anaE\iw[~ipJ5wh

95f,-2
'>%
{K_W>dtP7ss]v']qK7W%{7_y[>^Ia
      w~H$S;_^,Uv3R%mYu
,k_?'>_2       ^`}c       1       Cp       p!8              \       .        c       1              Bp       p!8              \       .        cz-w       ^O*W}k       kV        c       1              Bp       p!8              \       .        c       1              >`^=qicT\:7u<b[:s2       ljkZqlBr83thkM+|k^!-X';27VZ       65u
 ~M}u3|[Zj\;:+Kxyc2X       5K8vJ=%EywCKwfuMhkUk       5/[kw@=U;?m5#       lvkr<w,~?-uP       Kq*4QUa|C_N
Y-\q       !8<~P(DO_;>z3n:;{N        V:nxIKKxydr`:j<E       XCks
I7tkIDz%z^m;h-7td|3-?nd|q{7vv)       ap^8WXkKkZZS
l;\	DMC       VX[/tQ_):5V5qv3W>}*P?L=wetqfj "e*lw[PeT[r      X59v$7pJCGz5tGgM@{|^>iMiGz&}%)hWgwnv3x6 g lteW=q#cKs
]jpz)#       
//zN@=kw%E],3T4mTIzlKvHi3~a?C\;vc5g_[%JTgGj=|      T@$5INie/tgV8bB?[hq}l|,W1^fW1gEDm+xi$Gf       =\tKzE93+7Am^G~^{Cn[X@=%{!G0%z'--oq*u)[CiS      $Qq 3m#$}6]-L@v{K5Ja7>0m{:K?CO.1y
u!-DLqj?!}JUtEw@TvN>H
       
'8rQRmo%7+u\iyI$s7ka-{I~_}M;aog*$c,{Vqqf-}pP"fGqGW;eO^';)	o      Hw~H$S;_`L?#+,$L      PqMz1      c       @r<       1              Bp       p!8       |?i%&6z8       yw07K{      !GGF      &@p F      &@p 6AOc       k       Bp       p!8      T4Y=Mk     p/x#|
IBPAAi4 
7Q_{l\j<DTlX{FbkpN1'ySx'Jwlacmq]
!5|F}}$ybC!vttSR{z_o*?sDz-XIG2g     VCzXu;>
HC!2>=PHCs>5cqMns,6#\/C5pHa]:oE}#	CjA1=Rcd ~!~c{RchH9U~.&r-C8     0^c*"v%PfP7rE/TI]iW#4=WWgT1:*0;#rUD;+mvsV*=emRcsU}WqT:4P<]_N{
^k|eR|B1I*#S"Vs\bvg*r238uuW*ms}^i+i^k\vXv-G)g0rRkUi4]Vfz=KYTpeqc:^<k9S^z~7K5{HD:$_     p8;	+[LhWCi2<$o0 BXg+E_;S{ t,'>_EdGW!YYG2i(	.^/P}@iXsYeK$2UjHymOwLfW#{pGVhS_.&TY6UOry	C#>:<=%JkYuGw53Z2# ~TmkBypf<r~bDu@F	LDi;9:4FrXi{O,yQ|cs5|0>_$G<gC>=?/_2}.=tU/|IN7+V[D1k[7     Z8k[ZdC
;`&JEcYN+^iD{>Y!<vi4u~\X\	jDs9
[
f^9Qtvui(ChJ#yT9WL{Oypn=HD2yK^cFz+c}-d+6W]f}/WwM%-\~w|LqlQu1Ncg<2)>a~(v]{'tiR    x?E<g/NTks}CB`F]\WFd
mNbw]2_tD)Y[U)=]jiR
8+v6l=cKJV$gmdmK6|#     !2Qi"hFurF]zI]I+ve3c#n6T$')VY kRCFvtl^JV]YaDIo 
m?O.jcR_UAK:wsZ7;WN:c|AzjWoA
^\mcii2UK~r!.miq/WY}{8zovUv     Fpiy}ksqI2$ur,O	M"\
LAM_J&jdpu5l=pU(V
Mg:mY9c`O=TP'W;ZL9o::h02?2=5ABxNsJ^z/-SL>`eG|"&}<;uOD9{yB`_wu=>seKW]U\sqy<+s
ktUuB9-T}-c.{hZGh    x}%	7z<6zI	Ps     vI~m}YI_8p=~_            l         c       1       aI:Z%>cG'IsCj-.wNHWu(>>:Qjzs,pR'5<:]U:Wd:#Rkt<DTlXcHgBNGBW@rcA
']pNaDsgw3a
o	ff)R'3Ru}]`+7<G{#}l46|\:s#h@K     `#-Yje;3N~IGTT&t,4>oE{sW!*38:YT}OH&{IDRTp`zBtmKsot{zC'l"3"4Toe?.y3+6/FBk@z"ps*lRcvH#@t&Oxm@YZ nReNu"ag<p24vf5{/zXgW47y?Y[iS6[e
     "ZUlvk]<+VpTYcr]K'v8}|9ImL_]]u+3/% K,|sJ+Wt-{s^3b,/

9e!%M2	R6%[r1
s$i.dXc#\g
>	3!5/gy*9_/x     `PqY*z`^*=oUW_prejRzYh:PN4;Um
gTwR+4$'>Yq5uJElD{BOq+}O=S-Q\Rm;q8>\`YVS#oXiJD*&yz5Y1"{_jcl\wzn     M9Objl*/k<YJ+p3"y*bQTcmDnev5WZQ cKV=s}
wmz/\^d
f;w~Qmd+#ERi
/9a=tt(F&rE&5$TK*ql5m,i'[U	Uk50     X.ZUlR}#84@gql(j[9VG$\*.i:9C!5s?:C!OME=}g58<vi+ziesX{99r:CT9)K5tS:<:z'@&6=!gU}n(W&_XX%2!M/c\uT[vYb#*Wm     of~5j?I6|WPY7Ek4euGV\vrq-V
GMMv&{t6:B+|=TaaB\2ia^_i{Nu/.6jGet+9[%=F$
j    IDAT^     V*6Hu>K'^fSjPtQj:Cr-[v;eqQMo* RkrVq$6+q'M$+s8NXfKeFkd$YkRs:Zyz5dA'/9Sjm&D4(Sn+C:?8W)D8_[jiE}K|6]F$     w^otj=O=gzH00#uh45~PYzw6Ww@    `p$O?]>$?nUtDF      58U\	l(      5        .        c k.oVF      B\OG=:Z\:h;=>:?#EKu9L<~nx2yk6}PHY/nfa{~1\4'Uwt>V2cM:k:2tt,Xzr2cu]     l>2I~xurW47tQg	;|*mL@*]
9'Z_G-(2UJ#1I^=Th<;tG=h2ao:2iz=WXG;~YC\F~&f;Ba&d     oT,6hGU!XjUnk&G%dMP5JU]jLgZUa*D)o8
L0U=>&*uh4}?m3jyg]/g\s;SeYIj<OsQgYwnN*+3YjW)[s={(
84-`U)~IKsD|:5>NU{+u6Wtq}#i4}^:7Gda_s5&vE
MZ@-)#
s~\dg{4pl!)3e;*[}s72@    &hO(=RW
j&Wc(s2_(FkCC	6C!5:2iZS
S^lz|e%cq}k
JQsvui_-2M>aK-tl".OMzV[s'tlT%S&
BjvWaf2wx41<xY@}EY!Y]([a{+jZ'9Gk7rE>.bK--hNH[<R|AV{2C>L
+\~O}T%K1=.=z:Ix"@"t.s5
    ApekW;;W,.y$9G?yf-{vuG%zyAqy]:5Y!6yvh^3c
Ca]U:w'KT8JmsdvxhBTtjsg.:BeHtXvei7`#UL2doweN$Z!u}gW~,aq <]tkTzNki+sR\O{W*2)>{}Y^uhWw    MHjyBSBhz$)[7L)VS2WmKwnN^27uU}`K_sZyJ$[zqej2_ MS?Y9
!MoszSsR{ruYmEG^b]ymHD}ny= 6
}%     c>_=Tv:TK;KL%.=]$]/hR=UXb*X4gtrbQG#?k	\]B%+
D}"VeqM^_a)SODIVJ(NPWYkv%?G+d^Y%ap6'y/6ca;tWD[;=Rvwz
{K|q_FV,=    x9<!0So_'IKJV=eE@@q	geZ99Etk.2ke-HWwK|n6Rq?$utKjf_ WhP%d!?5N+W~i^!Eh9	K=:'{|y'>=K^X6V0/KLp`ixb{RW!<:}2A\kd5Ax     8|%	7z<6z9ttS@_Y1zVKDXs0/od~C{r:4yKsg]{rU    &tI~m}YI_89#3cK&5wVw'tl$>UWcz}O    1VEoIU1     f        .        c U`P25T:+Sk/fi5    aDcP!AES48v)SkKca
TC
k`[bs+PsZ^uwL$j,<,YZ[T(M,sI,9Ul(~HOy.my?}Y_*kUKr$58Kmkypl~Wq    #`
[U1C [Tx|G\WV4LJJ$$GYoQt.<:4_1?NYgPK>yN
TCSf	QsYi!C1	Rr
clf<

{sMYYY.$ifc     Z7;-:UU*JL3P2
Q,Y9Q/p
\Uj	VUY\%
OmE=^k;TU(1s1.wcnMd&Yi6cH]"Gi0mH^RmRUs!p8/RUgq]O+TY*%1HRQe45o2UZesU[U&f$X"IE[UU,rXU[:;

<k0l	10C0)3	Wf	0
},j|"P    X58p41u.Kic#XxoG?"pXcQN2]C
-mPgQ4+l62}A5i*Q43%}nV0eg;"j:fT<3:ZU%4u-Ua~\#\<$)s96&mTU[!-NvB*W2etb\pr\ic=.j(KH;uj*RtT	OV&Ri*.oEepq5U[UXZerW1[4{M\SQIE^k;31U[UxHJ%*|cXO79=3\PZ\ER"u*Xi]Z[l}8Wy0}Tc]ytS    `ToRvRSiJj*J*,:gkBHQ>Y!OUmSL63chJ4Tjx	eTw.K*<QTZVXzU

cRX9
HRjo]=U
2V@=f_iCP+LM]UIv Ue1_L:VIcq_,D\_ZvNP-z0(sa-Vl(F18vl*{jRWEX!Wuf5.sC{.-     kxZT8fVo#vY[h8EX3k[jKd{1R[:w^q]%ZV]3_^%Um8u[*T-LG
y9ELk.V]Mi6h2eoU/:~a/hq
VyM j=U<cY|IReE"%t;g,im}A33hC2c~9:LkUX(6YwyJV(uil     ,*6a*jINi=~:*e}&vv&IX+R_{\PtT.B5l}(<\zjTn/Y<kS	VUYt{Me3(4UE|OV%-Eu<GtJX/KLWv{g4U
jqNHSP*l0w?g/ 
'km8],2{-^<Ks7vW|    nfX/VVXaQcZ/_owmmQUK0Y-T!TCbJSRW)o\cUK/Oiky4S:tT
J.ilpV(<4uO8u[*xO{%_X*J]BZ,NvEU	M]KJ/	6}&\/=#:<A5~cK\UYd]3vaqJUSn;'YD56!L~U-AU%tnpJUZ:~{y9oZ`RcyX~     
w~H$S;_ !` -QP]tQ2rzN~c[mDMhy     .H>eoJVuL@1pBV_WON'\Suj1    Gpf
&7bd^Ea.iq\Ya
    x        c       1              BpL~~Ul8       lcdVTj <      6%cd19M%JlU]F      zF kU-UEO	MTCfTca$\`X1P
-Ay      M!8>xe3aU"w      Gp,*64
      f@cd$4      61*ifXac      `       Bp       p!8              \q*Sk0bG      l*~U!AES486GPiTca`luj*2?$tnp\k2

E[oT"{)<lR2s:75U      a*{[:3a
AM[M*V]McaMWUSWdU)kN-
WhXW3:}V)vk`@     AFpZUu}^`
..SkKL^k+EI,SSf,zT$B)tPUGJz4*;NWKn2VU1'*lXxfZ51     #81'(&T#9>/F-UJ58ch+ rQQqMBb$%bfnkbX=\ERt6WIuShaCc&4.k5q-ZsZ?^ag$UT>_j`      XS]q)ss>!`a*)VQB[[
*y,JKf%hML=wJq'OJ-U-
VeIyXPzuz_TCKP%]      &8>p`CPl&]dqF9[UP4DNE5UBRi'<S\@qJ";ZB$[7 990%
sC4x     Z/= la*jQ_^V\`Pu3u\	"
k
*+n7ib4kI*"):}ol"%G\W

'{</*TZnq\sJH**ra=U/     5*7aSkKg@,~,mP0h1kS5jI{9ErU;1
f?
{^jhhQ^l:wRj$iq|B`]e<3ll}!%x=+F%**U      k{w.H?FvAmC}4/BZ78A
     p$O?]>$?nUtT/
     `Ua"            \       .Rk     G\?M_?'w~/^?o)+>Bo?^C7%IPS^<|v{jOE[[]IoRBc
5yW%J9m_}?oN_UO]h3~5\7Z      8UMzV=4WEwy7Jdsj+orzny|zvzowvWo-EWaxbYG6kX/Ew?R$MM}$mM6}\TTv6mD.^     u@fC=rOjh*B4UJm$\cUzq3<Nj,?$8qQxa?3'ST}}M\&vt4}YueqJvx%yS_v6m
e^)R96*)DvEssd#Gr_S{    IDAT/^$I?w.~z%k}#dB%eV[Mz])BM	E~Og     x3Lk*}39+'*W'4}&m}K_ool?/O&U_0~ZA2u{>x2T6oR]{G&#JHmzi{^qono7'{T--?tz^nEZ/o;
uw	hUkJ:%s
YGn^6%~O&     >EpI*o]Z=y;T>[?;-?{QRq?/eol}Q?|e=YK?Wj~G*,Iv}5}t]z##h|Wz?fmTw)j
Pw3XB|DVd+Sy{m{b}m!;3AV^3(+7}~Cn)[F\oMzcIr~y<tM,{]k     p"8v]E+?v]_9aU~9^wwvy9W;>HaI?QqWZ'*VP|Ue*}rVZm+b7L_?|yY/R&)/'%U;zuWj/%-yV_lVt-n=]}oI_$*aI~sKz_$HMoZ~CT|      A3G~}izrzN\E]UK7wK9U'WdGU`el+8Zmt,hm~[l&=;`QU*#hb%M' QUUjE$g3'hhnDF8$*0'fs}{HB	^ZMZe,Lo%JK3`6>2f1u;FWJ16-0\&-`^vp%33Lrv5/DDDDDDDDjZnc9afg';	kv9/b4[YM@g+I|@CjV&k:3l0sLqm)<9rn'0YI6k4H"Dy\n
nw8=rtdCi:^dBr0`OcN27%Yk 3]igH|:'+$X&z2~"Skr/DDDDDDDD_NyoS)jwJJ>[.7K46o!yqaYBDDDDDDDkW_-xeoYpwlUn[
??
McwvW
Hszm=#YfEcqQg{rV'"""${h<yb{"vt9&\;q,mx(:
l-&T'Gxx_`OWY&m|'mW}ki	aXc.1YCGdZQ7&oxo<;d)~epkfTgpkmWr<,B,#Ml`pq)l~H%Q>p98^9cW\ZyQ5[5+ =9-lqX~-K\+U{!^V\9Vscg\PzSL
q
*"""pR\u-S`(0v6lj8mN|=8}=']v} Fb}=$tsk/!g_X3O<zqW|'!_}=&Y@>^l"OGsP[nST#u[7V|S5;*M/?kio@btpu~'9WI>,Ut-tn2u~sf:wJ\V\7.unV}r.v3.}_c!>[d7	~o|"""RC,/dvDTV;s{aGUk_%_9%{3bg|u;LB]>HnY&^< _
>fl} *jUA?kxukXZD;z2[L /[; RW&~~&N>-q0j<0;A|KmxT	?!zj?* Z6QIFOcoe!>Z	|-TYv& e6mfSMc:;1	2w.ey_$|C&mogfz0V20!$}ffd?F(	7lvl
\fYO>Cl[Rl_;K3g2r!'?]u^Tt*rH#7jtgnim.?s ?=;f&S]W{w~
+t$]u}V1!4&:B:R	0|LW/DsLvn]a~S`JdMF|J<K+t$Q'\@ycBSMqc=z79y
`kn}3~!7F:k%:
}o>k82 B""""mh~ L`S( 'bd3Xc5ljbO aR#Qe4.PCwX^f+PE+82}I4%1^?!cfQ5 i6`CD:?Jsf1`b~:}am NPCrQ	U0& ;'0=)	_WIo68=F-ipi6 U-$/`w)s\&Y
Kg}^
2O"*z,Ug@=V 7*{Be4m6}jT0Ica.+Ds/2jRDN!l,vJK8/*FCfONm$V3X=gg|t}!;Plo!v6_QA'xfo'9f?3AcWK4F}pk-Q-~}7XTE&Ndy<]Y92EF:)+V>'l~}g:<^[D!G\ycgc-y
wSP&p_Pk;MYV F\cvoFz+C<2&_ZREf~TFo3cx-u@]C4f~8F6z5z/Q6mrd*.rW6wxN1`9 3kvFW0WaBPS@mS/V0F>#Nd=NN}XL8aFG#XOH~ot;4z{L;Fm66egmw>>~G\f6
6Y3n,_e?f1~J
hG=S~-f G]QKNP/)il0t5q{Kw;vx@Na>!6#$}2Yq$'Owmgg6):vw}-(q*3:cszL-{=odoNXeL?
Hs<7s$?.RbS{L/J /szYy"[1Z0=FkL')'D5{,OU*?_~~qww2}vXr!2g%>/D ,fTuvPDDDd)pP0ci6W?5y./42kz03- !g	<+S7h'-f;MbW@},Y}Yd) u7T>QVb cTi	[F:KLNOqjo1PG,~!ag;;{?X's!|+T|uE?VOco=f]~/uIR{10%^e1/|>kvH}~g)G.XkL[	%aQEkT>2N9k}ZYj5H<j=B+mfu
v,Fg|_MKcr1)5*w]fsmog88SR`yI`^PL/""""k(GX(*7^.lyc7$Rlw7?)4S
2~;@~71v6vpx1Vqqgc{=!t]} {^[}y Y>vGv`uCYMifh5-t.]8,T,H^<dgE~(?i<MM43}If=#0)hJN7Tjh`&@Q_Rxie/5TS KIce]dqv9)<v:/\uiQUu^sd}d5bsw'\{OP=1Qh87{_g;;~,%xEX~yaKEP\:7_r>&7zzlV][{,bw>[SDs~}D|!V~%xS3G2"""PN5Ng9SuMX!zM-w?2Gf[v>q<"zd=C46cH>utID;"l/<Oxq^;"<bVMTPKoPKkDC6#dqA.{	dxDECJL(}<8&2Qibg[bgy!eK}7K~B9Vf1@~Yi2I&o`b|VVOl
a[l*(w^2k91qW,*pdE[CkTmQx&Ar 3[}G3ynwey ypC8}p
PBcoY[,c}hln$y3E$GB9:/_>X	o!V{u(5vfrvMXY}/
}dvz15u
w|*9Ca2@O
XIVDDDdOtWv?5~vDDdzed&{[iAEDcxc>"clf
HM/&m&,ay+z;vUwv _}}/Z>q,""ksz(,;3gDdbcae!Iq _\_'	]D_[u}Ykq(XDDDDDDDDV359(p,"""""""""".
wm<<tBwC=v
$IY5OChlBb+S9f`0!xVNH} |zwn},(:"Z5/2;^Df_=9-4ml{(&7<wn#x;:<|DFb9G&C /	oN8d~Vol-JfQuK<9/d>{L!{Kvs^wgTYo8bQ#!\;KDDDDD.e";[Js2AJ!:[
=a85|o+c]H[6l- 9^.,16x6h0^ -|k>+8vu9(Z|
vDKiy g_g^0<Ob5`~'Dbi<m/nVgvs?;kaU~;oG?NY.{	GDDDDD*e?{jx_oe 0Q\Ky@azK$1|fHrf{9_7# gtey	}":<$2 UuI+/=vU9!7A9Sb1u+S^zec["~$[+XY?;Dv0q5F1n2?u36<->f1v?8e6mfSm:\A?>Z}Q0nQif;KdKfcaB5e?Uy}BI'6-o;600|9=aKGyz=Fxz?|=fl} *jAVu{	a+Vt"	>\_uz6n:fVr|;&Kd^X.'<2_yV<[	?!:^f*/deX~B[=O7inag
%""""")p,c{s-TChl=;F<rw[apmx&@l6#j' }p@ D;mtn}7hNtl=P x`v\IxU1l]}6e&vY|
]r}|cVJ\=*6]HDclTo+cjwfo!v>u
    IDATWKxofgY3xO%[90'7=[wq< | >clc FGh2~q|}f>1gbV0H	Pr~8.qCw$[Cs<t@KAy"5Vg{,RyCHV& 225}ZT,qB`R!N86I =;])R1Yvq/u>^Xq^
z?&Bc]#L pwP$Vwpw](_bJDDDDD4rVF+xki~	f>/j*,y
T9n$SD0g4P`}VM{\d][XXEG@uYz|_,73+lwP}r6<(Mnj6macm~z+
f0o:MP$Y0mGvpWnW0b!Dx$yX(HV$09'@/'0[H"_-{=Pm_dbcO0 -La3(yY.!1jen;^C5L5Yq9&gS_&3NX7rX
:2~uVr"L!vb/
Kf,q4|q
-4Yo[GTKf34~5b3E`NH;4!^N.jfoZ |o+cvsQ{_f}XXU^}*q\+#<2"v5'-' -H/cgxT{tLPhe]2C#aM2RdN9<#j5jL+|n OAeKA2]F)7
|G a|-+r#m l{j;r
Z&33KM#5C!]] g1ff%}A6535Nge{AU}o- u(31EyQA.0{5[~=[*B
;fIVWqA`6u@a=C%+\$8d_q(
VwN*z~QS2|kYcUoo IkJ#Sc{#:OQ}"$Ved)JZ,\qkB0$FE4,V^KNVqdwfvfZGiiK2kLxTn+3602%gf3V%\`0lm&j#=9LW%&SGq6s]Vv=3|dk`_,AQyaf9dE]=G<DcNW2+b1~>?=L|Lx,3xkfmg:&=]"D('.?`j(L^)ggO,gB}m#Onq-<v9k2>otr7\'&.{<JPt_<&n0s7cwdWVy88?;(]2'<'yr={AAc/~J_fPD8S[$o2F-=<VdI+5{8aY*{[V0zze#kW_-xeo[gw'`cQ{h=,VVHHc6GCDB(IhY;Zx. `e7$@jBOnv(xY"""""""""8uAcqQXYC4<=Y]y8eE}	|C\n&lXRNFFDND?W NEZG+k{2kvDDDDDDDDHcy@g1]{LE2um__ ?L;g^+>	|"+kp6pf>?|%]\qYGxo[WTdyf`bw=9:k]5zQ{QnU&ko9[DIp{#Vo{px@=U$z+p2~\9u7""""""""scYyvrbL`3[8}`Yw05LNS<gS.X	|Gq&G.mem66	D_AU'[A*fW71K5fPwVp-l=""""""""Ch<7U\ja@5Pl9m]U^mA&`6tDQ>u3>yh_1=]v&7J2/ ,jDDDDDDDDH*de]=X,#Gi<]e
8trp5bca
o8Yg^Q,.Grwa3o~VYmqe#Yg36t
,@M94,0wv>g*ez Ol8pCTsj^8/~J_fD'5w	qMgqk [jnLZ[f"9sr_%&{Oy~y"}0HYx;n`qZ """"""""+k |W^_iYVx=YChlZVq, \=jL'""""""""q,"""""""""".
""""""""""k!|V'"d{h<yb{"""""""493'Pm5s 58;?&{Lgib# s$/2[D;:I
caBuet_?ip[
$16U;c=k{_`|+&K:NK6s`q9%Cf/h0=_"c?XcZ' K8d^#<w	l`O]:Lj-=I[u.>gs+uO~r]3}n@1r\si	a_\
?s\ZdMwv|=F9!l_*NcmUcrQf{ChlJ(p,cI3$X?/gO,F]|o+c%[w0,/{l7
<C
hS$+:0"G8LrgA)#91]58/&@@st\`
'k}S~,7h+X\xcg7	m!$Inuy<D{wdP2bz`]GM>; lMk~MCUww;Vx0(Xu_kF_9StqaIWYV :1LK,\Y>mOmA+hw28`YFeS]f&S]l[w~YC2A {6V\2VwoCv3_62?tt8Xg6Sh|5L;v`[d^kHq'fN~4XDo}-&{ph6uW1i/s;Qq =ve_pebi
bo"	>9`rpH%;D3
 Ftv36sW4p+;Lq62?uuy~>K8,!rnU~XyQx}:RW&),c{%@-{=ce,w_&1Ehykp	 d2x$J\{9UI Os=ukXz;,x=w8O^ tLPZWc{Ie/},1Y}rEMs$a&*FQ](x,""""%)p,+mAe&Pm/G>k?qdZV &ick4@b{;=>`c=%f[y!#v0|3]-,v(}o\ rQz:}ls=h#<b=
{}#!4&d=tBGBVW <:}xcHy*mCnwdq+UQ.[7^l,vL:nxO]O#b^J`x_U(,6W&8b3P{|.)53pf:;Y&tAVx;wlCaz%tg6]iaRE<
mo!v>u
WO8y{q"QX[O#x_$|<FBq+s5'1
/jBtXl
i8L28u,8h| 7:R$Xb?AyFkp+Jvg0!{]@2uJ:ap|/o1iEqEE97;yVV|CSDDDD~~8e=<X9?XSsP
&!s&3HP,3&/b47z&y_[230p^`3/|
\#VJ+kUWq}QW-7.Q/aW|}XBv3s 3\"In9`=Vv)}Xp&	8U%AvF4*PC51jSw`^vs$>|Avs Tnj+B6
Pf97
Gi}}zE<PkSXGK'plTMA_k&Ed_?[(Vx	gU2L '"H	 9Y3'b;f8@w%[u(Z7fd?FU2i?5athE f9B%0~c:Vawj<*WbK=&>GbKwqRXVVjzjo3oAG0 hfBL!Wp3K
gz*^[,i(/F4"\x*9JA"LYqd3Hq	NGkZ%`B~$s	?z^E&wFVj73Q2w : xUB\V#o9k-H'[Cr.LDkyVu|tJ0yus,}?[_y1_N,C%[H<8cp{K97a3kobS.FfTj3r3$o>f ;8|%r~ LH;,8cug#I=Nx,;Gr,&p2!v6@XwB]'zR5h%HyP~zN[rM_z&Ftg0"aX/I@S+"vUJ*`nm1fOe5/W5we
]n;IW		3|q wZ	Ds9n "u~	:b.:E9nb8vM	>\19~N.SK>&vn,]""""<ccf=PZFee$.>b{[ syC46g&6Nwf&|ry}h[_9!rvNVgOp;!cmLc2Ts$=qd he*tpy&o=I*8vMXd&JFG1km&z$X}^n}\fG6$eK>l+vn(43-9J#yqew,pJns?XYUI5Y\{W`ta#b.!we0f]YV
tYpd(Sd&+u< yj[Gl7[lX1OYz	K:&nGxoyLAc/~J_fVp6&z5EJSkMy)P`{QsoW(t]Y7]W_}uq OgeC";oSXD"vzG^8IQbYAWDDDDdq<t"""""""""?sk5EY8EDDDDDDDDDE~yN c<%3lEfvik*;;rv%|9f<A}?cwchp{;m}M `/jcT?n+Wr<	=Qmc?w_6' y?1\Sv\'wv?n+W=izGX{<l+yWXP<Oxj$2wtN2oXws2~]g^'iV;| - ;Uc24WMNv`6U'.SC5?XgZ?,}qgO)p,+)W03+'?'_|i/;uPTM+L_vfG<@55 =;2v{'v+OYk019S
V5=<kGsEl>dgrez?/?yTCDDDDDDDd9(p,+M1WJEXs2x.    IDATgV~Swx`or|f'_'+pJ:<2ex~7|q/@z!v.	
Tx r}OgejK[ggw;u>Z%|	?$/?_,5EDDDDDDD~8	]\o"o-X;&&th 4c}gim7UrOx43>5 &n<ae4OLWn)k_pj~(}(M&Y
~LlP3.]O.6r &v{r+|,qcAv{\nUbvvnS_\)46[#1$<VUcfViy,R+
3EDDDDDDDd~177S:]O
v7%+8n lh>o}Zlr u<,*i7Z_\?s'sge6gsT](^
^n+-een4k |W^_iYVx=YcqZc+lQOcYroY[8uiC {63s '[+q,YC[ONxr0-bwK'-gr
	BW"""""""":`nyu-G$w1!_SqKw052`ml{wWR`?{r{'Y\pVb:j/w6{cSv
NIuIx tD	`_v=UTN{XoLBw0=wn];}
?K46o(7dO|u{d3?J)	B2a`=AGBdFzF
9WDIp{#|3p;Z 'gHtXxO`[]i(;pb?=[apk-Qi 9Ci+8y;}8}Lvyi+fe,C~z wo}dYcc3n'{-T[r'5osyrd`0Ld)`_OBvp5vERNB5fD{[;t4|}L(p,+
byw_2P\l7?+eaSi61jf:CQ+:IekGL5d Dx$y5yQ,k|j;${(S`lQOKU;RWVHxZj
eodzYGZh|As2Wo2!2t*lpd[e7=
Gi}*canA+KDWBvF{nb"%~>
Ylw	L&Sncdk`$xf"BK&C,0PkezugL {1 );;F6oy
,%;1QAI&S -4j}Vy5Kp\(p,FqP>3g!H-0qog)N1:ik
nH2oI`q[(II;"4,VaMYc-?ZJ^Tan=2!!]y]g$}w,s1=7Y91GJtV:g|$z`p?	'M2c8vDI9Wj"""""""")N.WW?RobQG {ZeDDDDDDDDD~][rO2e%3Z4Ye
z=YoY[8!s_ejwE>{h<yb{""""""""8uwme+i4J2/[a[QS|Jf%>4N4#vm/N4oW:qo97cwhYgCulf/
=3$01+Y<vxn[>=X `b^|/H|$a>s%x/$^Y7Tnd{:{`ogf 58 +^\=b>^(}5P8p8Q3/x{L%K3X9*ZwzcDDDDDDDDdyHqml{(9@;{rc5{ke),ou9 R_O2{^^(ko6~dnf>x@=U$z Ux.F0o3dbxgBv !moa6W(z=1.SSTd3`
{c cqGk))p&60Gb+S13W1"s<H85.;w05f w]\,:xO;h.qfK\=Q2YOEtgfZ{}dwVTMf&pN9h4JS{}45U0;15WL2pyBYAdyn^0ysSD% 3Pn!vx
;`+8[\;`{+ <JS	|Gu#Z2v6vy)p,k	|G1VaR-u@]Y6;FkMMSP1\ys"2 "oR?mCyHpY>alb;lq"||~,}iCT%P~ 5P
T73NEnN<
tD9PVgP} o+UgV(q|*B?lg*@U*fd(x{LfP P`vJLvwO 5C|LAL''06s?YeW	=rCY&
as$ow66X8p]0Me]*M{$4*^uLC3[:GB"E#O$%g
h;)$8)2 C>:kddkGz0v^{|T7^('SXge<-8AV7LsI87'
-C!|sn& 
de{e~7tRmkR?8>vCQ7<Ig>pQf >BP(
BP(Zd2|/<"va[pNs:@xuec@;4gN
w\ 32hH&I\Uc$x9nK\\XdUvAj08I%{ws07>9K{FVl}P1/{{wl,,s~I^*g6\a W x:r*tybm2e])
BP(
Bpx[
J?_A'^

EMGg/m
BP(
BP(rT[
\P<

xpgOP(
BP(
=B
j[qXP(
BP(
B!Tp<BP(
BP(
BPPcBP(
BP(
BPPcb{|]Ba.[mwQ
BP(
B(q;2q?MG=$wbp{t3mlOu-x.^mp/ > f8xfBDOL'
q @LE)o$Ol!VF'L\+RFWwfu>fc3+cti<pv^$N87E!}bWnGPRuPW=/AOs5(%7W[e`0c0M"_xDNnYWZyu Hm6Yl7	BP(
C);FIstOlgFq}}{{ZUr.p6o4$v	w
].awp20+?fEmF"o?<R.{|w7P-Eo$ThHXE:>f06@(xyfg[3d0i3
	2@fyF6}hINhD)fgH59y-z-4`vH\>{z_drxi=%%)
BP(GX	hFS-Y&Pe<cMKw}AmvyqH5}B
\ |e:f8p w& G(4B1v
dB\Up@#Rq5'9puwsB]7D{>r7+RX|Mj|!|u.d
\J3M(iV%",3Mu]L/rjH.{|lP"GKVv7mfn?z5^>jI	'Ge=mH6ndh8`kehAf
;FaDq2x?CZ
eScY;i `fN|gR}>wZ4Q4:N42$XrWkiyY1~N
=mE%R%P0Bs%=ztsBP(
A;`be4MSS^_IGC_==xQ_ut >r7 {b4
Or__F}YctX:%Y6Oq<Y{-({xo!`Re4Lqq{"!dmI#"&c{${D7	4	=Do 	R@v+q(%W-&Y}FH<Lgx]Net=:{ksyFc--F|s~hFswE]3TQW
. ?c*"	dx4^4Vw.
&=gxFHW~Gr[MmD2n00fB@cpwJ?2,^uW]NdHL
rku?zxs)-Q?MVSEu""
APA "v!*4OHj[ZiLj4 %
Asm.2;	xfA(<
|a3
AX>Gp]g80.A0jF]d
r6Yr7Pw =5yoneBP(
@;i(}'
nXQu	:L^ /^i}4C:yaGXK*HSw x{
fvy`
@)n:i0H7.	cWA)<OGaiI~4 
SpPio 4N[X{'q
oM4JuD686(
#b-@&>#/P>w O3U Y+	f~'U>[Wq@.VHrIW%8h7u{%?_+7hzzBf;6W@:]_8X8gr2+y@4?C{SK|ri4Hl8 @"q
\ 11yP|V2m;%7X(0\&l&m5/e&qr{
BP(e8V@~oouo,>YF
`m,(8hd>RYcIj14?_v\2s3Nr8dA,x7^h0*kdNFH%B
N uR<fu<vS^5&u_]<[<;w7YWg
~hL\Sa\0dDFB)h(~OsloY@}l.
`vJ}f'
BP(
Dys[R|=]Z+xv	GJK)kN?:vYMV"~]/C
n{3+p.xF(gH!qVFc1,IYjq2xpgS,J)MyeK7\{qSs%\)vGI -__'@"`oLe~nC P6WF7U2L@|j6[ryVD7hGo-w1wllPCt:#wo$1
.ga$gh6Dc>b}mQn11AK
BP(nPcFl_fy Qe2X{0
._U\'^4j<O_NMFXlch}!B} LK	qf]f"!|9}udHWP(5_!P/OYgYH*968	A25+Y*Vs fE.xT^qxwE1ub'*fe$MwGs/tyUB5x|
yc(TlJnt Wr>)QrmY-5f7-"O`p_lxk@Jc.Mi-nw
=@[$8GT& 3$X,6N}m.U;'    IDATbh%Az
BP(;X]][&u|Q?Ma_:[X@.(6sV#u(V~rWomRk%<GP(
b;x[
J?_A)\]lT{K1P(UAw4k#`L;]=wr 
BP(8~PcBP(
BP(
b*Up<BP(
BP(
BPPcBP(
BP(
BPPcBP([e\t~s.mo$DhvpKlC?m}% pN9>vBBP(*8=2q?us`zH>xPCL>7RH_K2e$PG{[3:@f'0S34dkrkxdfY1KUC<e
e.\b9H3|9WhK;`\AMOp1Gg>k~>C
Orc<jXkD$ZAh&CP9>CCs.,XkMLe3fN3d{*JYYYL4n7n`#] C:FOUSM
:&z9Gp<)=jNd3(&<hS-qn|r5mYs<'[y|
pU$"KD'gy|\v
\,J2
)RW1e,BXqN9zyv]+a?-9#Oh,[8w7O4 A^'JbEDc>$v	Lq2s[lTv"u3u4lP.|3A=/kd`}C{g57nG{zb&cLt	wB|&a	w[~\d~?C_%]{/_[&udx/qCm{x6cV>A?'=Y):zC]zuhU/_Mi/
Gc"s"o$`='*2SJ=FsZ7x78N^^/'"i<6
\^C^/qV\^1i^.y|
W{(R;jNn
D);]J;J0:9~=4>XnwV)$~J?9i:{1 ,	eWAie>qP|Ou?sp7fZY
>&>R,efuTnn99E+M*r	@EYLJye|%#8!D_4xD*11yasx6AX7Eq[Za {rN>+RV~U4&k?<FNHT[[F'G`0)G76 c8y'e}.:Cs7 Ik7sJuLiUvy?*u yZ2W=SB	$CCcd
\
HW,B4s>OR
L,T0o|u*;Af`bRJS<i3[MvTWgL1`JBT{(e{3p\6sUZYc>.}rP#mra2=J=
?T2,jJ,d?.q6~Y/':5)w}_=!{q57(O
9W.v+q4
W9KHD;~<>BTFM5s{"+pcWPO'i h>~~{n9aUe5^/WbAP(6e8VPtC,ul
O_i\B3QT
h'Ar_4r8zx&
EZ?$:E/_>ibgb7~L$"US/$:>AI3N-KXkX
9i0#)F*5QAQs/S0"~c6O^ pp3;mXqn6lJMu h-
`e;RSO}eQ={
i K*7`Kv"-
z][jZ`e'\ elm\2w!2HV"8Kws7M/f[=@"^
2De(GgDYv9`<q}S?-FcEm2:
Ou]#4@'b0``,5/}ZYnerI3 q3TDN~Q0kF'a2:#src0a8g^`ZHk5fkdaP|sn
c@m&]O//@SmR!e)5ZkdFMC=u NajCs>+/'@8]umBv1D1K[20E:B8hw*~:}VC5X0]Q>9YB:uHj[Zqc7h?(o.EYf4.SBIL]T
WWm\US-ox9{_>9K{P_7aen8-_I\>YjD\
XT_v"OF.6f9{x|Q7Yrk=R~@u^?W;jM{P(MB;ygg"@k7wqH _KnBL^fEo"!mQ9G>WI:(L4R?xr>@e? 1\U+C5%1 LR<S'L9=tU`4cA Oo.{ b0La kXm,39#T4x|x?:zx?H^ 3<blZ<@Gh.$#w
)?_? #@"[e3)9CW!CKNNmX?zPE>~!4Ko.EySyU!`]FAk DRW@Z0->P .XI0s
L2VDuv;oz58cUA)^!i74<NmXa^R%9S-E}{rrCj[1O9w`I1QVC7LbQuRry9Pg3Sqj,_rxC~vI2m#	o-?iF<c.rWs; Oy%J,>}wxV||Y qH
OErZZu=\>vrcwFx<8=oo =4W@
A;$0xZA~x=Bse$)D1+|!Jn*g[]=$>5RugtP~&I&c7T#3*!gj#4Q,\d?`T H;hm_(s[Nl(=B[,7#kTWo%b6	TRiSo8LFtVz~sfwX6s|w!gYeuM"Y\C*.O?mpMT>2p~RG%xn~&N,:9b9Xyz}u/wm{cC08TX(0BcTE<.-w ~90%E.O,iHEIN\!z']{9}<GAnVXdhiBPl*8boIGY[-=7[^U%n-I~{MGo@$.|54o?_P=6
#\B"^o1vvC YD3HV;=4vr:28ToYI0p?*`}8v`z+jj~A!wBq73vcfCg'h\_
5_,;R{Fc#6D%BG8 1fCi+#B*
ht.f%`oWRqv!m	esqzJSp*C6;mvc`4!b=
MQvMYi{.v`~n%EZY&rHw$1Mc?1T>1=CJx&WwEs.lbE|.@z=J?2=tJ"$bV
_}$7XH>F+Q.zI~_XnQn1"+4"]+Ksrq[L+8]Ms@=;_PYBPlJq7}z7q}~c<0nZ~R>^8>f4.@=V^`P_F52 \<H?J^'^7q*BZVA~Ft;O;W\Y1"`rlmQ8ubln;|j2\rky6}Z
w`,@r2
%issz0>wB@9
5 ;N-\},8ur ,xZ
2^4NkP`_YIyEuLDw34Uf?.zdi~Osm6@E)5GBV\}k~.X[lkUX+:e4`b1IA
Sj1v4%d	}erTF66>Wze[
CBXFBB1^o_rH!??c7T3
zu$zeH=],aAj,<B~5Gb#

Nq*J\6&}b^pn|t_8|JdH$Tz~87W S1bKy	y.N(n&hg=J,W}>G:E,W >13CMe7^ud(3^?d}[YvAP(6X]][&u|Q?
BQcOCX?[t>
Ba~!
b
?$R+
b{*6S=[ipE
zKP(
BSP8V(
BP(
BP(>,vXS(
BP(
BP(
	e8V(
BP(
BP(
	e8V(
B\5>M7Xv{l.IFBf$ctIx$^
BP(>LTp<{e\~j$IYCT'72 ]MHyG{[3:@f'mz%7
&L\Z<S{2eFy"9C8e]jm
sg*s.9ZwOtsmL#C"'|8\eRP7<ajkuM
Pf
hXv(G"iOsSgl A` `Za>:E i'sa nX
)Z7)TZy|O
`N.=&y?BP(,Pc1:~bZv,v[p`. +?rSa
A?C5w k|Ib!ghXkH=4%F{'o?#'gV2dGi]g6l9.uc&n1&c/ kR5*\}|cVg}"}T`Z<el0&#/%L(`L'2~s|wOyyyU3/@"/bbmMs i@!=0kSvu31p<nz#!|_ybsA~Q'_kE-7~pAP(pUR	Iq`RY* _TQzad'!_gTCkyGY	O'YdVV="_g>W,ZpvM`;G:7yI%54W.>=B(w3)@"M=X2hE*^a
3*YI\ ^;M3o66iF.%httj8"bUcH1S
ul +Cbrg34h$ZUbAQs%P4Z*w	|go-JvIT*|'$ShfQX\nC4s>O1S
L;,4a~no3Zs1-J8WDZglIQ_:
jI	'!9>|e\{X# @Vz"hy
"]V2	j~5S%m2g?.ApKOe4:9"}68	J4"EXs$.JVlV<bV
SI-TI(e#.b2$uc(S->*c;B4e,z?5?U=boGY_Y{6AP(@;imToF_i\B3QT-l'!_fX_QHYg7MLlyo4&qGI&E%^Iu|( g2*A?'-
#)F!c1J80|e**I9UxZ"6VV= {${Q[~Vfax gLNn/~hX3)~O^CA<2*yTz;L?Q.4C    IDATlHd3`;5JX+?	JW bTg)nIEu?zF_*5%{<Q]f6)<.xe\,'D@<M<o10*mggiX|Jtfn&ZDn7?.IeO3KR$MP#	{'uBL	_i6bD?X0]`$}NW\w	wymk3"Ud-cOu}Ocze\rlN<ieQc>)fN RX]])4'v{@<7tV8g2Zf	HF< 66@uwU3DRX'%G{PBYDys)4w:rW"7~pxUH}sw1gt5p'ds?}Or@2?'Dy|S+\-&/79CDDOLqxK =]`o4}w=7xy	=:{_zAP(@;zgg"@k7wqH _KY2>;CwOI= ssJBVI:(L4R?xr>@e?jKe"*b_J%Six:fV%:b4N lCKhcr.$UOsw	<<y9w-7C7zXeV,mU2~'RG oivD{zpgU|Y#@q?c*m=
SpP*Jm?)? xsws>%g|-ql`5wnWOwS`yOs?~(o**2+JAk#43	c
h ri;bH`)l1eqk1.!1j*[d^OSW4B X&X TB+n3oN=K2gDBm=;f 6;~afk,_rxCg7kBpXTw ES1b8l;H.##.X3]wpy.Wr[>>Mg=VeEfDmpmPr$z2=(
BQXNR^, zKex
#4WQMb7Jp	pnV'.VWF	rqk?]H/,%Nc>C1_Uy*~JR1+is[{sem=<_+ x>1val*JNG{AuOY5yfuDY2u{ZU {$R;J^X+O`g`49;9Yy-c$0d,;; zZ$ rh'42_#dR9$*7
yW<{'4![X8fh|K5qb;p'=9BZV^%up*'~Op(;9Z1Xm0L\>xic>&(= uGKSP(
E?] (o.}K>O3<f[S8eX]wtV
DaSewM{x6l		3rL![5jP= lUhd3)x#!B!#@K+mR?&<K=Jhs7
*Y.c-=
zzzXHA?vcfC">P	2dHqf~E6x&S~ht KV+E#SX~fCMm.h]~Yu	G+79v* 	.v`n;%$M03I;O)~}T9+H	X;3d_gruWl?'7O}L<"o	5
un+v?$b}Joq)HDs+B;r,xo_Z>t"t3<{]5Bml@@{woIQZBP(*C);?O/&]'QOCRNwr=V^`P_F52 \<H?J^'^7q*R\A@we	F{JJ[~XxWJyWFp`lL<*CwW<)@Cs <J&f=}H[X=pm6Do@yFxe)kK2+

.~<fr{;MgAf}4993E-d\7~|
EmL-<_{$n]>r\X[l|@!
N}!cKZ7O9a~
f*HUxQM NQzHb,W;}Smw)fZ=1>=q.>g?i';XBf'Ug-Vq.bSYOdx*F,MlII(9-pB\*d\9fH_B"(6}A:~DIZQ>-BP(6X]][&u|Q?
BQcoC@i?
B9GkQ(
Go5(I|8V(
UVTwBqv4x;
C\3RY0
 
BPD=B)
BP(
BP(8V
BP(
BP(
BaB
BP(
BP(
BaB
BPl1qO-("4b`lKR;!.s|=.wx^(
BxP!qU$-]?!}*C
Ggo3xKy#!|-	#Noe*RA20r16gtPtO,JPn4_fH(ghMCrpA\&RFB\y/Rjm
sg*s.9Z!g'/ W]&L\+O[z2Cw5!]^'9Ty5,5_m-?nwz}34XU>?eGxa<m`s] 3d9jmcxObu7^i4[,BeW7&NsE<
\-;ctOyyP9mcBP(ct>85lw)X"J06@a"C+?#S,X{Ky_ 5Xy+=4%FIbA{qsT^fV2duyKz`}fS2\'9<f2Po6c;,K&%9akTOpQ3[MmEQ&#}>A?'=Y4^~?}W0&wT?9>b;Uc! Hu`/&44s 2zhy10cbP}W>-=Fd{N;Sv|,Ti_UBs6BP(e8V`t*^8Oy0|X[zMfw/r]tR03*vD5}#Nt 2_+%ir3+[h-8lm7*fsEs
WTRCs{ 4K4(l04h]7U~BphPQWTB}FGeB6RJ6_u)A
9ZgcM%4RTBJy}qR_kWyY&n@wFnOo2;Ch
z$T';dd*%QAW$CCcd
\
bR-Z/t.	5fZibA2YmX.cV]Q\m0)TZQA?]-	7d"RN|+	G@Lx8!D.j6vY8'XpX]TE9h#Cm;2bLhw 49i S' NAo_t
N:=}I7(
p_E$.j+
wscX\4EbCs9nIk~n	{
~:GQcBP(6e8VPG4z%j0B
_`]=xN-u;j=4r8zx&
EZ?$:E/_>ibg>+|I<(H30<p<c&F%<'-
e)F!c1J8(U{
QTyFE=EmQz6>Qw}V`,k
q(C`,Pd;HQ_P+#SOg807xc(|0mPW%,nLa\
t"6<6%4ntGoQ+}fa=:{d'W|f}7x>Ch'S?Jd("	FbO?Xk^<'(&gaO5&m~R,]`$wpxuZc}{VcVQ?MV"*>Iq9@NWGp79H9fci\|+wQt<Zb@!-$V[Y^s*vT3naZrF< 66@uwU3DRX'%G{PBYDys)4w:hoi9yt\-c.z}\sq(=}y-\dPe:F%}!GY_Kh_blY>O7{XTR(
BQ
pH;;\CR &X{&a6q@!SR4b	WIE(B
(^e~or"*b_R&Six:fV%Zhq5d-|GW?:zx?H^ 3<7`y0s=
6-
@&R}[O
]*,7O
-Q;+wU?<IsR7gYB+i8\ >B90pH	-gxNhF^C3HaH(lE/L=.^q	hlfGVcV	i=
N1W_9ktw`I>bpR	a2 K;aZ7.,mk

aP4?CtHD~r!SHIN\NEB'g'LqVLZanx_z$gp>HCbl_$b|:4~OAV(
bPc:IzxhAljY!^@BUu(*k=d&
{5J~Ch~e}6ljuf k)+opH(-{I.OD;rp2=fw9.Ye3Bh/7n\)eR=oVGty?/ShV/E+.=ygX/Uh2rvs[rHM{dx~$ |	+8]&I'
wLTeb*7
yW<{'4![X8fh|K5qb;p'=9"&BW TWNmFpy|;[&9e/r(
Be8VP|sfZ<cTn)K
LbR[[\e}/jQm
)MdYk-&:xAn7V`laDWIuc)tuY>T!+t9JC2kOJ}YA6]hO{(|+A `V8*`-l<A?iW-]!\d'2sel|7<6iR"B]}B1 SPf\QIues"/"iN.>) \>M7y@p>*+7M~6Sc]"T]6k;bqW$\+[ 0O;;Z2^iWhG2@%yw$]roygtN'Yxc
BQcv0}]'iAG=,CRNwr=V^`P_F52 \[uW%:
qrk1`KqY->cL5SR?{o}9rD: a%h\?"B k'elu}m|7a1F[80XP0UJajX)*y3CP3|< 536{f"aY1oo9^%Ym5~Rmv,AOSFe&e:*lT%i]O,Iye0M$f h)3(}$TQQ}mg|?gWTFE.
a/:vIk_[?3Q$'pch3c3lX$V'd	sAf}{^77<xS,!M9Tbf6>B!/GB^klO$Wi-.elmfZA>JZXh']oS_
$l8$e"v],aMn#7^xQUYzG%>$TfC]&&yAc1ILBd.\FNx B-V{$:M,GtK$D,\|/~^_k    IDATD"Ye'ne^7H$MJUZ;%D"Yeng<yRq,H$!!IQq+lS)cD";kJJ"H$E:%D!<W,lx
(k+H$D"H$?H$D"H$D"H$ds!D"H$D"H$D"1 D"H1ZG8{6:'fxqZ|M	=}H%D"H$kq,y9sGjmH:Hf*szRWx_GyA
	"CMLxtI/xwkwkvW;	vDub<+5zXF[U;-ps9q]$i*jQeYXA>n$'wC)t+lWE- ,<g{Z<S?=[wx	u2W$-
CH7"=AEQz]@:1\XJQmmIjr((C}unR(yi>h}_poC1:q6RqNsXL?_A7vLot
qsv9G\NEbt|z\|x)
s({f$y_}H|>nx}xLo#
:w_fEi^fXwqz{(+^oD"l:XqIDj6Y(OfIv;:#Cp|u)xnzQR@|N9i3D{P)CUMT)*Y$xU_e;8.?>H->sXcs@8qBNCVe2jr{"`!BC+JG7mn6oUS%8pB2eDWSWW/4Q:g/U3_fjF5@:\K;B':eOOxxnD8X=O	pvN#94t4IdtVGuF;?^aGHbw9U$H_	h)1pg&N<	}65#%a$8X#nqK"@3`\#c^yJS-'D"HdO.">\=i9nP[4u:s.Y uqZq(Tl<hyc(1]7(4QGbT
)4E#b	3l[qQ<zu+MW{I%g~O)i~@v6,;Ht3NRLPA4)v!;|MKF&{N3iD_'x[hdF60l;ZFL+(;SVR+ `/^K!`KSi:n-w+lbP->KQ(*Y1?:5e(VTOI
]5dmLr*=]PXhQMjS<MSPV	xC[]Raq*HnA_c$
W_`4STjwA|q\x.{1v&lsF{[`o% ^2	A+c;See g=[XiR|6T]Az	hCUG8 M4Xv:!'vYDO~8~PN1a2ffMCsVC_Ih	{$4uOts
[3qe`fo!r7\3}en-`dt\:!;qgi8|$
u
$d+#s;&f/9SIy~1`c&%D1Hd98_tpbIVMp:`YHEyGrEGx\8K?QOMVwJ$GU1wN87>Qgx<'BRA?= YCseB6	
U1o=rlP yx D2T!#O<EQm9'PK@~j'[@`-1x7@R?u`h'G01{iJm(c)ZnYqZ#ZUvE||-M*y
QU!~DQIN8G|?(oQA.G_uTm9DO@ >$}M(as
<K/Qf<	SD>n/MAm5*+,B8/ rDCU/MuzMRh=UQ3*Fj21z6!?'c*A
Tz'HS(n#Q#xVy{.JlQ'g]&rB=p#_,Sj;j&qjpq,Y1NS3b@DR
_#]UJ
U4!35e8@8334tr
QXX2TmoKIOs>"s:nQTuu4n[4 u^:
3?b*:a>o9U,4tECQcw]q$/TwO/$Wiy>I"<8PG:e(D" Mh4Os+{$?g>=#hmf 5Qm}kKy6RX:WR|vjzRHwtt].'\(]A]e})B#1<~08kU`lUP@ulq/\!%i<N]^^dse5pj^<gLsw2*#/EX-<R6zdJs}Js;# aEPbci'Y6Y6gT::WS]0}[$gX:kL&AXCX.lq}y}]7::I6jC Zmf+"=Jhg;+H0H5SS_nXJtT/XS+U1X>uQZ;+sU]_c9v,hg_gX*
MwDHkguXEp4h*/
vzY~NhVha}c
xZ0{k[(:i-XJbgNWjY43LMS7whvB!itQ`N#SS6:EAG
D{}VuH=}+~7y\|oUvO1vwFqJq%Dc&,IU{jeKua5`suex*FSZ m%"5We[*g f7v5eX/cN	-sO0/]c_:5uN Iq`G3s+K)^BqlIX4/WS"x	Fbv0X:6c'`c
ZjtWkw:AZTXLjn?2&VS8'<O~BQc$Zf$t>UGa!>)kmR"WUd/.*	MMYKPqJ'v4MjVWx!;	-iN>1Bq;6OU'fhY&(Oos.V %D8pZxTh80\Sr
5'z9DvZ?6pt?46UOs/aAl^x[fm9>7wbE.k5&GK}D"lxMJFjq;;!|#!u;85a$%"W+L#b5~Y\lc3lh!J~
{BB:.i^AaOAJ[&zi<{Lu{q$ygI_SIiCVr1F)iy^pOT0/B"#Tk4^N/@SFNw1s;KhK4mF+`]I{m|Sx\~|{M>(:vjn7BLD}A7
/NGJl6^lV*n3}mjL]&ADCUj,_]ZDbkR5ci>.P
nN-Ztvw8op.g8$M9='S
]%n,Sy;k{[/\
b	|u#m.krHNcD"1!M.;'y^@hKou08SDKcmL	.VDQxD\){v_[%yM|r=`t?$h(RZHLB!,a1&mX&l+u"J~m9|&ACu&"fA(sA0&DX(|6,)fM@.,',7qx-f~'6.[Cjz{iqRSR]E27im7w%^(GtmfF("I
f^"V'3PFUS-'^Qd(1>e(6lTa&[$".~>mu9XUxQ
Es@=,VT'pLRj,M4@m79ex6)hUb2W X=qh^M;85a.}>[	Sq#8.'b83zY{ [FN`xXy;Km\u8Dn^GMMV]NC50&q27H$K+,5~IgYH$X 8/42D"H*D'e<aD"yusWBN_JD"HRMT$kN1%K)"nE/~a7:D""/Rq,H$D"H$D"\lVO"H$D"H$D"H$X"H$D"H$D"H$X"H$:sHO/PY1b l,(#c\HJ$D"$/ 'qJw- 9"aJkgl^	6$
41]Qty\;pnnqNj'.hNg"vhmS,T+o-9A(PNwUjb{	v9H54UP%O8YXg]UB|7rnWx7./
jg{bj*u&r^>m@CG-=8czQW$Y}uJ7fY16;4%7>Ep/i"rc
}YeqKG?OEulg->tc9G\S7O|`K$D"D*%P
*nt0y{!q>D=DF\p<	nzQCRCObMyXJS<a?.DC'H]mE*z'vav
?}<%O\xl:V'N=dp\Qy!B&6.J+t'_I"}'98l7x_8x-wxfv/Q_dPgIOAQ;X`8 }]JZL8<< x<n$z:T1 gOSD"H8lb4+BbP9NqX`n9f_yU(U1&l<*P<bi8[9zd+FY9~D]vR>am+\IW5wG+M*8{NYd4;vPlpLXl8kwA|q;S)Iuyi: myM+qEQ)<	e=J`w@p9HK^5PUuh.i+wzVvDJ<NP~-[aZJyYm3@AdoJVITP'yEP6fM`FAoP(Z/p.	5f
]w.'V 4;ez$NPf=0Rac56Q$`PW5>DQc/2@&0h=xhCU?8 [qNgK=0UJLd^1#26miON
4a#O h	eP:m(S+!r\}	oM)^ty{$D"q,hNr'XW?&P'XVq?$17CQvt^C;W<n3A(_P_`>~Bh::H6E_![cuq!o|gOdRA?= $;f]&w^|2]{P\oC,SpFN<B,CUU^LR,)BZYv8sA]A] ?"G/Yi6(dZ'2#Ea/USE
okJ,0I/p$'Vo&]|sJ8VPpf9K:/cjFOW:?TJkl!n+pd?[|0eDayUiQV6{#&h    IDATT~7LO6 &#L?SCq.$DL1	r8b<7}]3kH'dZstNr@@Wl;i8v*KDZ1X[:3s\_iGz=Wj8SjZ5j(x2^\c.m:.g06Mn2bi.u,t38k}t=kVC5f,pho{	embLfr/IW6\ 3MJ}pL%]8a"#,
Eq$JS[
@YLM%q8\8.tPoW~l{0:I]}z JkgCm7
 H$~fx3DTtK6!Zhi:VmcHxIH}zG( qwBjQN[Nwzm^1RtL!1zq<z7w&K	JW`W e)!Y6]4g@EuT9'xpR@"88&X[[elfptN`UIQ`lUP@Uq/lg]+}Ys>%|-$Q[z>GRGn3?Ic+eUe`d9pLE8UOpi4a:ya
2	&aQf-<-sOO=z1l*d^N/@+s:kiz`N>dATv.1Nq%^j+"n*
X}nhp%>C]H1vwFqYLoskVNZh[QDQx;YaH$_.|g]m+tK6!gI#W+2Xf.f\]C~ m%TzDHUb^!x/?w{P&}gt*&msN8t4PV9JR~jzE(zEZpU(-]ymPQ3g f76(|NK]7{eOY5qZuDg$5ooxFLZm5^MJ1X+O``489u##2TeeNQ~Z$ on+!rp+u/
ML
i5>l&qSw	G7}~&<v'tj;kM8]x'{,4{h6HzAiWr=i&F?PicsE$8-M$DR^y%og}^*EIH-ngGYbDX^Rt_{nwXL=Wm.Be+Xu={c_Wz
qzn*b"X[3V6'b	E6|O%KY]*ey6mRny8cvrkFOU7B(p:_v'>U`)ykHfJ"<X(mnR4<z7$xgViD+ ^$S5+'Q/kB\?oc(l}~=DNG'R=3PYXU#k@f6mW#Ic(U/@1=Z7>L= bWn^5})~~Gjc\+'{F6v$DR{m
}x?qpHdkIgwZ [5k/g!wD}+ocMur$#R
Js#|(,+o:4T8-).Fw:U^
lw)-nm&!6UB~3<c&q[%q^.hy(P5s_$hobW=N%*:\'It5}ls<<A=J2*
u!o{QS\6I}Z9
|"f?8CC;&GX\a"i:W&K(5VUEnJ|-dlqV(#QPk)9/o>+ek04
QZB
KNIpVI:Z1S"lxD h_6W,yL~"nc1Hr|Qcx	qH:![	dmjm];]sjO.$F/E:D.V{$DR?n<{@,2#VHgl?,H$*>q3%/o"`D"l$bl~=rDco~[oX",uJ_TyRq,H$!6.GB>qmj4%sxH$!J$dm?Ok94m?<of|'"HTK$D"H$D"H*ed
x"fU!H$D"H$D"|Y'Oky~,R:%/q,H$D"H$D"LAw?-S< D"H8G~z	NpdFdcqEi
FBU"H$h~,< 79f6Ir_u[$G_<"=Wi'u}' 84+H*HyG|LF|7f'}`L_'3AQ;Rc!! Yeb~Mw$eM3
,xNGtmr8k8i0!YQZEIDSSm ^&]=Wio? ]^{\X<#:UV6$+(?T;TnhczQ*2zw1*[ X( Vxur4hf[+;]x/"a.}>[Inx>!4^g->tc9G\S7O
<iTXwsoSc(;<UDOxo8,]88lzCQZNnt6$Dj6Y(OfIv;~Hk(
Q<, +(NeF|7u$}\jgQzMTJ	qaeeW(y^L*?8opwpwC>\W;gIwpWy3N'28f]_bvoG\Pu{yvwl:o$i>>!)98"!Q=>T6!Um^8x-wxfv/Q_dPgIOAQ;X`8 .\
nKsi~<7@F=sv7=9dD*HRq_@i/ Q$_	9^3??=RL*%8lb4+BbP9NqX+-B%5^,Kx?*Be^G7muy+T>1Zl 2>+kn<W'maZGU4=.9WG+M*8{!>)`gEwbbl/)>DWAYN]]m8tJ<qin;!9,SymGznW:5CCHqGuok M`,J0+c4u5T{+.jka)xik
^#61E_`KV#R^VP?Y[sUBe`^*Mdmd)p*r}QW5e1yB"b30YF)|Td;AKbPjJ8WDZAQ_:*[r	xxg }'6B*FFzY8'hS%&u2@?vN'dXzhT,e"~t6=Q7C&cq/?|9L"AbA~;
N}lT0g4K|>f6thc_=w{v#2RdLO=NCxoH$^	3E6=q,^t'XZ?&P'XVq?$17CQvt^C;Wm3A(/(;|8nupWu)=UmBR8w9CGs8{"$"U]?1vy(p&A%{e4e"Xrx\pxb9Gc6B(pa<"fBbbH:L3!lfdI"9Vx\<7[y"oN>iY]L*Z=s#`dP8dj(c*A\nY"xzZ7MT]J}QP.P3zD%yJUZ;?
w%`yp,^#Hw>C({9>+2*pjfj	dVcjH?!%)&A|/:vB>b#ci9]A\:^fsqUfcV!uf,tX{q:6'FjQa/.{16VOCdx
`&7x4W:t38k}]q3A8Hl_7NubMY"Tp5lx^ai(#Qjj 2M`j*L2+bmP qrgJ8{NB<{Ug S09.}S\4&ORop(YW;I$u3RGyzG9oh2&$ds~9d?eYQ#MZrpj=!BvFj5C]Z[Cj;	y\!X OB"z>y840
I~Bz$*;!6'ag;e`qMWab<^80ox=K|xmZf <4)00+R>?oE}U%4Us8b<Ql	qHhOm7c3h`9QPv,w9cH/6YVGnw,WJ]H'#<U!mI#`[	%8w	Btdyqi<
U/5$8I
KNrGy+Bzbf.HKFMlqcU1YN/@+s:kizZ`N>dArc.1N/^jBs,'{Jz>7
,Pv>7LCe8g6^_ecz%<n2GOuN&%bUDOM~fjJ;dE'H{,H^X[
V<n~_I8lBTUYx!+#W5b;Jbn6*R;i>0,.w>3&[h$u^29l_pU(-]y7+y7K)b%[^Pr:7eOY5y:M|7wY.KryU".5^MJ1X{a`489u##>I9 on+!rp+u/
ML
i5SeQP*7uyz{Gb[iOh1'8&](njl-OMYC4$NICsE{u7\nBf0Dl@3i'QlO8'Z	BH$gf_?60oCn'Dxgts/MMuK=qA=,&m
Rm.B>E,Xu={c_Wp
qMl
BDIg2Ve[fRi hFji		N&>&i"B2\*s;K5_[`v6%4,bNs/\,@8'>P8Bd8N*Z~Hn5$OwXjI/z;BZ+E\n`}A7M64I#B4,7l^a!kVN^8~x-u'9[Q1j53F{$"ilJN@uLgJf6mW#Ir
__~T{$A*jz;RKLetb;]"SftV#QsM~xDUY\-{5@>q#:@L ii,H^uFA(?RdK61:;'y>h#Q#[5k/g!wD}+ocMur$#R
JS]([.!>+XR\t?$h(RZLB!,a1&m'{l+u"J~m)z&ADS1WE8qy,C77Ck hB]b d)d9%i]Z:=gE|g2	"f.7;lb]emja*<~aiB]H/^uFi-Q6}(CLI4	flgQC7II2T;,oyU1EG|_N(slqV(#QPk)9z**,Vs"}V')"h6AX]L;X.7g/*~^5SHXA 9}*B,k8g~4n6<'??7c9^ub`y%rje5<_&xwB7z	MGWH$'QO&    IDAT&w-
~2jWeY>k7:?"| H$TQD"*xD_|[}?[lr/R*%D1R#)J>qmj4%sxH$9|iMco&VusJD"8^U9;5~I8~cD"H$D"H$zG%x7^	9^cu?n6Xn'H$D"H$D"H+[^5|G:%D"H$D"H$

|RWUcvo9$/9q,H$d9s$Jxd		NltN6P1potN^.]%D"y7?<dK) 79f6Ir_u[$hc3aJkg=+]<< _&_f<\AUDCG<c55;A`:~Ib~M2K"wN[ovIN
Hkspa+8^mVp/.~JgB\?cxZY-=A=Wivcy0Gu_Ic;QXDxANfX^>?eGxYVss J36@&Gh%x M=Znq|z]5za.}>{g/};qnxX7NIxO}%DRN3Y~2^yk(	</Rq,8lHVlvP,0
!B!&6\G}4%9A=Dh,}z"A=D|+T}]
 ^4j.l&ZJSf?(~g=D$L1x >z5myNc(1'U:Zopwpw$;	za^'c\oj pC'@,"n
hskn=.MM5$-GyAsCnf|a(x(:^T?9ic]qo?A*z~:8)8YSPv\p_UTA7yx8Op.g@qVvVcD")}NWp//.[HdH]*zeqZG3uPm5tFi1a3PW8My]
`}g#[1*4u#b	3l[qTiWOP;>^iRIt:$1*)`gE@BC)FF[m=tB6~Of6Cw 473	i.J# #pPY(0l;Zt4,|BEid wbRp/^g4M]M'UJnZXJ>,8^mbyz7WmeH4`typEQRH6OVdDuPW
e`Ylvd

\aT(VU[P,\j)\@O/d2GEYTX.z1XkMsET5:	UcMphmwT'x*Ip/a
w	lxQ8'hS%&u2@?vPvYo6#c@96='$o5kD~)oq
&ek6VhzgA"H$+jd?<o_JtK6):Iw-	'T	k\ItPWq$PNx.y1e"MWVnCq;'y3!x{8@3	c	.L
'yq.7!s)8f#{!b**/{n)mS,;  .;7Lp5(`#
N&
mAfj7TetX<TJ<ceako =:wQFKU4%G)Tax<gKOx_hM"%q^1fg)at[A#
xb\o*kipRmnNjw$CDUz!o#,[x[g87Ge<"'?DGgq`2;*2*pjfj	(VcjH?!VFH.!jxG:#
>-ih,`]jj[ZV4fa+gi5<~T1m3VXxQVh5OaQ5SyH(e;W.=v)`V^s%so{=ssLK9N$wq$m60w:/zK52Rn&01`Z>p9<"|
i5*2q0R'8v~F"~ x=r-B` i
pz]=vnLi9T!f5z]^fuW,v3*RE^,
Av^6K/0

B6|d1x|dal{
b5Qb"BKV>PBG)^Ty0|i%]:m:^$pg[I1"|i>#oAvHxy7rE:[-~-c.bs}F&x0`UBA8vZq4}c028eutM?k/a>vSQ>[|Zu|h]R?m&r
>`C:Jrs:vt)X~l:olSkV@FnttR
hW?f~KE"v{+;UNFIOQbpO29M+5IW\ qo),D)Nb{lTP}Yd^-B4m{sYsA0')2'9X*'W^q cg_6#(!,!Co~]oQ4.
Fk KgC>5}V[xKqt|({H/GP(~l6&9Q[w%?(X	9ED^^o&byi]JPQ@^L!kNI?
Y\zDH3\Am!xU;-] >>&xx+*<[C\1et#aIOQ[el22w!c& C2#88c|q`Sq\n05+el4NS]=~?^~''8xSM{Y{z[fzb*dWlirso?9X	s_=}VI,(SD$ _ r:'_<MB9[xs:;vd^Z#Yi BRT6II[okz"{BP(6Om68K: o~o*8b{]<7}=byXd)B"\VBs	_>eX^+'lK9$C,c/V*sBDI-F`U*w17^@NE

r7g\ _DH?2s}8r:.wZg,	h,X4`91=K)Guypi^JLiyODc$BERk??
	'N>kr6on\fL}/9[oX8mFk/ Ub1-'&x28
6kwm}!_x?6;9)Fl7
0"DEIMQ9 Mmm O]r,/|SE7!)4pZQEZQ
}sgKv5wC4'-BPlOmm%L?8-l\r+61fnH ;E~,O\y
B}u8/m-L	 ;:qpE \M7J#"rq;NP^yM)te/4,
qs:YPC2BjI.gQy2_MEO98}29>::ujy#@_bx( E1.b)^"|r! e"/9-ep)
4`SoN^&:S}oq<<$Au2:
u!>o{Q<N}-TrRdffiq3`+;3I35x?w
o
"1E]p/s	bf2o`iaG87~[KSd$U}Vgh42QcaYS
P#pjbubPeWHu'tYqa67HWn ,N+C"$X9 	dum;w1>yG$beg+BPly)h[)rn>ObKR'Eyeqqa<{(2BXe'nI T(D"OV{P(~_-mf|o><[RcBPl	g`qA?q]lS9aBP
u]xTP(Ow~?(8UyoGq
BP(
BP(L?rzn
-[lOo`6/qS(
BP(
BP(tg>E>as%+
BP(
BP(UOltsO}LJ8V(
B98Ng)P}9XFBJ?`(
g6$9u**1A/EH,V6y]Ta?azj>s<"R'\xWjW|f'lw#$KWD<~DG|2UAdp',2]<*nw|Lc*)#_]q}aL{ _@%w?e[rKFQ({U9]-B{h~OjuB\r(yN.f*E6JktE
$GJ1 Yq<kJ]aU9d*;>08\lu6E>;WIxU@-8}0k@;~:MK>e
'Os|_|9C7qm6"%+6=Dxrsafg\xqBtbgc Ad^^sC|l)<8j_#.1`R4nb`=L_}kkH'?#OP4-{z
|vNN=f
{2}A`J6@gHs8JrM|'F
V
3
qC#l:R|
o6Pd.vf tpHjeqbS wr* ay8xV<~rsxY\o	n:k%)T)G>SOM7z[D	
bs4K8Mb~~mac&Fs--+{P?xt1faRp,+x3*nfC3OBkv<,<%RYd<W=P^/*m#<zw+tcF8{CF]2me9,(4dH#5BE&Fvq$& I\dF&/}{PNNQM]p	p5eqgL}pe"_U 9nJJ`9r;e{gs7+P^S
HnD+exr_6~|G"%0G" }",<Tg`RLBP
qle6(8mhp931KP%Gn_4:$c#I>*;A
`z1dZ?Z'@wi2L6!etTQS?6j^aq_	|W s_){2)k,OE}B`{CQn6(]fQ K~F;a#!H:^7eXz\e6Nw=M7dh,%IqYuV"lH;7W<^$S$e4d8OeXi}irnv==M?|c|7C;`>6GpucB)3m7K<P/SQWO+x3c&E8)YEB>6<>K#s8^(<Dl<YCGpOPh,SRJ9c|}[c
kmo>-@$"RWy[=c' )b$)BJhL
<T>RcH>`!ac`aO3nSEniOJ_aXj8Me~ RK__dNl4(/f`:kD]{lklFr3*!,\EEYF.CUX!`t]m^pXW+\1Ob-|9^D?S GxdR[

S
0q&LCeH}\>$txjKWlx`F8)s7G'zzxe6},K,i}m&u~G`+     IDATZ>p9<"|
i[VHr3xhl ps]*	/pU:9^~Vo;`Olsce}Me]vD;IG*pai}>&vaNU w||8 VBP4uCCBpmv4]9z[{	K `Hnjgej NM(b5eQ[YD7^:Vd	#<)~*g9I#Lu"V~#oE
Y\D/6%!?8%rq;>vNel'W[Tfo*M%q*~|Vn!=@	MndT4GJ{7D?8,p0kZ[Atol+VeH7@H{m?K|xLVGK4i~g{>TjcUL!Nis%s]zLBfR$\@G]D&~{.|-]x
e<k/a>vSQ>S.oCoB^3 T |6@tj-K@lfi,Y!>-,	~6E!
	v:< @
QbpO29M+5IW|A4qo),D)F5eige(0&WbP#|+'Dg-M^\f0')2N+.`vcWNaZmG%"
XCsxr1v
2_{=HXn|PC COCMAd\
'niuR!
wo/{mYS8%<
69%kd`kTS-eD?Ab{|j_"o.,9SnCG	; ;|)#oA9/l_
Bh>?RB?/f(SMSDu` Y!FukW}K8s!Sp=c^])m'JufOKSlIp"$8+4NfW{mBh XH's?Em"AB@2kb
rx,JNU3'f9SRXON7fV
)=FIgzj`]`*eW=|`-7h9~#gTQ~*2 tafI{wjezeh)L8>rk/_cVaQg^vatpsVW59]x{/M]h(7
O$8N4cmI`aq:f@^:Vbm?Ed-8j`j>m"PhmP(w72vxV97
J8VlR};Gs, SWjN"K/Z<9e='AkV:pJ~ [&{$iJkNw6
A`tVu9-WYJK#Nz4b"Zux2P/Z py^r QQ9I4G}ciU|Qf
1%MxU0F-AB\7.^HZM"q~sN/{!c\)*HD/HxV;c|;`!(zu-(QXw/i)EpY6*:}'9-hT5{OS}p'|}3g.p.|7dU3mQZ#Fr
D*ncAju_ai4C4	1'SSDL!m`vy/ '|k7\H	)~vpC3O1 &19}^Rg_*'	5nkZ;Ku0M,a]^IBX/mV</#H6<%+61nSDogO"Z MjD_gg8N{#t4-6AG8(V)&CidS2i(nDBw_iIYD's!,t |Cr1<]5oo3{MG|e0=5IZ^OF#_;<Z;?$	8iaK&8y YS7(oYJ0v(:g"CN`" |&!zlez'{4=wf kvqSxW#ov1^|R1k3%f6/:?mZ/1>Omzxr{nYa[[Fi>Z1 56.Vw
Lr.`G+DX_XQP]hq-DcCR=|@
E}n/rHD#;31a;C3:<N[U.<'b+[' 
o!tZ.Rw A,,PVD	8^f4q,r3]T%" 'BSI8^iU;)B)XPlvl?_Bxoy<p,..>!|E~P(T<	P
Bup?@R.EqV
"~w5l[X^vUI?+8V(
p{$?Q)	3
b3qfVq MgE6|<ziofBP(gBR<~K?x3qP(
BP(
Bxpoq"M+
BP(
BP(
-ye{N/QpuEQEn(XP(
BP(
BPl8KWmKlUcBP(1p	Oia?.~|Q^potN,T*
BXMV__
=l[<TQ)CRZ$l`j)H%+^lnhmOX&Kx3#,"}{(wG{kvvo}	>Lr:tyyv'::>mKO\s0A[{ZyN_;>jR1YeqCk'l1mNbr<YxO#txw]y_vE,iug\X<#r*D$xiP\uAUk,m'769"?Yq<kJ]aU9d*>C_/X8k`!t^f?qCNv<nYq;
}}BP<<!K%7K|70o?%p&:3:)s9T}qH|G/9)(_d1gvs]H~
}>3mchUd*F{	f;8KI!3c!N .M7|XO!w^a^)"a~Oj yRvR^mAs8JrM|'FDT:,~b";~jK:aLYm! BG	4!,g'v 8Mf-p&o/]=<xV<~rsxY\oj,B(TCiMO{=,l5;IP<<uslN<Wp|~?^@	MZZ$zQ+{P?xt1faRp,+x3*nfC3O/w!>N:J}g)F\	CylcFt=.|>4WF##qp&w
Gk >veP9&(b.'Cc07A|eF<vZ\QNNQM]pj8ijZj?(P4^))LCyM) }Y<Nq	|f
n|fIqO:bjc+Dh;EP)a_n(Q8t:dc#I>*;A
`z1dZ?Z'@wi2L6$m	&h13XUA:WaXsiYaS &dR;mYJ~vl%lD!Jpv]>
8:G2la@5GH6z#|q"@fH":Zf#tt!tB	dz1cMt:s.[%Zg&k*/5k0?jpdcSwKqxP(HJ-:K?nQZc&EwN]-"TQVo\	qv6_B9l3!g?>72:*6M|*"GDFd/Rwn6/|\{mD}m"bQ};H w]H	
U18@:oxH}0s&]yXPZNFDH
p 6;Z[ )@{}X(mC~W65Bn
5#>vvSFucKx_q1|.O5(a"a}<k|@$YJl]~OwC2lD}sVcb7[Tks8?c^D$k`dR[

S
0q&LCeH}\>%txj'a?WDvm\jvRs]=~\={<s
L>}W [%Cof}V6^.yk|6}sxDFi;k0ls]*	/pU:9^~Vo;`Olsce}Me]vD;IG*pai_O]kzU",gg8%aNJ]?HS&BKM\i[g,5U2iajAZsa;Tq{{a*p+Oq`<q5zm3U9:@0QCd3 (ziZ~CYoX{-vz`{k
B9>}=p7,wO>J8VlB[hJ4GJ3|\,p0kZ[Atl+4no"-'R|@
%>BiBs1m~oRx*Xm)"AIta.lFtaig'/~KM
0FJV#23}[0k/a>vSQ>[|Z@L@S4It6l)k-o}Bru9c@4XFbrttR
hW?f~KE"v{X!NUdRQbpO29M+5IW|A4qoC9-!wS/!j*(v>Q`L
,+Epmp.kA ?Q$D|0i5^iW;-iE?3S9t,e}c02qegW.b7y({qmPC!f)u8Y 4"sB!!~nb6'2.D<'_gk`Gx| 7.Bq3s7\rP8oQj),!^m57)/!2$z,Lre:J8$=TdBD#?\~<-cs<=(X	9ED^~ Y!FukW}K8s!Sp=c^An8E&9fD[`>eJ;]y;v^t:SV!B$ndM+c& k"<6'	wv88c27aj~WrB1
jN17eg1,b9p#EzY4kF>+gZOQOfh:F8pN >?oL!f9[x2FrsmOJ7qx:+a_tpdxY@ "spGwQ"j3 =8j49#Z9A(
[p^,_?Xb%Omt
s}CK;98Y]HS/GpA=,EH,_RkR"?r^QeMlH!2fT7\7h!iN@AnTB\)B,J"oALK.
4y7_J=KL{BK0qhSrtUgs-X&x
kblEc\c\  TVQm5!'4Kf9`mY=[p6go6P<L)W'Y6do"N46k-L/hW^/
7'6'.C9XsAU@GQYIb47.|!Yj)[q(e4N$BTSl!d3fxf}^j,fb)DkBd/%wdrjpC{/KqtS9l}dLqC>z\X 
OmVrie+lSlbt^=AvIY,G)~9Z 8;q"_Cv    IDATGr[nANwt[rl)i4^)"irJ&a0YNeAtQ;"aN'2jg8^>$Yj"?`H67*e.|t0oMoN=8~6M-C~!,5Q)~CquJ0FJ;3!LH:)qgtmc+;3I35Nm^At{_vJ$-x7DC9Mq<9JyfGrgh4\?\?F&"X^337PkoX \pg[I]SIw,8m:C'~kY[n8AkS/8-	7H90t .gXj	[
#veKdN85zX)N
ku~e>^{k_,;9J'G;
P()|	%Jy_/`2S+rcx<Qd~7:
BOL@=/P(MM%Q
b!'Y?{_=oe:FXy7W>1:XP(C2$Fs?Bxi55,[~X2
BQlo;:e%+>|_[0E9#XP(
BP(
B<8<7q ]V&o6X-P(
BP(
BP(;|	O7-%<V
(XP(
BP(
BP<,m?fvx\~u
BP3pSYx>O{FdOvl,{sdUP(
Fe3O[oy!pz)ZEgqu6rH\l<"mo z=gOce/\`l8tF 	>&]&3E9TX<K!7oAQ5y4:-HOc	:ikb'|5SAkqv2H_6::dR+8adyF%?$.+mo!Hug\X<#r*d=>0,br_^div=I9>Nwb 3$@"yG#=7~]+[~R}]hZ%[LYe._bu~VXqaC.HgvV&iO{BP(6?>eG .+(Xg'cMt6+f7u8U9{)42u^i}oy
oG1G~FG`(+aj`!e4<txi/XS1H0_@/>_;wS~=8b=b=]>^xyQxYwV;tGn3= H)Bc\L)yn{GIbz{O$(HJ_ OsC cMof/
6Pt;J>rB iX?=Y1,Nl
,}9Z]osS^dzwcf}Wn[<GB=ksa48uW:p	5akH'F]5T(
F|}Nn}P\Qb^G@cn&	"7q*l<:!afo3:RS(@s%cc5;1{6:SwM;iA8p8?I4>veP9&(Wj]!pAC9WamytlG5eqg^Ueb[RvVVQ
25gg!0TtTSW+|BpBBx[ G]82Fg45-5);eywSR)3=YR@w#z_);y.8*).)i=. )WLu&T!Vf#-vk=Sd'Q8pf gZkUvbgekI~.HOG}X6KCex-mI"LHcg07A@.<UY|9
`kc\>brO&x=](;`cl/yv>l*:uo6WQB#6~e(g8+n&kKfGn1e%%+
C}iOsXwYY1*qn(XINth{%u:[Q"Mx/
p7l!n=qvyQf\mS)9 2"-Rv{wCy/w%lTs|tg	@IqJ/:f:u2'BUqa8#58::R3a'u*T	<4m!wu&]I1L.=2hrjoFtAucS8	*5M%l]}8{k7QGM@+!JniT}!e.>
[?S0,YRe[Tt|/hD=TuFa%we|~=,]!6Z!|m{"jDVKUdR[

S
0q&LCjN|}\>{X 2bF.^=(20pRq
	+Iio8N$wq$m60!pY\;"O%Cof}V6^.yk|6}sxDFi;ku0ldb(3%^k-=lE!fc 4p;7]&\ewJ3Dp
6X&*7Sanj^p 9N~l
lX gB(,"|*zJ`alt Z3B^ksptP(
E~D~/|}oV"%*%&DN3|;K=E,!SK!nt9tn"{?J`U.))RbyE} O@!S4~GK4i~g{>TjcU4%o)J;i1dc_tv&p=	a]mis27X8	.&g
5ewC[i->- .k]'0h4D=6l(%[_:/ j\3)Lm,&}
IiIg)%NwecTA,bln,lTux;O&?!<J2i,yND/s[BE/;_BTP}Yd^-B4m{sYs-0')2'9X C+.`vckV=sk2CQ@rPv/]n7.az]{v"~wy	7@/qVCMa$>r8
Y)r}fQ^P(/[^w({B~p#`OS,C<ijA~v)}^Ey]#291:'4dqU!\H\ma k593v:P^kfMNS5}]o(Nt4NvQuvR
XIOQ[el2216@Yh 3r0h'?\8V?9!Z5DEw'wu.oXleKc>{J`4{P?zn	A+up~*2o?-| TeXVu"'_|A9[xs:;v.jMJ.v(3{imB+DifVg%/oXn+f{8+ks:2mmf|>C)BP<W\{h]~^~|7OS wvBVR9fo7`mku"Kn?}%(lDB"ay1w^Y'\_n7AU"8Zn.D8`0|TXdMOiO$B1D?J!]~G
'47\7h!T%"V?f=;XNLGRQ]n7\">e)Ff&Tu@26I_'|f
D`9vbGkH77XA3m h1.^ <|MnFzS
6kwllxccRKOmvc  2MJo_2oolkx2eY~s4q}^\Z^2s|5CS,P?W'M?90, CiJ]q->zG?{vjZW9SBPn6+KsS[x!4/?=(bJ1SDogO5G8NMj/ W<":t rswZe+zS^)"1

0YNeAtQ;"aN'2jg8^>$d 1	S19Z;?p2rytIN==rb~A.de.|ZX0|[3
6-gd;nwwTFG.'
a/o*ZM+Nis&2$J>{H	a#,D[!vO,x?w
o
b->.w;P*}m&l!qopQK7'@)mT!EAXg2D(\oZ]Aj{?28b
1O80tZo+AB>p>	]Z+A{	{ec@$@YeJdr!
5"{uBja<B3O9hZ-8ov?FG?,(
*>q3O/8`BP<I*
Bxz#?+8V(
pT!865f<(
E:^7fQB9pxdO"yBP(G1B9
BP(
BP('8V)
BP(
BP(

BP(
BP(
Ba@	
BP(c8,Eu9Y=<~]x;6:'8<YrU(
Bx2Q)CEH,VZy3aiD}s-
<~+~q'J` b j&(
2A%+r# N`*pV,J`6X[5d#+17s&CAx4@ Cz}3RJ%{~~}}^S3HWEs[(WG}sj7k.:4%'eFpLK0yQ-us'TWZZ{Je<b_KjRc*uMY(_zHxx<pFxwumm>G,(bQ2G_w'kB$aj[z(yN.f]}rtI~4QCeb9]e
 Yy<7hvE[zTw)}Zd/Z!U4{ez]nq?v3i9gqi#0--78~f]|AHp,zlG!DPb)p.CN!U4FNloE3S3v	`mhiC1lv,F[	&+A3?A}&N
$SaSS2'zmI^ @< 
	&A..7Zdh t~Jl7Fq_.oP;wa_x?g{5O^6 C Vg'k6 f-4=8	>MWO{\_ZNgI5b%:y]Ti&>\0zLiNu%|A/7DGn^D5vO}XoG\kJQ~y_D8V1kiY%0+[8p 1-dm~3K+jMmdFVCRyFosvo3;R)}v@1I1~}dnM<7{x:HEu#y
.,pmE=6-k)#c 5TW4xawVQyu3/wNI?TSVjhWn>Pe4_jZqCv-n"Rgz673?k1R70`;<r5ka\oKy^{f
6W93;bjSV^F<Dav(]fbLiLmrcFb^ng<J3(Uo8ciQ@Uf.Y%YZkRGr-1Q_;6muT&pv-D(C0`
o[Mm9DhY
(2~NaM%0IsO&x=]e=4WY,Umkn_}-BM;tfj8B"o5=By(USv^tSs $GA&eM&{*JNq=/6'8p}8n?xmnx,BQ}#k[SF%/g7w{'    IDAT.D8V)Ky::u{HP~ExPv[dQTPTu/nP.r]|awR{wBX
emptDTi>Cl6"&AR2:]i^Ua>RcHa6uofO00Sfc>Mr pp0U%)"@fjfw9L"NE'|"YmhLU_-2Hz~/#>S=M	Q"<A
C2B*%\K]~.C28Gcwp.U{gsk\h?y+btF*{2qR[k`Dvj.}{p=vinmQk/r;L=MjIio 6'y]wtZ:mY`<\hJfvME:9u~]=ph}sxPK;VHogyA"k6;X(eIS5NxLR7~3 g4cK[N;^D"E8bYUF9z-y5:G(u]@Wo+TfgKAB47y?s2QO3?~{#v;ieheUV?ND{Z3Apl%g#W!os?Xhn{Vg:<T8Bi[4ko} u%ri^qwC!kDHR{NgH9/yOorm#%<	w

MGs`&Ck[hQW?I7hj^sO{nk74Q&+?kNu
,>uuOyLe-|WO^@LuCthXUe@OWs&XXK?L!Hru2g3	_6%i,O'>-\K(xn*; 1Kw^
p'Cr6%M6cs $u Mxj. %Cm(r,[B%Omf
U6hT[G`YUpcr.A 
l,PATE'3si83!sJDsugwwYZ{-m#87AC%Tr|o]rW/"7p`eS,T'\'57x/2 BUunbbU.*GTVv~*$9%i\!4ee>qnE%c,.S]*$3x!ma`D 8)G\%er&)4.|tTP	&/cj\S\Q{1><08Nr+0rjauQc8z^8nQas~XiH*kk"=>gS-MTXYF RW-mRO@@:'5'b~~\KLK9i48|n']=!}ve8<UKCbmKs-HWrUELpT-fQ{!.<s-E_uQR> {1"kWo`:@;;GE0 /I$6X%&{b}VRivGkS^pt,A}|v*:eXQH
fk\6XzGeUH!*`% oL6;s}WM73!;)c-mbKFQn[M7B^HF0)w]D8q'@4.\
bH,4~tZ4g:r2WS?3@U55v>|m{N^2
lRkmP7gl/4f]f=0I1c~F#P
q=~DtcZG.era(Q
PuG#$kp`QZ_3Sq_cr^Aq\^TnZ!#F<q7nf_9dc)h, ~q,b=AvlIY,G.f~yR+qAFp&0*(69Dojeq"g7bjs'D^Ssf;+/0M=)r	89ed4bq|H.bqm%.FsXyC7SL
__~<HD5mhr
$]jM/(q] `c7ByKF!!9k*Rm#4Efz<9M0sgG85.dF'HsJfgKy16*m>%J6n;!8#_PEZ/5?OhlKnm>+,LF1&j,]*jUJkQXZ1.
P}Xh/
h]epci{1f/&g8P)8 S%H(QHst0Dk`""!M3H)>-n}9+)XckbXYOD
 ? ?,1;;x<?=2JA,OLA /) P` 8w=_[O_V{.8m>e}9~q, 	g`qA	8.v =}^W_3/w
A>lbuO>O'/$E?e/)u 6go"+Pd>ho]<B?.-I  "X-n%fT;+]bs|DA>Lk	 ]4>$cSN7
lek`7+]"    +/EQKj+!=x/ p_y?
!W	YG?qz ~/6\OnM=_ocLluFXAAAAP9bQd7j5kJsRDIXX%~sQ8kzx/C_D
8/)yhHS{bJS	?Ho+/V^jDcAAHQk|AW8)*#:$DG#N+]CV%z;.  ~
+o%%`+/cBQlt%z1~TTj7$QuOPVLM?;
ND|J=jK[lo\M^u/S@"jCc>D	%Z$uggyPD:V,|~-Ua]=(b$ef\m
pjp_Mat/M	P*=8-5nq-<^Ppb}Q/
y_KjR5|HSs.$kcC|G{s:Q6uL%Cl:1Xhb"<?z(yN.f]}r40h{sj}li*k .'@"yG'=1;op^VRM0{9}4{fE`Z o&H nDns8 nBkuO(A>XO6n?XSQQ/~\|'Gm|gnnqz)9 szKw$OXXeY7Z)ya1t'?IbD8Bp_CJ	&9et '?',"f0IxyE+G:!oO3W-faX8G~->C1M1}.?#OPl`7.J$SaSS2[^fG{R`3? uX<w<t:OeQmIOf3'e[}QkA{a&XHWjy@A!}+35v@	PoM0f/>Z^rs?D$|Z OLh=1
6wT .t\s;ju WU>z!~%%dL[(+M~rc-'6>~he;~IXXYg
. >L&k5m#4UY
eHu;Jhj#K!;bv Jtm*j7ecj	jvhisG;7Q;|5d9|YCvxh47X
;$/~a&
?p|D/#
X;81&PRNYsZm54#
	;QFuQ!mM	9fJKa.r+e{gs3PQo-u#}Y;}L#+gYSHgHiXkD^STK2(8%Cjh SdQimg|,M9
hJ6T^@>$K_kMJs\%{09tmnNH5_rReZrVS[+n}SxXS	Lb@I1^G {O<9nYUVZzC6mk)#5Mki".sNNLH:}rX&i|<e{df]t$nBo~8}zf(//g&r.{`A?"
jb{6Jz~|;CnE[FcAV
.cknVle<eSK 3mO&/n-XXe"LG?3~w!cc|i9jQSbTUXXy::-{HP~ExPv[dQgPTu/nP.r]|awR{wBX
mptDTi>Cl6"&AR2:]i^UaC
; ^|340Sfc>Mr pp@9B \!eZ51<59]6pU]WJ2L|NTP"w8yBszk	P;6%D\~Lq3tUJ%
-*9<BS')4rUa]eqY
0\dK[%C 6u![QW(F! {2qR[k`D5(ckw$i!ff!9+xtfn;vCkmN|ElS>\9{<spImlSiYL[rhc=k!ZjFkYetg?$1PsO8'*5o?gK4Z{B>@Wk//m=8xjggU5N`faw9('tQE;;<B]*Bo4&^>3#\]Looh3	uWwpt1a0Hn>iN
Gw |j7Zuoh.cwU7s#|zgO9Vf>hug[%O	Gm@;f9) xvm
q?*]L|_9"L|_DbeDMO9XXF
PX@1?D_?@]FnDn%(C]gHwA0-t`])dUDt,L}m)M8v7PT`m29\\0w75Mx0K-hJ(+a4^)<ado}Dw_y/:aaOcUlD|B>)wBIusAs`#ej{5G>-2]Jye
@
U9?%9enSk6
f?as-w~]X.y5Yp7ajRSc]|Ws]:M fIg61/(yj3k%AJ>2]dB4[sY
_mTz*J>iu~\[xy`Yt(!l2MFiV_itZ^k!
ZO9z{af*N`{T!|*[9=e8spl03WP7tJ7nU"o.Zs{Q$PphA4aU.+T<sU/"j+/NwLL}Rugr!u9V*.]JU%SS?qciXX'^o&jP2h80<B+B&5f_UMY\z5THg1#>B_& 74L{LrI8U;qs$Nh!/4'c%*AB@Z=N}#7Is|!]xV88bZ	nqa?h>eI7iuQc8z^8nQas~XiH*PV`l{  IDATeK~`-7h9q#\'#T`M%f> XaAmgxb1ro48|n']=!c?gwp8<UK\EsO0mi^:%Eqz	jrmbvz/,qZj}@o9QO
OyMpNby}U!x
@o'VtA$8EOo/]p98-o~#lK|oRd|IXX*rMthNm:wB+UYEHiSsr<>a*5,drfm1'!.)?Vv)oL!*4(*)%@d	`(-[bq!pF(3$KCfX;I)G}n>]AW%o`3HfgMWO-urgs)"K8.2]c3'8Nnao-N&mZK([n?c|l5QwU9c!<!;12M)O1c0~-A%')^.7\
q7SuM5HV\"-Xbb`1=auq<EAo.L@1#uqP9ryDc|<G^ ayMr@obfI[!zg!,Gx X	 XX>-)Q^je^_8 um#8m-L
Mw`6ps3d(z\4N7Bww4,5zCSfAFCMDI.gqm%L.F]HS\00W&HojU]xq};G]MN/RUNY>{o> =zrm>Z!:tFa)K_d;wo^cnBCHv^Tfp_.F3!DH:u{sf
$BF'HsJfgKy16*m>%J6`n_~QKSdP}VmYm. ^j%(4`]w0gmLk<z|^+`pQo<~^XwZDOx="q$s.*pK0PW}#Q<~~$PJj=CTZfS)mzGDeu	RD4aiZV8/<JG+]A"<q3JH BbO%W\U"DI[$ ;WF]p*? 2$OH2 #etx Xi'E_ALp, -|NX
q Ky?S,gQ<+/ ||a      .D8AAAAALp, 2sdS]%)~]+]vx!V$R PR/1C8k$IM_j%2x?tv6f0oZOaG,"-Bb<Z
S	_;n@p)E=*38x*]_P~nzF9s'lF2xNEov?<-]\P?rKcC|GJ96j}rK!lE,CzTZua{2q=0^?a].Qz;=I9>Yw5 f}gH,
EP<N{ocvnM3ka;\#KEns
39=}^o7{ ,2E+> M23;jA`j2JU7%%vwjQgg~E*K:/p_CJ	&9Y+gFn0A
-=wfsP?gia.6a)eiaO_giU&=uPBQw'(6[?I6ee8N;86@8A.lb=> )&A..7(g6$u(=x\>_e-fh~lk]W2/]m! B	AtOm@c+}.H'17]Z?rv-_rs?D$|f)w//j=WL~QO3G9B <?,!,{q6`Sa|O.sxqoUz4.mEXXYg`rl=@0|s	,5m#4UY
eHu;Jhj#K!;bv Jt7m*j7ecj	l6u4'OsA
F:Vh<vU^-()y@khbQx22^7IvPF5v6oCd4Kor~WwT(MUV_zndvxJC(_)+z6P['|"ovBC+30=q]hP)]z{\3OVhgDd30)*!8ee#Qp)Kk=k=SdQ3ze7>:ydC%0\u1
dJz?Zb'@wl6*LZ0T%'eQFi%a&
?((D:kO?\O'b .xr,xme>&TW4x^kz^h9d96'<_wV~&w
XO	%^SQ9|OuOoGyneXSUU2=/K[RU.pC+J}y7[umEEP;yNg[
eHuPI
R+5ANZ*z-gq't_;v GGHD}=a# )b$)QvrU1ocHa6uofO003:rpmSE!A3-2\S1;5TqevjSRIO?xQndTQaoI~CnCf*\lAN?<\|AlwXh`%nir?mW2@it}P+q,Z*uOd=Tufa9^d+[|p-m;;;X@;6>!nE][-e00Wk6M]#0'4v6|n= xt4W.^tvjN|Ej0-%HIy+Nlhy:12vSN`rhc=k!ZjFkYetgz61j"bwC2s&aM;y(zi*+
tdJ[N;^D"E8bYU2'o]Nl5	]kP9B~$`720P7v{)!2gL`7^}[70jgF |tt|*tm{??[}DcXXhn{Vg:<T8Bi[4ko} uEvi^qwC!kDHR{Ng8Y=Ml&}O;^(d*69i0KQi~YDu~\mzsAs:u#)Hyj(Sj@~SX;Tw4%LZ[Q/50=4 Ue@OWs-W&P@vu$?3}Q07	$k)u rwMe"vP&ejRSc]|WsxFB(D^FLc`(r,[B%Omf
U6hT[G`YUpcr.E 
l,PATE'3sk=K2gDR( C=d;.n@-CliMvhyO 
m#iT'\'57{h3 xLr *I;2f,:-WY@YL!kN>jNuocma i]c1q'k)X4q4jJ]KfrNl8!5VYPs:vPQB$nMk]e*(-^1GLpc,z%6\%;{|s]To7rc1!:Clte|'8{|8Y47c%0Dhx.:jq?Fo~z$ Z] 8<+Msmm&M~3$
$N.Zb}-xmKs-HLQ{x)hz9u;eFYt7Px8AtxEc/,{&{b}VRivGkS^pt,A}|v*:eXQH
fk\6rBTI57<-s^1M%HOPkU{-JXc'k(V<4c8w[%|)uW-C=S^R&1@	Q8 j9~k'N>K\1WVfLy59[n!@
L(al!y|w!'4ZU'F*VO<u_|psPxXBk5;iXC:v!nlg4DF7w53ohr/V+A.l%P<Ywac1qe8>_ZxX48)>>cS3P^O^C9OJfiZS{KY 
lpD4A/VB4qTgq''zs[R;Gw_t}B}vh/m-L
Mw`6ps3z
8;	Y~7LSOx\#zE;g*aN
5X/X\hfUg&GN-zT_b"'r-=>/:0	_aJ~d3EviEOzL
__;(7v/@G0&ByKF!!9k*Rm#4Efz9%\	a#Tr=C4:S#Y)`nB>\Bk3NlbC9u<_TR<Hq|mmT!EAXTg:{z"Si]w/
58~r/ ^Th
A668&@w^^-D
dHdJB$fL[x 1;;x<?=2JA,OLA /) o
AtuJ7-2+]ApTcbg	 4{]5zg/v=#tK?/ #8Zb    P$VXO/d     ">;/o
ek'AAAAA( q|+]AAAAA)nb!cAAAAA     	AAAAA"     &D8AAAAALp,     XAAAAA0!     `BcAAAAA     	AAAAAj g?8F7uNU@I.gX9:gxK     p|$r*SAAAA }}&h7j;Iv<L:l:?hwz.Bn{;k?}zveQ|#|mW+pv;AAAAEb|6~DA	;}y[;qS:xvMsIF_w^9c`~cq+tZK;V|kr9j^|v[|Ccz-lp~    p+s|F     IENDB`PNG

   
IHDR     C   E  
<IDATxOhw~gdJHzFKe:M n0fCalbAK8=]v+,'Y>JkS/uVh4eG|>`3wb~w9     4?;N'H[[[J}_     hrBg     
#t    0Bg     
#t    0Bg     
#t    0r3     hc     htaR    !t    hv0    \:    P3     :    P3     :    PF     4^\n,CTjw%t   &V.s|~Zn=7l=Lk    @*2/C|yuV.__2&WfMg    hRO$g{.7oTs||b"~I:5k=V]6   	LOO+I?y[a}I7ndzzzn;   I};IaOT?j    	    (        (    }I:+Vy_F    oK>7$;>;*|Ho|;;z>]2h{w?G'=    
l<L
t2Q
pd[GkO=sdd4?{=WlOWOJ/N3,<1~^m</Mk/xss    v3cT~r$'+#I~;ySuj7_8?O2oO]3|rs~>8gtujL0L^h%z>Ot^[~?7hTd4{7TsORs/+[g]8s    3s==Kg<8p?Fsd6CtX`y#IGF2{B1$5*6{ q97b$d/dJsy$u6Ogr`r?G'f;~9ydlsO<    Pv9dx0eW^X&g83KEJ\/yHz2PneXHJ:v^utyvl$_zj1u9qyxjR;Ee;{JR;tlUQ    -mwLW
09r@g]w+VW8};=9663wSZtrgsfl\iK}{r(g* 3[w     /52+Kzp5v]_ssK5E|%7	M.~I$K9DcC90A~=;w|    4yaO2<r*r5ko'ksss%V6,!se}_5:+&s}9g g~|vfrwkQsj5    jFdu:w]9wkUQKgTnV^N9yw\hCw|({L|_$Fyczcw-;xN}    HZclLyc+csKs^GX~uC     V\G%I_5x[=NM&mJB?ui|o|l:    ,+UYNg    9:s;U   UhczOcpk     P3     :   @{$]k$M3~f    oKrOR__}Qlr#A    hRR);s|>1'&=S*=l:   @*Jiiiw;?A6mF{l9wU<t   &kl-N\N\nh(JmmmYvmZZV>    TT5k6a*W
Ws"t   V
W,sVs\%t    0=qa8_+1?'75z    e3    @a     F    @a[9     xt    hrRk|oR.k    
uMR`=mCH    eS*RKm{u    Rj)     	    P3     :    P3     /!b =    IENDB`# CLM Common Library

## Common Core Module

When extract the common code for every CDA in CLM that is generated this module.

### Dependency

```xml
<dependency>
    <groupId>com.hsbc.gbm.bd.clm</groupId>
    <artifactId>clientlifecycle-common-core</artifactId>
    <version>${common-clm.version}</version>
</dependency>
```

### SparkSession

There is provide two approaches to get SparkSession.

#### First Approach

* Main class must need extend `com.hsbc.gbm.bd.clm.env.SparkJob`
* Main class must need provide constructor 
  `(jobName: String, configPath: String, params: RawJobParams)`
* Main class must follow annotation `@SparkJobName(<jobName>)`

**Notes: if you use this approach that entrypoints.class must is** `com.hsbc.gbm.bd.clm.runner.GenericSparkJobRunner`

##### Demo

```scala
import com.hsbc.gbm.bd.clm.annotation.SparkJobName
import com.hsbc.gbm.bd.clm.env.{RawJobParams, SparkJob}

@SparkJobName("clm-gmfx-cda")
class Main(jobName: String, configPath: String, params: RawJobParams) extends SparkJob {
  override def appName(): String = jobName

  override def configLocation(): String = configPath

  override def process(): Unit = {
    //do some things
  }

}
```

#### Second Approach 

* Main class must extend `com.hsbc.gbm.bd.clm.env.SparkEnv`

##### Demo

```scala
import com.hsbc.gbm.bd.clm.env.SparkEnv

object Main extends SparkEnv {
  override def appName(): String = this.getClass.getName

  override def configLocation(): String = "application.yml"
    
  def main(args: Array[String]): Unit = {
    }
}
```

### Submit Job

as of now, we just provide two render template.

#### run in hdp06

```xml
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.6.0</version>
                <executions>
                    <execution>
                        <phase>compile</phase>
                        <goals>
                            <goal>java</goal>
                        </goals>
                        <configuration>
                            <mainClass>com.hsbc.gbm.bd.clm.renderer.GenericTemplateRenderer</mainClass>
                            <arguments>
                                <argument>${artifactId}</argument>
                                <argument>${version}</argument>
                                <argument>config/deployment.yml</argument>
                                <argument>${project.basedir}/src/main/resources/scripts/submit-job-fg-hdp06.sh</argument>
                                <argument>submit-job-fg-hdp06</argument>
                                <argument>1.1</argument>
                                <argument>false</argument>
                            </arguments>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
```

#### run in hdp45

```xml
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.6.0</version>
                <executions>
                    <execution>
                        <phase>compile</phase>
                        <goals>
                            <goal>java</goal>
                        </goals>
                        <configuration>
                            <mainClass>com.hsbc.gbm.bd.clm.renderer.GenericTemplateRenderer</mainClass>
                            <arguments>
                                <argument>${artifactId}</argument>
                                <argument>${version}</argument>
                                <argument>config/deployment.yml</argument>
                                <argument>${project.basedir}/src/main/resources/scripts/submit-job-fg-hdp45.sh</argument>
                                <argument>submit-job-fg-hdp45</argument>
                                <argument>1.1</argument>
                                <argument>false</argument>
                            </arguments>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
```

**Notes: don't forget update** `clm-<product>-cda.sh` **for run script name**

### Utils Class

* when you `import com.hsbc.gbm.bd.clm.utils._` that you can use follow function

  * `rename(pairs: Map[String, String])`, `applyRenames(originalToRename: Map[String, String])`
    input Map that can rename field name for Dataframe
    **notes: when source field name is not exist, that applyRenames will jump but rename will throw exception**

  * `skewjoin(right: DataFrame, usingColumns: Seq[String], joinType: String, divide: Int = 100, samplePercent: Int = 10)`
    `skewJoin(right: Dataset[_]
                 , usingColumns: Seq[String]
                 , joinType: String
                 , filterKey: Seq[String]
                 , skewValues: Seq[String])`

    `skewJoin(right: Dataset[_]
                 , joinExprs: Column
                 , joinType: String
                 , filterKey: Seq[String]
                 , skewValues: Seq[String])`

    instead of raw join, it need provide a skew value list, it implement by separate skew rows.
    and if skewjoin is deprecated that it implement by random key distribute skew rows.

  * `dropDuplicateCols(rmDF: DataFrame)`
    it can drop duplicate columns base on parameter dataframe

  * `dropDuplicateColumns(dfs: DataFrame*)`
    like dropDuplicateCols, but the parameter order is necessary, remain order is big  column 

  * `trimAllColumnsToNull`, `trimColumnsToNull(cols: Seq[String])`
    if value is blank char that will transform null, all

  * `applyInserts(inserts: Map[String, String])`
    insert hard code as field value

  * `storeAsParquet(path: String, partition_col: Option[String] = None, retention: Int = 7)`
    store as parquet, will base on partition_col to select whether need partition

  * `write2Es(rawIndexName: String, cdaLocation: String)`
    write to es , the second parameter is add for index to make da can see the cda location.

  * `buildFullSchema(schema: StructType)`
    will base on schema that output special field and if not exist will add by null value.

  * `getMetaDataResponse(host: String, port: Int, batchId: Option[String])`
    will base on host and port to get mda path metadata object, the batchId is very important that if not None will get the date which batchId point.

* when you `import com.hsbc.gbm.bd.clm.utils.CsvUnitTestUtils` that you can use follow function

  * `csvToParquet(path: String)`
  * `csvToHiveTable(path: String, target: Option[String] = None)`

## Common Data Quality Module

### Dependency 

```xml
<dependency>
    <groupId>com.hsbc.gbm.bd.clm</groupId>
    <artifactId>clientlifecycle-common-measure</artifactId>
    <version>${common-clm.version}</version>
</dependency>
```

### Measure

* you need to make Main class extend `com.hsbc.gbm.bd.clm.measure.MeasureDQ`

* you need to implement follow properties

  ```scala
  //this is dir for measure file, if in cluster it need hdfs://
  override def measureDirPath: String = s"${appConfig("measurePath")}/${appConfig("name")}"
  
  //this is some data sources for measure input
  override def dsParams: Seq[DataSourceParam] = (pathMap + ("cda" -> s"${appConfig("outputGMFX").toString}/date=$date"))
      .map(x => (x._1.replaceAll("[-]", "_"), x._2))
      .map(x => {
        val dcParam = DataConnectorParam("file", x._1, Map(
          "format" -> "parquet",
          "paths" -> List(x._2),
          "skipOnError" -> false
        ), null)
        DataSourceParam(x._1, dcParam)
      }).toList
  
  //this is some sink for issue rows data and measure metrics
  override def sinkParams: Seq[SinkParam] = List(
      SinkParam("consoleSink", "CONSOLE", Map("max.log.lines" -> 10)),
      SinkParam("hdfsSink", "HDFS", Map("max.lines.per.file" -> 10000, "max.persist.lines" -> 10000, "path" -> appConfig("metricsPath")))
    )
  ```

* you need to call measure() when cda is generated

## Common Data Lineage Module

### Dependency

```xml
<dependency>
    <groupId>com.hsbc.gbm.bd.clm</groupId>
    <artifactId>clientlifecycle-common-datalineage</artifactId>
    <version>${common-clm.version}</version>
</dependency>
```

### Usage

#### Scala

import package before use 

```scala
import org.apache.spark.sql.DataLineage
```

* get dot content or svg file by Dataframe

  ```scala
  DataLineage(details = true).getDot(coreData())
  DataLineage(details = true).save(coreDataDF, "target/core-data-lineage.svg")
  ```

* get dot content or svg file by temp view

  ```scala
  coreDataDF.createOrReplaceTempView("coreDataDF")
  DataLineage(details = true).getDot("coreDataDF")
  DataLineage(details = true).save("coreDataDF", "target/core-data-lineage.svg")
  ```

**notes**:  the save path must be local file systems. and we suggest just generate in dev environment(**dev environment comes from unit test or say jenkins**), it means that don't run in cluster. so it should follow `if (config("env").toString == "dev") DataLineage(details = true).save(coreDataDF, "target/core-data-lineage.svg")`. and we will auto upload the svg file to jira as test evidence.

#### Jupyter

* get spark session

  ```python
  import os
  import sys
  os.environ["SPARK_HOME"] = "/usr/hdp/2.6.5.0-292/spark2-client"
  os.environ["PYLIB"] = os.environ["SPARK_HOME"] + "/python/lib"
  os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars <path to common jar file> pyspark-shell'
  sys.path.insert(0, os.environ["SPARK_HOME"] +"/python/lib/pyspark.zip")
  sys.path.insert(0, os.environ["PYLIB"] +"/py4j-0.10.6-src.zip")
  
  from common.py36.app.sparkEnv import get_spark
  from common.py36.app.configuration import SPARK_OPTS
  
  SPARK_OPTS.update([("spark.sql.broadcastTimeout","36000"),('spark.yarn.queue','platfora-finance'),\
                     ('spark.dynamicAllocation.maxExecutors','50'),\
                    ('spark.jars', '<path to common jar file>')])
  spark = get_spark(spark_options=SPARK_OPTS)
  
  import pyspark.sql.functions as F
  from pyspark.sql.types import *
  from pyspark.sql import Window
  ```

* get jvm object

  ```python
  jvm = spark.sparkContext._active_spark_context._jvm
  ```

* make Dataframe  register to temp view

  ```python
  df.createOrReplaceTempView("df")
  ```

* get data lineage (content is dot language)

  ```python
  print(jvm.DataLineageApi.get("df", True))
  ```

  the second parameter is able to appear detail 

* transform dot to svg 
  https://apprunner.hk.hsbc/swagger/docs/?url=https://apprunner.hk.hsbc/graph/openapi.yaml#/default/post_api_v1_graph

  * select request type
    ![image-20210706144817057](README.assets/image-20210706144817057.png)

  * select response type
    ![image-20210706145005645](README.assets/image-20210706145005645.png)

  * you need to click try it out then you can fill dot content
    ![image-20210706145501356](README.assets/image-20210706145501356.png)

  * fill dot and execute
    ![image-20210706145149147](README.assets/image-20210706145149147.png)

  * download svg content and rename file
    ![image-20210706145234210](README.assets/image-20210706145234210.png)

    this is xml file that you need to rename to svg file then you can open it by chrome or IE 

## FAQ

### 1- How can I get the es index name?

answer: We have ingest date and timestamp in sparksession, and use it for index name to write in es, so you can get by `spark.sparkContext.getConf.get("spark.runtime.config.time.date")`or `spark.sparkContext.getConf.get("spark.runtime.config.time.timestamp")`

date format is `yyyyMMdd` and timestamp format is `yyyyMMddhhmmss`cat: ./src: Is a directory
cat: ./src/site: Is a directory
<document xmlns="http://maven.apache.org/DOCUMENT/1.0.1"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/DOCUMENT/1.0.1 http://maven.apache.org/xsd/document-1.0.1.xsd"
          outputName="${artifactId}-${version}-test-report">

    <meta>
        <title>SCSAI Party Asset pre-processing Test Report</title>
    </meta>

    <toc name="Table of Contents">
    </toc>

    <cover>
        <coverTitle>Project: ${project.name}</coverTitle>
        <coverSubTitle>Version: ${project.version}</coverSubTitle>
        <coverType>Test Report</coverType>
        <projectName>${project.name}</projectName>
    </cover>

</document>
cat: ./src/site/resources: Is a directory
cat: ./src/site/resources/css: Is a directory
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  margin: 0px;
  padding: 0px;
}
table {
  padding:0px;
  width: 100%;
  margin-left: -2px;
  margin-right: -2px;
}
acronym {
  cursor: help;
  border-bottom: 1px dotted #feb;
}
table.bodyTable th, table.bodyTable td {
  padding: 2px 4px 2px 4px;
  vertical-align: top;
}
div.clear{
  clear:both;
  visibility: hidden;
}
div.clear hr{
  display: none;
}
#bannerLeft, #bannerRight {
  font-size: xx-large;
  font-weight: bold;
}
#bannerLeft img, #bannerRight img {
  margin: 0px;
}
.xleft, #bannerLeft img {
  float:left;
}
.xright, #bannerRight {
  float:right;
}
#banner {
  padding: 0px;
}
#breadcrumbs {
  padding: 3px 10px 3px 10px;
}
#leftColumn {
 width: 170px;
 float:left;
 overflow: auto;
}
#bodyColumn {
  margin-right: 1.5em;
  margin-left: 197px;
}
#legend {
  padding: 8px 0 8px 0;
}
#navcolumn {
  padding: 8px 4px 0 8px;
}
#navcolumn h5 {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn ul {
  margin: 0;
  padding: 0;
  font-size: small;
}
#navcolumn li {
  list-style-type: none;
  background-image: none;
  background-repeat: no-repeat;
  background-position: 0 0.4em;
  padding-left: 16px;
  list-style-position: outside;
  line-height: 1.2em;
  font-size: smaller;
}
#navcolumn li.expanded {
  background-image: url(../images/expanded.gif);
}
#navcolumn li.collapsed {
  background-image: url(../images/collapsed.gif);
}
#navcolumn li.none {
  text-indent: -1em;
  margin-left: 1em;
}
#poweredBy {
  text-align: center;
}
#navcolumn img {
  margin-top: 10px;
  margin-bottom: 3px;
}
#poweredBy img {
  display:block;
  margin: 20px 0 20px 17px;
}
#search img {
    margin: 0px;
    display: block;
}
#search #q, #search #btnG {
    border: 1px solid #999;
    margin-bottom:10px;
}
#search form {
    margin: 0px;
}
#lastPublished {
  font-size: x-small;
}
.navSection {
  margin-bottom: 2px;
  padding: 8px;
}
.navSectionHead {
  font-weight: bold;
  font-size: x-small;
}
.section {
  padding: 4px;
}
#footer {
  padding: 3px 10px 3px 10px;
  font-size: x-small;
}
#breadcrumbs {
  font-size: x-small;
  margin: 0pt;
}
.source {
  padding: 12px;
  margin: 1em 7px 1em 7px;
}
.source pre {
  margin: 0px;
  padding: 0px;
}
#navcolumn img.imageLink, .imageLink {
  padding-left: 0px;
  padding-bottom: 0px;
  padding-top: 0px;
  padding-right: 2px;
  border: 0px;
  margin: 0px;
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

body {
  padding: 0px 0px 10px 0px;
}
body, td, select, input, li{
  font-family: Verdana, Helvetica, Arial, sans-serif;
  font-size: 13px;
}
code{
  font-family: Courier, monospace;
  font-size: 13px;
}
a {
  text-decoration: none;
}
a:link {
  color:#36a;
}
a:visited  {
  color:#47a;
}
a:active, a:hover {
  color:#69c;
}
#legend li.externalLink {
  background: url(../images/external.png) left top no-repeat;
  padding-left: 18px;
}
a.externalLink, a.externalLink:link, a.externalLink:visited, a.externalLink:active, a.externalLink:hover {
  background: url(../images/external.png) right center no-repeat;
  padding-right: 18px;
}
#legend li.newWindow {
  background: url(../images/newwindow.png) left top no-repeat;
  padding-left: 18px;
}
a.newWindow, a.newWindow:link, a.newWindow:visited, a.newWindow:active, a.newWindow:hover {
  background: url(../images/newwindow.png) right center no-repeat;
  padding-right: 18px;
}
h2 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #999;
  color: #900;
  background-color: #ddd;
  font-weight:900;
  font-size: x-large;
}
h3 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  color: #900;
  background-color: #eee;
  font-weight: normal;
  font-size: large;
}
h4 {
  padding: 4px 4px 4px 6px;
  border: 1px solid #bbb;
  color: #900;
  background-color: #fff;
  font-weight: normal;
  font-size: large;
}
h5 {
  padding: 4px 4px 4px 6px;
  color: #900;
  font-size: medium;
}
p {
  line-height: 1.3em;
  font-size: small;
}
#breadcrumbs {
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #aaa;
  background-color: #ccc;
}
#leftColumn {
  margin: 10px 0 0 5px;
  border: 1px solid #999;
  background-color: #eee;
  padding-bottom: 3px; /* IE-9 scrollbar-fix */
}
#navcolumn h5 {
  font-size: smaller;
  border-bottom: 1px solid #aaaaaa;
  padding-top: 2px;
  color: #000;
}

table.bodyTable th {
  color: white;
  background-color: #bbb;
  text-align: left;
  font-weight: bold;
}

table.bodyTable th, table.bodyTable td {
  font-size: 1em;
}

table.bodyTable tr.a {
  background-color: #ddd;
}

table.bodyTable tr.b {
  background-color: #eee;
}

.source {
  border: 1px solid #999;
}
dl {
  padding: 4px 4px 4px 6px;
  border: 1px solid #aaa;
  background-color: #ffc;
}
dt {
  color: #900;
}
#organizationLogo img, #projectLogo img, #projectLogo span{
  margin: 8px;
}
#banner {
  border-bottom: 1px solid #fff;
}
.errormark, .warningmark, .donemark, .infomark {
  background: url(../images/icon_error_sml.gif) no-repeat;
}

.warningmark {
  background-image: url(../images/icon_warning_sml.gif);
}

.donemark {
  background-image: url(../images/icon_success_sml.gif);
}

.infomark {
  background-image: url(../images/icon_info_sml.gif);
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#banner, #footer, #leftcol, #breadcrumbs, .docs #toc, .docs .courtesylinks, #leftColumn, #navColumn {
	display: none !important;
}
#bodyColumn, body.docs div.docs {
	margin: 0 !important;
	border: none !important
}
/* You can override this file with your own styles */cat: ./src/site/resources/images: Is a directory
GIF89a          !
  ,       D`c5
 ;GIF89a          !
  ,       j
 ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  P   tRNS @*   PIDATxb`&& @P6#@` X 2 d@ A3 (  * t    IENDB`GIF89a       
q	
x
v
m;@GJf46_
i
[
~
}
s	q	p	n	XVL


v
l	~"",9?6;{6:df$')OW-/*"4"3s}%8&:(=*@+A*?5H=MM\+B.F/G0I2M3O2N<S5R6S7U:Y:Y<\;[=^?a@bBeH 1K"4H!3F 1I"4g1Lm9Xr@bxHn>%9J.FprN3Ovx{@-EL6RO8V08G3O1:bGm3=4>7 C5AgOx?2ME7TmW|d}gu^V                                                                                                                                                                                                                                                                                                      !   ,        ;	H AJ gNYc*
FrM/]HIh $5i
%BT0P8$2,,Z(DL@;G k$<Hd
>`' !\2?vhp
u@
.X=	q0	)f$Tp"FQGrC ;GIF89a       r	'SG]}]yVpOgLcJ`EYxCWtI^~FZyehgjlmpXltvdyw|~wunzw@TpAUqy}                                       !  r ,       r`fg`l\d
LG^qqopnQmbj.MObhNgr\oP6[ig%dmE4k_^eL	PRahYcHBjfV<JD@>
F`\-5Z=97D`]
I'130@KICA@," c
$XA ?tr!8h2@Pab
 ;GIF89a       c~wSq*/%(#8='*!37,/(CH'AF&>B/2*,)FI"!PrkjtFcO7G;2B.]B]>frmjb7Jw*U0W2im;Ag$r?o=e8f8b7Z2Y1S.Jt)Qy1WYuQeFcE{{hGv@u?k:h7`3Jr(E^3Q|,S~-Dh%}DEi&QIm&Qx+Ot)Mo(Ll&Ig%Gc$                                                                                                                                                                                                                                                                                                                                                            !   ,        	HA!,XE
aCF5pA*01#"NT0 A=Y& 	NRI1bJ':o8b3-e>|(q4h(mT"J;*d("4lp	,P(  ;GIF89a       b`awr}~j|fs[M@K?
kTN
GJ$wsrpiffecb_[ZTQOJ
EDE?i2	nm]XWVH
A=h2	]-bz{z&Zi&i(j&127m-D2%p4BI|Du@Ya\}R[v[mI:2`vs=.()+*.!<,&5&!:*%y*vp*,-~z                           !  v ,       v	oaed][sv
iT
u^PW\ZMIgXUGADSRQ?4VONK)#CnJHFE0=7'6*vB@%+8,l !5"9.:/Lt"$&(-3:cYf3c#
;  ;PNG

   
IHDR      	   &   gAMA  7   tEXtSoftware Adobe ImageReadyqe<   PLTEuuu  8   tRNS @*   FIDATxb`fff f b @   8@ !
@`6  L  & ^    IENDB`<xsl:stylesheet
        version="1.0"
        xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
        xmlns:fo="http://www.w3.org/1999/XSL/Format">

    <xsl:attribute-set name="layout.master.set.base">
        <xsl:attribute name="page-width">8.26in</xsl:attribute>
        <xsl:attribute name="page-height">11.69in</xsl:attribute>
        <xsl:attribute name="margin-top">0.5in</xsl:attribute>
        <xsl:attribute name="margin-bottom">0.5in</xsl:attribute>
        <xsl:attribute name="margin-left">1in</xsl:attribute>
        <xsl:attribute name="margin-right">1in</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="body.pre" use-attribute-sets="base.pre.style">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.layout">
        <xsl:attribute name="table-omit-footer-at-break">false</xsl:attribute>
        <!-- note that table-layout="auto" is not supported by FOP 0.93 -->
        <xsl:attribute name="table-layout">fixed</xsl:attribute>
        <xsl:attribute name="width">100%</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.title.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.heading.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">8pt</xsl:attribute>
        <xsl:attribute name="font-weight">bold</xsl:attribute>
    </xsl:attribute-set>

    <xsl:attribute-set name="table.body.block" use-attribute-sets="base.block">
        <xsl:attribute name="font-size">7pt</xsl:attribute>
    </xsl:attribute-set>

</xsl:stylesheet>
