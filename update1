 cat: .: Is a directory
*.class
*.log
*.iml
/.idea

cat: ./allcode.txt: input file is output file
#!/usr/bin/env groovy
// Declarative //

library identifier: "data-products-pipeline-library@master", changelog: false

def jiraId
def jiraSummary
def poms = ['pom_2.11.xml', 'pom_2.12.xml']

def getArtifactAndVersion() {
    return pom0.artifactId + "-" + pom0.version
}

def getGitLogMessages() {
    return sh(script: "git log -10 --format=\"%s\"", returnStdout: true)
}

pipeline {
    agent {
        label 'gcp-agent-2'
    }
    tools {
        maven 'M3'
        jdk 'JDK1.8'
    }
    triggers {
        githubPush()
    }
    environment {
        scmVars = null
        nexusBaseUrl = "https://dsnexus.uk.hibm.hsbc:8081"
        service_account_creds = credentials("GB-GBM-BDNEXUS")
        defaultVersionUpdatePattern = "=+0"
    }
    options {
        disableConcurrentBuilds()
        buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '1'))
    }
    stages {
        stage('Prepare Env') {
            steps {
                echo "Preparing Environment"
                cleanWs()
                script {
                    scmVars = checkout scm
                    pom0 = readMavenPom file: poms[0]
                    gitLog = getGitLogMessages()
                    jiraId = pipelineApi.getLastCommitJiraId(gitLog)
                    println "Retrieved JIRA ID from GIT log: '${jiraId}'"
                    jiraSummary = pipelineApi.getJiraSummary(service_account_creds_usr, service_account_creds_psw, jiraId)
                    println "JIRA Summary: '${jiraSummary}'"
                }
            }
        }

        stage("Update POM Versions") {
            steps {
                echo "Updating POM version"
                script {
                    if (env.GIT_BRANCH == "master") {
                        def gitLogVersionUpdatePattern = pipelineApi.getVersionUpdatePattern(gitLog)
                        def versionUpdatePattern = gitLogVersionUpdatePattern != null ? gitLogVersionUpdatePattern : defaultVersionUpdatePattern
                        newVersion = pipelineApi.getNextReleaseVersion(nexusBaseUrl, service_account_creds_usr, service_account_creds_psw, pom0.groupId, pom0.artifactId, versionUpdatePattern)

                    } else {
                        newVersion = jiraId.replaceAll("-", "") + "-SNAPSHOT"
                    }
                    println "Version to be used: ${newVersion}"
                }

                script {
                    for (String pomFile : poms) {
                        sh "mvn versions:set -DnewVersion=${newVersion} -f ${pomFile}"
                    }
                }

                script {
                    pom0 = readMavenPom file: poms[0]
                    String buildName = "[${env.BUILD_NUMBER}] ${pom0.version}"
                    currentBuild.displayName = buildName
                    currentBuild.description = jiraSummary
                }

            }
        }
        stage('Build / Test / Upload Artifacts') {
            steps {
                script {
                    for (String pomFile : poms) {
                        echo "Running ${env.BUILD_ID} for ${pomFile} on ${env.JENKINS_URL}"
                        sh "mvn -U clean deploy -f ${pomFile}"
                    }
                }
            }
        }

        stage('Tag Release') {
            when {
                branch 'master'
            }
            steps {
                echo "Creating a tag for version: ${getArtifactAndVersion()}"
                sh "git tag -a ${getArtifactAndVersion()} -m \"Tagging version ${getArtifactAndVersion()} \""
                echo "Pushing to remote repository..."
                sh "git push origin ${getArtifactAndVersion()}"
            }
        }
        stage('Clean-Up') {
            steps {
                cleanWs()
            }
        }
    }
}
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>
    <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
    <artifactId>common-spark-mv_2.11</artifactId>
    <version>0-SNAPSHOT</version>

    <properties>
        <jdk.version>1.8</jdk.version>
        <scala.binary.version>2.11</scala.binary.version>
        <scala.version>2.11.8</scala.version>
        <spark.version>2.3.0</spark.version>
        <dataproducts.data-schema.version>1.43.0</dataproducts.data-schema.version>
        <akka.version>2.5.21</akka.version>
        <akka-http.version>10.1.1</akka-http.version>
        <elastic.version>7.4.2</elastic.version>
    </properties>

    <distributionManagement>
        <snapshotRepository>
            <id>dsnexus-snapshots</id>
            <uniqueVersion>true</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots</url>
        </snapshotRepository>
        <repository>
            <id>dsnexus-releases</id>
            <uniqueVersion>false</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases</url>
        </repository>
    </distributionManagement>

    <dependencies>
        <dependency>
            <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
            <artifactId>common-encryption</artifactId>
            <version>1.36.0</version>
        </dependency>
        <dependency>
            <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
            <artifactId>data-schema</artifactId>
            <version>${dataproducts.data-schema.version}</version>
        </dependency>

        <!-- Spark Dependencies -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <groupId>commons-codec</groupId>
                    <artifactId>commons-codec</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <groupId>commons-codec</groupId>
                    <artifactId>commons-codec</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>commons-codec</groupId>
            <artifactId>commons-codec</artifactId>
            <version>1.15</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.25</version>
        </dependency>
        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>1.25</version>
        </dependency>
        <dependency>
            <groupId>io.github.classgraph</groupId>
            <artifactId>classgraph</artifactId>
            <version>4.8.90</version>
        </dependency>

        <!-- Scala Library -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- Akka Dependencies -->
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
            <version>${akka.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-stream_${scala.binary.version}</artifactId>
            <version>${akka.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-http_${scala.binary.version}</artifactId>
            <version>${akka-http.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-http-spray-json_${scala.binary.version}</artifactId>
            <version>${akka-http.version}</version>
        </dependency>

        <!-- Test Dependencies -->
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <version>3.0.5</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.github.tomakehurst</groupId>
            <artifactId>wiremock-standalone</artifactId>
            <version>2.27.2</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.awaitility</groupId>
            <artifactId>awaitility-scala</artifactId>
            <version>4.1.1</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.assertj</groupId>
            <artifactId>assertj-core</artifactId>
            <version>3.18.1</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.6.0</version>
                <executions>
                    <execution>
                        <id>delete-hive-temp-dir</id>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <phase>clean</phase>
                        <configuration>
                            <executable>rm</executable>
                            <workingDirectory>/tmp</workingDirectory>
                            <arguments>
                                <argument>-rf</argument>
                                <argument>hive</argument>
                            </arguments>
                        </configuration>
                    </execution>
                    <execution>
                        <id>delete-project-temp-dir</id>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <phase>clean</phase>
                        <configuration>
                            <executable>rm</executable>
                            <workingDirectory>${project.basedir}</workingDirectory>
                            <arguments>
                                <argument>-rf</argument>
                                <argument>tmp</argument>
                            </arguments>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>3.1.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>test-jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.17</version>
                <executions>
                    <execution>
                        <id>standard-unit-tests</id>
                        <phase>test</phase>
                        <goals>
                            <goal>test</goal>
                        </goals>
                        <configuration>
                            <excludes>
                                <exclude>**/IT.java</exclude>
                            </excludes>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <version>1.0</version>
                <configuration>
                    <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                    <junitxml>.</junitxml>
                    <filereports>common-test-suite.txt</filereports>
                </configuration>
                <executions>
                    <execution>
                        <id>test</id>
                        <goals>
                            <goal>test</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.4.0</version>
                <executions>
                    <execution>
                        <id>scala-compile</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>
    <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
    <artifactId>common-spark-mv_2.12</artifactId>
    <version>0-SNAPSHOT</version>

    <properties>
        <jdk.version>1.8</jdk.version>
        <scala.binary.version>2.12</scala.binary.version>
        <scala.version>2.12.10</scala.version>
        <spark.version>3.1.1</spark.version>
        <hadoop.version>3.1.1</hadoop.version>
        <dataproducts.data-schema.version>1.43.0</dataproducts.data-schema.version>
        <akka.version>2.5.21</akka.version>
        <akka-http.version>10.1.1</akka-http.version>
        <elastic.version>7.4.2</elastic.version>
    </properties>

    <distributionManagement>
        <snapshotRepository>
            <id>dsnexus-snapshots</id>
            <uniqueVersion>true</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots</url>
        </snapshotRepository>
        <repository>
            <id>dsnexus-releases</id>
            <uniqueVersion>false</uniqueVersion>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases</url>
        </repository>
    </distributionManagement>

    <dependencies>
        <dependency>
            <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
            <artifactId>common-encryption</artifactId>
            <version>1.36.0</version>
        </dependency>
        <dependency>
            <groupId>com.hsbc.gbm.bd.dataproducts</groupId>
            <artifactId>data-schema</artifactId>
            <version>${dataproducts.data-schema.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-sql_2.11</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <!-- Spark Dependencies -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <groupId>commons-codec</groupId>
                    <artifactId>commons-codec</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <groupId>commons-codec</groupId>
                    <artifactId>commons-codec</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <groupId>commons-codec</groupId>
                    <artifactId>commons-codec</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>servlet-api</artifactId>
                    <groupId>javax.servlet</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>slf4j-log4j12</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>commons-codec</groupId>
            <artifactId>commons-codec</artifactId>
            <version>1.15</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.25</version>
        </dependency>
        <dependency>
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>1.25</version>
        </dependency>
        <dependency>
            <groupId>io.github.classgraph</groupId>
            <artifactId>classgraph</artifactId>
            <version>4.8.90</version>
        </dependency>

        <!-- Scala Library -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- Akka Dependencies -->
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
            <version>${akka.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-stream_${scala.binary.version}</artifactId>
            <version>${akka.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-http_${scala.binary.version}</artifactId>
            <version>${akka-http.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-http-spray-json_${scala.binary.version}</artifactId>
            <version>${akka-http.version}</version>
        </dependency>

        <!-- Test Dependencies -->
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <version>3.1.4</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.github.tomakehurst</groupId>
            <artifactId>wiremock-standalone</artifactId>
            <version>2.27.2</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.awaitility</groupId>
            <artifactId>awaitility-scala</artifactId>
            <version>4.1.1</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.assertj</groupId>
            <artifactId>assertj-core</artifactId>
            <version>3.21.0</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>1.6.0</version>
                <executions>
                    <execution>
                        <id>delete-hive-temp-dir</id>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <phase>clean</phase>
                        <configuration>
                            <executable>rm</executable>
                            <workingDirectory>/tmp</workingDirectory>
                            <arguments>
                                <argument>-rf</argument>
                                <argument>hive</argument>
                            </arguments>
                        </configuration>
                    </execution>
                    <execution>
                        <id>delete-project-temp-dir</id>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <phase>clean</phase>
                        <configuration>
                            <executable>rm</executable>
                            <workingDirectory>${project.basedir}</workingDirectory>
                            <arguments>
                                <argument>-rf</argument>
                                <argument>tmp</argument>
                            </arguments>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>3.1.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>test-jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.17</version>
                <executions>
                    <execution>
                        <id>standard-unit-tests</id>
                        <phase>test</phase>
                        <goals>
                            <goal>test</goal>
                        </goals>
                        <configuration>
                            <excludes>
                                <exclude>**/IT.java</exclude>
                            </excludes>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <version>1.0</version>
                <configuration>
                    <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                    <junitxml>.</junitxml>
                    <filereports>common-test-suite.txt</filereports>
                </configuration>
                <executions>
                    <execution>
                        <id>test</id>
                        <goals>
                            <goal>test</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.4.0</version>
                <executions>
                    <execution>
                        <id>scala-compile</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project># common-spark-mv
Common library with utilities for Spark jobs supporting multiple Scala versions
cat: ./src: Is a directory
cat: ./src/main: Is a directory
cat: ./src/main/scala: Is a directory
cat: ./src/main/scala/com: Is a directory
cat: ./src/main/scala/com/hsbc: Is a directory
cat: ./src/main/scala/com/hsbc/gbm: Is a directory
cat: ./src/main/scala/com/hsbc/gbm/bd: Is a directory
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts: Is a directory
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark: Is a directory
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/annotation: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.annotation;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
public @interface LifecycleEventListenerName {
    String value();
}package com.hsbc.gbm.bd.dataproducts.spark.annotation;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
public @interface SparkJobName {
    String value();
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/env: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.env

class AppConfig(config: Map[String, Any]) {

  def apply[T](key: String): T = {
    appConfig(key).asInstanceOf[T]
  }

  def apply[T](key: String, defaultValue: T): T = {
    appConfig.getOrElse(key, defaultValue).asInstanceOf[T]
  }

  private lazy val appConfig: Map[String, Any] = {
    config
      .filter({ case (k, v) => k.startsWith("app.") })
      .map({ case (k, v) => (k.substring("app.".length), v) })
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.env

import java._
import java.io.FileInputStream
import java.security.{InvalidAlgorithmParameterException, InvalidKeyException, NoSuchAlgorithmException}

import com.hsbc.gbm.bd.dataproducts.encryption.EncryptionUtils
import javax.crypto.{BadPaddingException, IllegalBlockSizeException, NoSuchPaddingException}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

import scala.collection.JavaConverters._
import org.yaml.snakeyaml.Yaml

trait SparkEnv {

  def appName(): String

  def configLocation(): String

  lazy val sparkConfig: SparkConf = {
    val sparkProperties: Map[String, String] = config
      .filter({ case (k, v) => k.startsWith("spark.") })
      .map({ case (k, v) =>
        val truncatedKey = k.substring("spark.".length)
        (truncatedKey, decryptValueIfApplicable(truncatedKey, v))
      })

    val kryoClasses = sparkKryoClasses(sparkProperties)
    new SparkConf().setAll(sparkProperties).registerKryoClasses(kryoClasses)
  }

  lazy val appConfig: AppConfig = new AppConfig(config)

  lazy val spark: SparkSession =
    SparkSession
      .builder()
      .master(sparkConfig.get("spark.master", "local"))
      .appName(appName())
      .config(sparkConfig)
      .enableHiveSupport()
      .getOrCreate()

  def configProfile: String = appConfig("configuration-profile", "na")

  private lazy val config: Map[String, Any] = {
    val inputStream = new FileInputStream(configLocation())
    val valuesMap: util.Map[String, Object] = new Yaml().load(inputStream)
    flattenKeys("", valuesMap)
  }

  private def flattenKeys(parentKey: String, value: Any): Map[String, Any] = {
    val keySeparator = if (parentKey.isEmpty) "" else "."
    value match {
      case jMap: util.Map[String, Any] =>
        val sMap = jMap.asScala.toMap
        sMap.flatMap({ case (k, v) => flattenKeys(s"""$parentKey$keySeparator$k""", v) }) ++ Map((parentKey, sMap))
      case jList: util.List[Any] =>
        Map((parentKey, jList.asScala.toList))
      case _ =>
        Map((parentKey, value))
    }
  }

  private def sparkKryoClasses(sparkProperties: Map[String, String]): Array[Class[_]] = try {
    val kryoClassesString = sparkProperties.getOrElse("kryoserializer", "")
    if(kryoClassesString.nonEmpty) {
      kryoClassesString.split(",").map(className => Class.forName(className))
    } else Array()
  } catch {
    case e: Exception =>
      throw new RuntimeException("Error loading kryo classes ", e)
  }

  private def decryptValueIfApplicable(key: String, value: Any): String = try
    key match {
      case "es.net.http.auth.pass" =>
        EncryptionUtils.decrypt(value.toString)
      case "es.xpack.security.user" =>
        val fragments = if (value != null) value.toString.split(":") else Array()
        if (fragments.length > 1) fragments(0) + ":" + EncryptionUtils.decrypt(fragments(1))
        else if (fragments.length > 0) fragments(0)
        else null
      case _ =>
        value.toString
    }
  catch {
    case e@(_: InvalidKeyException | _: NoSuchAlgorithmException | _: NoSuchPaddingException | _: InvalidAlgorithmParameterException | _: IllegalBlockSizeException | _: BadPaddingException) =>
      throw new RuntimeException(e)
  }
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/io: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.net.URI

import com.hsbc.gbm.bd.dataproducts.schema.{DataEntity, JsonSchema, SchemaField, SchemaUtils}
import com.hsbc.gbm.bd.dataproducts.spark.job.{OutputWriterStats, WithOutputWriterStats}
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{col, lit}
import org.slf4j.LoggerFactory

import scala.collection.JavaConverters._
import scala.collection.mutable.ListBuffer

case class AssetPartition(pairs: List[(String, String)]) {

  def hiveSuffix: String = {
    pairs.map(pair => s"${pair._1}='${pair._2}'").mkString(",")
  }

  def hdfsSuffix: String = {
    pairs.map(pair => s"${pair._1}=${pair._2}").mkString("/")
  }
}

class AssetWriter(spark: SparkSession, dbName: String, dataEntity: DataEntity,
                  tableName: Option[String] = Option.empty, baseLocation: String, updateTableSchema: Boolean = false)
  extends WithOutputWriterStats {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  private val hdfsFileSystem = FileSystem.get(new URI("/"), spark.sparkContext.hadoopConfiguration)
  private val actualTableName = if (tableName.isDefined) tableName.get else dataEntity.getDefaultTableName
  private val outputTableName: String = s"$dbName.$actualTableName"
  private val outputTableLocation = s"$baseLocation/$actualTableName"
  private val writtenPartitions: ListBuffer[AssetPartition] = ListBuffer[AssetPartition]()

  override def writerStats: OutputWriterStats = OutputWriterStats(outputTableName, outputCount())

  def writeToTable(records: Dataset[Row]): Unit = {
    val jsonSchema = SchemaUtils.getSchemaOf(dataEntity)
    writeToTable(records, jsonSchema, actualTableName)
  }

  private def outputCount(): Long = {
    if (writtenPartitions.isEmpty) {
      0
    } else {
      val writtenPaths = writtenPartitions.map(partition => s"$outputTableLocation/${partition.hdfsSuffix}")
      spark.read.format("parquet").load(writtenPaths: _*).count()
    }
  }

  private def writeToTable(records: Dataset[Row], jsonSchema: JsonSchema, tableName: String): Unit = {

    val fullTableName = s"$dbName.$tableName"
    val tableLocation = s"$baseLocation/$tableName"
    val partitionColumns = jsonSchema.getPartitionFieldNames.asScala

    val updatedRecords = alignToSchema(records, jsonSchema)

    if(updateTableSchema) {
      checkAndUpdateTableSchema(jsonSchema)
    }

    val datasetPartitions = dropExistingPartitions(records, jsonSchema, tableName)

    logger.info(s"Appending records to table: [$fullTableName] at location [$tableLocation]")

    updatedRecords
      .repartition(partitionColumns.map(col): _*)
      .write
      .partitionBy(partitionColumns: _*)
      .format("parquet")
      .option("path", tableLocation)
      .mode(SaveMode.Append)
      .saveAsTable(fullTableName)

    writtenPartitions.appendAll(datasetPartitions)
  }

  def alignToSchema(records: Dataset[Row], jsonSchema: JsonSchema): Dataset[Row] = {
    val partitionFieldNames = jsonSchema.getPartitionFieldNames.asScala

    val partitionFields = jsonSchema.getFields.asScala
      .filter(field => partitionFieldNames.contains(field.getName))

    val schemaFields = jsonSchema.getFields.asScala
      .filter(field => !partitionFieldNames.contains(field.getName)) ++ partitionFields

    val recordFieldNames = records.schema.fieldNames
    val missingFields = schemaFields.filter(field => !recordFieldNames.contains(field.getName))
    val presentFields = schemaFields.filter(field => recordFieldNames.contains(field.getName))

    val withMissingFields = missingFields
      .foldLeft(records)((df, field) => df.withColumn(field.getName, lit(null).cast(field.getDataType)))

    val withTypeEnforcement = presentFields
      .foldLeft(withMissingFields)((df, field) => df.withColumn(field.getName, col(field.getName).cast(field.getDataType)))

    val schemaColumns = schemaFields.map(field => col(field.getName))

    withTypeEnforcement.select(schemaColumns: _*)
  }

  private def dropExistingPartitions(records: Dataset[Row], jsonSchema: JsonSchema, tableName: String): Seq[AssetPartition] = {
    val partitions: Seq[AssetPartition] = getDatasetPartitions(records, jsonSchema)
    partitions.foreach(partition => dropAssetPartition(dbName, tableName, partition))
    partitions
  }

  private def dropAssetPartition(dbName: String, tableName: String, partition: AssetPartition): Unit = {
    val partitionPath = s"$outputTableLocation/${partition.hdfsSuffix}"

    if (spark.catalog.tableExists(s"$dbName.$tableName")) {
      logger.info(s"Deleting partition if exists: [$partitionPath]")
      spark.sql(s"alter table $dbName.$tableName drop if exists partition (${partition.hiveSuffix})")
      hdfsFileSystem.delete(new org.apache.hadoop.fs.Path(partitionPath), true) // isRecursive = true
    }
  }

  private def getDatasetPartitions(records: Dataset[Row], jsonSchema: JsonSchema): List[AssetPartition] = {
    val fieldNames: List[String] = jsonSchema.getPartitionFieldNames.asScala.toList
    val rows: List[Row] = records
      .select(fieldNames.map(col): _*)
      .distinct()
      .sort(fieldNames.map(col): _*)
      .collectAsList()
      .asScala.toList

    rows.map(row => mapRowToPartition(row, fieldNames))
  }

  private def mapRowToPartition(row: Row, fieldNames: List[String]): AssetPartition = {
    val tuples: List[(String, String)] = fieldNames.map(fieldName => extractFieldValuePair(fieldName, row))
    AssetPartition(tuples)
  }

  private def extractFieldValuePair(fieldName: String, row: Row): (String, String) = {
    val value: Object = row.getAs(fieldName)
    (fieldName, value.toString)
  }

  private def checkAndUpdateTableSchema(jsonSchema: JsonSchema): Unit = {
    logger.info(s"Checking data schema of table [$dbName.$actualTableName]")
    if (spark.catalog.tableExists(dbName, actualTableName)) {
      val existingColumns = spark.catalog.listColumns(dbName, actualTableName).collect().map(_.name)
      val schemaFields = jsonSchema.getFields.asScala
      val outstandingFields = schemaFields
        .filter(field => !existingColumns.contains(field.getName))
      if(outstandingFields.nonEmpty) {
        updateHiveSchema(outstandingFields)
      } else {
        logger.info(s"Table [$dbName.$actualTableName] is up-to-date")
      }
    }
  }

  private def updateHiveSchema(outstandingFields: Seq[SchemaField]): Unit = {
    logger.info(s"Updating data schema of table [$dbName.$actualTableName]")
    val columnAndTypeList = outstandingFields.map(field => s"${field.getName} ${field.getDataType}").mkString(",")
    logger.info(s"Columns to be added [$columnAndTypeList]")
    val sqlStatement = s"alter table $dbName.$actualTableName add columns ($columnAndTypeList)"
    logger.info(s"Executing sql statement [$sqlStatement]")
    spark.sql(sqlStatement)
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import com.hsbc.gbm.bd.dataproducts.spark.param.RawJobParams
import org.json4s.DefaultFormats
import org.json4s.jackson.JsonMethods

import scala.io.Source

case class MdaSource(source: String, asset: String, path: String)

class MdaLocationResolver(endpointUrl: String, rawJobParams: RawJobParams) {

  private implicit val formats: DefaultFormats.type = DefaultFormats

  def getAssetLocationsFromCommandLine: Map[String, String] = {
    rawJobParams.pairs
      .filter(_._1.trim.nonEmpty)
      .filter(_._1.endsWith("MdaLocation"))
      .map(pair => (paramToAssetType(pair._1), pair._2))
      .toMap
  }

  private def paramToAssetType(param: String): String = {
    param
      .substring(0, param.indexOf("MdaLocation"))
      .split("(?=\\p{Lu})").mkString("-").toLowerCase
  }

  def getAssetLocations(assetTypes: Set[String]): Map[String, String] = {
    val cmdLineLocations = getAssetLocationsFromCommandLine

    val outstandingAssetTypes = assetTypes.filter(assetType => ! cmdLineLocations.contains(assetType))

    if(outstandingAssetTypes.isEmpty) {
      cmdLineLocations
    } else {
      getLocationsFromEndpoint(outstandingAssetTypes) ++ cmdLineLocations
    }
  }

  def getLocationsFromEndpoint(assetTypes: Set[String]): Map[String, String] = {

    val response = get(endpointUrl)

    val json = JsonMethods.parse(response)

    val sources = (json \ "sources").extract[List[MdaSource]]

    val level1BestRef = (json \ "level1BestRefPath").extract[String]

    val allSources = sources ++ List(MdaSource("level1-best-ref", "level1-best-ref", level1BestRef))

    allSources
      .filter(mdaSource => assetTypes.contains(mdaSource.asset))
      .map(mdaSource => (mdaSource.asset, mdaSource.path))
      .toMap
  }

  @throws(classOf[java.io.IOException])
  @throws(classOf[java.net.SocketTimeoutException])
  private def get(url: String,
                  connectTimeout: Int = 30000,
                  readTimeout: Int = 30000,
                  requestMethod: String = "GET") = {
    import java.net.{HttpURLConnection, URL}
    val connection = new URL(url).openConnection.asInstanceOf[HttpURLConnection]
    connection.setConnectTimeout(connectTimeout)
    connection.setReadTimeout(readTimeout)
    connection.setRequestMethod(requestMethod)
    val inputStream = connection.getInputStream
    val content = Source.fromInputStream(inputStream).mkString
    if (inputStream != null) inputStream.close()
    content
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.util.concurrent.atomic.AtomicLong

import com.hsbc.gbm.bd.dataproducts.spark.job.{InputReaderStats, WithInputReaderStats}
import com.hsbc.gbm.bd.dataproducts.spark.param.RawJobParams
import com.hsbc.gbm.bd.dataproducts.spark.utils.LoggingUtils
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.slf4j.LoggerFactory

class MdaTypeParquetReader(spark: SparkSession, mdaType: String,
                           resolverEndpointUrl: String, rawJobParams: RawJobParams) extends WithInputReaderStats {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  private val inputCount: AtomicLong = new AtomicLong(0)
  private val mdaLocationResolver = new MdaLocationResolver(resolverEndpointUrl, rawJobParams)
  private val inputLocation: String = {
    val location = mdaLocationResolver.getAssetLocations(Set(mdaType)).getOrElse(mdaType, "")
    if (location == "") {
      logger.error(s"Unable to retrieve location for MDA type [$mdaType]")
      throw new RuntimeException(s"Unable to retrieve location for MDA type [$mdaType]")
    }
    location
  }

  override def readerStats: InputReaderStats = InputReaderStats(Set(inputLocation), inputCount.get())

  def read(runDateFrom: Option[String] = Option.empty,
           runDateTo: Option[String] = Option.empty): DataFrame = {

    logger.info(s"""Reading input from: [$inputLocation]""")

    val input = spark.read.parquet(inputLocation)

    val filteredInput = (runDateFrom, runDateTo) match {
      case (from, to) if from.isDefined && to.isDefined =>
        logger.info(s"Reading for date range: [$runDateFrom-$runDateTo]")
        input.filter(col("run_date").geq(runDateFrom.get).and(col("run_date").leq(runDateTo.get)))
      case (from, to) if from.isDefined && to.isEmpty =>
        logger.info(s"Reading for dates equal to or after: [$runDateFrom]")
        input.filter(col("run_date").geq(runDateFrom.get))
      case (from, to) if from.isEmpty && to.isDefined =>
        logger.info(s"Reading for dates before or equal to: [$runDateTo]")
        input.filter(col("run_date").leq(runDateTo.get))
      case _ =>
        logger.info("Reading complete date range")
        input
    }

    inputCount.addAndGet(filteredInput.count)

    LoggingUtils.showDataset("Showing input dataset", filteredInput)

    filteredInput
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.util.concurrent.atomic.AtomicLong

import com.hsbc.gbm.bd.dataproducts.spark.job.{InputReaderStats, WithInputReaderStats}
import com.hsbc.gbm.bd.dataproducts.spark.utils.LoggingUtils
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.{Dataset, Row, SparkSession}
import org.slf4j.LoggerFactory

class NamedInputParquetReader(spark: SparkSession, inputName: String, inputLocationPrefix: String) extends WithInputReaderStats {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  private val inputCount: AtomicLong = new AtomicLong(0)
  private val inputLocation: String = s"""$inputLocationPrefix/$inputName/$inputName.parq"""

  override def readerStats: InputReaderStats = InputReaderStats(Set(inputLocation), inputCount.get())

  def read(runDateFrom: String, runDateTo: String): Dataset[Row] = {

    logger.info(s"""Reading input from: [$inputLocation]""")

    val input = spark
      .read
      .parquet(inputLocation)
      .filter(col("run_date").geq(runDateFrom).and(col("run_date").leq(runDateTo)))

    inputCount.addAndGet(input.count)

    LoggingUtils.showDataset("Showing input dataset", input)

    input
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.net.URI

import com.hsbc.gbm.bd.dataproducts.spark.job.{OutputWriterStats, WithOutputWriterStats}
import com.hsbc.gbm.bd.dataproducts.spark.utils.LoggingUtils
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}
import org.slf4j.LoggerFactory

import scala.collection.JavaConverters._
import scala.collection.mutable.ListBuffer


case class HdfsPartition(pairs: Seq[(String, String)]) {

  def hiveSuffix: String = {
    pairs.map(pair => s"${pair._1}='${pair._2}'").mkString(",")
  }

  def hdfsSuffix: String = {
    pairs.map(pair => s"${pair._1}=${pair._2}").mkString("/")
  }
}

class NamedOutputParquetWriter(spark: SparkSession, outputName: String,
                               outputLocationPrefix: String, partitionColumns: Seq[String]) extends WithOutputWriterStats {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  private val outputLocation: String = s"""$outputLocationPrefix/$outputName/$outputName.parq"""
  private val hdfsFileSystem = FileSystem.get(new URI("/"), spark.sparkContext.hadoopConfiguration)
  private val writtenPartitions: ListBuffer[HdfsPartition] = ListBuffer[HdfsPartition]()

  override def writerStats: OutputWriterStats = OutputWriterStats(outputLocation, outputCount())

  private def outputCount(): Long = {
    if(writtenPartitions.isEmpty) {
      0
    } else {
      val writtenPaths = writtenPartitions.map(partition => s"$outputLocation/${partition.hdfsSuffix}")
      spark.read.format("parquet").load(writtenPaths: _*).count()
    }
  }

  def write(output: Dataset[Row]): Unit = {

    LoggingUtils.showDataset("Showing output dataset", output)

    logger.info(s"Deleting overlapping partitions...")
    val datasetPartitions = dropExistingPartitions(output, partitionColumns, outputLocation)

    logger.info(s"Writing output to: [$outputLocation]")

    val columnList = partitionColumns.map(col)

    output
      .repartition(columnList: _*)
      .write
      .partitionBy(partitionColumns: _*)
      .mode(SaveMode.Append)
      .parquet(outputLocation)

    writtenPartitions.appendAll(datasetPartitions)
  }

  private def dropExistingPartitions(records: Dataset[Row],
                                     partitionColumns: Seq[String],
                                     baseLocation: String): Seq[HdfsPartition] = {
    val partitions: List[HdfsPartition] = getDatasetPartitions(records, partitionColumns)
    partitions.foreach(partition => dropHdfsPartition(baseLocation, partition))
    partitions
  }

  private def dropHdfsPartition(baseLocation: String, partition: HdfsPartition): Unit = {
    val partitionPath = s"$baseLocation/${partition.hdfsSuffix}"
    logger.info(s"Deleting partition if exists: [$partitionPath]")
    hdfsFileSystem.delete(new org.apache.hadoop.fs.Path(partitionPath), true) // isRecursive = true
  }

  private def getDatasetPartitions(records: Dataset[Row], partitionColumns: Seq[String]): List[HdfsPartition] = {
    val rows: List[Row] = records
      .select(partitionColumns.map(col): _*)
      .distinct()
      .sort(partitionColumns.map(col): _*)
      .collectAsList()
      .asScala.toList

    rows.map(row => mapRowToPartition(row, partitionColumns))
  }

  private def mapRowToPartition(row: Row, partitionColumns: Seq[String]): HdfsPartition = {
    val tuples: Seq[(String, String)] = partitionColumns.map(columnName => extractFieldValuePair(columnName, row))
    HdfsPartition(tuples)
  }

  private def extractFieldValuePair(columnName: String, row: Row): (String, String) = {
    val value: Object = row.getAs(columnName)
    (columnName, value.toString)
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.util.concurrent.atomic.AtomicLong

import com.hsbc.gbm.bd.dataproducts.spark.job.{InputReaderStats, WithInputReaderStats}
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.{Column, Dataset, Row, SparkSession}

import scala.collection.mutable

class TableInputReader(spark: SparkSession, countedSources: String*) extends WithInputReaderStats {

  private val sources: mutable.Set[String] = mutable.Set()
  private val count: AtomicLong = new AtomicLong(0)

  override def readerStats: InputReaderStats = InputReaderStats(sources.toSet, count.get())

  def read(tableName: String, runDateFrom: Option[String], runDateTo: Option[String]): Dataset[Row] = {
    readWithColumnMappings(tableName, runDateFrom, runDateTo, Option.empty)
  }

  def readWithColumnList(tableName: String, runDateFrom: Option[String], runDateTo: Option[String],
                         columnNames: Option[List[String]]): Dataset[Row] = {
    val columnMappings: Option[Map[String, String]] =
      if (columnNames.isDefined) Option(columnNames.get.map(name => (name, name)).toMap)
      else Option.empty
    readWithColumnMappings(tableName, runDateFrom, runDateTo, columnMappings)
  }

  def readWithColumnMappings(tableName: String, runDateFrom: Option[String], runDateTo: Option[String],
                             columnMappings: Option[Map[String, String]]): Dataset[Row] = {

    val sqlStatement =
      if (runDateFrom.isDefined && runDateTo.isDefined)
        s"select * from $tableName where run_date >= '${runDateFrom.get}' and run_date <= '${runDateTo.get}' "
      else if (runDateFrom.isDefined)
        s"select * from $tableName where run_date >= '${runDateFrom.get}' "
      else if (runDateTo.isDefined)
        s"select * from $tableName where run_date <= '${runDateTo.get}' "
      else
        s"select * from $tableName"

    val frame = spark.sql(sqlStatement)

    val finalFrame = if (columnMappings.isDefined) {
      val selectColumns: Seq[Column] = columnMappings.get.values.map(col).toSeq
      columnMappings.get.foldLeft(frame)((df, columnMapping) => df.withColumn(columnMapping._2, col(columnMapping._1)))
        .select(selectColumns: _*)
    } else frame

    sources.add(tableName)

    if (countedSources.contains(tableName)) {
      count.addAndGet(finalFrame.count())
    }
    finalFrame
  }
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/job: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.job

case class InputReaderStats(sources: Set[String], sourceCount: Long)
package com.hsbc.gbm.bd.dataproducts.spark.job

case class OutputWriterStats(outputLocation: String, outputCount: Long)
package com.hsbc.gbm.bd.dataproducts.spark.job

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.param.RunDateRange

trait SparkJob extends SparkEnv with WithInputOutputStats {

  lazy val uuid: String = java.util.UUID.randomUUID.toString

  def process(): Map[String, String]

  def close(): Unit = spark.close()

  def runDateRange: Option[RunDateRange] = Option.empty

  def component: String = getClass.getSimpleName

  override def readerStats: Option[InputReaderStats] = Option.empty

  override def writerStats: Option[OutputWriterStats] = Option.empty

  def applicationId: String = spark.sparkContext.applicationId
}
package com.hsbc.gbm.bd.dataproducts.spark.job

trait WithInputOutputStats {
  def readerStats: Option[InputReaderStats]

  def writerStats: Option[OutputWriterStats]
}
package com.hsbc.gbm.bd.dataproducts.spark.job

trait WithInputReaderStats {
  def readerStats: InputReaderStats
}
package com.hsbc.gbm.bd.dataproducts.spark.job

trait WithOutputWriterStats {
  def writerStats: OutputWriterStats
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/lifecycle: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.lifecycle

import com.hsbc.gbm.bd.dataproducts.spark.lifecycle

object EndState extends Enumeration {
  type EndState = Value
  val Success: lifecycle.EndState.Value = Value("success")
  val Failure: lifecycle.EndState.Value = Value("failure")
}
package com.hsbc.gbm.bd.dataproducts.spark.lifecycle

object EventType extends Enumeration {
  type EventType = Value
  val Started, Finished = Value
}
package com.hsbc.gbm.bd.dataproducts.spark.lifecycle

import java.time.LocalDateTime

import com.hsbc.gbm.bd.dataproducts.spark.job.{InputReaderStats, OutputWriterStats}
import com.hsbc.gbm.bd.dataproducts.spark.lifecycle.EndState.EndState
import com.hsbc.gbm.bd.dataproducts.spark.lifecycle.EventType.EventType
import com.hsbc.gbm.bd.dataproducts.spark.param.{RawJobParams, RunDateRange}

case class JobLifecycleEvent(jobId: String,
                             eventDateTime: LocalDateTime,
                             eventType: EventType,
                             endState: Option[EndState],
                             component: String,
                             configProfile: String,
                             readerStats: Option[InputReaderStats],
                             writerStats: Option[OutputWriterStats],
                             jobParams: RawJobParams,
                             runDateRange: Option[RunDateRange],
                             tags: Map[String, String])
package com.hsbc.gbm.bd.dataproducts.spark.lifecycle

import akka.actor.Actor
import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv

abstract class LifecycleEventListener(sparkEnv: SparkEnv) extends Actor
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/monitoring: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

case class MonitoringEntry(uid: String,
                           startTime: Option[String],
                           endTime: Option[String],
                           sources: List[String] = List(),
                           outputLocation: Option[String],
                           sourceCount: Long,
                           outputCount: Long,
                           tags: List[String] = List(),
                           component: Option[String],
                           user: Option[String],
                           runDate: Option[String],
                           status: Option[String],
                           client: Option[String]) {

}
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

import java.time.format.DateTimeFormatter
import java.time.{ZoneId, ZonedDateTime}

import com.google.common.collect.Lists
import com.hsbc.gbm.bd.dataproducts.spark.env.AppConfig
import org.apache.parquet.Strings
import org.apache.spark.sql.{RowFactory, SaveMode, SparkSession}
import org.slf4j.LoggerFactory

class MonitoringEntryWriter(spark: SparkSession, appConfig: AppConfig) {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)
  private val dateFormat = DateTimeFormatter.ofPattern("yyyyMMdd HH:mm:ssZ")
  private val location: String = appConfig("monitoring.location")
  private val tableName: String = appConfig("monitoring.table.name")

  def save(entry: MonitoringEntry): Unit = {

    val row = RowFactory.create(
      entry.uid,
      if (entry.startTime.isEmpty) Option(ZonedDateTime.now(ZoneId.systemDefault()).format(dateFormat)) else entry.startTime.get,
      if (entry.endTime.isEmpty) Option(ZonedDateTime.now(ZoneId.systemDefault()).format(dateFormat)) else entry.endTime.get,
      Strings.join(entry.sources.toArray, ","),
      if(entry.outputLocation.isEmpty) "N/A" else entry.outputLocation.get,
      entry.sourceCount.asInstanceOf[Object],
      entry.outputCount.asInstanceOf[Object],
      Strings.join(entry.tags.toArray, ","),
      if(entry.component.isEmpty) "N/A" else entry.component.get,
      if(entry.user.isEmpty) Option(System.getProperty("user.name")) else entry.user.get,
      if(entry.runDate.isEmpty) "N/A" else entry.runDate.get,
      if(entry.status.isEmpty) "N/A" else entry.status.get,
      if(entry.client.isEmpty) "N/A" else entry.client.get)

    logger.info(s"Writing monitoring row with uid: [${entry.uid}] to [$location]")

    val ds = spark.createDataFrame(Lists.newArrayList(row), MonitoringSchema.schema())

    logger.info("Monitoring entry created")

    ds.show(false)

    ds.write
      .format("parquet")
      .option("path", location)
      .mode(SaveMode.Append)
      .saveAsTable(tableName)
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

import java.sql.Timestamp

import akka.actor.typed.scaladsl.Behaviors
import com.hsbc.gbm.bd.dataproducts.spark.annotation.LifecycleEventListenerName
import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.lifecycle.{EventType, JobLifecycleEvent, LifecycleEventListener}
import org.slf4j.LoggerFactory

@LifecycleEventListenerName("monitoring-hdfs-agent")
class MonitoringHdfsAgent(sparkEnv: SparkEnv) extends LifecycleEventListener(sparkEnv) {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  override def receive: Receive = {
    updated(list = List())
  }

  private def updated(list: List[JobLifecycleEvent]): Receive = {
    case event: JobLifecycleEvent if event.eventType == EventType.Started =>
      logger.info("Received Started lifecycle event: [{}]", event)
      context.become(updated(event :: list))
      Behaviors.same
    case event: JobLifecycleEvent if event.eventType == EventType.Finished =>
      logger.info("Received Finished lifecycle event: [{}]", event)
      processEvents(event :: list)
    case event: Any =>
      logger.info("Ignoring unsupported event class [{}]", event)
      throw new RuntimeException("Unsupported event class")
  }

  private def processEvents(events: List[JobLifecycleEvent]): Unit = {
    val startEvent = events.filter(event => event.eventType == EventType.Started).head
    val finishEvent = events.filter(event => event.eventType == EventType.Finished).head
    writeMonitoringRecord(startEvent, finishEvent)
  }

  private def writeMonitoringRecord(startEvent: JobLifecycleEvent, finishEvent: JobLifecycleEvent): Unit = {
    val appConfig = sparkEnv.appConfig
    val monitoringLocation: String = appConfig("monitoring.location")
    val monitoringTable: String = appConfig("monitoring.table.name")
    val monitoringClient: String = appConfig("monitoring.client", "data-products-1")

    //    val monitoringEntryBuilder = new MonitoringEntryBuilder(monitoringLocation, monitoringTable).withUid(finishEvent.jobId)

    val readerStats = finishEvent.readerStats
    val writerStats = finishEvent.writerStats
    val runDateRange = finishEvent.runDateRange
    val jobStartTime = Timestamp.valueOf(startEvent.eventDateTime)
    val jobFinishTime = Timestamp.valueOf(finishEvent.eventDateTime)
    val tags = finishEvent.tags
      .map(entry => s"${entry._1}=${entry._2}")
      .toList

    val monitoringEntry = MonitoringEntry(
      uid = finishEvent.jobId,
      startTime = Option(jobStartTime.toString),
      endTime = Option(jobFinishTime.toString),
      sources = if (readerStats.isDefined) readerStats.get.sources.toList else List(),
      outputLocation = if (writerStats.isDefined) Option(writerStats.get.outputLocation) else Option("N/A"),
      sourceCount = if (readerStats.isDefined) readerStats.get.sourceCount else -1L,
      outputCount = if (writerStats.isDefined) writerStats.get.outputCount else -1L,
      tags = tags,
      component = Option(finishEvent.component),
      user = Option.empty,
      runDate = if (runDateRange.isDefined) Option(runDateRange.get.toString) else Option("N/A"),
      status = if (finishEvent.endState.isDefined) Option(finishEvent.endState.get.toString) else Option("N/A"),
      client = Option(s"$monitoringClient-${sparkEnv.configProfile}"))

    val monitoringEntryWriter = new MonitoringEntryWriter(sparkEnv.spark, sparkEnv.appConfig)
    monitoringEntryWriter.save(monitoringEntry)

    logger.info("Monitoring record has been persisted")

    Behaviors.stopped
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

import org.apache.spark.sql.types.{DataTypes, StructField, StructType}

object MonitoringSchema {

  def schema(): StructType = DataTypes.createStructType(
    Array[StructField](
      DataTypes.createStructField("uid", DataTypes.StringType, true),
      DataTypes.createStructField("start_time", DataTypes.StringType, true),
      DataTypes.createStructField("end_time", DataTypes.StringType, true),
      DataTypes.createStructField("sources", DataTypes.StringType, true),
      DataTypes.createStructField("output_location", DataTypes.StringType, true),
      DataTypes.createStructField("source_count", DataTypes.LongType, true),
      DataTypes.createStructField("output_count", DataTypes.LongType, true),
      DataTypes.createStructField("tags", DataTypes.StringType, true),
      DataTypes.createStructField("component", DataTypes.StringType, true),
      DataTypes.createStructField("user", DataTypes.StringType, true),
      DataTypes.createStructField("run_date", DataTypes.StringType, true),
      DataTypes.createStructField("status", DataTypes.StringType, true),
      DataTypes.createStructField("client", DataTypes.StringType, true)))
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/param: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.param

case class RawJobParams(rawParams: Array[String]) {

  val pairs: Array[(String, String)] = rawParams.map(param => toKeyValuePair(param))

  def apply(position: Int): String = {
    if (pairs.length > position) pairs(position)._2 else null
  }

  def apply(name: String): String = {
    val found = pairs.find({ case (k, v) => k.equals(name) })
    if(found.isDefined) found.get._2 else null
  }

  private def toKeyValuePair(param: String): (String, String) = {
    val fragments = param.split("=")
    if (fragments.length == 1) ("", fragments.head) else (fragments(0), fragments(1))
  }

  override def toString: String = {
    val paramString = pairs.map(pair => if(pair._1 == "") s"${pair._2}" else  s"${pair._1}=${pair._2}").mkString(",")
    s"RawJobParams[$paramString]"
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.param

case class RunDateRange(runDateFrom: String, runDateTo: String) {

  override def toString: String = {
    if (runDateFrom.equals(runDateTo)) runDateFrom else s"""$runDateFrom-$runDateTo"""
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.param

import java.time.LocalDate
import java.time.format.{DateTimeFormatter, DateTimeParseException}

object RunDateRangeParamParser {

  def parse(params: RawJobParams): RunDateRange = {

    val runDateFrom =
      if (params("runDateFrom") != null) params("runDateFrom")
      else if (params(0) != null) params(0)
      else
        throw new RuntimeException("runDateFrom is required")

    val runDateTo =
      if (params("runDateTo") != null) params("runDateTo")
      else if(params(1) != null) params(1)
      else runDateFrom

    val dateFrom = parseDate(runDateFrom, "runDateFrom")
    val dateTo = parseDate(runDateTo, "runDateTo")

    if (dateFrom.isAfter(dateTo))
      throw new RuntimeException(s"""'runDateTo' [$runDateTo] should be same or after 'runDateFrom' [$runDateFrom]""")

    RunDateRange(runDateFrom, runDateTo)
  }

  private def parseDate(dateString: String, paramName: String): LocalDate = {
    val dateFormatter = DateTimeFormatter.ofPattern("yyyyMMdd")
    try {
      LocalDate.parse(dateString, dateFormatter)
    } catch {
      case e: DateTimeParseException =>
        throw new RuntimeException(s"""Invalid format of '$paramName': [$dateString]. Expected format: yyyyMMdd""")
    }
  }
}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/runner: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.runner

import java.time.LocalDateTime

import akka.actor.{ActorSystem, Props}
import com.hsbc.gbm.bd.dataproducts.spark.annotation.{LifecycleEventListenerName, SparkJobName}
import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.job.SparkJob
import com.hsbc.gbm.bd.dataproducts.spark.lifecycle.{EndState, EventType, JobLifecycleEvent, LifecycleEventListener}
import com.hsbc.gbm.bd.dataproducts.spark.param.RawJobParams
import io.github.classgraph.ClassGraph

import scala.collection.JavaConversions._
import scala.concurrent.Await
import scala.concurrent.duration._
import scala.language.postfixOps

class GenericSparkJobRunner(jobName: String, configPath: String, params: Array[String]) {

  protected def closeJob(job: SparkJob): Unit = {
    job.close()
  }

  protected def run(): Unit = {
    val jobClassName = jobMappings()(jobName)
    val jobClass = Thread.currentThread.getContextClassLoader.loadClass(jobClassName)
    runScalaJob(jobClass.asInstanceOf[Class[SparkJob]])
  }

  protected def runScalaJob(jobClass: Class[SparkJob]): Unit = {

    val jobParams: RawJobParams = RawJobParams(params)

    val sparkJob: SparkJob = instantiateSparkJob(jobClass, jobParams)
    val sparkEnv = sparkJob
    val listeners: Map[String, Class[_]] = getEventListenerClasses(sparkEnv)

    val system: ActorSystem = ActorSystem("lifecycle-event-system")
    val actorMatchPattern = "/user/*"
    val actorRefs = listeners
      .map({ case (name, class_) => system.actorOf(Props(class_, sparkEnv), name) })
      .toList

    try {
      val startedEvent = createStartedEvent(sparkJob, jobParams)
      system.actorSelection(actorMatchPattern) ! startedEvent

      val tags = sparkJob.process() ++ Map(
        "implementation_title" -> sparkJob.getClass.getPackage.getImplementationTitle,
        "implementation_version" -> sparkJob.getClass.getPackage.getImplementationVersion,
        "configuration_profile" -> sparkJob.configProfile,
        "job_name" -> sparkJob.appName,
        "application_id" -> sparkJob.applicationId
      )

      val finishedEvent = createFinishedEvent(sparkJob, jobParams, EndState.Success, tags, Option.empty)
      system.actorSelection(actorMatchPattern) ! finishedEvent
    } catch {
      case e: Throwable =>
        val finishedEvent = createFinishedEvent(sparkJob, jobParams, EndState.Failure, Map(), Option(e.getMessage))
        system.actorSelection(actorMatchPattern) ! finishedEvent
        throw e
    } finally {
      Thread.sleep(5000) //Give a chance to agents to receive last event
      system.terminate()
      Await.result(system.whenTerminated, 600 seconds)
      closeJob(sparkJob)
    }
  }

  private def createStartedEvent(sparkJob: SparkJob, jobParams: RawJobParams): JobLifecycleEvent = {
    JobLifecycleEvent(
      sparkJob.uuid,
      LocalDateTime.now(),
      EventType.Started,
      Option.empty,
      sparkJob.component,
      sparkJob.configProfile,
      Option.empty,
      Option.empty,
      jobParams,
      sparkJob.runDateRange,
      Map("job_name" -> sparkJob.appName())
    )
  }

  private def createFinishedEvent(sparkJob: SparkJob, jobParams: RawJobParams,
                                  endState: EndState.EndState, jobTags: Map[String, String],
                                  errorMessage: Option[String]): JobLifecycleEvent = {

    val errorMessageTag =
      if (errorMessage.isDefined) {
        val truncatedMessage = errorMessage.get.split("\n")(0).replaceAll(",", ";")
        Seq(("error_message", truncatedMessage))
      } else Nil

    val allTags = jobTags.toList ++ errorMessageTag

    JobLifecycleEvent(
      sparkJob.uuid,
      LocalDateTime.now(),
      EventType.Finished,
      Option(endState),
      sparkJob.component,
      sparkJob.configProfile,
      sparkJob.readerStats,
      sparkJob.writerStats,
      jobParams,
      sparkJob.runDateRange,
      Map(allTags: _*)
    )
  }

  private def instantiateSparkJob(jobClass: Class[SparkJob], jobParams: RawJobParams): SparkJob = {
    jobClass
      .getConstructors
      .filter(_.getParameterTypes.size == 3)
      .find(constructor => {
        constructor.getParameterTypes()(0).isAssignableFrom(classOf[String]) &&
          constructor.getParameterTypes()(1).isAssignableFrom(classOf[String]) &&
          constructor.getParameterTypes()(2).isAssignableFrom(classOf[RawJobParams])
      })
      .get
      .newInstance(jobName, configPath, jobParams)
      .asInstanceOf[SparkJob]
  }

  private def getEventListenerClasses(sparkEnv: SparkEnv): Map[String, Class[_]] = {
    val appConfig = sparkEnv.appConfig
    val listenerScanPackages: List[String] = appConfig("spark-job-runner.lifecycle-listener-packages",
      List("com.hsbc.gbm.bd.dataproducts"))
    val enabledListenerNames: List[String] = appConfig("spark-job-runner.lifecycle-listener-names", List())
    val listenerClassMappings: Map[String, String] = getEnabledListenerClasses(listenerScanPackages, enabledListenerNames)
    listenerClassMappings
      .map({ case (name, className) => (name, Thread.currentThread.getContextClassLoader.loadClass(className)) })
      .toMap
  }

  protected def jobMappings(): Map[String, String] = {
    val scanResult = new ClassGraph().enableAllInfo.acceptPackages("com.hsbc.gbm.bd").scan
    val classInfoList = scanResult.getClassesImplementing(classOf[SparkJob].getName)
    classInfoList.toList
      .map(classInfo => (classInfo.getAnnotationInfo(classOf[SparkJobName].getName), classInfo.getName))
      .filter(_._1 != null)
      .map(tuple => (tuple._1.getParameterValues.getValue("value").toString, tuple._2))
      .toMap
  }

  private def getEnabledListenerClasses(packages: List[String], names: List[String]): Map[String, String] = {
    if (packages == null || names == null) {
      Map()
    } else {
      val scanResult = new ClassGraph().enableAllInfo.acceptPackages(packages: _*).scan
      val classInfoList = scanResult.getSubclasses(classOf[LifecycleEventListener].getName)
      classInfoList.toList
        .map(classInfo => (classInfo.getAnnotationInfo(classOf[LifecycleEventListenerName].getName), classInfo.getName))
        .filter(_._1 != null)
        .map(tuple => (tuple._1.getParameterValues.getValue("value").toString, tuple._2))
        .filter(tuple => names.contains(tuple._1))
        .toMap
    }
  }
}

object GenericSparkJobRunner {

  def main(args: Array[String]): Unit = {
    val jobName = args(0)
    val configPath = args(1)
    val params = args.slice(from = 2, until = args.length)

    new GenericSparkJobRunner(jobName, configPath, params).run()
  }

}



package com.hsbc.gbm.bd.dataproducts.spark.runner

import com.hsbc.gbm.bd.dataproducts.spark.job.SparkJob

class GenericSparkJobTestRunner(jobName: String, configPath: String, params: Array[String])
  extends GenericSparkJobRunner(jobName, configPath, params) {

  override def closeJob(job: SparkJob): Unit = {}

}


object GenericSparkJobTestRunner {

  def main(args: Array[String]): Unit = {
    val jobName = args(0)
    val configPath = args(1)
    val params = args.slice(from = 2, until = args.length)

    new GenericSparkJobTestRunner(jobName, configPath, params).run()
  }

}
cat: ./src/main/scala/com/hsbc/gbm/bd/dataproducts/spark/utils: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.utils

import org.apache.parquet.Strings
import org.apache.spark.sql.{Dataset, Row}
import org.slf4j.LoggerFactory

object LoggingUtils {

  private val logger = LoggerFactory.getLogger("com.hsbc.gbm.bd.dataproducts.spark.utils.LoggingUtils")

  def showDataset(message: String, dataset: Dataset[Row], maxNumberOfRows: Integer): Unit = {
    if (logger.isDebugEnabled) {
      if (!Strings.isNullOrEmpty(message)) logger.info(message)
      logger.info("Row count: [{}]", dataset.count)

      if (maxNumberOfRows != null) dataset.show(maxNumberOfRows, false)
      else dataset.show(false)
    }
  }

  def showDataset(message: String, dataset: Dataset[Row]): Unit = {
    showDataset(message, dataset, null)
  }

  def showDataset(dataset: Dataset[Row]): Unit = {
    showDataset(null, dataset, null)
  }

}
cat: ./src/test: Is a directory
cat: ./src/test/resources: Is a directory
spark:
  spark.master: "local"
  spark.local.dir : "tmp/spark-temp"
  spark.sql.catalogImplementation: hive
  spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
  spark.serializer: org.apache.spark.serializer.KryoSerializer
  derby.system.home: "tmp/hive/"
  javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
  #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow
app:
  loaded-from: "classpath"
  app_name: "dataproducts-common"
  monitoring:
    location: "tmp/monitoring"
    table-name: "monitoring"
  another-key:
    list:
      - item1
      - item2
      - item3
cat: ./src/test/resources/config: Is a directory
spark:
    spark.master: "local"
    spark.local.dir: "{workDirectory}/tmp/spark-temp"
    spark.sql.catalogImplementation: hive
    spark.sql.warehouse.dir: "{workDirectory}/tmp/hive/warehouse"
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    derby.system.home: "{workDirectory}/tmp/hive/"
    javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName={workDirectory}/tmp/hive/metastore_db;create=true"
    #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow

app:
    app_name: "dataproducts-common"
    configuration-profile: dev
    monitoring:
        location: "{workDirectory}/tmp/monitoring"
        table:
            name: "test_db.monitoring"
        client: "data-products-1-common"
    table-name: "test_db.trade"
    output-location: "src/test/resources/out/asset"
    spark-job-runner:
        lifecycle-listener-packages:
            - com.hsbc.gbm.bd.dataproducts.spark.monitoring
        lifecycle-listener-names:
            - monitoring-hdfs-agent
cat: ./src/test/resources/configuration: Is a directory
spark:
  spark.master: "local"
  spark.local.dir : "tmp/spark-temp"
  spark.sql.catalogImplementation: hive
  spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
  spark.serializer: org.apache.spark.serializer.KryoSerializer
  derby.system.home: "tmp/hive/"
  javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
  #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow
app:
  app_name: "dataproducts-common"
  monitoring:
    location: "tmp/monitoring"
    table-name: "monitoring"
  another-key:
    list:
      - item1
      - item2
      - item3
  spark:
    - spark.master: "local"
    - spark.local.dir : "tmp/spark-temp"
    - spark.sql.catalogImplementation: hive
    - spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
    - spark.serializer: org.apache.spark.serializer.KryoSerializer
    - derby.system.home: "tmp/hive/"
    - javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
    #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow
  app:
     - app_name: "dataproducts-common"
     - monitoring.location: "tmp/monitoring"
     - monitoring.table.name: "monitoring"
  elastic:
     - es.nodes: localhost:9200
     - es.net.http.auth.user: elastic
     - es.net.http.auth.pass: NONENC_changeme
     - es.port: 9200
     - es.index.auto.create: true
     - cluster.name: elasticsearch
     - client.transport.sniff: true
     - client.transport.ignore_cluster_name: true
     - es.xpack.security.user: elastic:byiF7KeOP9EtCtkmgMlyttOWcO55La2WasVQb1oKnss=
     - index.auto.create: true
     - index.number_of_replicas: 0
     - index.refresh_interval: 1s
  encryption:
     - password.need.to.encrypt : changeme

cat: ./src/test/resources/data: Is a directory
cat: ./src/test/resources/data/io: Is a directory
trade_identifier|trade_identifier_source|originating_system|originating_system_trade_id|version|associate_trade_identifier|event|status|execution_timestamp|capture_date|trade_date|settlement_date|maturity_date|product_type|product_family|isda_plus_taxonomy|security_type|security_hsbc_id|security_bloomberg_id|security_isin|security_cusip|security_sedol|security_ric|underlying_security_id|underlying_security_name|buy_amount|buy_currency|sell_amount|sell_currency|settlement_amount|settlement_currency|buy_sell_indicator|margin_account|fixing_date|quantity|trade_price|hsbc_entity|hsbc_entity_po|book_name_source|book_id_source|cpty_treats_acronym|cpty_grid|cpty_ber_id|executing_broker|customer_identifier|option_type|option_length|option_strike_price|option_premium|settlement_delivery_method|trader_id|trader_peoplesoft_id|trade_type|composite_id|composite_product_code|hsbc_entity_country|trader_name|sales_person|processing_timestamp|trade_tresata_id|cpty_tresata_id|executing_broker_tresata_id|troux_id_source|booking_country|trade_hti|clean_price|dirty_price|bd_dp_lineage|book_id_hms|execution_type|execution_facility|clearing_broker|clearing_broker_tresata_id|ratesense|primary_currency|counter_currency|primary_value_date|counter_value_date|primary_option_end_date|counter_option_end_date|option_end_date|original_trade_id|spot_date|swap_points|exchange_id|mkt_fwd_rate|corporate_margin|sales_margin|spot_rate|branch|cpty_name|cpty_treats_parent_acronym|run_date|physical_source
SPT192389984|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 09:08:40|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||5779592|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|122|AARC WEST MECH INS (P)||20190829|TREATS_FWD
SPT192389985|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 09:08:41|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||6551742|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|100|AARC W MECH INS 20 CRISP ADJ|AARC|20190830|TREATS_FWD
SPT192389986|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 19:08:42|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||6421758|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|101|ABCELLERA BIOLOGICS INC 270|ABCG|20190829|TREATS_SPOT
SPT192389987|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 19:08:43|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||2473672|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|719|ABBOTT LABORTORIES||20190830|TREATS_SPOTtrade_identifier|trade_identifier_source|originating_system|originating_system_trade_id|version|associate_trade_identifier|event|status|execution_timestamp|capture_date|trade_date|settlement_date|maturity_date|product_type|product_family|isda_plus_taxonomy|security_type|security_hsbc_id|security_bloomberg_id|security_isin|security_cusip|security_sedol|security_ric|underlying_security_id|underlying_security_name|buy_amount|buy_currency|sell_amount|sell_currency|settlement_amount|settlement_currency|buy_sell_indicator|margin_account|fixing_date|quantity|trade_price|hsbc_entity|hsbc_entity_po|book_name_source|book_id_source|cpty_treats_acronym|cpty_grid|cpty_ber_id|executing_broker|customer_identifier|option_type|option_length|option_strike_price|option_premium|settlement_delivery_method|trader_id|trader_peoplesoft_id|trade_type|composite_id|composite_product_code|hsbc_entity_country|trader_name|sales_person|processing_timestamp|trade_tresata_id|cpty_tresata_id|executing_broker_tresata_id|troux_id_source|booking_country|trade_hti|clean_price|dirty_price|bd_dp_lineage|book_id_hms|execution_type|execution_facility|clearing_broker|clearing_broker_tresata_id|ratesense|primary_currency|counter_currency|primary_value_date|counter_value_date|primary_option_end_date|counter_option_end_date|option_end_date|original_trade_id|spot_date|swap_points|exchange_id|mkt_fwd_rate|corporate_margin|sales_margin|spot_rate|branch|cpty_name|cpty_treats_parent_acronym|run_date|physical_source
SPT192389984|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 09:08:40|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||5779592|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|122|AARC WEST MECH INS (P)||20190829|TREATS_FWD
SPT192389986|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 19:08:42|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||6421758|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|101|ABCELLERA BIOLOGICS INC 270|ABCG|20190829|TREATS_SPOT
SPT222222222|TREATS_UK|EMX LOH|382000046222222||||NOR|2019-08-29 20:08:43|2019-08-30|2019-08-30|2019-08-30||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||2473672|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|719|SOME NAME||20190829|TREATS_SPOTtrade_identifier|trade_identifier_source|originating_system|originating_system_trade_id|version|associate_trade_identifier|event|status|execution_timestamp|capture_date|trade_date|settlement_date|maturity_date|product_type|product_family|isda_plus_taxonomy|security_type|security_hsbc_id|security_bloomberg_id|security_isin|security_cusip|security_sedol|security_ric|underlying_security_id|underlying_security_name|buy_amount|buy_currency|sell_amount|sell_currency|settlement_amount|settlement_currency|buy_sell_indicator|margin_account|fixing_date|quantity|trade_price|hsbc_entity|hsbc_entity_po|book_name_source|book_id_source|cpty_treats_acronym|cpty_grid|cpty_ber_id|executing_broker|customer_identifier|option_type|option_length|option_strike_price|option_premium|settlement_delivery_method|trader_id|trader_peoplesoft_id|trade_type|composite_id|composite_product_code|hsbc_entity_country|trader_name|sales_person|processing_timestamp|trade_tresata_id|cpty_tresata_id|executing_broker_tresata_id|troux_id_source|booking_country|trade_hti|clean_price|dirty_price|bd_dp_lineage|book_id_hms|execution_type|execution_facility|clearing_broker|clearing_broker_tresata_id|ratesense|primary_currency|counter_currency|primary_value_date|counter_value_date|primary_option_end_date|counter_option_end_date|option_end_date|original_trade_id|spot_date|swap_points|exchange_id|mkt_fwd_rate|corporate_margin|sales_margin|spot_rate|branch|cpty_name|cpty_treats_parent_acronym|run_date|physical_source
SPT192389984|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 09:08:40|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||5779592|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|122|AARC WEST MECH INS (P)||20190829|TREATS_FWD
SPT192389985|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 09:08:41|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||6551742|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|100|AARC W MECH INS 20 CRISP ADJ|AARC|20190830|TREATS_FWD
SPT192389986|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 19:08:42|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||6421758|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|101|ABCELLERA BIOLOGICS INC 270|ABCG|20190829|TREATS_SPOT
SPT192389987|TREATS_UK|EMX LOH|382000046274142||||NOR|2019-08-29 19:08:43|2019-08-29|2019-08-29|2019-08-29||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||2473672|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|719|ABBOTT LABORTORIES||20190830|TREATS_SPOT
SPT222222222|TREATS_UK|EMX LOH|382000046222222||||NOR|2019-08-29 20:08:43|2019-08-30|2019-08-30|2019-08-30||SPT|FX|||||||||||1.3E9|NZD|-8.3213E8|USD|||||||0.6401|HRFB|||101|MIDLLOH||2473672|*PH|||||||VAT||E|SWPC44657954|SWP|UK|||2020-11-13 21:20:11|333|||237633|||||cd3cb789-8f0b-4e45-b2bb-da02e534efe7||||||D|NZD|USD|2019-08-29|2020-02-20||||B0000213|||B0000213||0.9969||0.6401|719|SOME NAME||20190829|TREATS_SPOTtrade_id|product_id|trade_date_time|settlement_date|quantity|trade_price|trade_currency|settle_currency|trader_name|cpty_id|book_id|trade_status|entered_date|entered_user|comments|exchange_traded|exchange_id|accrual|update_date_time|bundle_id|trade_action|internal_reference|external_reference|split_book_price|split_base_price|split_currency|sales_person|mirror_trade_id|neg_trade_price|neg_price_type|custom_xfrule_b|le_role|market_type|market_info|alternate_date|version_num|custom_data|mirror_book_id|run_date|SourceSystemGID|FOTradeBuySell|producttype|productfamily|maturity|counterparty_acronym|hsbc_entity|hsbc_entity_country|hsbc_entity_po|portfolio_name|pkey|source_ref|bd_dp_lineage|tresataId_sub
23262345|24487671|2019-07-08 23:00:00.0|2019-09-11 00:00:00.0|1500000|250.50|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-10-03 13:21:30.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||13||0|20191003|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|c1654f9b-47f0-49e4-bd5b-28c25562d92d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|2
23262345|24487671|2019-07-08 23:00:00.0|2019-09-27 00:00:00.0|1500000|1106.46|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-09-24 11:10:54.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||11||0|20190924|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|c2078fdd-0820-4117-a0b5-a13d53022505|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|4
23262345|24487671|2019-07-08 23:00:00.0|2019-11-13 00:00:00.0|1500000|100.22|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-11-14 15:08:27.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||34||0|20191114|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|0097ef87-43e6-47ef-9294-18dfe239015b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|3
23262346|24487673|2019-07-08 23:00:00.0|2019-10-10 00:00:00.0|1500000|200.00|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-10-22 17:45:01.0|0|AMEND|15606578|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||26||0|20191022|SOPHIS|BUY|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|24b5b4ca-14ca-47fe-b4de-060f7234b782|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|5
23262346|24487673|2019-07-08 23:00:00.0|2019-07-11 00:00:00.0|1500000|300.00|USD|USD|LABIDURIE|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-07-10 13:03:15.0|0|NEW|15606578|LONEQD|0.0|0.0||LABIDURIE|0|0.0||0|CounterParty||0.0||0||0|20190710|SOPHIS|BUY|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|e3ae5c91-ea27-4e12-8f06-9b5ddd64b24d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|6
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:05.146|admin||0|1|0.0|2019-08-21 15:26:10.076|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||50||0|20190821|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|caea28f9-2c7d-4f7b-a007-d5331f31ab1e|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|10
9917455|24487679|2013-01-04 10:15:57.0|2013-01-08 00:00:00.0|2000000|315.00|USD|USD||121325|1642421|VERIFIED|2013-01-04 15:25:13.146|admin||0|1|0.0|2019-04-04 16:09:28.086|0|FIXING|1175540N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||41||0|20190408|*|sell|Swap|Swap|2025-01-08 00:00:00.0|FHLMSWP|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|92179bf2-8fae-4ff3-b403-74968bd32301|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|11
23440031|23558060|2019-08-09 18:25:17.0|2019-05-23 00:00:00.0|2000000|315.00|USD|USD||121586|3298105|VERIFIED|2019-08-09 18:54:27.186|admin||0|1|0.0|2019-08-21 16:49:18.486|0|FIXING|4245780N1|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||1||0|20191118|*||Swap|Swap|2023-11-23 00:00:00.0|NYKUUSD|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|382edd46-ff12-4670-9263-aee081ccd66b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|14
9917455|24487679|2013-01-04 10:15:57.0|2013-01-08 00:00:00.0|2000000|315.00|USD|USD||121325|1642421|VERIFIED|2013-01-04 15:25:14.346|admin||0|1|0.0|2019-08-07 09:04:16.046|0|AMEND|1175540N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||48||0|20190807|*||Swap|Swap|2025-01-08 00:00:00.0|FHLMSWP|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|8a968eeb-b6dc-43f2-bdd7-931f94575fd6|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|15
23262345|24487671|2019-07-08 23:00:00.0|2019-11-06 00:00:00.0|1500000|32992.463|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-11-01 12:07:03.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||33||0|20200120|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|717516fb-8f7b-400a-a665-c31ef944576b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|1
14066340|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.346|admin||0|1|0.0|2019-08-26 15:28:39.783|0|AMEND|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||62||0|20190826|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|2c4e3b0f-732f-4743-9b13-bd3725ad7dac|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|7
14066340|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.173|admin||0|1|0.0|2019-08-23 15:28:28.01|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||58||0|20190823|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|3ac8ddc6-7091-46f7-98c3-0127529e752d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|8
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:13.893|admin||0|1|0.0|2019-09-06 15:27:25.633|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||98||0|20190906|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|4372bdff-9537-4a8c-8216-d46259338394|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|9
23440031|23558060|2019-08-09 18:25:17.0|2019-05-23 00:00:00.0|2000000|315.00|USD|USD||121586|3298105|VERIFIED|2019-08-09 18:54:27.186|admin||0|1|0.0|2019-08-21 16:49:18.486|0|FIXING|4245780N1|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||1||0|20190823|*|Buy|Swap|Swap|2023-11-23 00:00:00.0|NYKUUSD|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|2f142a0f-1dd7-4266-9555-247750d3d43b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|12
19384116|23558063|2017-09-28 19:11:36.0|2017-10-02 00:00:00.0|2000000|315.00|USD|USD|TSUN|2111734|1642421|VERIFIED|2017-09-28 18:12:39.213|admin|EXERCISE OF SWAPTION|0|1|0.0|2019-12-11 19:07:12.483|0|UPDATE|3180987N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||18||0|20191211|*|Sell|Swap|Swap|2019-12-12 00:00:00.0|CHEXCME|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|ccb17b12-1d2a-4b7e-a773-097c2bd600ba|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|13trade_id|product_id|trade_date_time|settlement_date|quantity|trade_price|trade_currency|settle_currency|trader_name|cpty_id|book_id|trade_status|entered_date|entered_user|comments|exchange_traded|exchange_id|accrual|update_date_time|bundle_id|trade_action|internal_reference|external_reference|split_book_price|split_base_price|split_currency|sales_person|mirror_trade_id|neg_trade_price|neg_price_type|custom_xfrule_b|le_role|market_type|market_info|alternate_date|version_num|custom_data|mirror_book_id|run_date|SourceSystemGID|FOTradeBuySell|producttype|productfamily|maturity|counterparty_acronym|hsbc_entity|hsbc_entity_country|hsbc_entity_po|portfolio_name|pkey|source_ref|bd_dp_lineage|tresataId_sub
23262345|24487671|2019-07-08 23:00:00.0|2019-09-11 00:00:00.0|1500000|250.50|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-10-03 13:21:30.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||13||0|20191003|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|c1654f9b-47f0-49e4-bd5b-28c25562d92d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|2
23262345|24487671|2019-07-08 23:00:00.0|2019-09-27 00:00:00.0|1500000|1106.46|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-09-24 11:10:54.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||11||0|20190924|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|c2078fdd-0820-4117-a0b5-a13d53022505|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|4
23262345|24487671|2019-07-08 23:00:00.0|2019-11-13 00:00:00.0|1500000|100.22|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-11-14 15:08:27.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||34||0|20191114|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|0097ef87-43e6-47ef-9294-18dfe239015b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|3
23262346|24487673|2019-07-08 23:00:00.0|2019-10-10 00:00:00.0|1500000|200.00|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=true|0|0|0.0|2019-10-22 17:45:01.0|0|AMEND|15606578|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||26||0|20191022|SOPHIS|BUY|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|24b5b4ca-14ca-47fe-b4de-060f7234b782|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|5
23262346|24487673|2019-07-08 23:00:00.0|2019-07-11 00:00:00.0|1500000|300.00|USD|USD|LABIDURIE|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-07-10 13:03:15.0|0|NEW|15606578|LONEQD|0.0|0.0||LABIDURIE|0|0.0||0|CounterParty||0.0||0||0|20190710|SOPHIS|BUY|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|e3ae5c91-ea27-4e12-8f06-9b5ddd64b24d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|6
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.346|admin||0|1|0.0|2019-08-26 15:28:39.783|0|AMEND|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||62||0|20190826|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|2c4e3b0f-732f-4743-9b13-bd3725ad7dac|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|7
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.173|admin||0|1|0.0|2019-08-23 15:28:28.01|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||58||0|20190823|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|3ac8ddc6-7091-46f7-98c3-0127529e752d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|8
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:13.893|admin||0|1|0.0|2019-09-06 15:27:25.633|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||98||0|20190906|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|4372bdff-9537-4a8c-8216-d46259338394|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|9
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:05.146|admin||0|1|0.0|2019-08-21 15:26:10.076|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||50||0|20190821|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|caea28f9-2c7d-4f7b-a007-d5331f31ab1e|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|10
9917455|24487679|2013-01-04 10:15:57.0|2013-01-08 00:00:00.0|2000000|315.00|USD|USD||121325|1642421|VERIFIED|2013-01-04 15:25:13.146|admin||0|1|0.0|2019-04-04 16:09:28.086|0|FIXING|1175540N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||41||0|20190408|*|sell|Swap|Swap|2025-01-08 00:00:00.0|FHLMSWP|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|92179bf2-8fae-4ff3-b403-74968bd32301|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|11
23440031|23558060|2019-08-09 18:25:17.0|2019-05-23 00:00:00.0|2000000|315.00|USD|USD||121586|3298105|VERIFIED|2019-08-09 18:54:27.186|admin||0|1|0.0|2019-08-21 16:49:18.486|0|FIXING|4245780N1|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||1||0|20190823|*|Buy|Swap|Swap|2023-11-23 00:00:00.0|NYKUUSD|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|2f142a0f-1dd7-4266-9555-247750d3d43b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|12
23440031|23558060|2019-08-09 18:25:17.0|2019-05-23 00:00:00.0|2000000|315.00|USD|USD||121586|3298105|VERIFIED|2019-08-09 18:54:27.186|admin||0|1|0.0|2019-08-21 16:49:18.486|0|FIXING|4245780N1|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||1||0|20191118|*||Swap|Swap|2023-11-23 00:00:00.0|NYKUUSD|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|382edd46-ff12-4670-9263-aee081ccd66b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|14
9917455|24487679|2013-01-04 10:15:57.0|2013-01-08 00:00:00.0|2000000|315.00|USD|USD||121325|1642421|VERIFIED|2013-01-04 15:25:14.346|admin||0|1|0.0|2019-08-07 09:04:16.046|0|AMEND|1175540N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||48||0|20190807|*||Swap|Swap|2025-01-08 00:00:00.0|FHLMSWP|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|8a968eeb-b6dc-43f2-bdd7-931f94575fd6|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|15trade_id|product_id|trade_date_time|settlement_date|quantity|trade_price|trade_currency|settle_currency|trader_name|cpty_id|book_id|trade_status|entered_date|entered_user|comments|exchange_traded|exchange_id|accrual|update_date_time|bundle_id|trade_action|internal_reference|external_reference|split_book_price|split_base_price|split_currency|sales_person|mirror_trade_id|neg_trade_price|neg_price_type|custom_xfrule_b|le_role|market_type|market_info|alternate_date|version_num|custom_data|mirror_book_id|run_date|SourceSystemGID|FOTradeBuySell|producttype|productfamily|maturity|counterparty_acronym|hsbc_entity|hsbc_entity_country|hsbc_entity_po|portfolio_name|pkey|source_ref|bd_dp_lineage|tresataId_sub
23262345|24487671|2019-07-08 23:00:00.0|2019-11-06 00:00:00.0|1500000|32992.463|USD|USD|SWAPONE_ADMIN|11644453|3619276|VERIFIED|2019-07-10 13:00:43.0|admin|InstrumentModifMessage=false|0|0|0.0|2019-11-01 12:07:03.0|0|AMEND|15606577|LONEQD|0.0|0.0||SWAPONE_ADMIN|0|0.0||0|CounterParty||0.0||33||0|20200120|SOPHIS|SELL|DTPEquityPortfolioSwap|DTPEquityDerivative|2023-02-08 00:00:00.0|MCAMM65|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|717516fb-8f7b-400a-a665-c31ef944576b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|1
14066340|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.346|admin||0|1|0.0|2019-08-26 15:28:39.783|0|AMEND|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||62||0|20190826|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|2c4e3b0f-732f-4743-9b13-bd3725ad7dac|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|7
14066340|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:06.173|admin||0|1|0.0|2019-08-23 15:28:28.01|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||58||0|20190823|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|3ac8ddc6-7091-46f7-98c3-0127529e752d|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|8
14066342|24487677|2015-05-07 16:40:29.0|2015-05-11 00:00:00.0|2000000|275.00|EUR|EUR|LUCASB|1224612|3517223|VERIFIED|2015-05-07 15:43:13.893|admin||0|1|0.0|2019-09-06 15:27:25.633|0|UPDATE|2020286L|LONSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||98||0|20190906|*|BUY|Swap|Swap|2020-05-11 00:00:00.0|BGIN85B|HBEU|UNITED KINGDOM|HBEU|HSBC BANK PLC|4372bdff-9537-4a8c-8216-d46259338394|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|9
23440031|23558060|2019-08-09 18:25:17.0|2019-05-23 00:00:00.0|2000000|315.00|USD|USD||121586|3298105|VERIFIED|2019-08-09 18:54:27.186|admin||0|1|0.0|2019-08-21 16:49:18.486|0|FIXING|4245780N1|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||1||0|20190823|*|Buy|Swap|Swap|2023-11-23 00:00:00.0|NYKUUSD|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|2f142a0f-1dd7-4266-9555-247750d3d43b|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|12
19384116|23558063|2017-09-28 19:11:36.0|2017-10-02 00:00:00.0|2000000|315.00|USD|USD|TSUN|2111734|1642421|VERIFIED|2017-09-28 18:12:39.213|admin|EXERCISE OF SWAPTION|0|1|0.0|2019-12-11 19:07:12.483|0|UPDATE|3180987N|NYKSUM|0.0|0.0||NONE|0|0.0||0|CounterParty||0.0||18||0|20191211|*|Sell|Swap|Swap|2019-12-12 00:00:00.0|CHEXCME|HBUS|UNITED STATES|HBUS|HSBC Bank USA, National Association|ccb17b12-1d2a-4b7e-a773-097c2bd600ba|DTP_GB|5d2d182b-2cb8-4442-bb3a-850c25855c4d|13
cat: ./src/test/resources/data/job: Is a directory
trade_identifier|trade_originating_system|trade_originating_pts_identifier|run_date|trade_identifier_source|trade_productfamily|trade_producttype
22244364|SOPHIS|14639607|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244365|SOPHIS|14639609|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244366|SOPHIS|14639621|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244370|SOPHIS|14639615|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244372|SOPHIS|14639603|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144364||14639607|20190122|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144365||14639609|20190122|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144366||14639621|20190122|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144370||14639615|20190122|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144372||14639603|20190122|DTP_GB|DTPEquityDerivative|DTPEquityForwarda|b|c|d
1|2|3|4
2.0|3.0|4|5cat: ./src/test/resources/data/rest: Is a directory
{
  "runId": "2021-03-04_AF",
  "location": "/user/hdp06-ss-tresata-uk/data-factory-prod/global-tree-entity/2021-03-04_AF",
  "sources": [
    {
      "source": "avox",
      "asset": "avox",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/avox/20210305.parq"
    },
    {
      "source": "ngt",
      "asset": "ngt",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/ngt/20210305.parq"
    },
    {
      "source": "erds",
      "asset": "erds",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/erds/erds.parq"
    },
    {
      "source": "cv-cis",
      "asset": "cv-cis",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/cis/cis.parq"
    },
    {
      "source": "cdu",
      "asset": "cdu",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/cdu/cdu.parq"
    },
    {
      "source": "obs",
      "asset": "obs",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/obs/obs.parq"
    },
    {
      "source": "ebcc",
      "asset": "ebcc",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/ebcc/ebcc.parq"
    },
    {
      "source": "hub",
      "asset": "hub",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/hub/hub.parq"
    },
    {
      "source": "mdm",
      "asset": "mdm",
      "path": "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/mdm/mdm.parq"
    }
  ],
  "level1BestRefPath": "src/test/resources/out/global-tree-entity/2021-03-03_AF/best/level1.parq",
  "level1aBestRefPath": "src/test/resources/out/global-tree-entity/2021-03-03_AF/best/level1a.parq",
  "level1bBestRefPath": "src/test/resources/out/global-tree-entity/2021-03-03_AF/best/level1b.parq"
}cat: ./src/test/resources/data/tree: Is a directory
nace_code|SWIFT_legal_name_3|Country_of_primary_operation|gicd_id|source_system_unique_id|MAIN_ADDRESS_ADDRESS_LINE_2|Name|registered_address_line_1|Fund_type|tresataId_sub|MAIN_ADDRESS_ADDRESS_CITY|Cbid|DTCC|main_address_postal_code|CICI|DNB|Primary_SIC_percentage|swift_address_country|Country_of_incorporation|UP|REGISTERED_ADDRESS_ADDRESS_STATE_PROVINCE|Source_system|Message_version|Strategic_client|US_CIN|SWIFT_address_line_1|Parent_Name|SIC_Code|tresataId_sub_b|LEI_registration|LEI|asset_name|VERSION_NUMBER|tresataId_sub_a|REGISTERED_ADDRESS_ADDRESS_LINE_2|GBM|Address_2|Date_of_incorporation|GOV_ID_TIN|immediate_parent_id|REGISTERED_ADDRESS_ADDRESS_ISO_COUNTRY|duplicate_of_grid|Cin|GOV_ID_CRN|sic_code_percentage|correspondence_address_country|correspondence_address_postal_code|Entity_Status|swift_full_name|Status|SWIFT_full_legal_name|Primary_SIC_code|Originating_system|main_address_line_2|SWIFT_legal_name_2|bbg_id|cici_id|branch_name|legal_form|Bvd|main_address_city|lei_id|GICD|BO_AFFL|is_line_of_business_both|registered_address_line_2|SWIFT_compliant_flag|source_system_application_id|BO_BRCH|REGISTERED_ADDRESS_ADDRESS_LINE_1|main_address_line_1|irisk_id|Legal_name|Country_of_residence|CMB|Parent_grid|correspondence_address_city|Country_of_Inc|Nace2_code|Address_1|SWIFT_BIC|swift_address_city|REGISTERED_ADDRESS_ADDRESS_CITY|MAIN_ADDRESS_ADDRESS_STATE_PROVINCE|Alternate_Name_Type|SWIFT_legal_name_1|HSBC_affilifate|BBG|main_address_state_province|City|MARKITRED|DuplicateOfId|registered_address_country|Customer_business_type|tax_id_number|XDS_ID|SWIFT_address_line_2|branch_parent_id|Head_office_country|Country|source_input_path|correspondence_address_line_1|ultimate_parent_id|source_date|Grid|MAIN_ADDRESS_ADDRESS_ISO_COUNTRY|correspondence_address_line_2|bvd_id|Entity_Type|main_address_country|source_system_name|dtcc_id|Country_of_Operations|Short_Name|swift_bic_code|Epp_indicator|MAIN_ADDRESS_ADDRESS_POSTAL_CODE|functional_business_unit|is_hsbc_affiliate|line_of_business|UTI_provider|markitred_id|avox_id|registered_address_state_province|AVX|MAIN_ADDRESS_ADDRESS_LINE_1|registered_address_postal_code|Published_date_time|MIFiD_Investment|IRISK|Parent_full_name|BO_EFFECTIVE|SWIFT_bank_indicator|is_strategic_client|company_registration_number|SWIFT_country_state_city|Message_info|dnb_id|Avox|IP_SUB|LegalForm|correspondence_address_state_province|REGISTERED_ADDRESS_ADDRESS_POSTAL_CODE|registered_address_city|Booking_Country|source|run_date
6430||LU||GS_Client_Entitydata/official/20210217/266816||BNP Paribas Funds - US Multi-Factor Equity|10 rue Edward Steichen||c451ad6bd471c737152d3c0360f31b29|Luxembourg|BMPU||L-2540|213800R4HBLFN2D54E31||100|LU|LU|266816||ERDS|2.8|false||10 RUE EDWARD STEICHEN||6430|c451ad6bd471c737152d3c0360f31b29|ISD|213800R4HBLFN2D54E31|erds|15|c451ad6bd471c737152d3c0360f31b29||GBM|||||LU|25928672|||100|||ACT|BNP PARIBAS FUNDS - SUSTAINABLE US MULTI-FACTOR EQUITY|ACT|BNP PARIBAS FUNDS - SUSTAINABLE US MULTI-FACTOR EQUITY|6430|ERDS||US MULTI-FACTOR EQUITY||213800R4HBLFN2D54E31|BNP Paribas Funds - US Multi-Factor Equity|I|LUFS0000FLQA|Luxembourg|213800R4HBLFN2D54E31|||false||Y|243659||10 rue Edward Steichen|10 rue Edward Steichen||BNP Paribas Funds - US Multi-Factor Equity|||||LU|6430|10 rue Edward Steichen||LUXEMBOURG|Luxembourg||SHT|BNP PARIBAS FUNDS - SUSTAINABLE|false|||Luxembourg||25928672|LU|CLP||GS_Client_Entitydata/official/20210217/266816|||LU|LU|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20210217/part-00039-2132447f-5145-432f-9a5f-c784fb0c6c64.c000.snappy.orc||266816|2021_03_14|266816|LU||LUFS0000FLQA|Corporate|LU|ERDS||LU|BNP Paribas Fds-US M-Fac Eqt|||L-2540|Corporate|false|GBM|PAV||38070308||38070308|10 rue Edward Steichen|L-2540|2021-02-17T01:20:27.259Z|false||||N|false||3/LU/LUXEMBOURG|Client Central Message Version 2.8||38070308||I||L-2540|Luxembourg||erds|20210313
4690||US||GS_Client_Entitydata/official/20200615/25280570||UTC Fire & Security Corporation|Corporation Trust Center||601dca551164162a4296e764021ae5a9|Farmington|||06034|||100|US|US|72608|DE|ERDS|2.7|false||ONE CARRIER PLACE||4690|cbb3ab9735e5c3d2960ace7e5f75bed0|||erds|4|601dca551164162a4296e764021ae5a9|1209 Orange Street|GBM|1209 Orange Street|2003-12-22Z|US~341977388|72608|US|||3743724|100|||ACT|UTC FIRE + SECURITY CORPORATION|ACT|UTC FIRE + SECURITY CORPORATION|4690|ERDS|||||UTC Fire & Security Corporation|I|US2-45579|Farmington||||false|1209 Orange Street|Y|243659||Corporation Trust Center|One Carrier Place||UTC Fire & Security Corporation|||||US|4690|Corporation Trust Center||CT FARMINGTON|Wilmington|CT|SHT|UTC FIRE + SECURITY CORPORATION|false||CT|Wilmington|||US|CLP|US~341977388|GS_Client_Entitydata/official/20200615/25280570|||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200615/part-00188-35d94869-b5e0-47dd-b3f5-ffd89fa68ef8.c000.snappy.orc||72608|2021_03_14|25280570|US||US2-45579|Corporate|US|ERDS||US|UTC Fire & SECCORP|||06034|Corporate|false|GBM|PAV||12832899|DE|12832899|One Carrier Place|19801|2020-06-15T10:45:49.054Z|false||||N|false|3743724|3/US/CT FARMINGTON|Client Central Message Version 2.7||12832899|72608|I||19801|Wilmington||erds|20210313
4677||US||GS_Client_Entitydata/official/20200119/4555220||SMM - North America Trade Corporation, Houston Branch|251 Little Falls Drive||63a5e88bc3e4aa8b7309e430eb1601be|Houston|||77079|||100|US|US|4165103|DE|ERDS|2.6|false||11767 KATY FREEWAY||4669|cc3f028ea3d936660f06f0b82e982766|||erds|4|63a5e88bc3e4aa8b7309e430eb1601be||GBM||1989-01-04Z||25919715|US|||0801142283|100|||ACT|SMM - NORTH AMERICA TRADE CORPORATION, HOUSTON BRANCH|ACT|SMM - NORTH AMERICA TRADE CORPORATION, HOUSTON BRANCH|4669|ERDS||CORPORATION, HOUSTON BRANCH|||SMM - North America Trade Corporation, Houston Branch|I|US*B000080475|Houston||||true||Y|243659||251 Little Falls Drive|11767 Katy Freeway||SMM - North America Trade Corporation, Houston Branch||CMB|||US|4677|251 Little Falls Drive||TX HOUSTON|Wilmington|TX|SHT|SMM - NORTH AMERICA TRADE|false||TX|Wilmington|||US|CLP||GS_Client_Entitydata/official/20200119/4555220|||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200131/part-00177-acc53cdc-d1e9-499e-b78a-e6765c2049b3.c000.snappy.orc||4165103|2021_03_14|4555220|US||US*B000080475|Corporate|US|ERDS||US|SMM-Nrth Amrca TC Houston Br|||77079|Corporate|false|GBM|FUV||32129238|DE|32129238|11767 Katy Freeway|19808|2020-01-19T09:09:04.363Z|false||||N|false|0801142283|3/US/TX HOUSTON|Client Central Message Version 2.6||32129238|25919715|I||19808|Wilmington||erds|20210313
6430||US||GS_Client_Entitydata/official/20200907/4345565|Suite 1118|Rothko Emerging Markets All Cap Equity Fund, L.P.|1105 N. Market Street|SF|f41c68baa03ff6335455709f1e41a518|Wilmington|DJGY||19801|5493002LLHS7Z0XZ3183||100|US|US|4345565|DE|ERDS|2.7|false||1105 N. MARKET STREET||6430|f41c68baa03ff6335455709f1e41a518|ISD|5493002LLHS7Z0XZ3183|erds|7|f41c68baa03ff6335455709f1e41a518|Suite 1300|GBM|Suite 1300|2013-06-03Z|US~462902364||US|||5344030|100|||ACT|ROTHKO EMERGING MARKETS ALL CAP EQUITY FUND, L.P.|ACT|ROTHKO EMERGING MARKETS ALL CAP EQUITY FUND, L.P.|6430|ERDS|Suite 1118|EQUITY FUND, L.P.||5493002LLHS7Z0XZ3183|Rothko Emerging Markets All Cap Equity Fund, L.P.||USLEI106139|Wilmington|5493002LLHS7Z0XZ3183|||false|Suite 1300|Y|243659||1105 N. Market Street|1105 N. Market Street||Rothko Emerging Markets All Cap Equity Fund, L.P.|||||US|6430|1105 N. Market Street||DE WILMINGTON|Wilmington|DE|SHT|ROTHKO EMERGING MARKETS ALL CAP|false||DE|Wilmington|||US||US~462902364|GS_Client_Entitydata/official/20200907/4345565|SUITE 1118||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200907/part-00048-78fa2d5f-b1e3-4ca4-961f-01f87027077a.c000.snappy.orc||4345565|2021_03_14|4345565|US||USLEI106139|Fund|US|ERDS||US|Rothko Emrg Mkt ACEF LP|||19801|Fund|false|GBM|PAV||30197083|DE|30197083|1105 N. Market Street|19801|2020-09-07T14:17:31.310Z|false||||N|false|5344030|3/US/DE WILMINGTON|Client Central Message Version 2.7||30197083||||19801|Wilmington||erds|20210313
6430||US||GS_Client_Entitydata/official/20191030/4151738||MacAndrews & Forbes Incorporated Master Trust|50 South LaSalle Street|SF|4f73e7de289c394504c56bbb2531af34|New York|ZJCJ||10004|549300WLYSWQUKBZ0B74||100|US|US|4151738|IL|ERDS|2.6|false||50TH FLOOR,ONE NEW YORK PLAZA||6430|4f73e7de289c394504c56bbb2531af34|LPD|549300WLYSWQUKBZ0B74|erds|15|4f73e7de289c394504c56bbb2531af34||GBM|||US~367370049||US||||100|||ACT|MACANDREWS + FORBES INCORPORATED MASTER TRUST|ACT|MACANDREWS + FORBES INCORPORATED MASTER TRUST|6430|ERDS||MASTER TRUST||549300WLYSWQUKBZ0B74|MacAndrews & Forbes Incorporated Master Trust||USLEI49623|New York|549300WLYSWQUKBZ0B74|||false||Y|243659||50 South LaSalle Street|50th Floor,One New York Plaza||MacAndrews & Forbes Incorporated Master Trust|||||US|6430|50 South LaSalle Street||NY NEW YORK|Chicago|NY|SHT|MACANDREWS + FORBES INCORPORATED|false||NY|Chicago|||US||US~367370049|GS_Client_Entitydata/official/20191030/4151738|||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191030/part-00017-9eb2d1f1-8b11-4bc6-9b65-e99c16ed8a60.c000.snappy.orc||4151738|2021_03_14|4151738|US||USLEI49623|Fund|US|ERDS||US|MacAndrews+Forbes Inc Mstr T|||10004|Fund|false|GBM|PAV||25706018|IL|25706018|50th Floor,One New York Plaza|60603|2019-10-30T11:18:42.213Z|false||||N|false||3/US/NY NEW YORK|Client Central Message Version 2.6||25706018||||60603|Chicago||erds|20210313
6430||US||GS_Client_Entitydata/official/20200926/734096||PIMCO Funds - PIMCO Low Duration Fund|155 Federal Street|IT|8fec234452e2cb8aeff6bf7aba606c28|Newport Beach|JTG1||92660|MGFS63GRW2G1SZOESY83||100|US|US|734096|MA|ERDS|2.7|false||650 NEWPORT CENTER DRIVE||6430|8fec234452e2cb8aeff6bf7aba606c28|ISD|MGFS63GRW2G1SZOESY83|erds|20|8fec234452e2cb8aeff6bf7aba606c28|Suite 700|GBM|Suite 700||US~330239887||US|73501|||100|||ACT|PIMCO FUNDS - PIMCO LOW DURATION FUND|ACT|PIMCO FUNDS - PIMCO LOW DURATION FUND|6430|ERDS||FUND||MGFS63GRW2G1SZOESY83|PIMCO Funds - PIMCO Low Duration Fund|||Newport Beach|MGFS63GRW2G1SZOESY83|||false|Suite 700|Y|243659||155 Federal Street|650 Newport Center Drive||PIMCO Funds - PIMCO Low Duration Fund|||||US|6430|155 Federal Street||CA NEWPORT BEACH|Boston|CA|SHT|PIMCO FUNDS - PIMCO LOW DURATION|false||CA|Boston||73501|US||US~330239887|GS_Client_Entitydata/official/20200926/734096|||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200926/part-00060-28bc5a08-fcdb-45fb-a897-93b0328e33b3.c000.snappy.orc||734096|2021_03_14|734096|US|||Fund|US|ERDS||US|PIMCO Low Duration Fnd||false|92660|Fund|false|GBM|PAV||25702499|MA|25702499|650 Newport Center Drive|02110|2020-09-26T09:22:29.993Z|false||||N|false||3/US/CA NEWPORT BEACH|Client Central Message Version 2.7||25702499||||02110|Boston||erds|20210314
7022||HK||GS_Client_Entitydata/official/20200430/4425043|Kowloon Bay,Hong Kong|Dick's International Sourcing Holdings Limited|23 Wang Tai Road||67d0ff07d6a500a9854cb8d6fd819b24|Hong Kong||||||100|HK|HK|4355732||ERDS|2.6|false||23 WANG TAI ROAD,KWUN TONG,KWN||7020|1bca10c01df60eb56587932426d8b31a|||erds|5|d175b7ee91d40f5e613b0f81da3dc183|Kowloon Bay,Kwun Tong,Kowloon|GBM|Kowloon Bay,Kwun Tong,Kowloon|2011-11-14Z||4355732|HK|||1680429|100|||ACT|DICK'S INTERNATIONAL SOURCING HOLDINGS LIMITED|ACT|DICK'S INTERNATIONAL SOURCING HOLDINGS LIMITED|7020|ERDS|Kowloon Bay,Hong Kong|HOLDINGS LIMITED|||Dick's International Sourcing Holdings Limited|I||Hong Kong||||true|Kowloon Bay,Kwun Tong,Kowloon|Y|243659||23 Wang Tai Road|23 Wang Tai Road,Kwun Tong,Kwn||Dick's International Sourcing Holdings Limited||CMB|||HK|7022|23 Wang Tai Road||HONG KONG|Hong Kong||SHT|DICK'S INTERNATIONAL SOURCING|false|||Hong Kong|||HK|CLP||GS_Client_Entitydata/official/20200430/4425043|KOWLOON BAY,HONG KONG||HK|HK|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200430/part-00174-ed8313c0-a36a-46bb-aaad-66b8bf9f7aa2.c000.snappy.orc||4355732|2021_03_14|4106479|HK|||Corporate|HK|ERDS||HK|Dicks Intl Sourcng Hdg Ltd||||Corporate|false|GBM|PAV||34254294||34254294|23 Wang Tai Road,Kwun Tong,Kwn||2020-04-30T01:54:25.765Z|false||||N|false|1680429|3/HK/HONG KONG|Client Central Message Version 2.6||34254294|4355732|I|||Hong Kong||erds|20210314
4649||MX||GS_Client_Entitydata/official/20200614/4300707|Polanco, Chapultepec, Miguel Hidalgo|G.S.E.B. Mexicana, S.A. de C.V.|Piso 4,Goldsmith 38-401||d513f88f711e810c76710bf9dc239bea|Mexico City|||11560|||100|MX|MX|2605835|Ciudad de Mexico|ERDS|2.7|false||PISO 4,GOLDSMITH 38-401||4649|efc0f0717bf3f980bca37ac6479080e3|||erds|10|d513f88f711e810c76710bf9dc239bea|Polanco Chapultepec,Miguel Hidalgo|GBM|Polanco Chapultepec,Miguel Hidalgo|||4077835|MX|||GSB9107195A0|100|||ACT|G.S.E.B. MEXICANA, S.A. DE C.V.|ACT|G.S.E.B. MEXICANA, S.A. DE C.V.|4649|ERDS|Polanco, Chapultepec, Miguel Hidalgo||||G.S.E.B. Mexicana, S.A. de C.V.|L|MX*190521664274|Mexico City||||false|Polanco Chapultepec,Miguel Hidalgo|Y|243659||Piso 4,Goldsmith 38-401|Piso 4,Goldsmith 38-401||G.S.E.B. Mexicana, S.A. de C.V.|||||MX|4649|Piso 4,Goldsmith 38-401||CIUDAD DE MEXICO MEXICO CITY|Mexico City|Ciudad de Mexico|SHT|G.S.E.B. MEXICANA, S.A. DE C.V.|false||Ciudad de Mexico|Mexico City|||MX|CLP||GS_Client_Entitydata/official/20200614/4300707|POLANCO, CHAPULTEPEC, MIGUEL HIDA||MX|MX|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200614/part-00156-c166fe86-781f-4529-a0d7-48080c51ff50.c000.snappy.orc||2605835|2021_03_14|4300707|MX||MX*190521664274|Corporate|MX|ERDS||MX|GSEB Mexicana SA de CV|||11560|Corporate|false|GBM|PAV||28246864|Ciudad de Mexico|28246864|Piso 4,Goldsmith 38-401|11560|2020-06-14T08:33:11.488Z|false||||N|false|GSB9107195A0|3/MX/CIUDAD DE MEXICO MEXICO CITY|Client Central Message Version 2.7||28246864|4077835|L||11560|Mexico City||erds|20210314
6630||US||GS_Client_Entitydata/official/20201211/74740|Suite 1801|Twinfields Capital Management, LLC|Corporation Trust Center||f89b711230514650678c580c9c048f74|Chicago|||60604|||100|US|US||DE|ERDS|2.8|false||141 W JACKSON BOULEVARD||6630|f89b711230514650678c580c9c048f74|||erds|14|f89b711230514650678c580c9c048f74|1209 Orange Street|GBM|1209 Orange Street|2004-05-25Z|US~030542783||US|74610||3805263|100|||ACT|TWINFIELDS CAPITAL MANAGEMENT, LLC|ACT|TWINFIELDS CAPITAL MANAGEMENT, LLC|6630|ERDS|Suite 1801|LLC|||Twinfields Capital Management, LLC|L||Chicago||||false|1209 Orange Street|Y|243659||Corporation Trust Center|141 W Jackson Boulevard||Twinfields Capital Management, LLC|||||US|6630|Corporation Trust Center||IL CHICAGO|Wilmington|IL|SHT|TWINFIELDS CAPITAL MANAGEMENT,|false||IL|Wilmington||74610|US|IMA|US~030542783|GS_Client_Entitydata/official/20201211/74740|SUITE 1801||US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20201211/part-00137-fd2fb07d-385d-41c8-be8d-6241832ed688.c000.snappy.orc|||2021_03_14|74740|US|||FinancialInstitution|US|ERDS||US|TWCM-Twinfields Cap Mng LLC|||60604|FinancialInstitution|false|GBM|PAV||25969834|DE|25969834|141 W Jackson Boulevard|19801|2020-12-11T05:20:26.292Z|false||||N|false|3805263|3/US/IL CHICAGO|Client Central Message Version 2.8||25969834||L||19801|Wilmington||erds|20210314
6120||KR||GS_Client_Entitydata/official/20191220/4306745|3001|Syniverse Technologies, LLC, Seoul Branch|The Corporation Trust Company,||294137e8dccf47d9e52f98655018fd25|Gangnam-gu|||06164|||100|KR|US|4116950|DE|ERDS|2.6|false||30F,TRADE TOWER,511 YOUNGDONG-DAE|Syniverse Technologies LLC|6120|aeba4318f24791b727b83368d1ff28be|||erds|8|294137e8dccf47d9e52f98655018fd25|Corporation Trust Center,1209 Orange|GBM|Corporation Trust Center,1209 Orange|1989-02-15Z|||US|||2187578|100|||ACT|SYNIVERSE TECHNOLOGIES, LLC, SEOUL BRANCH|ACT|SYNIVERSE TECHNOLOGIES, LLC, SEOUL BRANCH|6120|ERDS|3001|SEOUL BRANCH|||Syniverse Technologies, LLC, Seoul Branch|L||Gangnam-gu||||false|Corporation Trust Center,1209 Orange|Y|243659|5167556|The Corporation Trust Company,|30f,Trade Tower,511 Youngdong-Daero|||||5167556||US|6120|The Corporation Trust Company,||SEOUL GANGNAM-GU|Wilmington|Seoul|AKA|SYNIVERSE TECHNOLOGIES, LLC,|false||Seoul|Wilmington|||US|CLP||GS_Client_Entitydata/official/20191220/4306745|3001|5167556|US|US|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191231/part-00022-8640c9fc-cb50-4100-8e99-0f3533ccd714.c000.snappy.orc||4116950|2021_03_14|4306745|KR|||Branch|KR|ERDS||KR|Syniverse|||06164|Branch|false|GBM|PAV||31988212|DE|31988212|30f,Trade Tower,511 Youngdong-Daero|19801|2019-12-20T02:44:22.858Z|false||Syniverse Technologies LLC||N|false|2187578|3/KR/SEOUL GANGNAM-GU|Client Central Message Version 2.6||31988212||L||19801|Wilmington||erds|20210314
6430||DE||GS_Client_Entitydata/official/20201013/609118||Barmenia Renditefonds DWS|Brienner Strasse 59|IT|cea0b719df4843837454d98291fa2194|Frankfurt am Main|ETX1||60329|549300O0P8V33VP75Q04||100|DE|DE|609118|Bayern|ERDS|2.7|false||MAINZER LANDSTRASSE 11-17||6430|cea0b719df4843837454d98291fa2194|ISD|549300O0P8V33VP75Q04|erds|13|cea0b719df4843837454d98291fa2194||GBM|||||DE||||100|||ACT|BARMENIA RENDITEFONDS DWS|ACT|BARMENIA RENDITEFONDS DWS|6430|ERDS||||549300O0P8V33VP75Q04|Barmenia Renditefonds DWS||DEECB0008474248|Frankfurt am Main|549300O0P8V33VP75Q04|||false||Y|243659||Brienner Strasse 59|Mainzer Landstrasse 11-17||Barmenia Renditefonds DWS|||||DE|6430|Brienner Strasse 59||HESSEN FRANKFURT AM MAIN|Munich|Hessen|SHT|BARMENIA RENDITEFONDS DWS|false||Hessen|Munich|||DE|||GS_Client_Entitydata/official/20201013/609118|||DE|DE|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20201013/part-00023-5c705f3a-1ae9-4a3a-bfe9-20c12c7643df.c000.snappy.orc||609118|2021_03_14|609118|DE||DEECB0008474248|Fund|DE|ERDS||DE|Barmenia RenditeFds DWS|||60329|Fund|false|GBM|PAV||25877654|Bayern|25877654|Mainzer Landstrasse 11-17|80333|2020-10-13T08:13:18.280Z|false||||N|false||3/DE/HESSEN FRANKFURT AM MAIN|Client Central Message Version 2.7||25877654||||80333|Munich||erds|20210314
4752||CA||GS_Client_Entitydata/official/20200327/25300200||Realterm Energy Corp.|56 Portland Street||67d0ff07d6a500a9854cb8d6fd819b24|Montreal|||H3G 2T3|549300NBXN8AV8L7BB96||100|CA|CA|25300200|NS|ERDS|2.6|false||600-2160 RUE DE LA MONTAGNE||4752|c9b12c2aeee8c45b30a55fa1011d62a6|LPD|549300NBXN8AV8L7BB96|erds|9|c9b12c2aeee8c45b30a55fa1011d62a6|Suite 200,Dartmouth|GBM|Suite 200,Dartmouth|2015-03-25Z|||CA|4106479||3288189|100|||ACT|REALTERM ENERGY CORP.|ACT|REALTERM ENERGY CORP.|4752|ERDS||||549300NBXN8AV8L7BB96|Realterm Energy Corp.|I||Montreal|549300NBXN8AV8L7BB96|||false|Suite 200,Dartmouth|Y|243659||56 Portland Street|600-2160 rue de la Montagne||Realterm Energy Corp.|||||CA|4752|56 Portland Street||QC MONTREAL|Halifax|QC|SHT|REALTERM ENERGY CORP.|false||QC|Halifax|||CA|CLP||GS_Client_Entitydata/official/20200327/25300200|||CA|CA|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200331/part-00121-eb72a495-4a9a-49f3-8b4e-be25d9ba231f.c000.snappy.orc||25300200|2021_03_14|25300200|CA|||Corporate|CA|ERDS||CA|Realterm Energy Corp|||H3G 2T3|Corporate|false|GBM|PAV||36502501|NS|36502501|600-2160 rue de la Montagne|B2Y 1H2|2020-03-27T07:57:23.420Z|false||||N|false|3288189|3/CA/QC MONTREAL|Client Central Message Version 2.6||36502501||I||B2Y 1H2|Halifax||erds|20210314
6530||IT||GS_Client_Entitydata/official/20201013/25463509||CONCRETO - Fondo Nazionale Pensione Complementare|Via Giovanni Amendola 46|PF|67d0ff07d6a500a9854cb8d6fd819b24|Rome|ZSEO||00144|5493009BUJQ3CYLELD96||100|IT|IT|25463509||ERDS|2.7|false||PIAZZA GUGLIELMO MARCONI 25||6530|93d22d40942f9913968fcbac346ea444|ISD|5493009BUJQ3CYLELD96|erds|10|93d22d40942f9913968fcbac346ea444||GBM|||||IT|4106479|||100|||ACT|CONCRETO - FONDO NAZIONALE PENSIONE COMPLEMENTARE|ACT|CONCRETO - FONDO NAZIONALE PENSIONE COMPLEMENTARE|6530|ERDS||PENSIONE COMPLEMENTARE||5493009BUJQ3CYLELD96|CONCRETO - Fondo Nazionale Pensione Complementare||ITLEI201798|Rome|5493009BUJQ3CYLELD96|||false||Y|243659||Via Giovanni Amendola 46|Piazza Guglielmo Marconi 25||CONCRETO - Fondo Nazionale Pensione Complementare|||||IT|6530|Via Giovanni Amendola 46||ROME|Rome||AKA|CONCRETO - FONDO NAZIONALE|false|||Rome|||IT|||GS_Client_Entitydata/official/20201013/25463509|||IT|IT|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20201013/part-00101-5c705f3a-1ae9-4a3a-bfe9-20c12c7643df.c000.snappy.orc||25463509|2021_03_14|25463509|IT||ITLEI201798|Fund|IT|ERDS||IT|Concreto - Fondo Nazionale Pensione Complementare a Capitalizzazione Per I Lavoratori Dell'industria del Cemento, Della Calce e Suoi Derivati, del Gesso e Relativi Manufatti, Delle Malte e Dei Materiali Di Base Per Le Costruzioni|||00144|Fund|false|GBM|PAV||31120573||31120573|Piazza Guglielmo Marconi 25|00185|2020-10-13T08:25:53.585Z|false||||N|false||3/IT/ROME|Client Central Message Version 2.7||31120573||||00185|Rome||erds|20210314
4649||GB||GS_Client_Entitydata/official/20180716/4248877|Park Royal|Surridge Dawson Limited|C/O Ernst & Young LLP||43cea2d8a4c33451ad60389be46fe75b|London|||NW10 7NZ|||100|GB|GB|4090415||ERDS|2.6|false||2 PREMIER PARK ROAD||4649|8c0fa88d13d11bfbf09a646b7ac4f626|||erds|8|8c0fa88d13d11bfbf09a646b7ac4f626|1 Bridgewater Place Water Lane|GBM|1 Bridgewater Place Water Lane|1962-12-19Z|||GB|||00744679|100|||ACT|SURRIDGE DAWSON LIMITED|ACT|SURRIDGE DAWSON LIMITED|4649|ERDS|Park Royal||||Surridge Dawson Limited Branch|I|GB00744679|London||||false|1 Bridgewater Place Water Lane|Y|243659||C/O Ernst & Young LLP|2 Premier Park Road||Surridge Dawson Limited|||||GB|4649|C/O Ernst & Young LLP||LONDON|Leeds||SHT|SURRIDGE DAWSON LIMITED|false|||Leeds|||GB|CLP||GS_Client_Entitydata/official/20180716/4248877|PARK ROYAL||GB|GB|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191030/part-00094-9eb2d1f1-8b11-4bc6-9b65-e99c16ed8a60.c000.snappy.orc||4090415|2021_03_14|4011597|GB||GB00744679|Branch|GB|ERDS||GB|Surridge Dawson Limited|||NW10 7NZ|Corporate|false|GBM|FUV||15783387||15783387|2 Premier Park Road|LS11 5QR|2018-07-16T19:44:15.740Z|false||||N|false|00744679|3/GB/LONDON|Client Central Message Version 2.6||15783387||I||LS11 5QR|Leeds||erds|20210314
5229||MX||GS_Client_Entitydata/official/20200521/4134718|Col Centro|Compania De Transportes Del Mar De Cortes S.A. De C.V.|Av Constitucion No 444 Poniente||43cea2d8a4c33451ad60389be46fe75b|Monterrey|||64000|||100|MX|MX|73314|Nuevo Leon|ERDS|2.7|false||AV CONSTITUCION NO 444 PONIENTE||5229|dc9e3d11365270ff6dd792f508e9db8d|||erds|13|42def36fd3ebc07902f6da0b87847d99|Col Centro|GBM|Col Centro|1982-11-16Z|||MX|4011597||TMC821116NUA|100|||ACT|COMPANIA DE TRANSPORTES DEL MAR DE CORTES S.A. DE C.V.|ACT|COMPANIA DE TRANSPORTES DEL MAR DE CORTES S.A. DE C.V.|5229|ERDS|Col Centro|DE CORTES S.A. DE C.V.|||Compania De Transportes Del Mar De Cortes S.A. De C.V.|L|MX58-978-3153|Monterrey||||false|Col Centro|Y|243659||Av Constitucion No 444 Poniente|Av Constitucion No 444 Poniente||Compania De Transportes Del Mar De Cortes S.A. De C.V.|||||MX|5229|Av Constitucion No 444 Poniente||NL MONTERREY|Monterrey|Nuevo Leon|SHT|COMPANIA DE TRANSPORTES DEL MAR|false||Nuevo Leon|Monterrey|||MX|CLP||GS_Client_Entitydata/official/20200521/4134718|COL CENTRO||MX|MX|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200528/part-00043-e71475fd-c8e9-4a8f-b68e-d862379b1602.c000.snappy.orc||73314|2021_03_14|4134718|MX||MX58-978-3153|Corporate|MX|ERDS||MX|CIA De Transportes Del Mar D|||64000|Corporate|false|GBM|FUV||6687891|Nuevo Leon|6687891|Av Constitucion No 444 Poniente|64000|2020-05-21T01:07:26.425Z|false||||N|false|TMC821116NUA|3/MX/NL MONTERREY|Client Central Message Version 2.7||6687891||L||64000|Monterrey||erds|20210314
6619||HK||GS_Client_Entitydata/official/20210111/808729|Two Pacific Place,Suite 3301|Schroder Investment Management (Hong Kong) Limited|Level 33,88 Queensway,C+W Dt, HKI||8914a23ebc5835447c377be94c5566ca|Hong Kong|SIHR|||5493002J24EYQ8LYV855||100|HK|HK|270314||ERDS|2.8|false||LEVEL 33, 2 PACIFIC PLACE,88 QUEE||6619|16b0e6013bca3a6a328e535ba94adc36|ISD|5493002J24EYQ8LYV855|erds|23|8914a23ebc5835447c377be94c5566ca|Two Pacific Place,Suite 3301|GBM|Two Pacific Place,Suite 3301|1974-03-29Z||4333843|HK|1780152||0037798|100|||ACT|SCHRODER INVESTMENT MANAGEMENT (HONG KONG) LIMITED|ACT|SCHRODER INVESTMENT MANAGEMENT (HONG KONG) LIMITED|6619|ERDS|Two Pacific Place,Suite 3301|(HONG KONG) LIMITED||5493002J24EYQ8LYV855|Schroder Investment Management (Hong Kong) Limited|I||Hong Kong|5493002J24EYQ8LYV855|||false|Two Pacific Place,Suite 3301|Y|243659||Level 33,88 Queensway,C+W Dt, HKI|Level 33,88 Queensway,C+W Dt,HKI||Schroder Investment Management (Hong Kong) Limited|||||HK|6619|Level 33,88 Queensway,C+W Dt, HKI||HONG KONG|Hong Kong||SHT|SCHRODER INVESTMENT MANAGEMENT|false|||Hong Kong||1780152|HK|CLP||GS_Client_Entitydata/official/20210111/808729|HONG KONG||HK|HK|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20210111/part-00173-f0ad70c6-3313-41ed-9bbb-ae87fd4dac61.c000.snappy.orc||270314|2021_03_14|808729|HK|||Corporate|HK|ERDS||HK|SCHRODER INV (HK)-SAF||||Corporate|false|GBM|PAV||2370693||2370693|Level 33,88 Queensway,C+W Dt,HKI||2021-01-11T12:53:27.916Z|false||||N|false|0037798|3/HK/HONG KONG|Client Central Message Version 2.8||2370693|4333843|I|||Hong Kong||erds|20210314
6430||FR||GS_Client_Entitydata/official/20200621/4106859||LFP CDS 2 ANS|128 BD Raspail|FCR|43cea2d8a4c33451ad60389be46fe75b|Paris|QMPI||75006|969500KR2QC96YFU9K43||100|FR|FR|4106859||ERDS|2.7|false||128 BD RASPAIL||6430|e1dc319e710e6ff02a4525a68037170d|RTD|969500KR2QC96YFU9K43|erds|12|e1dc319e710e6ff02a4525a68037170d||GBM|||||FR|4011597|||100|||ACT|LFP CDS 2 ANS|ACT|LFP CDS 2 ANS|6430|ERDS||||969500KR2QC96YFU9K43|LFP CDS 2 ANS||FRECB0011154996|Paris|969500KR2QC96YFU9K43|||false||Y|243659||128 BD Raspail|128 BD Raspail||LFP CDS 2 ANS|||||FR|6430|128 BD Raspail||PARIS|Paris||SHT|LFP CDS 2 ANS|false|||Paris|||FR|||GS_Client_Entitydata/official/20200621/4106859|||FR|FR|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200621/part-00026-b6744d61-54c9-47b6-ae86-42b44268fd74.c000.snappy.orc||4106859|2021_03_14|4106859|FR||FRECB0011154996|Fund|FR|ERDS||FR|LFP CDS 3 Ans|||75006|Fund|false|GBM|FUV||30332801||30332801|128 BD Raspail|75006|2020-06-21T05:26:02.786Z|false||||N|false||3/FR/PARIS|Client Central Message Version 2.7||30332801||||75006|Paris||erds|20210314
2932||CN||GS_Client_Entitydata/official/20210309/26402911|Economic and Technological Development|CITIC Dicastal Co., Ltd.|No 185 Longhai Road||b05445678eddc4653f9230c2930a74d7|Qinhuangdao||||655600NHN8YB250MP414||100|CN|CN|26402911|Hebei|ERDS|2.8|false||NO 185 LONG HAI ROAD||2930|b05445678eddc4653f9230c2930a74d7|ISD|655600NHN8YB250MP414|erds|7|b05445678eddc4653f9230c2930a74d7|Economic and Technological Development|GBM|Economic and Technological Development|1988-05-26Z|||CN|||91130300601149636T|100|||ACT|CITIC DICASTAL CO., LTD.|ACT|CITIC DICASTAL CO., LTD.|2930|ERDS|Economic and Technological Development|||655600NHN8YB250MP414|CITIC Dicastal Co., Ltd.|L|CN9366688857|Qinhuangdao|655600NHN8YB250MP414|||true|Economic and Technological Development|Y|243659||No 185 Longhai Road|No 185 Long Hai Road||CITIC Dicastal Co., Ltd.||CMB|||CN|2932|No 185 Longhai Road||HEBEI QINHUANGDAO|Qinhuangdao|Hebei|NLN|CITIC DICASTAL CO., LTD.|false||Hebei|Qinhuangdao|||CN|CLP||GS_Client_Entitydata/official/20210309/26402911|ECONOMIC AND TECHNOLOGICAL DEV||CN|CN|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20210309/part-00119-6ebaa838-9be1-4f46-a674-e02d963908a6.c000.snappy.orc||26402911|2021_03_14|26402911|CN||CN9366688857|Corporate|CN|ERDS||CN|中信戴卡股份有限公司||||Corporate|false|GBM|PAV||38975135|Hebei|38975135|No 185 Long Hai Road||2021-03-09T09:55:02.258Z|false||||N|false|91130300601149636T|3/CN/HEBEI QINHUANGDAO|Client Central Message Version 2.8||38975135||L|||Qinhuangdao||erds|20210314
5912||GB||GS_Client_Entitydata/official/20200617/4243408|North Orbital Road,Denham|Composite Image Systems London Limited|Film House,co Deluxe Limited||58423db36775a32b9ec53c1c7fd2c791|Uxbridge|||UB9 5HQ|||100|GB|GB|26204383||ERDS|2.7|false||DENHAM MEDIA PARK||5912|625c7906545f7037d6377307187301b5|||erds|12|58423db36775a32b9ec53c1c7fd2c791|142 Wardour Street|GBM|142 Wardour Street|2005-09-28Z||4238134|GB|||05577229|100|||ACT|COMPOSITE IMAGE SYSTEMS LONDON LIMITED|ACT|COMPOSITE IMAGE SYSTEMS LONDON LIMITED|5912|ERDS|North Orbital Road,Denham|LIMITED|||Composite Image Systems London Limited|I|GB05577229|Uxbridge||||true|142 Wardour Street|Y|243659||Film House,co Deluxe Limited|Denham Media Park||Composite Image Systems London Limited||CMB|||GB|5912|Film House,co Deluxe Limited||UXBRIDGE|London||SHT|COMPOSITE IMAGE SYSTEMS LONDON|false|||London|||GB|CLP||GS_Client_Entitydata/official/20200617/4243408|NORTH ORBITAL ROAD,DENHAM||GB|GB|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200617/part-00137-d21629b9-49a3-44ba-af5f-3f08745a899a.c000.snappy.orc||26204383|2021_03_14|4243408|GB||GB05577229|Corporate|GB|ERDS||GB|Composite Imge Sys Lndon Ltd|||UB9 5HQ|Corporate|false|GBM|FUV||31448057||31448057|Denham Media Park|W1F 8DD|2020-06-17T01:21:26.042Z|false||||N|false|05577229|3/GB/UXBRIDGE|Client Central Message Version 2.7||31448057|4238134|I||W1F 8DD|London||erds|20210314||GB||GS_Client_Entitydata/official/20171220/2620989|403 ALCESTER ROAD SOUTH|MR COLIN + MRS CAROLE JEAN BARLOW|||08c91a4026fd659b11f571102f998558|BIRMINGHAM|||B14 6ES|||100|GB|GB|||ERDS|2.5|false||403 ALCESTER ROAD SOUTH||9998|08c91a4026fd659b11f571102f998558|||erds|5|08c91a4026fd659b11f571102f998558||GBM|||||||||100|||ACT|MR COLIN + MRS CAROLE JEAN BARLOW|ACT|MR COLIN + MRS CAROLE JEAN BARLOW|9998|ERDS|403 ALCESTER ROAD SOUTH||||MR COLIN + MRS CAROLE JEAN BARLOW|I||BIRMINGHAM||||false||Y|243659|||403 ALCESTER ROAD SOUTH||MR COLIN + MRS CAROLE JEAN BARLOW|||||GB||||BIRMINGHAM|||SHT|MR COLIN + MRS CAROLE JEAN BARLOW|false|||||||CLP||GS_Client_Entitydata/official/20171220/2620989|403 ALCESTER ROAD SOUTH||GB||hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191030/part-00039-9eb2d1f1-8b11-4bc6-9b65-e99c16ed8a60.c000.snappy.orc|||2021_03_14|2620989|GB|||Corporate|GB|ERDS||GB|BARLOW COLIN + CAROLE JEAN|||B14 6ES|Corporate|false|GBM|PAV|||||403 ALCESTER ROAD SOUTH||2017-12-20T14:38:17.818Z|false||||N|false||3/GB/BIRMINGHAM|Client Central Message Version 2.5||||I|||||erds|20210314
6430||GB||GS_Client_Entitydata/official/20200831/608521||AXA Framlington UK Select Opportunities Fund|250 Bishopsgate|SF|e67d7b0fbd49b58451d1d409b2cd31a4|London|ACI1||EC1A 7NX|2138003V7DIS99CJCU53||100|GB|GB|608521||ERDS|2.7|false||7 NEWGATE STREET||6430|e67d7b0fbd49b58451d1d409b2cd31a4|ISD|2138003V7DIS99CJCU53|erds|15|e67d7b0fbd49b58451d1d409b2cd31a4||GBM|||||GB||||100|||ACT|AXA FRAMLINGTON UK SELECT OPPORTUNITIES FUND|ACT|AXA FRAMLINGTON UK SELECT OPPORTUNITIES FUND|6430|ERDS||OPPORTUNITIES FUND||2138003V7DIS99CJCU53|AXA Framlington UK Select Opportunities Fund||GBLEI581831|London|2138003V7DIS99CJCU53|||false||Y|243659||250 Bishopsgate|7 Newgate Street||AXA Framlington UK Select Opportunities Fund|||||GB|6430|250 Bishopsgate||LONDON|London||SHT|AXA FRAMLINGTON UK SELECT|false|||London|||GB|||GS_Client_Entitydata/official/20200831/608521|||GB|GB|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200831/part-00146-e752779a-df13-438c-af19-a7deeb9c4773.c000.snappy.orc||608521|2021_03_14|608521|GB||GBLEI581831|Fund|GB|ERDS||GB|AXA Fram UK Sel Opps Fd|||EC1A 7NX|Fund|false|GBM|PAV||29252314||29252314|7 Newgate Street|EC2M 4AA|2020-08-31T12:39:24.952Z|false||||N|false||3/GB/LONDON|Client Central Message Version 2.7||29252314||||EC2M 4AA|London||erds|20210314||HK||GS_Client_Entitydata/official/20171220/2641428||CHEUNG SAU LEE SHIRLEY|||19e4a4f4b9b39fc86bb20c69555b6237|||||||100||HK|||ERDS|2.5|false||||9998|19e4a4f4b9b39fc86bb20c69555b6237|||erds|4|19e4a4f4b9b39fc86bb20c69555b6237||GBM|||||||||100|||ACT||ACT||9998|ERDS|||||CHEUNG SAU LEE SHIRLEY|I||||||false||N|243659|||||CHEUNG SAU LEE SHIRLEY|||||HK|||||||SHT||false|||||||CLP||GS_Client_Entitydata/official/20171220/2641428|||HK||hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191030/part-00173-9eb2d1f1-8b11-4bc6-9b65-e99c16ed8a60.c000.snappy.orc|||2021_03_14|2641428||||Corporate||ERDS||HK|CHEUNG SAU LEE SHIRLEY HK||||Corporate|false|GBM|PAV|||||||2017-12-20T16:29:12.204Z|false||||N|false||3//|Client Central Message Version 2.5||||I|||||erds|20210314
6499||SE||GS_Client_Entitydata/official/20200525/4006872||Pareto Securities AB|PO Box 7415,Berzelii Park 9||de997ff39403bd41124b1d19584fae6a|Stockholm|||103 91|549300446KJF7NHIXJ61||100|SE|SE|4329836||ERDS|2.7|false||BERZELII PARK 9||6499|135b005458749b8552d0840726dcb36d|ISD|549300446KJF7NHIXJ61|erds|21|de997ff39403bd41124b1d19584fae6a||GBM||||2612600|SE|||556206-8956|100|||ACT|PARETO SECURITIES AB|ACT|PARETO SECURITIES AB|6499|ERDS||||549300446KJF7NHIXJ61|Pareto Securities AB|I|SE5562068956|Stockholm|549300446KJF7NHIXJ61|||false||Y|243659||PO Box 7415,Berzelii Park 9|Berzelii Park 9||Pareto Securities AB|||||SE|6499|PO Box 7415,Berzelii Park 9|OHMJSESS|STOCKHOLM|Stockholm||SHT|PARETO SECURITIES AB|false|||Stockholm|||SE|BDS||GS_Client_Entitydata/official/20200525/4006872|||SE|SE|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200528/part-00059-e71475fd-c8e9-4a8f-b68e-d862379b1602.c000.snappy.orc||4329836|2021_03_14|4006872|SE||SE5562068956|FinancialInstitution|SE|ERDS||SE|Pareto Securities AB|OHMJSESS||103 91|FinancialInstitution|false|GBM|FUV||5948263||5948263|Berzelii Park 9|103 91|2020-05-25T09:58:53.248Z|true||||N|false|556206-8956|3/SE/STOCKHOLM|Client Central Message Version 2.7||5948263|2612600|I||103 91|Stockholm||erds|20210314
6832||GB||GS_Client_Entitydata/official/20200224/2622178|MULCTURE HALL ROAD|MR RAFFAELE SCALFARO|||30e40f5f286f0ed9168af453c189274d|HALIFAX|||HX1 1DP|||100|GB|GB|||ERDS|2.6|false||WOOL MERCHANT HOTEL||6820|92bccfe84c04fccc92b845aff4ebdfcd|||erds|3|92bccfe84c04fccc92b845aff4ebdfcd||GBM|||||||||100|||ACT|MR RAFFAELE SCALFARO|ACT|MR RAFFAELE SCALFARO|6820|ERDS|MULCTURE HALL ROAD||||MR RAFFAELE SCALFARO|I||HALIFAX||||false||Y|243659|||WOOL MERCHANT HOTEL||MR RAFFAELE SCALFARO|||||GB|6832|||HALIFAX|||SHT|MR RAFFAELE SCALFARO|false|||||||CLP||GS_Client_Entitydata/official/20200224/2622178|MULCTURE HALL ROAD||GB||hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200229/part-00181-ff668f79-c5ed-412b-abde-3967e5f44e56.c000.snappy.orc|||2021_03_14|4440730|GB|||Corporate|GB|ERDS||GB|SCALFARO MR RAFFAELE|SWIFTBIC||HX1 1DP|Corporate|false|GBM|PAV|||||WOOL MERCHANT HOTEL||2020-02-24T10:53:07.527Z|false||||N|false||3/GB/HALIFAX|Client Central Message Version 2.6||||I|||||erds|20210314
4511||HK||GS_Client_Entitydata/official/20200915/1296253|Central and Western District,HKI|AAC Technologies Limited|100 Queen's Rd Central||30e40f5f286f0ed9168af453c189274d|Hong Kong|BYRT|||||100|HK|HK|4328860||ERDS|2.7|false||100 QUEEN'S RD CENTRAL||4510|5bbfe924b8407e30d37075dfa2bf918b|||erds|14|1b42f62c63ab52984426bed73b1e0f1b|C+W Dt,HKI|GBM|C+W Dt,HKI|2004-12-07Z||4420183|HK|4440730||0938665|100|||ACT|AAC TECHNOLOGIES LIMITED|ACT|AAC TECHNOLOGIES LIMITED|4510|ERDS|Central and Western District,HKI||||AAC Technologies Limited|I|HK0000096036|Hong Kong||||true|C+W Dt,HKI|Y|243659||100 Queen's Rd Central|100 Queen's Rd Central||AAC Technologies Limited||CMB|||HK|4511|100 Queen's Rd Central||HONG KONG|Hong Kong||SHT|AAC TECHNOLOGIES LIMITED|false|||Hong Kong|||HK|CLP||GS_Client_Entitydata/official/20200915/1296253|CEN AND WESTERN DISTRICT,HKI||HK|HK|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200915/part-00052-90c43234-f9b8-4ec7-bfde-9bbcd36516cf.c000.snappy.orc||4328860|2021_03_14|4011598|HK||HK0000096036|Corporate|HK|ERDS||HK|AAC Technologies Limited|OHMJSESS|||Corporate|false|GBM|PAV||8145940||8145940|100 Queen's Rd Central||2020-09-15T08:10:26.524Z|false||||N|false|0938665|3/HK/HONG KONG|Client Central Message Version 2.7||8145940|4420183|I|||Hong Kong||erds|20210314
6820||HK||GS_Client_Entitydata/official/20200414/25948671|183 Queens Road,East HK|EBA Hong Kong (Liao Ning) Developments No.3 Limited|||30e40f5f286f0ed9168af453c189274d|Hong Kong||||||100|HK|HK|25948671||ERDS|2.6|false||LEVEL 54 HOPEWELL CENTRE||6810|bc757aeb24d01586124ef33dfeb74371|||erds|2|bc757aeb24d01586124ef33dfeb74371||GBM||2013-11-07Z||||4440730||1992813|100|||ACT|EBA HONG KONG (LIAO NING) DEVELOPMENTS NO.3 LIMITED|ACT|EBA HONG KONG (LIAO NING) DEVELOPMENTS NO.3 LIMITED|6810|ERDS|183 Queens Road,East HK|DEVELOPMENTS NO.3 LIMITED|||EBA Hong Kong (Liao Ning) Developments No.3 Limited|I||Hong Kong||||false||Y|243659|||Level 54 Hopewell Centre||EBA Hong Kong (Liao Ning) Developments No.3 Limited|||||HK|6820|||HONG KONG|||SHT|EBA HONG KONG (LIAO NING)|false|||||||CLP||GS_Client_Entitydata/official/20200414/25948671|183 QUEENS ROAD,EAST HK||HK||hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200430/part-00098-ed8313c0-a36a-46bb-aaad-66b8bf9f7aa2.c000.snappy.orc||25948671|2021_03_14|25948671|HK|||Corporate|HK|ERDS||HK|EBA HK Liao Ning De No 3 Ltd||||Corporate|false|GBM|PAV||31761754||31761754|Level 54 Hopewell Centre||2020-04-14T17:11:23.651Z|false||||N|false|1992813|3/HK/HONG KONG|Client Central Message Version 2.6||31761754||I|||||erds|20210315
6492||IN||GS_Client_Entitydata/official/20210126/26523459|G- Block|Trust AMC Trustee Private Limited|802, 8th Floor, G - Block, Naman Center||8abd0b55aadb149c084a12235210b5f6|Mumbai|||400051|||100|IN|IN|26523459|Maharashtra|ERDS|2.8|false||802, 8TH FLOOR||6492|8abd0b55aadb149c084a12235210b5f6|||erds|2|8abd0b55aadb149c084a12235210b5f6|Bandra Kurla Complex,Bandra (E)|GBM|Bandra Kurla Complex,Bandra (E)|2017-12-13Z|||IN|||U65929MH2017PTC302821|100|||ACT|TRUST AMC TRUSTEE PRIVATE LIMITED|ACT|TRUST AMC TRUSTEE PRIVATE LIMITED|6492|ERDS|G- Block||||Trust AMC Trustee Private Limited|I|IN0017010364|Mumbai||||false|Bandra Kurla Complex,Bandra (E)|Y|243659||802, 8th Floor, G - Block, Naman Center|802, 8th Floor||Trust AMC Trustee Private Limited|||||IN|6492|802, 8th Floor, G - Block, Naman Center||MAHARASHTRA MUMBAI|Mumbai|Maharashtra|SHT|TRUST AMC TRUSTEE PRIVATE LIMITED|false||Maharashtra|Mumbai|||IN|FIG||GS_Client_Entitydata/official/20210126/26523459|G- BLOCK||IN|IN|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20210126/part-00042-cfa5cf38-cb8a-43f7-a903-57bad198715c.c000.snappy.orc||26523459|2021_03_14|26523459|IN||IN0017010364|FinancialInstitution|IN|ERDS||IN|Trust AMC Trustee Privat Ltd|||400051|FinancialInstitution|false|GBM|PAV||40916510|Maharashtra|40916510|802, 8th Floor|400051|2021-01-26T15:56:26.760Z|false||||N|false|U65929MH2017PTC302821|3/IN/MAHARASHTRA MUMBAI|Client Central Message Version 2.8||40916510||I||400051|Mumbai||erds|20210315
6420||CA||GS_Client_Entitydata/official/20201109/4083069||Brookfield BRP Holdings (Canada) Inc.|181 Bay Street Suite 300||bde0f275fb7cd329ee60d5188e8312a0|Toronto|CFUU||M5J 2T3|549300NSPD7QJ50GZW26||100|CA|CA|73258|ON|ERDS|2.7|false||181 BAY STREET SUITE 300||6420|b5474c7e1fabff1c686538f23d1434d1|ISD|549300NSPD7QJ50GZW26|erds|22|bde0f275fb7cd329ee60d5188e8312a0||GBM||2011-03-08Z||4215168|CA|||2277041|100|||ACT|BROOKFIELD BRP HOLDINGS (CANADA) INC.|ACT|BROOKFIELD BRP HOLDINGS (CANADA) INC.|6420|ERDS||INC.||549300NSPD7QJ50GZW26|Brookfield BRP Holdings (Canada) Inc.|I|CA*906117576|Toronto|549300NSPD7QJ50GZW26|||true||Y|243659||181 Bay Street Suite 300|181 Bay Street Suite 300||Brookfield BRP Holdings (Canada) Inc.||CMB|||CA|6420|181 Bay Street Suite 300||ON TORONTO|Toronto|ON|AKA|BROOKFIELD BRP HOLDINGS (CANADA)|false||ON|Toronto|||CA|CLP||GS_Client_Entitydata/official/20201109/4083069|||CA|CA|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20201109/part-00043-d124162e-a8d8-44c8-8c16-be60a83ca131.c000.snappy.orc||73258|2021_03_14|4083069|CA||CA*906117576|Corporate|CA|ERDS||CA|CanHoldco|||M5J 2T3|Corporate|false|GBM|PAV||26105057|ON|26105057|181 Bay Street Suite 300|M5J 2T3|2020-11-09T18:30:27.787Z|false||||N|false|2277041|3/CA/ON TORONTO|Client Central Message Version 2.7||26105057|4215168|I||M5J 2T3|Toronto||erds|20210315
6430||US||GS_Client_Entitydata/official/20200618/340942|1370 Avenue of the Americas|Pharo Macro Fund, Ltd.|PO Box 31106,2nd Floor||606dc969a8c362da269e6f2978cd9b2a|New York|PMFU||10019|7J8V0LJ3O9OMBWX17035||100|US|KY|340942||ERDS|2.7|false|205400681|26TH FLOOR||6430|606dc969a8c362da269e6f2978cd9b2a|ISD|7J8V0LJ3O9OMBWX17035|erds|34|606dc969a8c362da269e6f2978cd9b2a|89 Nexus Way,Camana Bay|GBM|89 Nexus Way,Camana Bay|2005-04-13Z|||KY|||147219|100|NL||ACT|PHARO MACRO FUND, LTD.|ACT|PHARO MACRO FUND, LTD.|6430|ERDS|1370 Avenue of the Americas|||7J8V0LJ3O9OMBWX17035|Pharo Macro Fund, Ltd.|I|KYIA8051725475327|New York|7J8V0LJ3O9OMBWX17035|||false|89 Nexus Way,Camana Bay|Y|243659||PO Box 31106,2nd Floor|26th Floor||Pharo Macro Fund, Ltd.||||Willemstad|KY|6430|PO Box 31106,2nd Floor||NY NEW YORK|George Town|NY|SHT|PHARO MACRO FUND, LTD.|false||NY|George Town|||KY|CLP||GS_Client_Entitydata/official/20200618/340942|1370 AVENUE OF THE AMERICAS||US|KY|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20200618/part-00000-03e8891b-0cbd-40c9-9a5f-87a11e1bd833.c000.snappy.orc|C/O Citco Fund Services (Curacao) BV|340942|2021_03_14|340942|US|Kaya Flamboyan 9, PO Box 4774|KYIA8051725475327|Corporate|US|ERDS||US|Pharo Macro Fund Ltd||false|10019|Corporate|false|GBM|FUV||25117064||25117064|26th Floor|KY1-1205|2020-06-18T09:36:17.980Z|false||||N|false|147219|3/US/NY NEW YORK|Client Central Message Version 2.7||25117064||I|Curacao|KY1-1205|George Town||erds|20210315
8411||RO||GS_Client_Entitydata/official/20190320/280137|Piata Victoriei Nr. 1,Sector 1|Government of Romania|Palatul Victoria,Sector 1||8bc9ac0fc1d9fda2f41370bb2025a6bc|Bucharest|XBRO||011791|||100|RO|RO|280137||ERDS|2.6|false||PALATUL VICTORIA||8411|8bc9ac0fc1d9fda2f41370bb2025a6bc|||erds|11|8bc9ac0fc1d9fda2f41370bb2025a6bc|Piata Victoriei Nr. 1|GBM|Piata Victoriei Nr. 1||||RO|268386|||100|||ACT|GOVERNMENT OF ROMANIA|ACT|GOVERNMENT OF ROMANIA|8411|ERDS|Piata Victoriei Nr. 1,Sector 1||7718618||Government of Romania|U||Bucharest||||false|Piata Victoriei Nr. 1|Y|243659||Palatul Victoria,Sector 1|Palatul Victoria||Government of Romania|||||RO|8411|Palatul Victoria,Sector 1||BUCHAREST|Bucharest||SHT|GOVERNMENT OF ROMANIA|false|7718618||Bucharest||268386|RO|GOV||GS_Client_Entitydata/official/20190320/280137|PIATA VICTORIEI NR. 1,SECTOR 1||RO|RO|hdfs://HDP06/apps/hive/warehouse/erds_xds_prod/erds_xds_prod.db/erds_maintable/run_date=20191030/part-00107-9eb2d1f1-8b11-4bc6-9b65-e99c16ed8a60.c000.snappy.orc||280137|2021_03_14|280137|RO|||Sovereign|RO|ERDS||RO|Government of Romani|||011791|Sovereign|false|GBM|PAV||4642367||4642367|Palatul Victoria|011791|2019-03-20T10:18:52.680Z|false||||N|false||3/RO/BUCHAREST|Client Central Message Version 2.6||4642367||U||011791|Bucharest||erds|20210315cat: ./src/test/resources/data/utils: Is a directory
trade_identifier|trade_identifier_source|originating_system|originating_system_trade_id|version|associate_trade_identifier|event|status|execution_timestamp|capture_date|trade_date|settlement_date|maturity_date|product_type|product_family|isda_plus_taxonomy|security_type|security_hsbc_id|security_bloomberg_id|security_isin|security_cusip|security_sedol|security_ric|underlying_security_id|underlying_security_name|buy_amount|buy_currency|sell_amount|sell_currency|settlement_amount|settlement_currency|buy_sell_indicator|margin_account|fixing_date|quantity|trade_price|hsbc_entity|hsbc_entity_po|book_name_source|book_id_source|cpty_treats_acronym|cpty_grid|cpty_ber_id|cpty_id_other|cpty_id_other_type|executing_broker|customer_identifier|option_type|option_length|option_strike_price|option_premium|settlement_delivery_method|trader_id|trader_peoplesoft_id|trade_type|composite_id|composite_product_code|hsbc_entity_country|trader_name|sales_person|run_date|processing_timestamp|trade_tresata_id|cpty_tresata_id|executing_broker_tresata_id|troux_id_source|booking_country|trade_hti|clean_price|dirty_price|bd_dp_lineage|physical_source|book_id_hms|execution_type|execution_facility|clearing_broker|clearing_broker_tresata_id|ratesense|primary_currency|counter_currency|primary_value_date|counter_value_date|primary_option_end_date|counter_option_end_date|option_end_date|original_trade_id|spot_date|swap_points|exchange_id|mkt_fwd_rate|corporate_margin|sales_margin|spot_rate|cleared_flag|clearing_timestamp|clearing_status|clearing_trade_id|conf_platform|conf_platform_trade_id|cpty_clearing_ber_id|cpty_clearing_grid|cpty_clearing_treats_acronym|cpty_clearing_lei|cpty_lei|cpty_original_ber_id|cpty_original_grid|cpty_original_treats_acronym|cpty_original_lei|cpty_remaining_ber_id|cpty_remaining_grid|cpty_remaining_treats_acronym|cpty_remaining_lei|cpty_transferee_ber_id|cpty_transferee_grid|cpty_transferee_treats_acronym|cpty_transferee_lei|cpty_transferor_ber_id|cpty_transferor_grid|cpty_transferor_treats_acronym|cpty_transferor_lei|end_date|execution_venue_type|ext_booking_platform|ext_booking_platform_trade_id|hsbc_desk|hsbc_entity_ber_id|hsbc_entity_grid|hsbc_entity_organizationtype|hsbc_entity_partyname|hti_version|leg_sequence|leg_day_count_fraction|leg_notional_currency|leg_floating_rate_index|leg_formula|leg_maturity_date|leg_notional_amount|leg_pay_receive|leg_rate|leg_spread|leg_start_date|conf_method|executing_broker_trade_id|executing_broker_name|collateralization_type|swaption_straddle|option_excercise_type|datasubmitter_id|option_style|product_name|product_subtype|reportable_flag|reporting_regime|salesperson_id|salesperson_name|salesperson_peoplesoft_id|start_date|supervisory_body|termination_timestamp|trade_timestamp|usi|usi_prefix|usi_prior|usi_prior_prefix|value_date|reporting_role|trade_category|trading_capacity|intent_to_clear|termination_reason|termination_type|mirror_trade_id|leg_fix_float|leg_far_near|branch|event_subtype|previous_trade_identifier|cpty_name|cpty_treats_parent_acronym|principal_amount|nominal_amount|currency|security_treats_master_number
22244364|DTP_GB|SOPHIS|14639607||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
22244365|DTP_GB|SOPHIS|14639609||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
22244366|DTP_GB|SOPHIS|14639621||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
22244370|DTP_GB|SOPHIS|14639615||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
22244372|DTP_GB|SOPHIS|14639603||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
11144364|DTP_GB||14639607||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
11144365|DTP_GB||14639609||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
11144366|DTP_GB||14639621||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
11144370|DTP_GB||14639615||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
11144372|DTP_GB||14639603||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||trade_identifier|trade_identifier_source|originating_system|originating_system_trade_id|version|associate_trade_identifier|event|status|execution_timestamp|capture_date|trade_date|settlement_date|maturity_date|product_type|product_family|isda_plus_taxonomy|security_type|security_hsbc_id|security_bloomberg_id|security_isin|security_cusip|security_sedol|security_ric|underlying_security_id|underlying_security_name|buy_amount|buy_currency|sell_amount|sell_currency|settlement_amount|settlement_currency|buy_sell_indicator|margin_account|fixing_date|quantity|trade_price|hsbc_entity|hsbc_entity_po|book_name_source|book_id_source|cpty_treats_acronym|cpty_grid|cpty_ber_id|cpty_id_other|cpty_id_other_type|executing_broker|customer_identifier|option_type|option_length|option_strike_price|option_premium|settlement_delivery_method|trader_id|trader_peoplesoft_id|trade_type|composite_id|composite_product_code|hsbc_entity_country|trader_name|sales_person|run_date|processing_timestamp|trade_tresata_id|cpty_tresata_id|executing_broker_tresata_id|troux_id_source|booking_country|trade_hti|clean_price|dirty_price|bd_dp_lineage|physical_source|book_id_hms|execution_type|execution_facility|clearing_broker|clearing_broker_tresata_id|ratesense|primary_currency|counter_currency|primary_value_date|counter_value_date|primary_option_end_date|counter_option_end_date|option_end_date|original_trade_id|spot_date|swap_points|exchange_id|mkt_fwd_rate|corporate_margin|sales_margin|spot_rate|cleared_flag|clearing_timestamp|clearing_status|clearing_trade_id|conf_platform|conf_platform_trade_id|cpty_clearing_ber_id|cpty_clearing_grid|cpty_clearing_treats_acronym|cpty_clearing_lei|cpty_lei|cpty_original_ber_id|cpty_original_grid|cpty_original_treats_acronym|cpty_original_lei|cpty_remaining_ber_id|cpty_remaining_grid|cpty_remaining_treats_acronym|cpty_remaining_lei|cpty_transferee_ber_id|cpty_transferee_grid|cpty_transferee_treats_acronym|cpty_transferee_lei|cpty_transferor_ber_id|cpty_transferor_grid|cpty_transferor_treats_acronym|cpty_transferor_lei|end_date|execution_venue_type|ext_booking_platform|ext_booking_platform_trade_id|hsbc_desk|hsbc_entity_ber_id|hsbc_entity_grid|hsbc_entity_organizationtype|hsbc_entity_partyname|hti_version|leg_sequence|leg_day_count_fraction|leg_notional_currency|leg_floating_rate_index|leg_formula|leg_maturity_date|leg_notional_amount|leg_pay_receive|leg_rate|leg_spread|leg_start_date|conf_method|executing_broker_trade_id|executing_broker_name|collateralization_type|swaption_straddle|option_excercise_type|datasubmitter_id|option_style|product_name|product_subtype|reportable_flag|reporting_regime|salesperson_id|salesperson_name|salesperson_peoplesoft_id|start_date|supervisory_body|termination_timestamp|trade_timestamp|usi|usi_prefix|usi_prior|usi_prior_prefix|value_date|reporting_role|trade_category|trading_capacity|intent_to_clear|termination_reason|termination_type|mirror_trade_id|leg_fix_float|leg_far_near|branch|event_subtype|previous_trade_identifier|cpty_name|cpty_treats_parent_acronym|principal_amount|nominal_amount|currency|security_treats_master_number
22244364|DTP_GB|SOPHIS|14639607||||||||||DTPEquityForward|DTPEquityDerivative||||||||||||||||||||||||||||||||||||||||||||||20190121||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||trade_identifier|originating_system|originating_system_trade_id|run_date|trade_identifier_source|product_family|product_type
22244364|SOPHIS|14639607|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244365|SOPHIS|14639609|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244366|SOPHIS|14639621|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244370|SOPHIS|14639615|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
22244372|SOPHIS|14639603|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144364||14639607|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144365||14639609|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144366||14639621|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144370||14639615|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward
11144372||14639603|20190121|DTP_GB|DTPEquityDerivative|DTPEquityForward# Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.out
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.logger.org.apache.orc=DEBUG
log4j.logger.com.hsbc.gbm.bd.dataproducts=INFO

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark-project.jetty=WARN
log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
  spark:
    spark.master: "local"
    spark.ui.enabled: "false"
    spark.app.id: "123456"
    spark.driver.host: "localhost"
    spark.local.dir : "tmp/spark-temp"
    spark.sql.catalogImplementation: hive
    spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    derby.system.home: "tmp/hive/"
    javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
    #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow

    es.nodes: localhost:9200
    es.net.http.auth.user: elastic
    es.net.http.auth.pass: NONENC_changeme
    es.port: 9200
    es.index.auto.create: true
    cluster.name: elasticsearch
    client.transport.sniff: true
    client.transport.ignore_cluster_name: true
    es.xpack.security.user: elastic:byiF7KeOP9EtCtkmgMlyttOWcO55La2WasVQb1oKnss=
    index.auto.create: true
    index.number_of_replicas: 0
    index.refresh_interval: 1s

  app:
    app_name: "dataproducts-common"
    monitoring:
      location: "tmp/monitoring"
      table.name: "monitoring"
    some-key:
      string-list:
        - item1
        - item2
        - item3
      int-list:
        - 101
        - 1045
        - 8625
      double-list:
        - 1.01
        - 104.5
        - 8625.16548
      map-of-strings:
          item1: value1
          item2: value2
          item3: value3
      map-of-ints:
          item1: 168
          item2: 2569
          item3: -14
      map-of-doubles:
          item1: 16.81256
          item2: 2.5690745
          item3: -0.1425456
      map-of-booleans:
          item1: true
          item2: false
          item3: true
    bool-property: true
    int-property: 123
    double-property: 456.789


spark:
  spark.driver.allowMultipleContexts: true
  spark.master: "local"
  spark.local.dir : "tmp/spark-temp"
  spark.sql.catalogImplementation: hive
  spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
  spark.serializer: org.apache.spark.serializer.KryoSerializer
  derby.system.home: "tmp/hive/"
  javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
  #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow
app:
  app_name: "dataproducts-common"


  spark:
    spark.master: "local"
    spark.ui.enabled: "false"
    spark.app.id: "123456"
    spark.driver.host: "localhost"
    spark.local.dir : "tmp/spark-temp"
    spark.sql.catalogImplementation: hive
    spark.sql.warehouse.dir: "/tmp/hive/common/warehouse"
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    derby.system.home: "tmp/hive/"
    javax.jdo.option.ConnectionURL: "jdbc:derby:;databaseName=tmp/hive/metastore_db;create=true"
    #kryoserializer: org.apache.spark.sql.catalyst.expressions.GenericInternalRow
  app:
    app_name: "dataproducts-common"
    monitoring:
      location: "tmp/monitoring"
      table.name: "monitoring"
    some-key:
      list:
        - item1
        - item2
        - item3
    bool-property: true
    int-property: 123
    double-property: 456.789
  elastic:
    es.nodes: localhost:9200
    es.net.http.auth.user: elastic
    es.net.http.auth.pass: NONENC_changeme
    es.port: 9200
    es.index.auto.create: true
    cluster.name: elasticsearch
    client.transport.sniff: true
    client.transport.ignore_cluster_name: true
    es.xpack.security.user: elastic:byiF7KeOP9EtCtkmgMlyttOWcO55La2WasVQb1oKnss=
    index.auto.create: true
    index.number_of_replicas: 0
    index.refresh_interval: 1s


cat: ./src/test/scala: Is a directory
cat: ./src/test/scala/com: Is a directory
cat: ./src/test/scala/com/hsbc: Is a directory
cat: ./src/test/scala/com/hsbc/gbm: Is a directory
cat: ./src/test/scala/com/hsbc/gbm/bd: Is a directory
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts: Is a directory
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark: Is a directory
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/env: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.env

import org.scalatest.FunSuite

class SparkEnvTest extends FunSuite with SparkEnv {

  def appName(): String = "spark-config-test"

  def configLocation(): String = "src/test/resources/test-application-scala.yml"


  test("Test getting a string value from appConfig") {
    val value: String = appConfig("monitoring.table.name")
    assert(value === "monitoring")
  }

  test("Test getting an integer value from appConfig") {
    val value: Int = appConfig("int-property")
    assert(value === 123)
  }

  test("Test getting a double value from appConfig") {
    val value: Double = appConfig("double-property")
    assert(value === 456.789)
  }

  test("Test getting a boolean value from appConfig") {
    val value: Boolean = appConfig("bool-property")
    assert(value === true)
  }

  test("Test getting a list of strings from appConfig") {
    val value: List[String] = appConfig("some-key.string-list")
    assert(value === List("item1", "item2", "item3"))
  }

  test("Test getting a map of strings from appConfig") {
    val value: Map[String, String] = appConfig("some-key.map-of-strings")
    assert(value === Map("item1" -> "value1", "item2" -> "value2" , "item3" -> "value3"))
  }

  test("Test getting a map of ints from appConfig") {
    val value: Map[String, Int] = appConfig("some-key.map-of-ints")
    assert(value === Map("item1" -> 168, "item2" -> 2569 , "item3" -> -14))
  }

  test("Test getting a map of doubles from appConfig") {
    val value: Map[String, Double] = appConfig("some-key.map-of-doubles")
    assert(value === Map("item1" -> 16.81256, "item2" -> 2.5690745 , "item3" -> -0.1425456))
  }

  test("Test getting a map of booleans from appConfig") {
    val value: Map[String, Boolean] = appConfig("some-key.map-of-booleans")
    assert(value === Map("item1" -> true, "item2" -> false , "item3" -> true))
  }

  test("Test getting a list of integers from appConfig") {
    val value: List[Int] = appConfig("some-key.int-list")
    assert(value === List(101, 1045, 8625))
  }

  test("Test getting a list of doubles from appConfig") {
    val value: List[Double] = appConfig("some-key.double-list")
    assert(value === List(1.01, 104.5, 8625.16548))
  }

  test("Test non-existing key") {
    val thrown = intercept[NoSuchElementException] {
      val value: List[Double] = appConfig("invalid-key")
    }
    assert(thrown.getMessage === "key not found: invalid-key")
  }

  test("Test non-existing list key with default value") {
    val value: List[Int] = appConfig("invalid-key", List(1, 2, 3))
    assert(value === List(1, 2, 3))
  }

  test("Test non-existing string key with default value") {
    val value: String = appConfig("invalid-key", "DefaultValue")
    assert(value === "DefaultValue")
  }

}
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/io: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.io

import com.hsbc.gbm.bd.dataproducts.schema.{DataEntity, SchemaUtils}
import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils._
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions.lit
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class AssetWriterTest extends FunSuite with BeforeAndAfterAll with SparkEnv {

  def appName(): String = "asset-writer-test"

  def configLocation(): String = "src/test/resources/test-application-scala.yml"

  private val workDir: String = System.getProperty("user.dir").replace('\\', '/')
  private val assetWriter = new AssetWriter(spark, "xds_uk_prod", DataEntity.TRADE_ASSET,
    Option.empty, s"file:///$workDir/src/test/resources/out/asset", false)

  private val resourcesBaseDir = "src/test/resources"

  override def beforeAll(): Unit = {
    createHiveDatabases(spark, "xds_uk_prod", "asset_db")
    csvToParquetFile(spark, s"$resourcesBaseDir/data/io/asset-writer-dataset-1.csv", "src/test/resources/out/source-trade-asset.parq")
    csvToParquetFile(spark, s"$resourcesBaseDir/data/io/asset-writer-dataset-2.csv", "src/test/resources/out/source-dataset.parq")

    val jsonSchema = SchemaUtils.getSchemaOf(DataEntity.TRADE_ASSET)
    val recordsToBeWritten = spark.read.parquet("src/test/resources/out/source-trade-asset.parq")

    val alignedRecords = assetWriter.alignToSchema(recordsToBeWritten, jsonSchema)

    alignedRecords
      .write
      .partitionBy("run_date", "physical_source")
      .option("path", s"file:///$workDir/src/test/resources/out/asset/trade")
      .mode(SaveMode.Overwrite)
      .saveAsTable("xds_uk_prod.trade")

    alignedRecords
      .drop("branch", "cpty_name", "cpty_treats_parent_acronym")
      .write
      .partitionBy("run_date", "physical_source")
      .option("path", s"file:///$workDir/src/test/resources/out/asset_db/trade")
      .mode(SaveMode.Overwrite)
      .saveAsTable("asset_db.trade")
  }

  override def afterAll(): Unit = {
    dropHiveDatabases(spark, "xds_uk_prod", "asset_db")
    deleteDirectoryIfExists("src/test/resources/out")
  }

  test("Test drop table partition and write") {

    val records = spark.read.parquet("src/test/resources/out/source-dataset.parq")
    assetWriter.writeToTable(records)

    val actualDataset = spark.sql("select * from xds_uk_prod.trade")
      .sort("trade_identifier", "trade_identifier_source")

    val df = TestUtils
      .loadCSV(spark, "src/test/resources/data/io/asset-writer-expected-dataset.csv", Option.empty)
    val expectedDataset = TestUtils.alignDataFrameToSchema(df, DataEntity.TRADE_ASSET).sort("trade_identifier", "trade_identifier_source")

    assertDatasetsEqualWithColumnsIgnored(expectedDataset, actualDataset, false,
      "processing_timestamp", "bd_dp_lineage")
  }

  test("Test update table schema and write") {

    val assetWriter = new AssetWriter(spark, "asset_db", DataEntity.TRADE_ASSET,
      Option.empty, s"file:///$workDir/src/test/resources/out/asset_db", true)

    val schemaBeforeWrite = spark.catalog.listColumns("asset_db", "trade")
      .collect().map(_.name)

    assert(!schemaBeforeWrite.contains("branch"))
    assert(!schemaBeforeWrite.contains("cpty_name"))
    assert(!schemaBeforeWrite.contains("cpty_treats_parent_acronym"))

    val records = spark.read.parquet("src/test/resources/out/source-dataset.parq")
    assetWriter.writeToTable(records)

    val schemaAfterWrite = spark.catalog.listColumns("asset_db", "trade")
      .collect().map(_.name)

    assert(schemaAfterWrite.contains("branch"))
    assert(schemaAfterWrite.contains("cpty_name"))
    assert(schemaAfterWrite.contains("cpty_treats_parent_acronym"))
  }

  test("Test asset writer output count") {
    val outputCount = assetWriter.writerStats.outputCount
    assert(outputCount == 3)
  }

  test("Test asset writer output location") {
    val outputLocation = assetWriter.writerStats.outputLocation
    assert(outputLocation == "xds_uk_prod.trade")
  }

  test("Test asset writer when zero records to write") {
    val assetWriter = new AssetWriter(spark, "xds_uk_prod", DataEntity.TRADE_ASSET,
      Option.empty, s"file:///$workDir/src/test/resources/out/asset", false)

    val df = spark.emptyDataFrame
      .withColumn("run_date", lit("20210222"))
      .withColumn("physical_source", lit("TREATS_FWD"))

    assetWriter.writeToTable(df)

    assert(assetWriter.writerStats.outputCount == 0)
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.io.IOException

import com.github.tomakehurst.wiremock.client.WireMock.{exactly, getRequestedFor, urlEqualTo}
import com.hsbc.gbm.bd.dataproducts.spark.param.RawJobParams
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class MdaLocationResolverTest extends FunSuite with BeforeAndAfterAll with ResolverEndpoint {

  private val wireMockPort = WireMockInstance.serverPort
  private val nonWireMockPort = WireMockInstance.serverPort + 1

  override def beforeAll(): Unit = {
    WireMockInstance.startAndWaitInSeconds(10)

    setUpResolverLocationStub(s"/test/url")
    setUpResolverLocationStub(s"/test/url2")
    setUpResolverLocationErrorStub(s"/test/error")
    setUpResolverLocationDelayStub(s"/test/delay")
  }

  override def afterAll(): Unit = {
    WireMockInstance.stopAndWaitInSeconds(10)
  }

  test("Resolver should return the MDA location when it exists") {

    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url", RawJobParams(Array()))
    val locations = resolver.getAssetLocations(Set("ngt"))

    assert(locations("ngt") == "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/ngt/20210305.parq")
  }

  test("Resolver should return empty result when the MDA does not exist") {

    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url", RawJobParams(Array()))
    val locations = resolver.getAssetLocations(Set("none"))

    assert(locations.get("none").isEmpty)
  }

  test("Resolver should use locations from command line if available") {
    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url",
      RawJobParams(Array("ngtMdaLocation=/mda/locations/ngt-from-cmd.parq")))

    val locations = resolver.getAssetLocations(Set("ngt", "erds"))

    assert(locations == Map(
      "ngt" -> "/mda/locations/ngt-from-cmd.parq",
      "erds" -> "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/erds/erds.parq"))
  }

  test("Resolver should return best ref 1 location") {
    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url", RawJobParams(Array()))
    val locations = resolver.getAssetLocations(Set("level1-best-ref"))
    assert(locations == Map("level1-best-ref" -> "src/test/resources/out/global-tree-entity/2021-03-03_AF/best/level1.parq"))
  }

  test("Resolver should not call REST endpoint if all locations are provided on command line") {
    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url2",
      RawJobParams(Array("ngtMdaLocation=/mda/locations/ngt-from-cmd.parq")))

    val locations = resolver.getAssetLocations(Set("ngt"))

    assert(locations == Map(
      "ngt" -> "/mda/locations/ngt-from-cmd.parq"))

    WireMockInstance.server.verify(exactly(0),
      getRequestedFor(urlEqualTo("/test/url2")))
  }

  test("Resolver should convert command line arguments to asset types") {

    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/url",
      RawJobParams(Array("ngtMdaLocation=l1", "param2", "test=l2", "cvCisMdaLocation=l3",
        "cduAccountMdaLocation=l4", "level1BestRefMdaLocation=l5")))
    val locations = resolver.getAssetLocationsFromCommandLine

    assert(locations == Map("ngt" -> "l1", "cv-cis" -> "l3", "cdu-account" -> "l4", "level1-best-ref" -> "l5"))
  }

  test("Resolver should throw exception when unable to process endpoint response") {

    val resolver = new MdaLocationResolver(s"http://localhost:$wireMockPort/test/error", RawJobParams(Array()))

    val thrown = intercept[Exception] {
      resolver.getAssetLocations(Set("ngt"))
    }

    assert(thrown.getMessage === s"Server returned HTTP response code: 500 for URL: http://localhost:$wireMockPort/test/error")
  }

  test("Resolver should throw exception when unable to connect to endpoint") {

    val resolver = new MdaLocationResolver(s"http://localhost:$nonWireMockPort/test/error", RawJobParams(Array()))

    val thrown = intercept[IOException] {
      resolver.getAssetLocations(Set("ngt"))
    }

    assert(thrown.getMessage.startsWith("Connection refused"))
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.io

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.param.RawJobParams
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class MdaTypeParquetReaderTest
  extends FunSuite with BeforeAndAfterAll with ResolverEndpoint with SparkEnv {

  override def appName(): String = "MdaTypeParquetReaderTest"

  override def configLocation(): String = "src/test/resources/config/application.yml"

  private val wireMockPort = WireMockInstance.serverPort

  override def beforeAll(): Unit = {
    WireMockInstance.startAndWaitInSeconds(10)
    setUpResolverLocationStub(s"/mda-locations/latest")
    TestUtils.csvToParquetFile(spark, "src/test/resources/data/tree/erds-tree-output.csv",
      "src/test/resources/out/global-tree-entity/2021-03-03_AF/post/erds/erds.parq")
  }

  override def afterAll(): Unit = {
    WireMockInstance.stopAndWaitInSeconds(10)
    TestUtils.deleteDirectoryIfExists("src/test/resources/out")
  }

  test("Read non-existing asset type") {

    val thrown = intercept[RuntimeException] {
      val reader = new MdaTypeParquetReader(spark, "gcal",
        s"http://localhost:$wireMockPort/mda-locations/latest", RawJobParams(Array()))
      reader.read()
    }

    assert(thrown.isInstanceOf[RuntimeException])
  }

  test("Read existing asset type without date range") {

    val reader = new MdaTypeParquetReader(spark, "erds",
      s"http://localhost:$wireMockPort/mda-locations/latest", RawJobParams(Array()))
    val df = reader.read()

    assert(df.count() == 28)

  }

  test("Read existing asset type with date range") {

    val reader = new MdaTypeParquetReader(spark, "erds",
      s"http://localhost:$wireMockPort/mda-locations/latest", RawJobParams(Array()))
    val df = reader.read(Option("20210314"), Option("20210314"))

    assert(df.count() == 18)

  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils.{assertDatasetsEqual, csvToParquetFile}
import org.apache.spark.sql.functions.col
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class NamedOutputParquetWriterTest extends FunSuite with BeforeAndAfterAll with SparkEnv {

  def appName(): String = "output-writer-test"

  def configLocation(): String = "src/test/resources/test-application-scala.yml"

  lazy val writer = new NamedOutputParquetWriter(spark, "output-dataset", "src/test/resources/out", Seq("run_date"))

  override def beforeAll() {
    csvToParquetFile(spark, "src/test/resources/data/io/test-dataset.csv",
      "src/test/resources/out/output-dataset/output-dataset.parq", "run_date")
  }

  override def afterAll() {
//    deleteDirectoryIfExists("src/test/resources/out")
  }

  test("Test parquet write output records") {
    val dataset = spark.read
      .option("header", "true")
      .option("delimiter", "|")
      .csv("src/test/resources/data/io/test-dataset2.csv")

    writer.write(dataset)

    val actual = spark.read
      .parquet("src/test/resources/out/output-dataset/output-dataset.parq")
      .withColumn("run_date", col("run_date").cast("string"))
      .sort("trade_id", "update_date_time", "FOTradeBuySell")

    val expected = spark.read
      .option("header", "true")
      .option("delimiter", "|")
      .csv("src/test/resources/data/io/expected-dataset.csv")
      .sort("trade_id", "update_date_time", "FOTradeBuySell")

    assertDatasetsEqual(expected, actual, false)
  }

  test("Test parquet writer output count") {
    val outputCount = writer.writerStats.outputCount
    assert(outputCount == 6)
  }

  test("Test parquet writer output location") {
    val outputLocation = writer.writerStats.outputLocation
    assert(outputLocation == "src/test/resources/out/output-dataset/output-dataset.parq")
  }

  test("Test parquet writer when zero records to write") {
    val writer = new NamedOutputParquetWriter(spark, "output-dataset-2",
      "src/test/resources/out", Seq("run_date"))

    assert(writer.writerStats.outputCount == 0)
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.io

import com.github.tomakehurst.wiremock.client.WireMock._

trait ResolverEndpoint {

  private val responseStream = this.getClass.getClassLoader.getResourceAsStream("data/rest/location-endpoint-response.json")
  private val body: String = scala.io.Source.fromInputStream(responseStream).getLines.mkString("\n")

  def setUpResolverLocationStub(urlRegex: String): Unit = {
    WireMockInstance.server.stubFor(
      get(urlPathEqualTo(urlRegex))
        .willReturn(aResponse()
          .withStatus(200)
          .withBody(body)))
  }

  def setUpResolverLocationErrorStub(urlRegex: String): Unit = {
    WireMockInstance.server.stubFor(
      get(urlPathEqualTo(urlRegex))
        .willReturn(aResponse()
          .withStatus(500)))
  }

  def setUpResolverLocationDelayStub(urlRegex: String): Unit = {
    WireMockInstance.server.stubFor(
      get(urlPathEqualTo(urlRegex))
        .willReturn(aResponse()
          .withFixedDelay(6000)
          .withStatus(200)
          .withBody(body)))
  }
}
package com.hsbc.gbm.bd.dataproducts.spark.io

import java.net.ServerSocket
import java.util.concurrent.Callable
import java.util.concurrent.TimeUnit.SECONDS

import com.github.tomakehurst.wiremock.WireMockServer
import org.awaitility.Awaitility._
import org.slf4j.LoggerFactory

object WireMockInstance {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  val serverPort: Int = {
    val freePorts = (for (port <- 8080 to 8200 if isPortFree(port)) yield port).take(1)
    if (freePorts.nonEmpty) {
      val serverPort = freePorts.head
      logger.info(s"WireMock instance port [$serverPort]")
      serverPort
    } else throw new RuntimeException("No available port in range for WireMock")
  }

  val server = new WireMockServer(serverPort)

  def startAndWaitInSeconds(duration: Int): Unit = {
    if (!server.isRunning) {
      server.start()
    }

    val waitCondition: Callable[java.lang.Boolean] = new Callable[java.lang.Boolean]() {
      override def call(): java.lang.Boolean = server.isRunning
    }
    await("Starting WireMock server").atMost(duration, SECONDS).pollInterval(1, SECONDS).until(
      waitCondition
    )
  }

  def stopAndWaitInSeconds(duration: Int): Unit = {
    if (server.isRunning) {
      server.stop()
    }

    val waitCondition: Callable[java.lang.Boolean] = new Callable[java.lang.Boolean]() {
      override def call(): java.lang.Boolean = !server.isRunning
    }
    await("Stopping WireMock server").atMost(duration, SECONDS).pollInterval(1, SECONDS).until(
      waitCondition
    )
  }

  private def isPortFree(port: Int): Boolean = try {
    new ServerSocket(port)
    true
  } catch {
    case e: Exception => false
  }

}
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/job: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.job
import com.hsbc.gbm.bd.dataproducts.spark.annotation.SparkJobName
import com.hsbc.gbm.bd.dataproducts.spark.io.{NamedOutputParquetWriter, TableInputReader}
import com.hsbc.gbm.bd.dataproducts.spark.param.{RawJobParams, RunDateRange, RunDateRangeParamParser}

@SparkJobName("sample-spark-job")
class SampleSparkJob(jobName: String, configPath: String, rawParams: RawJobParams) extends SparkJob {

  val dateRange: RunDateRange = RunDateRangeParamParser.parse(rawParams)

  override def appName(): String = jobName

  override def configLocation(): String = configPath

  override def runDateRange: Option[RunDateRange] = Option(dateRange)

  override def readerStats: Option[InputReaderStats] = Option(inputReader.readerStats)

  override def writerStats: Option[OutputWriterStats] = Option(outputWriter.writerStats)

  val tableName: String = appConfig("table-name")
  val outputLocation: String = appConfig("output-location")
  val inputReader: TableInputReader = new TableInputReader(spark, countedSources = tableName)
  val outputWriter: NamedOutputParquetWriter =
    new NamedOutputParquetWriter(spark, "sample-asset", outputLocation, List("run_date"))

  override def process(): Map[String, String] = {
    val frame = inputReader.read(tableName, Option(dateRange.runDateFrom), Option(dateRange.runDateTo))
    outputWriter.write(frame)
    Map("tag_name" -> "tag_value")
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.job

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.runner.GenericSparkJobTestRunner
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils.{createHiveDatabases, csvToHiveTable, deleteDirectoryIfExists}
import com.hsbc.gbm.bd.dataproducts.spark.utils.UnitTestBase
import org.apache.spark.sql.Encoders
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class SampleSparkJobTest extends FunSuite with SparkEnv with BeforeAndAfterAll with UnitTestBase {

  override def testConfigFileClassPath(): String = "/config/application.yml"

  override protected def beforeAll(): Unit = {
    createHiveDatabases(spark, "test_db")
    csvToHiveTable(spark, "src/test/resources/data/job/test-asset.csv", "test_db.trade", Option.empty)
    GenericSparkJobTestRunner.main(Array("sample-spark-job", configLocation(), "20190121", "20190122"))
  }

  override protected def afterAll(): Unit = {
    spark.sql("drop database if exists test_db cascade")
    deleteDirectoryIfExists("src/test/resources/out")
  }

  test("Test sample spark job output") {
    val actual = spark.read.parquet("src/test/resources/out/asset/sample-asset/sample-asset.parq")
    assert(actual.count === 10)
  }

  test("Test sample spark job tags") {
    val monitoringRecord = spark.sql("select * from test_db.monitoring")
    monitoringRecord.show(false)
    val tags = monitoringRecord
      .select("tags")
      .map(row => row.getAs[String]("tags"))(Encoders.STRING)
      .collect()

    assert(tags.length == 1)

    val actualTags = tags(0).split(",")
      .map(tag => {
        val fragments = tag.split("=")
        (fragments(0), fragments(1))
      }).toMap
    assert(actualTags("implementation_version") == "null")
    assert(actualTags("implementation_title") == "null")
    assert(actualTags("job_name") == "sample-spark-job")
    assert(actualTags("configuration_profile") == "dev")
    assert(actualTags("tag_name") == "tag_value")
    assert(actualTags("application_id") != null)
  }
}
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/monitoring: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

import java.time.LocalDateTime

import akka.actor.{ActorSystem, Props}
import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.job.{InputReaderStats, OutputWriterStats}
import com.hsbc.gbm.bd.dataproducts.spark.lifecycle.{EndState, EventType, JobLifecycleEvent}
import com.hsbc.gbm.bd.dataproducts.spark.param.{RawJobParams, RunDateRange}
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils.{createHiveDatabases, dropHiveDatabases}
import com.hsbc.gbm.bd.dataproducts.spark.utils.UnitTestBase
import org.apache.spark.sql.Encoders
import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll, FunSuite}

import scala.collection.JavaConverters._
import scala.concurrent.Await
import scala.concurrent.duration.Duration

class MonitoringHdfsAgentTest extends FunSuite with SparkEnv with BeforeAndAfterAll with BeforeAndAfter with UnitTestBase {

  override def testConfigFileClassPath(): String = "/config/application.yml"

  override protected def beforeAll(): Unit = {
    createHiveDatabases(spark, "test_db")
  }

  override protected def afterAll(): Unit = {
    dropHiveDatabases(spark, "test_db")
  }

  after {
    spark.sql("drop table if exists test_db.monitoring")
  }

  test("Test monitoring hdfs agent for success") {
    val system = ActorSystem("lifecycle-event-system")
    val agent = system.actorOf(Props(classOf[MonitoringHdfsAgent], this))

    agent ! createStartedEvent()
    Thread.sleep(200)
    agent ! createFinishedEvent(EndState.Success, Option.empty)

    Thread.sleep(500)

    system.terminate()
    Await.result(system.whenTerminated, Duration(30, "seconds"))

    verifyRecord("success", Option.empty)
  }

  test("Test monitoring hdfs agent for failure") {
    val system = ActorSystem("lifecycle-event-system")
    val agent = system.actorOf(Props(classOf[MonitoringHdfsAgent], this))

    agent ! createStartedEvent()
    Thread.sleep(200)
    agent ! createFinishedEvent(EndState.Failure, Option("Invalid argument"))

    Thread.sleep(500)

    system.terminate()
    Await.result(system.whenTerminated, Duration(30, "seconds"))

    verifyRecord("failure", Option("Invalid argument"))
  }

  private def verifyRecord(status: String, errorMessage: Option[String]): Unit = {
    val expectedTags =
      if (errorMessage.isDefined) s"job_name=test-job,error_message=${errorMessage.get}"
      else "job_name=test-job"
    spark.sql("refresh table test_db.monitoring")
    val monitoring = spark.sql("select * from test_db.monitoring")
    val columnMappings = monitoring.columns.map(snakeCase => {
      val snakeCaseFragments = snakeCase.split("_")
      val camelCase = (snakeCaseFragments.head +: snakeCaseFragments.tail.map(_.capitalize)).mkString
      (snakeCase, camelCase)
    })

    val monitoringRenamed = columnMappings.foldLeft(monitoring)({ case (df, (k, v)) => df.withColumnRenamed(k, v) })
    val entries: List[TestMonitoringEntry] = monitoringRenamed.as[TestMonitoringEntry](Encoders.product).collectAsList().asScala.toList

    assert(entries.size === 1)
    val entry = entries.head
    assert(entry.uid === "identifier")
    assert(entry.startTime != null && !entry.startTime.isEmpty)
    assert(entry.endTime != null && !entry.endTime.isEmpty)
    assert(entry.sources === "source1")
    assert(entry.outputLocation === "output/location.parq")
    assert(entry.sourceCount === 101)
    assert(entry.outputCount === 202)
    assert(entry.tags === expectedTags)
    assert(entry.component === "ComponentName")
    assert(entry.runDate === "20200926-20200927")
    assert(entry.status === status)
    assert(entry.client === "data-products-1-common-dev")
  }

  private def createStartedEvent(): JobLifecycleEvent = {
    JobLifecycleEvent(
      "identifier",
      LocalDateTime.now(),
      EventType.Started,
      Option.empty,
      "ComponentName",
      "dev",
      Option.empty,
      Option.empty,
      RawJobParams(Array("20200926", "20200927")),
      Option(RunDateRange("20200926", "20200927")),
      Map("job_name" -> "test-job")
    )
  }

  private def createFinishedEvent(endState: EndState.EndState, errorMessage: Option[String]): JobLifecycleEvent = {
    val error = if (errorMessage.isDefined) Seq(("error_message", errorMessage.get)) else Nil
    val tags = Seq(("job_name", "test-job")) ++ error

    JobLifecycleEvent(
      "identifier",
      LocalDateTime.now(),
      EventType.Finished,
      Option(endState),
      "ComponentName",
      "dev",
      Option(InputReaderStats(Set("source1"), 101)),
      Option(OutputWriterStats("output/location.parq", 202)),
      RawJobParams(Array("20200926", "20200927")),
      Option(RunDateRange("20200926", "20200927")),
      Map(tags: _*)
    )
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.monitoring

case class TestMonitoringEntry(uid: String,
                               startTime: String,
                               endTime: String,
                               sources: String,
                               outputLocation: String,
                               sourceCount: BigInt,
                               outputCount: BigInt,
                               tags: String,
                               component: String,
                               user: String,
                               runDate: String,
                               status: String,
                               client: String)
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/param: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.param

import org.scalatest.FunSuite

class RawJobParamsTest extends FunSuite {

  val params = RawJobParams(Array("value0", "name1=value1", "value2", "name2=value3"))

  test("Test getting param by position") {
    assert(params(0) === "value0")
    assert(params(1) === "value1")
    assert(params(2) === "value2")
    assert(params(3) === "value3")
    assert(params(4) === null)
  }

  test("Test getting param by name") {
    assert(params("name1") === "value1")
    assert(params("name2") === "value3")
    assert(params("unknown") === null)
  }

  test("Test raw params toString") {
    assert(params.toString === "RawJobParams[value0,name1=value1,value2,name2=value3]")
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.param

import org.scalatest.FunSuite

class RunDateRangeParamParserTest extends FunSuite {

  test("Test parsing when no dates provided") {
    val params = RawJobParams(Array())

    val thrown = intercept[RuntimeException] {
      RunDateRangeParamParser.parse(params)
    }
    assert(thrown.getMessage === "runDateFrom is required")
  }

  test("Test parsing when invalid runDateFrom") {
    val params = RawJobParams(Array("runDateFrom=2020091"))

    val thrown = intercept[RuntimeException] {
      RunDateRangeParamParser.parse(params)
    }
    assert(thrown.getMessage === s"""Invalid format of 'runDateFrom': [2020091]. Expected format: yyyyMMdd""")
  }

  test("Test parsing when invalid runDateTo") {
    val params = RawJobParams(Array("runDateTo=2020091", "runDateFrom=20200917"))

    val thrown = intercept[RuntimeException] {
      RunDateRangeParamParser.parse(params)
    }
    assert(thrown.getMessage === s"""Invalid format of 'runDateTo': [2020091]. Expected format: yyyyMMdd""")
  }

  test("Test parsing when runDateTo before runDateFrom") {
    val params = RawJobParams(Array("runDateTo=20200916", "runDateFrom=20200917"))

    val thrown = intercept[RuntimeException] {
      RunDateRangeParamParser.parse(params)
    }
    assert(thrown.getMessage === s"""'runDateTo' [20200916] should be same or after 'runDateFrom' [20200917]""")
  }

  test("Test parsing when only runDateFrom") {
    val params = RawJobParams(Array("runDateFrom=20200917"))
    val range = RunDateRangeParamParser.parse(params)

    assert(range.runDateFrom === "20200917")
    assert(range.runDateTo === "20200917")
  }

  test("Test parsing when both runDateFrom and runDateTo") {
    val params = RawJobParams(Array("runDateTo=20200918", "runDateFrom=20200917"))
    val range = RunDateRangeParamParser.parse(params)

    assert(range.runDateFrom === "20200917")
    assert(range.runDateTo === "20200918")
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.param

import org.scalatest.FunSuite

class RunDateRangeTest extends FunSuite {

  test("Test both dates empty") {
    val runDateRange = RunDateRange("", "")
    assert(runDateRange.toString === "")
  }

  test("Test when runDateFrom equal to runDateTo") {
    val runDateRange = RunDateRange("20200917", "20200917")
    assert(runDateRange.toString === "20200917")
  }

  test("Test when runDateFrom different from runDateTo") {
    val runDateRange = RunDateRange("20200917", "20200918")
    assert(runDateRange.toString === "20200917-20200918")
  }

}
cat: ./src/test/scala/com/hsbc/gbm/bd/dataproducts/spark/utils: Is a directory
package com.hsbc.gbm.bd.dataproducts.spark.utils

import java.io.{File, IOException}
import java.util.Objects

import com.hsbc.gbm.bd.dataproducts.schema.{DataEntity, JsonSchema, SchemaUtils}
import org.apache.commons.io.FileUtils
import org.apache.spark.sql._
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types.StructType
import org.assertj.core.api.Assertions

import scala.collection.JavaConversions._
import scala.collection.JavaConverters._

object TestUtils {

  def dropHiveDatabases(spark: SparkSession, dbNames: String*): Unit = {
    try
      dbNames.foreach(name => spark.sql(s"drop database if exists $name cascade"))
    catch {
      case e: Exception =>
        throw new RuntimeException(e)
    }
  }

  def createHiveDatabases(spark: SparkSession, dbNames: String*): Unit = {
    try {
      dbNames.foreach(name => spark.sql(s"create database if not exists $name"))
    } catch {
      case e: Exception =>
        e.printStackTrace()
    }
  }

  def dropHiveTables(spark: SparkSession, tableNames: String*): Unit = {
    tableNames.foreach(name => spark.sql(s"drop table if exists $name"))
  }


  def createDirectoryIfNotExists(relativePath: String): Unit = {
    val dir = new File(relativePath)
    if (!dir.exists) dir.mkdirs
  }

  def deleteDirectoryIfExists(relativePath: String): Unit = {
    val dir = new File(relativePath)
    if (dir.exists) try
      FileUtils.deleteDirectory(dir)
    catch {
      case e: IOException =>
        throw new RuntimeException(e)
    }
  }

  def deleteDirectoryIfExistsIgnoreErrors(relativePath: String): Unit = {
    val dir = new File(relativePath)
    if (dir.exists) try
      FileUtils.deleteDirectory(dir)
    catch {
      case e: IOException => println(s"Unable to delete directory at '$relativePath'. Error: ${e.getMessage}")
    }
  }

  def loadCSV(spark: SparkSession, relativeCsvPath: String, schema: Option[String]): DataFrame = {
    if (schema.isDefined) {
      spark
        .read
        .schema(schema.get)
        .format("csv")
        .option("header", "true")
        .option("delimiter", "|")
        .load(relativeCsvPath)

    } else {
      spark
        .read
        .format("csv")
        .option("header", "true")
        .option("delimiter", "|")
        .load(relativeCsvPath)
    }
  }

  def csvToHiveTable(spark: SparkSession, relativeCsvPath: String, tableName: String, schema: Option[String], partitionColumns: String*): Unit = {
    val df = loadCSV(spark, relativeCsvPath, schema)
    if (spark.catalog.tableExists(tableName)) {
      spark.sql(s"drop table '$tableName'")
    }
    if (partitionColumns.nonEmpty) {
      df
        .write
        .partitionBy(partitionColumns: _*)
        .format("parquet")
        .mode(SaveMode.Overwrite)
        .saveAsTable(tableName)
    } else {
      df
        .write
        .format("parquet")
        .mode(SaveMode.Overwrite)
        .saveAsTable(tableName)
    }
    spark.sql(s"refresh table $tableName")
  }

  def csvToHiveTableWithRunDate(spark: SparkSession, relativeCsvPath: String, tableName: String, runDate: String): Unit = {
    val df = loadCSV(spark, relativeCsvPath, Option.empty)

    df
      .withColumn("run_date", lit(runDate))
      .write
      .format("parquet")
      .mode(SaveMode.Overwrite)
      .saveAsTable(tableName)

    spark.sql(s"refresh table $tableName")
  }

  def csvToParquetFile(spark: SparkSession, relativeCsvPath: String, relativeParquetPath: String, partitionColumns: String*): Unit = {
    val dir = new File(relativeParquetPath)
    if (dir.exists) try
      FileUtils.deleteDirectory(dir)
    catch {
      case e: IOException =>
        throw new RuntimeException(e)
    }
    dir.mkdirs
    val df = loadCSV(spark, relativeCsvPath, Option.empty)

    if (partitionColumns.nonEmpty) {
      df
        .write
        .partitionBy(partitionColumns: _*)
        .mode(SaveMode.Overwrite)
        .parquet(relativeParquetPath)
    } else {
      df
        .write
        .mode(SaveMode.Overwrite)
        .parquet(relativeParquetPath)
    }
  }

  def assertDatasetsEqual(expectedDataset: DataFrame, actualDataset: DataFrame, matchColumnOrder: Boolean): Unit = {
    assertDatasetsEqualWithColumnsIgnored(expectedDataset, actualDataset, matchColumnOrder)
  }

  def assertDatasetsEqualWithColumnsIgnored(expectedDataset: DataFrame, actualDataset: DataFrame, matchColumnOrder: Boolean, ignoredColumns: String*): Unit = {
    println("Actual dataset")
    actualDataset.show(actualDataset.count.toInt, false)

    println("Expected dataset")
    expectedDataset.show(expectedDataset.count.toInt, false)

    var finalActualDataset: DataFrame =
      if (ignoredColumns != null && ignoredColumns.nonEmpty) {
        val fads = actualDataset.drop(ignoredColumns: _*)
        println("Showing masked actual dataset")
        fads.show(fads.count.toInt, false)
        fads
      } else {
        actualDataset
      }

    var finalExpectedDataset: DataFrame =
      if (ignoredColumns != null && ignoredColumns.nonEmpty) {
        val feds = expectedDataset.drop(ignoredColumns: _*)
        println("Showing masked expected dataset")
        feds.show(feds.count.toInt, false)
        feds
      } else {
        expectedDataset
      }

    assertColumnsEqual(finalExpectedDataset, finalActualDataset, matchColumnOrder)

    val actualCount: Long = finalActualDataset.count
    val expectedCount: Long = finalExpectedDataset.count

    if(actualCount != expectedCount) {
      throw new AssertionError("Datasets have different number of rows")
    }

    val columnOrdering: List[Column] = finalExpectedDataset.columns.map(functions.column).toList
    val actualRows: List[Row] = finalActualDataset.select(columnOrdering: _*).collectAsList().asScala.toList
    val expectedRows: List[Row] = finalExpectedDataset.select(columnOrdering: _*).collectAsList().asScala.toList
    val errorList: Seq[String] =
      for {
        index <- 0 to expectedCount.toInt - 1
        list <- compareRows(index, expectedRows.get(index), actualRows.get(index))
      } yield list

    if (errorList.nonEmpty) {
      errorList.foreach(error => println(error))
      throw new AssertionError("There are different values in the datasets")
    }
    println("The datasets are equivalent.")
  }


  def assertColumnsEqual(expectedDataset: DataFrame, actualDataset: DataFrame, matchColumnOrder: Boolean): Unit = {
    if (matchColumnOrder) {
      Assertions.assertThat(actualDataset.columns)
        .as("The datasets have different column lists")
        .containsExactly(expectedDataset.columns: _*)
    } else {
      Assertions.assertThat(actualDataset.columns)
        .as("The datasets have different column sets")
        .containsExactlyInAnyOrder(expectedDataset.columns: _*)
    }
  }

  def compareRows(rowIndex: Int, expectedRow: Row, actualRow: Row): List[String] = {
    val schema: StructType = actualRow.schema
    val fieldsNames: Array[String] = schema.fieldNames

    fieldsNames
      .map(fieldName => compareColumnValues(rowIndex, fieldName, expectedRow.getAs(fieldName), actualRow.getAs(fieldName)))
      .filter(message => message.isDefined)
      .map(message => message.get)
      .toList
  }

  def alignDataFrameToSchema(df: DataFrame, dataEntity: DataEntity): DataFrame = {
    val jsonSchema: JsonSchema = SchemaUtils.getSchemaOf(dataEntity)
    val schemaFromJson: StructType = SchemaUtils.jsonSchemaToStructType(jsonSchema)
    alignDataFrameToSchema(df, schemaFromJson)
  }

  def alignDataFrameToSchema(inputDF: DataFrame, schema: StructType): DataFrame = {
    val datasetColumnNames: Set[String] = inputDF.columns.toSet
    val missingColumnNames: List[String] = schema.fields
      .filter(field => !datasetColumnNames.contains(field.name))
      .map(field => field.name)
      .toList

    val missingColumnValues: List[Column] = schema.fields
      .filter(field => !datasetColumnNames.contains(field.name))
      .map(field => functions.lit(null))
      .toList

    val schemaColumns: Array[Column] = schema.fields
      .map(field => functions.col(field.name).cast(field.dataType))

    val tuples = missingColumnNames.zip(missingColumnValues)

    tuples.foldLeft(inputDF)((df, tuple) => df.withColumn(tuple._1, tuple._2)).select(schemaColumns: _*)
  }

  private def compareColumnValues(rowIndex: Int, columnName: String, expectedValue: Object, actualValue: Object): Option[String] = {
    val actualValueType = if (actualValue != null) actualValue.getClass.getSimpleName else null
    val expectedValueType = if (expectedValue != null) expectedValue.getClass.getSimpleName else null
    if (Objects.equals(actualValue, expectedValue)) {
      Option.empty
    } else {
      Option(s"Values are different for [row: $rowIndex, col: $columnName]. Actual: [$actualValue ($actualValueType)], expected: [$expectedValue ($expectedValueType)]")
    }
  }

}
package com.hsbc.gbm.bd.dataproducts.spark.utils

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import org.scalatest.FunSuite

class TestUtilsTest extends FunSuite with SparkEnv {

  import spark.implicits._

  def appName(): String = "test-utils-test"

  def configLocation(): String = "src/test/resources/test-application-scala.yml"


  test("Test assert column sets when same datasets and different column order") {
    val df1 = Seq(("va11", "va12", "va13"), ("va2l", "va22", "va23")).toDF("col1", "col2", "col3")
    val df2 = Seq(("vb11", "vb13", "vb12"), ("vb2l", "vb23", "vb22")).toDF("col1", "col3", "col2")
    TestUtils.assertColumnsEqual(df1, df2, false)
  }

  test("Test assert column sets when same datasets and different column order but same order expected") {
    val df1 = Seq(("va11", "va12", "va13"), ("va2l", "va22", "va23")).toDF("col1", "col2", "col3")
    val df2 = Seq(("vb11", "vb13", "vb12"), ("vb2l", "vb23", "vb22")).toDF("col1", "col3", "col2")
    TestUtils.assertColumnsEqual(df1, df2, false)
  }

  test("Test assert column sets when datasets are different") {
    val df1 = Seq(("va11", "va12"), ("va2l", "va22")).toDF("col1", "col2")
    val df2 = Seq(("vb11", "vb12", "vb13"), ("vb2l", "vb22", "vb23")).toDF("col1", "col2", "col3")

    val thrown = intercept[AssertionError] {
      TestUtils.assertColumnsEqual(df1, df2, false)
    }
    assert(thrown.getMessage.nonEmpty)
  }

  test("Test compare rows when same datasets") {
    val df1 = Seq(("v11", "v12", "v13")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v12", "v13")).toDF("col1", "col2", "col3")

    val errorMessages = TestUtils.compareRows(0, df1.first(), df2.first())
    assert(errorMessages.isEmpty)
  }

  test("Test compare rows when different datasets") {
    val df1 = Seq(("v11", "v12", "v13")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v10", "v13")).toDF("col1", "col2", "col3")

    val errorMessages = TestUtils.compareRows(0, df1.first(), df2.first())
    assert(errorMessages.nonEmpty)
  }

  test("Test assert datasets equal when same datasets") {
    val df1 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23")).toDF("col1", "col2", "col3")

    TestUtils.assertDatasetsEqual(df1, df2, false)
  }

  test("Test assert datasets equal when same datasets but different column order") {
    val df1 = Seq(("v11", "v13", "v12"), ("v21", "v23", "v22")).toDF("col1", "col3", "col2")
    val df2 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23")).toDF("col1", "col2", "col3")

    TestUtils.assertDatasetsEqual(df1, df2, false)
  }

  test("Test assert datasets equal when rows are swapped") {
    val df1 = Seq(("v11", "v13", "v12"), ("v21", "v23", "v22"), ("v31", "v33", "v32")).toDF("col1", "col3", "col2")
    val df2 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23"), ("v31", "v32", "v33")).toDF("col1", "col2", "col3")

    TestUtils.assertDatasetsEqual(df1, df2, false)
  }

  test("Test assert datasets equal when null values exist") {
    val df1 = Seq(("v11", "v13", null), ("v21", "v23", null), ("v31", "v33", null)).toDF("col1", "col3", "col2")
    val df2 = Seq(("v11", null, "v13"), ("v21", null, "v23"), ("v31", null, "v33")).toDF("col1", "col2", "col3")

    TestUtils.assertDatasetsEqual(df1, df2, false)
  }

  test("Test assert datasets equal when they have different columns") {
    val df1 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v13"), ("v21", "v23")).toDF("col1", "col3")

    val thrown = intercept[AssertionError] {
      TestUtils.assertDatasetsEqual(df1, df2, false)
    }
    assert(thrown.getMessage.nonEmpty)
  }

  test("Test assert datasets equal when they have different number of rows") {
    val df1 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v12", "v13"), ("v21", "v22", "v23"), ("v31", "v32", "v33")).toDF("col1", "col2", "col3")

    val thrown = intercept[AssertionError] {
      TestUtils.assertDatasetsEqual(df1, df2, false)
    }
    assert(thrown.getMessage.nonEmpty)
  }

  test("Test assert datasets equal when they have different values") {
    val df1 = Seq(("v15", "v12", "v13"), ("v21", "v20", "v23"), ("v31", "v32", "v33")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v12", "v13"), ("v23", "v22", "v23"), ("v31", "v32", "v34")).toDF("col1", "col2", "col3")

    val thrown = intercept[AssertionError] {
      TestUtils.assertDatasetsEqual(df1, df2, false)
    }
    assert(thrown.getMessage.nonEmpty)
  }

  test("Test assert datasets equal with columns ignored when they equivalent") {
    val df1 = Seq(("v11", "v12", "v44"), ("v21", "v22", "v55")).toDF("col1", "col2", "col3")
    val df2 = Seq(("v11", "v12", "v66"), ("v21", "v22", "v77")).toDF("col1", "col2", "col3")

    TestUtils.assertDatasetsEqualWithColumnsIgnored(df1, df2, false, "col3")
  }


}
package com.hsbc.gbm.bd.dataproducts.spark.utils

import java.io.PrintWriter

import com.hsbc.gbm.bd.dataproducts.spark.env.SparkEnv
import com.hsbc.gbm.bd.dataproducts.spark.utils.TestUtils.createDirectoryIfNotExists
import org.slf4j.LoggerFactory

import scala.io.Source

trait UnitTestBase extends SparkEnv {

  private val logger = LoggerFactory.getLogger(this.getClass.getName)

  def appName(): String = "spark-job-test"

  def testConfigFileClassPath(): String

  override def configLocation(): String = {
    val configLocation = "src/test/resources/out/test-application-updated.yml"
    val workDirectory: String = System.getProperty("user.dir").replace('\\', '/')

    val inputStream = getClass.getResourceAsStream(testConfigFileClassPath())
    val yml = Source.fromInputStream(inputStream).mkString
      .replaceAll("\\{workDirectory\\}", workDirectory)

    createDirectoryIfNotExists("src/test/resources/out")
    new PrintWriter(configLocation) {
      write(yml)
      close()
    }

    configLocation
  }

}
