cat: .: Is a directory
cat: ./.idea: Is a directory
trdslake<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CompilerConfiguration">
    <annotationProcessing>
      <profile name="Maven default annotation processors profile" enabled="true">
        <sourceOutputDir name="target/generated-sources/annotations" />
        <sourceTestOutputDir name="target/generated-test-sources/test-annotations" />
        <outputRelativeToContentRoot value="true" />
        <module name="trdslake" />
      </profile>
    </annotationProcessing>
    <bytecodeTargetLevel>
      <module name="trdslake" target="1.8" />
    </bytecodeTargetLevel>
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="HydraSettings">
    <option name="hydraStorePath" value="C:\Users\45143309\Downloads\TRDS_Lake-master\TRDS_Lake-master\.hydra\idea" />
    <option name="noOfCores" value="2" />
    <option name="projectRoot" value="C:\Users\45143309\Downloads\TRDS_Lake-master\TRDS_Lake-master" />
    <option name="sourcePartitioner" value="auto" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="MavenProjectsManager">
    <option name="originalFiles">
      <list>
        <option value="$PROJECT_DIR$/pom.xml" />
      </list>
    </option>
  </component>
  <component name="ProjectRootManager" version="2" languageLevel="JDK_1_8" project-jdk-name="1.8" project-jdk-type="JavaSDK">
    <output url="file://$PROJECT_DIR$/classes" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ProjectModuleManager">
    <modules>
      <module fileurl="file://$PROJECT_DIR$/trdslake.iml" filepath="$PROJECT_DIR$/trdslake.iml" />
    </modules>
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ScalaSbtSettings">
    <option name="customVMPath" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="GradleLocalSettings">
    <option name="projectSyncType">
      <map>
        <entry key="$USER_HOME$/IdeaProjects" value="PREVIEW" />
        <entry key="$USER_HOME$/IdeaProjects/emf-spark" value="PREVIEW" />
        <entry key="$USER_HOME$/IdeaProjects/emf-spark1" value="PREVIEW" />
      </map>
    </option>
  </component>
  <component name="ProjectFrameBounds" extendedState="6">
    <option name="x" value="345" />
    <option name="width" value="720" />
    <option name="height" value="572" />
  </component>
  <component name="PropertiesComponent">
    <property name="last_opened_file_path" value="$PROJECT_DIR$" />
    <property name="settings.editor.selected.configurable" value="preferences.pathVariables" />
  </component>
  <component name="SbtLocalSettings">
    <option name="projectSyncType">
      <map>
        <entry key="$PROJECT_DIR$/../../configexample-master/configexample-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../exercises-pureconfig-main/exercises-pureconfig-main" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../pure-spark-2.2-master/pure-spark-2.2-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../spark-config-example-master/spark-config-example-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../spark-seed.g8-master/spark-seed.g8-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../typesafe-config-example-master/typesafe-config-example-master" value="PREVIEW" />
      </map>
    </option>
  </component>
  <component name="VcsContentAnnotationSettings">
    <option name="myLimit" value="2678400000" />
  </component>
  <component name="masterDetails">
    <states>
      <state key="ProjectJDKs.UI">
        <settings>
          <last-edited>1.8</last-edited>
          <splitter-proportions>
            <option name="proportions">
              <list>
                <option value="0.2" />
              </list>
            </option>
          </splitter-proportions>
        </settings>
      </state>
    </states>
  </component>
</project>#!groovy
@Library('tr-jenkins-lib') _

simpleBuild()
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.hsbc.trds</groupId>
  <artifactId>trds-lake-launcher</artifactId>
  <version>1.0.0</version>
  <name>trdslake</name>
  <description>the common libs for spark jobs</description>

  <properties>
    <job.version>1.0-SNAPSHOT</job.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>com.hsbc.trdslake</groupId>
      <artifactId>trdslake</artifactId>
      <version>${job.version}</version>
    </dependency>
  </dependencies>

  <repositories>
    <repository>
      <id>dsnexus-releases</id>
      <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </repository>

    <repository>
      <id>dsnexus-snapshots</id>
      <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots</url>
      <releases>
        <enabled>false</enabled>
      </releases>
      <snapshots>
        <enabled>true</enabled>
      </snapshots>
    </repository>
  </repositories>
</project><?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.hsbc.trdslake</groupId>
  <artifactId>trdslake</artifactId>
  <version>1.0-SNAPSHOT</version>
  <properties>
    <maven.compiler.source>1.8</maven.compiler.source>
    <maven.compiler.target>1.8</maven.compiler.target>
    <java.version>1.8</java.version>
    <scala.version>2.11.12</scala.version>
    <scala.binary.version>2.11</scala.binary.version>
    <scalatest.version>3.2.2</scalatest.version>
    <spark.version>2.4.5</spark.version>
    <spark.binary.version>2.4.5</spark.binary.version>
    <slf4j.version>1.7.25</slf4j.version>
    <avro.version>1.8.2</avro.version>
    <aws-s3.version>1.11.319</aws-s3.version>
    <coherence.version>12.2.1.0.3.1</coherence.version>
    <lombok.version>1.18.10</lombok.version>
    <app.main.class>com.hsbc.trds.scala.driver</app.main.class>
    <elasticsearch.version>7.10.1</elasticsearch.version>
    <lombok.version>1.18.16</lombok.version>
    <open-csv.version>5.2</open-csv.version>
    <beam.version>2.26.0</beam.version>
    <jackson.version>2.10.2</jackson.version>
    <sol-jms.version>10.9.0</sol-jms.version>
  </properties>

  <scm>
    <connection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_Lake.git
    </connection>
    <developerConnection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_Lake.git
    </developerConnection>
    <url>https://alm-github.systems.uk.hsbc/CATS/TRDS_Lake.git</url>
    <tag>HEAD</tag>
  </scm>


  <distributionManagement>
    <!-- use the following if you're not using a release version. -->
    <repository>
      <id>dsnexus</id>
      <name>
      </name>
      <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases
      </url>
    </repository>
    <snapshotRepository>
      <id>dsnexus-snapshots</id>
      <name>
      </name>
      <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots
      </url>
    </snapshotRepository>
  </distributionManagement>


  <dependencies>

    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-lang3</artifactId>
      <version>3.8.1</version>
    </dependency>

    <dependency>
      <groupId>com.amazonaws</groupId>
      <artifactId>aws-java-sdk-s3</artifactId>
      <version>${aws-s3.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>log4j-over-slf4j</artifactId>
        </exclusion>
        <exclusion>
          <groupId>ch.qos.logback</groupId>
          <artifactId>logback-core</artifactId>
        </exclusion>
        <exclusion>
          <groupId>ch.qos.logback</groupId>
          <artifactId>logback-classic</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
      <exclusions>
        <exclusion>
          <groupId>com.google.guava</groupId>
          <artifactId>guava</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <!--

          <dependency>
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-aws</artifactId>
              <version>2.8.5</version>
          </dependency>


          <dependency>
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-common</artifactId>
              <version>2.8.5</version>
              <exclusions>
                  <exclusion>
                      <groupId>com.google.guava</groupId>
                      <artifactId>guava</artifactId>
                  </exclusion>
              </exclusions>
          </dependency>
          <dependency>
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-client</artifactId>
              <version>2.8.5</version>
              <exclusions>
                  <exclusion>
                      <groupId>com.google.guava</groupId>
                      <artifactId>guava</artifactId>
                  </exclusion>
              </exclusions>
          </dependency>

          -->


    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>


    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.13</version>
      <scope>test</scope>
    </dependency>


    <dependency>
      <groupId>org.scalatest</groupId>
      <artifactId>scalatest_${scala.binary.version}</artifactId>
      <version>${scalatest.version}</version>
    </dependency>
    <dependency>
      <groupId>com.typesafe</groupId>
      <artifactId>config</artifactId>
      <version>1.3.3</version>
    </dependency>
    <dependency>
      <groupId>com.oracle.coherence</groupId>
      <artifactId>coherence</artifactId>
      <version>${coherence.version}</version>
    </dependency>
    <dependency>
      <groupId>org.projectlombok</groupId>
      <artifactId>lombok</artifactId>
      <version>${lombok.version}</version>
      <scope>provided</scope>
    </dependency>

    <!--
    <dependency>
        <groupId>org.elasticsearch</groupId>
        <artifactId>elasticsearch-spark-20_2.11</artifactId>
        <version>${elasticsearch.version}</version>
    </dependency> -->

    <dependency>
      <groupId>org.elasticsearch.client</groupId>
      <artifactId>elasticsearch-rest-high-level-client</artifactId>
      <version>${elasticsearch.version}</version>
    </dependency>

    <dependency>
      <groupId>org.elasticsearch</groupId>
      <artifactId>elasticsearch</artifactId>
      <version>${elasticsearch.version}</version>
    </dependency>

    <dependency>
      <groupId>com.databricks</groupId>
      <artifactId>spark-xml_2.11</artifactId>
      <version>0.11.0</version>
    </dependency>
    <dependency>
      <groupId>org.springframework</groupId>
      <artifactId>spring-core</artifactId>
      <version>5.3.2</version>
    </dependency>
    <dependency>
      <groupId>org.springframework</groupId>
      <artifactId>spring-context</artifactId>
      <version>5.3.2</version>
    </dependency>
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpcore</artifactId>
      <version>4.4.14</version>
    </dependency>
    <dependency>
      <groupId>com.opencsv</groupId>
      <artifactId>opencsv</artifactId>
      <version>${open-csv.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-runners-direct-java</artifactId>
      <version>${beam.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-runners-spark</artifactId>
      <version>${beam.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-sdks-java-io-hadoop-file-system</artifactId>
      <version>${beam.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>jul-to-slf4j</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-databind</artifactId>
      <version>${jackson.version}</version>
    </dependency>

    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-annotations</artifactId>
      <version>${jackson.version}</version>
    </dependency>

    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-core</artifactId>
      <version>${jackson.version}</version>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.module</groupId>
      <artifactId>jackson-module-scala_${scala.binary.version}</artifactId>
      <version>${jackson.version}</version>
    </dependency>
    <!-- [BEAM-3519] GCP IO exposes netty on its API surface, causing conflicts with runners -->
    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>
      <version>${beam.version}</version>
      <exclusions>
        <exclusion>
          <groupId>io.grpc</groupId>
          <artifactId>grpc-netty</artifactId>
        </exclusion>
        <exclusion>
          <groupId>io.netty</groupId>
          <artifactId>netty-handler</artifactId>
        </exclusion>
        <exclusion>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-sdks-java-extensions-google-cloud-platform-core</artifactId>
      <version>${beam.version}</version>
    </dependency>
    <dependency>
      <groupId>com.solacesystems</groupId>
      <artifactId>sol-jms</artifactId>
      <version>${sol-jms.version}</version>
    </dependency>
    <dependency>
      <groupId>com.solacesystems</groupId>
      <artifactId>sol-jcsmp</artifactId>
      <version>${sol-jms.version}</version>
    </dependency>
    <dependency>
      <groupId>com.solace.connector.beam</groupId>
      <artifactId>beam-sdks-java-io-solace</artifactId>
      <version>1.0.0</version>
    </dependency>
    <dependency>
      <groupId>com.solace.connector.beam</groupId>
      <artifactId>solace-apache-beam-samples</artifactId>
      <version>1.0.0</version>
    </dependency>

    <dependency>
      <groupId>org.apache.beam</groupId>
      <artifactId>beam-sdks-java-io-amazon-web-services</artifactId>
      <version>${beam.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>29.0-jre</version>
    </dependency>

  </dependencies>

  <build>

    <plugins>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-jar-plugin</artifactId>
        <version>2.4</version>
        <configuration>
          <excludes>
           <!-- <exclude>**/*.properties</exclude>
            <exclude>**/*.conf</exclude> -->
            <exclude>**/*.sh</exclude>
            <exclude>**/*.json</exclude>
            <exclude>**/*.avsc</exclude>
          </excludes>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.6.1</version>
        <executions>
          <execution>
            <id>default-compile</id>
            <phase>none</phase>
          </execution>
        </executions>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest-maven-plugin</artifactId>
        <version>2.0.0</version>
        <configuration>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
          <junitxml>.</junitxml>
          <filereports>TestSuite.txt</filereports>
        </configuration>
        <executions>
          <execution>
            <id>test</id>
            <goals>

            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <version>3.2.2</version>
        <executions>
          <execution>
            <goals>
              <goal>compile</goal>
              <goal>testCompile</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <recompileMode>incremental</recompileMode>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-release-plugin</artifactId>
        <version>2.5.3</version>
        <configuration>
          <providerImplementations>
            <git>git</git>
          </providerImplementations>
        </configuration>
        <dependencies>
          <dependency>
            <groupId>org.apache.maven.scm</groupId>
            <artifactId>maven-scm-provider-gitexe</artifactId>
            <version>1.9.5</version>
          </dependency>
          <dependency>
            <groupId>org.apache.maven.scm</groupId>
            <artifactId>maven-scm-provider-git-commons</artifactId>
            <version>1.9.5</version>
          </dependency>
          <dependency>
            <groupId>org.apache.maven.release</groupId>
            <artifactId>maven-release-manager</artifactId>
            <version>2.5.3</version>
          </dependency>
        </dependencies>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-scm-plugin</artifactId>
        <version>1.9.5</version>
        <configuration>
          <providerImplementations>
            <git>git</git>
          </providerImplementations>
        </configuration>
        <dependencies>
          <dependency>
            <groupId>org.apache.maven.scm</groupId>
            <artifactId>maven-scm-provider-gitexe</artifactId>
            <version>1.9.5</version>
          </dependency>
          <dependency>
            <groupId>org.apache.maven.scm</groupId>
            <artifactId>maven-scm-provider-git-commons</artifactId>
            <version>1.9.5</version>
          </dependency>
        </dependencies>
      </plugin>


      <!--

                         <plugin>
                             <groupId>org.apache.avro</groupId>
                             <artifactId>avro-maven-plugin</artifactId>
                             <version>${avro.version}</version>
                             <executions>
                                 <execution>
                                     <phase>generate-resources</phase>
                                     <goals>
                                         <goal>schema</goal>
                                     </goals>
                                     <configuration>
                                         <sourceDirectory>${project.basedir}/src/main/resources/avro/</sourceDirectory>
                                         <outputDirectory>${project.basedir}/target/generated-sources/</outputDirectory>
                                     </configuration>
                                 </execution>
                             </executions>
                         </plugin>

-->

<!--

                    <plugin>
                        <artifactId>maven-assembly-plugin</artifactId>
                        <version>3.2.0</version>
                        <configuration>
                            <descriptors>
                                <descriptor>src/main/assembly/assembly.xml</descriptor>
                            </descriptors>
                        </configuration>
                        <executions>
                            <execution>
                                <phase>package</phase>
                                <id>make-assembly</id>
                                <goals>
                                    <goal>single</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
-->


      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>3.3.0</version>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>assemble-all</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>


      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>3.2.4</version>
        <configuration>
          <relocations>
            <relocation>
              <pattern>org.apache.avro</pattern>
              <shadedPattern>shaded.org.apache.avro</shadedPattern>
            </relocation>
            <relocation>
              <pattern>com.google.</pattern>
              <shadedPattern>com.shaded_pkg.google.</shadedPattern>
            </relocation>
            <relocation>
              <pattern>org.apache.http.</pattern>
              <shadedPattern>shdhttp.org.apache.http</shadedPattern>
            </relocation>
          </relocations>
          <artifactSet>
            <excludes>
              <exclude>net.sourceforge.saxon:saxon:jar:</exclude>
            </excludes>
          </artifactSet>
          <filters>
            <filter>
              <artifact>*:*</artifact>

              <excludes>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
                <exclude>net.sourceforge.saxon:saxon</exclude>
               <!-- <exclude>**/*.conf</exclude> -->
              </excludes>

            </filter>
          </filters>
          <transformers>
            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
              <manifestEntries>
                <Main-Class>${app.main.class}</Main-Class>
                <Class-Path>.</Class-Path>
              </manifestEntries>
            </transformer>
            <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
          </transformers>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
          </execution>
        </executions>

      </plugin>


    </plugins>
  </build>
  <profiles>
<!--    <profile>-->
<!--      <id>direct-runner</id>-->
<!--      <activation>-->
<!--        <activeByDefault>true</activeByDefault>-->
<!--      </activation>-->

<!--    </profile>-->
<!--    <profile>-->
<!--      <id>spark-runner</id>-->
<!--      &lt;!&ndash; Makes the SparkRunner available when running a pipeline. Additionally,-->
<!--           overrides some Spark dependencies to Beam-compatible versions. &ndash;&gt;-->
<!--      <properties>-->
<!--        <netty.version>4.1.17.Final</netty.version>-->
<!--      </properties>-->
<!--      <dependencies>-->

<!--      </dependencies>-->
<!--    </profile>-->
  </profiles>
</project># TRDS_Lake
cat: ./s3sparkwrite2: input file is output file
cat: ./src: Is a directory
cat: ./src/main: Is a directory
cat: ./src/main/assembly: Is a directory
<assembly
        xmlns="http://maven.apache.org/ASSEMBLY/2.0.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd http://maven.apache.org/ASSEMBLY/2.0.0 ">
    <id>bundle</id>
    <formats>
        <format>tar.gz</format>
    </formats>
    <fileSets>
        <fileSet>
            <directory>${project.build.directory}</directory>
            <outputDirectory>\</outputDirectory>
            <includes>
                <include>*.jar</include>
            </includes>
            <excludes>
                <exclude>**/*.xml</exclude>
                <exclude>**/*.properties</exclude>
                <exclude>**/*.conf</exclude>
                <exclude>**.*.sh</exclude>
            </excludes>
        </fileSet>

        <fileSet>
            <directory>${project.basedir}/src/main/resources</directory>
            <outputDirectory>bin</outputDirectory>
            <includes>
                <include>**/*.sh</include>
            </includes>
            <fileMode>0755</fileMode>
            <directoryMode>0755</directoryMode>
            <lineEnding>unix</lineEnding>
            <filtered>false</filtered>
        </fileSet>
        <fileSet>
            <directory>${project.basedir}/src/main/resources/avro</directory>
            <outputDirectory>schema</outputDirectory>
            <includes>
                <include>**/*.avsc</include>
            </includes>
            <filtered>false</filtered>
        </fileSet>
        <fileSet>
            <directory>${project.basedir}/src/main/resources</directory>
            <outputDirectory>conf</outputDirectory>
            <includes>
                <include>**/*.conf</include>
                <include>**/*.json</include>
                <include>**/*.properties</include>
            </includes>
            <filtered>false</filtered>
        </fileSet>
    </fileSets>
    <dependencySets>
        <dependencySet>
            <outputDirectory>/lib</outputDirectory>
            <unpack>false</unpack>
            <includes>
                <include>com.oracle.database.jdbc:ojdbc8:jar:19.8.0.0</include>
                <include>org.apache.avro:avro:jar:${avro.version}</include>
                <include>org.apache.avro:avro-mapred:jar:${avro.version}</include>
                <include>org.apache.parquet:parquet-avro:jar:1.8.1</include>
                <include>com.oracle.coherence:coherence:jar:12.2.1.0.3.1</include>
                <include>org.elasticsearch:elasticsearch-spark-20_2.11:jar:${elasticsearch.version}</include>
                <include>org.elasticsearch:elasticsearch:jar:${elasticsearch.version}</include>
                <include>org.elasticsearch.client:elasticsearch-rest-high-level-client:jar:${elasticsearch.version}</include>
                <include>commons-httpclient:commons-httpclient:jar:3.1</include>
                <include>com.databricks:spark-xml_2.11:jar:0.11.0</include>
                <include>org.elasticsearch.client:elasticsearch-rest-client:jar:7.10.1</include>
                <include>org.apache.httpcomponents:httpasyncclient:jar:4.1.4</include>
                <include>org.apache.httpcomponents:httpcore-nio:jar:4.4.12</include>
                <include>org.elasticsearch.plugin:mapper-extras-client:jar:7.10.1</include>
                <include>org.elasticsearch.plugin:parent-join-client:jar:7.10.1</include>
                <include>org.elasticsearch.plugin:aggs-matrix-stats-client:jar:7.10.1</include>
                <include>org.elasticsearch.plugin:rank-eval-client:jar:7.10.1</include>
                <include>org.elasticsearch.plugin:lang-mustache-client:jar:7.10.1</include>
                <include>com.github.spullara.mustache.java:compiler:jar:0.9.6</include>
                <include>org.elasticsearch:elasticsearch-core:jar:7.10.1</include>
                <include>org.elasticsearch:elasticsearch-secure-sm:jar:7.10.1</include>
                <include>org.elasticsearch:elasticsearch-x-content:jar:7.10.1</include>
                <include>org.yaml:snakeyaml:jar:1.26</include>
                <include>com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.10.4</include>
                <include>com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.10.4</include>
                <include>org.elasticsearch:elasticsearch-geo:jar:7.10.1      </include>
                <include>org.apache.lucene:lucene-core:jar:8.7.0             </include>
                <include>org.apache.lucene:lucene-analyzers-common:jar:8.7.0 </include>
                <include>org.apache.lucene:lucene-backward-codecs:jar:8.7.0  </include>
                <include>org.apache.lucene:lucene-grouping:jar:8.7.0         </include>
                <include>org.apache.lucene:lucene-highlighter:jar:8.7.0      </include>
                <include>org.apache.lucene:lucene-join:jar:8.7.0             </include>
                <include>org.apache.lucene:lucene-memory:jar:8.7.0           </include>
                <include>org.apache.lucene:lucene-misc:jar:8.7.0             </include>
                <include>org.apache.lucene:lucene-queries:jar:8.7.0          </include>
                <include>org.apache.lucene:lucene-queryparser:jar:8.7.0      </include>
                <include>org.apache.lucene:lucene-sandbox:jar:8.7.0          </include>
                <include>org.apache.lucene:lucene-spatial-extras:jar:8.7.0   </include>
                <include>org.apache.lucene:lucene-spatial3d:jar:8.7.0        </include>
                <include>org.apache.lucene:lucene-suggest:jar:8.7.0          </include>
                <include>org.elasticsearch:elasticsearch-cli:jar:7.10.1      </include>
                <include>net.sf.jopt-simple:jopt-simple:jar:5.0.2            </include>
                <include>com.carrotsearch:hppc:jar:0.8.1                     </include>
                <include>joda-time:joda-time:jar:2.10.4                      </include>
                <include>com.tdunning:t-digest:jar:3.2                       </include>
                <include>org.hdrhistogram:HdrHistogram:jar:2.1.9             </include>
                <include>org.apache.logging.log4j:log4j-api:jar:2.11.1       </include>
                <include>org.elasticsearch:jna:jar:5.5.0                     </include>
                <include>org.apache.httpcomponents:httpcore:jar:4.4.14</include>
                <include>com.solacesystems:sol-jcsmp:jar:10.9.0</include>
                <include>org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:${beam.version}</include>
                <include>org.apache.beam:beam-sdks-java-extensions-google-cloud-platform-core:jar:${beam.version}</include>
                <include>org.apache.beam:beam-runners-direct-java:jar:${beam.version}</include>
                <include>org.apache.beam:beam-runners-spark:jar:${beam.version}</include>
                <include>org.apache.beam:beam-sdks-java-io-hadoop-file-system:jar:${beam.version}</include>
                <include>org.apache.beam:beam-runners-spark:jar:2.26.0</include>
                <include>org.checkerframework:checker-qual:jar:3.7.0</include>
                <include>org.apache.beam:beam-runners-core-construction-java:jar:2.26.0</include>
                <include>org.apache.beam:beam-sdks-java-fn-execution:jar:2.26.0</include>
                <include>io.github.classgraph:classgraph:jar:4.8.65</include>
            </includes>
        </dependencySet>
    </dependencySets>

</assembly>cat: ./src/main/java: Is a directory
cat: ./src/main/java/com: Is a directory
cat: ./src/main/java/com/hsbc: Is a directory
cat: ./src/main/java/com/hsbc/trds: Is a directory
cat: ./src/main/java/com/hsbc/trds/beam: Is a directory
cat: ./src/main/java/com/hsbc/trds/beam/coder: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.coder;

import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInput;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.OutputStream;

import org.apache.beam.sdk.coders.AtomicCoder;
import org.apache.beam.sdk.coders.ByteArrayCoder;
import org.apache.beam.sdk.coders.CoderException;

import com.hsbc.trds.beam.model.DataLakeModel;

public class DataLakeModelCoder extends AtomicCoder<DataLakeModel> {

  private static final DataLakeModelCoder INSTANCE = new DataLakeModelCoder();
  private static final ByteArrayCoder BYTE_ARRAY_CODER = ByteArrayCoder.of();

  public static DataLakeModelCoder of() {
    return INSTANCE;
  }

  @Override
  public void encode(DataLakeModel value,
      OutputStream outStream) throws CoderException, IOException {
    encode(value, outStream, Context.NESTED);
  }

  public void encode(DataLakeModel value,
      OutputStream outStream,
      Context context)
      throws IOException, CoderException {
    checkNotNull(value, String.format("cannot encode a null %s", DataLakeModel.class.getSimpleName()));
    ByteArrayOutputStream bos = new ByteArrayOutputStream();
    ObjectOutputStream out = null;
    out = new ObjectOutputStream(bos);
    out.writeObject(value);
    out.flush();
    byte[] yourBytes = bos.toByteArray();
    BYTE_ARRAY_CODER.encode(yourBytes, outStream);
  }

  @Override
  public DataLakeModel decode(InputStream inStream) throws CoderException, IOException {
    byte[] decode = BYTE_ARRAY_CODER.decode(inStream);
    ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(decode);

    ObjectInput in = new ObjectInputStream(byteArrayInputStream);
    try {
      return (DataLakeModel) in.readObject();
    } catch (ClassNotFoundException e) {
      e.printStackTrace();
    }
    return null;
  }
}
package com.hsbc.trds.beam.coder;

import com.hsbc.trds.beam.model.DataLakeModel;
import com.hsbc.trds.beam.model.DslXmlDbModel;
import org.apache.beam.sdk.coders.AtomicCoder;
import org.apache.beam.sdk.coders.ByteArrayCoder;
import org.apache.beam.sdk.coders.CoderException;
import org.checkerframework.checker.initialization.qual.Initialized;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.UnknownKeyFor;

import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;

import java.io.*;

public class DslModelCoder extends AtomicCoder<DslXmlDbModel> {

    private static final ByteArrayCoder BYTE_ARRAY_CODER = ByteArrayCoder.of();
    private static final DataLakeModelCoder INSTANCE = new DataLakeModelCoder();

    public static DataLakeModelCoder of() {return INSTANCE; }

    @Override
    public void encode(DslXmlDbModel value, OutputStream outStream)
            throws CoderException, IOException {
        encode(value, outStream, Context.NESTED);
    }

    public void encode(DataLakeModel value,
                       OutputStream outputStream,
                       Context context)
        throws IOException, CoderException {
        checkNotNull(value, String.format("cannot encode a null %s", DslXmlDbModel.class.getSimpleName()));
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        ObjectOutputStream out = null;
        out = new ObjectOutputStream(bos);
        out.writeObject(value);
        out.flush();
        byte[] yourBytes = bos.toByteArray();
        BYTE_ARRAY_CODER.encode(yourBytes, outputStream);

    }

    @Override
    public DslXmlDbModel decode(InputStream inStream) throws CoderException, IOException {
        byte[] decode = BYTE_ARRAY_CODER.decode(inStream);
        ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(decode);

        ObjectInput in = new ObjectInputStream(byteArrayInputStream);

        try {
            return (DslXmlDbModel) in.readObject();
        } catch (ClassNotFoundException ex) {
            ex.printStackTrace();
        }
        return null;
    }
}
cat: ./src/main/java/com/hsbc/trds/beam/config: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.config;

import org.apache.beam.runners.spark.SparkRunner;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

import com.google.auth.oauth2.GoogleCredentials;

import com.hsbc.trds.beam.constants.GcpService;
import com.hsbc.trds.beam.gcp.CredentialsHelper;
import com.hsbc.trds.beam.options.GcsSparkOptions;

public class GcsSparkConfig {

  public GcsSparkOptions gcsSparkOptions() {
    GcsSparkOptions gcsSparkOptions = PipelineOptionsFactory.as(GcsSparkOptions.class);

    GoogleCredentials gcpCredentials = CredentialsHelper.getCredentials(GcpService.CLOUD_STORAGE);
    gcsSparkOptions.setGcpCredential(gcpCredentials);

    SparkConf conf = new SparkConf().setAppName("test-finods-beam").setMaster("local[*]");
    conf.set("spark.local.dir", "./target/temp/spark");
    conf.set("spark.rdd.compress", "true");
    conf.set("spark.default.parallelism", "16");
    conf.set("spark.sql.crossJoin.enabled", "true");
    conf.set("spark.sql.shuffle.partitions", "16");
    conf.set("spark.sql.autoBroadcastJoinThreshold", "10M");

//    JavaSparkContext context = new JavaSparkContext(conf);
    gcsSparkOptions.setTempLocation("./target/temp/beam");
//    gcsSparkOptions.setProvidedSparkContext(context);
//    gcsSparkOptions.setUsesProvidedSparkContext(true);
//    gcsSparkOptions.setRunner(SparkRunner.class);

    return gcsSparkOptions;
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.config;

import java.io.Serializable;
import java.util.Arrays;
import java.util.Properties;
import javax.jms.Connection;
import javax.jms.JMSException;

import com.solacesystems.jcsmp.JCSMPProperties;
import com.solacesystems.jms.SolConnectionFactory;
import com.solacesystems.jms.SolJmsUtility;

import com.hsbc.trds.beam.properties.SolaceProperties;
import com.hsbc.trds.core.resource.ResourceHandler;

import lombok.Getter;
import lombok.extern.slf4j.Slf4j;

@Slf4j
@Getter
public class SolaceConfig extends ResourceHandler implements Serializable {

  private static final long serialVersionUID = 7634432333080823303L;
  private SolaceProperties solaceProperties;
  private JCSMPProperties jcsmpProperties;

  public SolaceConfig() {
    super();
    solaceProperties = solaceProperties();
    jcsmpProperties = new JCSMPProperties();
    jcsmpProperties.setProperty(JCSMPProperties.HOST, solaceProperties.getHost()+":"+solaceProperties.getPort());
    jcsmpProperties.setProperty(JCSMPProperties.VPN_NAME, solaceProperties.getVpn());
    jcsmpProperties.setProperty(JCSMPProperties.USERNAME, solaceProperties.getUsername());
    jcsmpProperties.setProperty(JCSMPProperties.PASSWORD, solaceProperties.getPassword());
//    jcsmpProperties.setProperty(JCSMPProperties., solaceProperties.getHost());
  }

  public Connection createConnection() {
    log.info("Creating connection");
    SolConnectionFactory connectionFactory = null;
    try {
      connectionFactory = SolJmsUtility.createConnectionFactory();
    } catch (Exception e) {
      e.printStackTrace();
      return null;
    }
    connectionFactory.setHost(solaceProperties.getHost());
    connectionFactory.setPort(solaceProperties.getPort());
    connectionFactory.setVPN(solaceProperties.getVpn());
    connectionFactory.setUsername(solaceProperties.getUsername());
    connectionFactory.setPassword(solaceProperties.getPassword());
    Connection connection = null;
    try {
      connection = connectionFactory.createConnection();
    } catch (JMSException e) {
      e.printStackTrace();
      return null;
    }
    log.info("Connection created successfully");
    return connection;
  }

  private SolaceProperties solaceProperties() {
    String key = "trds.guest.solace.properties";
    Properties properties = loadProperties(key);
    return SolaceProperties.builder()
        .host(properties.getProperty("host"))
        .port(Integer.parseInt(properties.getProperty("port")))
        .vpn(properties.getProperty("vpn"))
        .username(properties.getProperty("username"))
        .password(properties.getProperty("password"))
        .queues(Arrays.asList(properties.getProperty("queues").split(",")))
        .build();
  }
}
cat: ./src/main/java/com/hsbc/trds/beam/constants: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.constants;

public enum GcpService {
  CLOUD_STORAGE, PUBSUB;
}
cat: ./src/main/java/com/hsbc/trds/beam/filenaming: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.filenaming;

import static com.hsbc.trds.beam.transformer.DataToGCS.DATE_TIME_FORMATTER;

import java.time.LocalDateTime;

import org.apache.beam.sdk.io.Compression;
import org.apache.beam.sdk.io.FileIO.Write.FileNaming;
import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
import org.apache.beam.sdk.transforms.windowing.PaneInfo;
import org.checkerframework.checker.initialization.qual.Initialized;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.UnknownKeyFor;

import lombok.AllArgsConstructor;

@AllArgsConstructor
public class XmlFileNaming implements FileNaming {

  private String tradeId;
  private String uuid;
  private String region;
  private String timestamp;

  @Override
  public @UnknownKeyFor @NonNull @Initialized String getFilename(@UnknownKeyFor @NonNull @Initialized BoundedWindow window,
      @UnknownKeyFor @NonNull @Initialized PaneInfo pane,
      @UnknownKeyFor @NonNull @Initialized int numShards,
      @UnknownKeyFor @NonNull @Initialized int shardIndex,
      @UnknownKeyFor @NonNull @Initialized Compression compression) {
    // TRADE_ID UUID REGION TRADE_ID timestamp
    String fileName = tradeId + "_" + uuid + "_" + region + "_" + tradeId + "_" + timestamp + ".xml";
    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": file = " + fileName);
    return fileName;
  }
}
cat: ./src/main/java/com/hsbc/trds/beam/function: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.function;

import javax.jms.JMSException;
import javax.jms.Message;
import javax.jms.TextMessage;

import org.apache.beam.sdk.transforms.DoFn;
import com.solacesystems.jms.SolJmsUtility;

public class ExtractMessageFn extends DoFn<String, String> {

  @ProcessElement
  public void processElement(@Element Message element, OutputReceiver<String> receiver) {
    String message = extractMessage(element);
    receiver.output(message);
  }

  private String extractMessage(Message receivedMessage) {
    if (receivedMessage instanceof TextMessage) {
      try {
        return ((TextMessage) receivedMessage).getText();
      } catch (JMSException e) {
        e.printStackTrace();
      }
    }
    return SolJmsUtility.dumpMessage(receivedMessage);
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.function;

import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.FormatStyle;
import java.util.Map;

import org.apache.beam.sdk.transforms.DoFn;
import com.solace.connector.beam.examples.common.SolaceTextRecord;

import com.hsbc.trds.beam.model.DataLakeModel;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class ExtractPayloadFn extends DoFn<SolaceTextRecord, DataLakeModel> {

  private static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL)
      .withZone(ZoneId.systemDefault());

  @ProcessElement
  public void processElement(@Element SolaceTextRecord record,
      OutputReceiver<DataLakeModel> receiver) {
//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + "------------------- Start: Message");
//    printProperties(record.getProperties());
    DataLakeModel dataLakeModel = buildModel(record);
//    Key = AUDIT_DATE, Value = 2021-01-15 11:02:02.0
//    Key = TRADE_VERSION, Value = 1.0
//    Key = BUSINESS_DATE, Value = 2021-01-19 06:56:30.0
//    Key = UUID, Value = 0x00000177196DED4F80A129280310731D4947B08A0D8B08C0CEE4F5F8C8F5F7D4
//    Key = TRADE_ID, Value = GCTRHBEU20210119065100002888
//    Key = SOURCE_SYSTEM, Value = XFOS
//    Key = REGION, Value = HBAP
//    Key = timestamp, Value = 1611574520361

//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": Object Type  : " + dataLakeModel.getDslObjectType());
//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": Business Date: " + dataLakeModel.getBusinessDate());
//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": Trade ID     : " + dataLakeModel.getTradeId());
//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": UUID         : " + dataLakeModel.getUuid());
//    System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + "------------------- End: Message");
    receiver.output(dataLakeModel);
  }

  private DataLakeModel buildModel(SolaceTextRecord record) {
    return DataLakeModel.builder()
        .payload(record.getPayload())
        .dslObjectType(String.valueOf(record.getProperties().get("DSL_OBJECT_TYPE")))
        .businessDate(String.valueOf(record.getProperties().get("BUSINESS_DATE")))
        .tradeId(String.valueOf(record.getProperties().get("TRADE_ID")))
        .uuid(String.valueOf(record.getProperties().get("UUID")))
        .region(String.valueOf(record.getProperties().get("REGION")))
        .timestamp(String.valueOf(record.getProperties().get("TIME_STAMP")))
        .build();
  }

  private void printProperties(Map<String, Object> properties) {
    properties.entrySet().forEach(entry ->
        System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue())
    );
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.function;

import java.util.Arrays;

import org.apache.beam.sdk.transforms.DoFn;

public class ExtractWordsFn extends DoFn<String, String> {

  @ProcessElement
  public void processElement(@Element String element, OutputReceiver<String> receiver) {
    Arrays.stream(element.split(" "))
        .forEach(word -> receiver.output(word));
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.function;

import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.beam.sdk.values.KV;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class FormatAsTextFn extends SimpleFunction<KV<String, Long>, String> {

  @Override
  public String apply(KV<String, Long> input) {
    String line = input.getKey() + "==> " + input.getValue();
    log.info("{}", line);
    return line;
  }
}
package com.hsbc.trds.beam.function;

import com.hsbc.trds.beam.model.DataLakeModel;
import com.hsbc.trds.beam.model.DslXmlDbModel;
import com.solace.connector.beam.examples.common.SolaceTextRecord;
import lombok.extern.slf4j.Slf4j;
import org.apache.beam.sdk.transforms.DoFn;

import java.io.ByteArrayInputStream;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.FormatStyle;

@Slf4j
public class SolaceToEcsPayloadFn extends DoFn<SolaceTextRecord, DslXmlDbModel> {
    private static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL)
            .withZone(ZoneId.systemDefault());

    public void processElement1(@Element SolaceTextRecord record, OutputReceiver<DslXmlDbModel> receiver) {
        System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + "------------------- Start: Message");
        DslXmlDbModel dslXmlDbModel = buildModel(record);
        System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": Object Type  : " + dslXmlDbModel.getDslObjectType());
        System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": Business Date: " + dslXmlDbModel.getBusinessDate());
        System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + "------------------- End: Message");
        receiver.output(dslXmlDbModel);

    }

//    @ProcessElement
//    public void processElement(@Element SolaceTextRecord record, OutputReceiver<ByteArrayInputStream>)

    private DslXmlDbModel buildModel(SolaceTextRecord record) {
        String businessDate = String.valueOf(record.getProperties().get("BUSINESS_DATE"));

        return DslXmlDbModel.builder()
                    .payload(record.getPayload())
                    .dslObjectType(String.valueOf(record.getProperties().get("DSL_OBJECT_TYPE")))
                    .businessDate(businessDate)
                    .tradeId(String.valueOf(record.getProperties().get("TRADE_ID")))
                    .uuid(String.valueOf(record.getProperties().get("UUID")))
                    .region(String.valueOf(record.getProperties().get("REGION")))
                    .build();

    }

}
cat: ./src/main/java/com/hsbc/trds/beam/gcp: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.gcp;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.URISyntaxException;
import java.nio.file.Path;
import java.nio.file.Paths;

import com.google.auth.oauth2.GoogleCredentials;

import com.hsbc.trds.beam.constants.GcpService;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class CredentialsHelper {

  public static GoogleCredentials getCredentials(GcpService gcpService) {
    String key = getKeyByGcpService(gcpService);
   //   String key = System.getProperty("keyFile");
    log.info("the key is: " + key);
    File credentialsFile = getCredentialsFile(key);
    return getCredentials(credentialsFile);
  }

  private static GoogleCredentials getCredentials(File credentialsFile) {
    try {
      return GoogleCredentials.fromStream(new FileInputStream(credentialsFile))
          .createScoped("https://www.googleapis.com/auth/devstorage.full_control",
              "https://www.googleapis.com/auth/cloud-platform",
              "https://www.googleapis.com/auth/userinfo.email",
              "https://www.googleapis.com/auth/datastore",
              "https://www.googleapis.com/auth/pubsub");
    } catch (FileNotFoundException e) {
      log.error("Something went wrong while building the credentials", e);
    } catch (IOException e) {
      log.error("Something went wrong while building the credentials", e);
    }
    return null;
  }

  private static File getCredentialsFile(String key) {
    try {
      Path path = Paths.get(Thread.currentThread().getContextClassLoader()
          .getResource(key)
          .toURI());
      return path.toFile();
    } catch (URISyntaxException e) {
      log.error("Something went wrong while reading the key file", e);
    }
    return null;
  }

  private static String getKeyByGcpService(GcpService gcpService) {
    switch (gcpService) {
      case CLOUD_STORAGE:
      default:
        return "hsbc-11007943-trds-dev-cloud-storage.json";
      case PUBSUB:
        return "hsbc-11007943-trds-dev-pubsub.json";
    }
  }
}
cat: ./src/main/java/com/hsbc/trds/beam/model: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.model;

import java.io.Serializable;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class DataLakeModel implements Serializable {

  private String dslObjectType;
  private String businessDate;
  private String uuid;
  private String tradeId;
  private String payload;
  private String region;
  private String timestamp;
//    Key = AUDIT_DATE, Value = 2021-01-15 11:02:02.0
//    Key = TRADE_VERSION, Value = 1.0
//    Key = BUSINESS_DATE, Value = 2021-01-19 06:56:30.0
//    Key = UUID, Value = 0x00000177196DED4F80A129280310731D4947B08A0D8B08C0CEE4F5F8C8F5F7D4
//    Key = TRADE_ID, Value = GCTRHBEU20210119065100002888
//    Key = SOURCE_SYSTEM, Value = XFOS
//    Key = REGION, Value = HBAP
//    Key = timestamp, Value = 1611574520361
  // 6485170999_0x000001752025345B80A4D46A712B3DA04C9EBE8AD72358F3C5D3CDC8756A1D08_HBEU_6485170999_1602543600000
  // TRADE_ID UUID REGION TRADE_ID timestamp
}
package com.hsbc.trds.beam.model;


import lombok.Builder;
import lombok.Data;

import java.io.Serializable;

@Data
@Builder
public class DslXmlDbModel implements Serializable {
    private String uuid;
    private String tradeId;
    private String region;
    private String dslObjectType;
    private String businessDate;
    private String payload;
}
cat: ./src/main/java/com/hsbc/trds/beam/options: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.options;

import org.apache.beam.runners.spark.SparkContextOptions;
import org.apache.beam.runners.spark.SparkPipelineOptions;
import org.apache.beam.sdk.extensions.gcp.options.GcsOptions;
import org.apache.beam.sdk.options.Default;

public interface GcsSparkOptions extends  GcsOptions {

//  SparkContextOptions, SparkPipelineOptions,

//  @Default.String("uk-gbm-fit-s3-nonprod-u")
//  String getEcsUser();
//  void setEcsUser(String ecsUser);
//
//  @Default.String("c8jMHx07RubVWMosecBn9GVMVaglp8aKtbHgwHgg")
//  String getEcsSecretKey();
//  void setEcsSecretKey(String ecsSecretKey);
//
//  @Default.String("http://wdctestlab-ecs1-node1.systems.uk.hsbc:9020")
//  String getEcsServiceEndpoint();
//  void setEcsServiceEndpoint(String ecsServiceEndpoint);
//
//  @Default.String("s3://uk-gbm-fit-s3-dev/john/output/result.parquet")
//  String getOutputFile();
//  void setOutputFile(String outputFile);
//
//  @Default.String("https://excalibur-authserver-dev.cf.wgdc-drn-01.cloud.uk.hsbc/oauth/token")
//  String getOauth2Url();
//  void setOauth2Url(String oauth2Url);
//
//  @Default.String("HBAPLDS-HKSSPIT")
//  String getServiceAccountId();
//  void setServiceAccountId(String serviceAccountId);
//
//  @Default.String("QxgzkcCC93m9g4Az")
//  String getServiceAccountPassword();
//  void setServiceAccountPassword(String serviceAccountPassword);
//
//  @Default.String("GBMFinance")
//  String getOauth2ClientId();
//  public void setOauth2ClientId(String oauth2ClientId);
//
//  @Default.String("clientAccessSecretKey")
//  String getOauth2ClientSecret();
//  public void setOauth2ClientSecret(String oauth2ClientSecret);
//
//  @Default.String("process")
//  String getOauth2ClientScope();
//  public void setOauth2ClientScope(String oauth2ClientScope);
//
//
//  @Default.String("http://finance-data-service-dev.cf.wgdc-drn-01.cloud.uk.hsbc/api/gbm-fit/fin-ods/1/trade/1/2019-08-21/vault/fxo/gb/hbeu?filters=&columns=*")
//  String getTradeUrl();
//  void setTradeUrl(String tradeUrl);
//
//  @Default.String("http://finance-data-service-dev.cf.wgdc-drn-01.cloud.uk.hsbc/api/gbm-fit/fin-ods/1/valuation/1/2019-08-21/vault/fxo/gb/hbeu?filters=&columns=*")
//  String getValuationUrl();
//  void setValuationUrl(String valuationUrl);
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.options;

import org.apache.beam.sdk.options.PipelineOptions;

import lombok.Data;

public interface WordCountOptions extends PipelineOptions {

  String getInputFile();

  void setInputFile(String value);

  String getOutput();

  void setOutput(String value);
}
cat: ./src/main/java/com/hsbc/trds/beam/partition: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.partition;

import java.util.Objects;

import org.apache.beam.sdk.transforms.SerializableFunction;

import com.hsbc.trds.beam.model.DataLakeModel;

public class ByBusinessDate implements SerializableFunction<DataLakeModel, String> {

  @Override
  public String apply(DataLakeModel input) {
    if(Objects.nonNull(input)) {
      return "/" + input.getBusinessDate();
    }
    return "";
  }
}
package com.hsbc.trds.beam.partition;

import com.hsbc.trds.beam.model.DslXmlDbModel;
import org.apache.beam.sdk.transforms.SerializableFunction;

import java.util.Objects;

public class ByBusinessDateForEcs implements SerializableFunction<DslXmlDbModel, String> {

    @Override
    public String apply(DslXmlDbModel input) {
        if (Objects.nonNull(input)) {
            return "/" + input.getBusinessDate();
        }
        return "";
    }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.partition;

import java.util.Objects;

import org.apache.beam.sdk.transforms.SerializableFunction;

import com.hsbc.trds.beam.model.DataLakeModel;

public class ByDslObjectType implements SerializableFunction<DataLakeModel, String> {

  private static final String SEPARATOR = "@@@";

  @Override
  public String apply(DataLakeModel input) {
    if(Objects.nonNull(input)) {
      // TRADE_ID UUID REGION TRADE_ID timestamp
      return input.getBusinessDate() + SEPARATOR
          + input.getDslObjectType() + SEPARATOR
          + input.getTradeId() + SEPARATOR
          + input.getUuid() + SEPARATOR
          + input.getRegion() + SEPARATOR
          + input.getTimestamp() + SEPARATOR;
    }
    return "";
  }
}
package com.hsbc.trds.beam.partition;

import com.hsbc.trds.beam.model.DslXmlDbModel;
import org.apache.beam.sdk.transforms.SerializableFunction;

import java.util.Objects;

public class ByDslObjectTypeForEcs implements SerializableFunction<DslXmlDbModel, String> {

    @Override
    public String apply(DslXmlDbModel input) {
        if (Objects.nonNull(input)) {
            return input.getDslObjectType();
        }
        return "";
    }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.partition;

import org.apache.beam.sdk.options.ValueProvider;

import lombok.AllArgsConstructor;

@AllArgsConstructor
public class PartitionProvider implements ValueProvider<String> {

  private String folder;

  @Override
  public String get() {
    return folder;
  }

  @Override
  public boolean isAccessible() {
    return true;
  }
}
cat: ./src/main/java/com/hsbc/trds/beam/pipeline: Is a directory
package com.hsbc.trds.beam.pipeline;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.options.PipelineOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;

public class BeamPipeLine {
    public static void main(String args[]) {
        PipelineOptions pipelineOptions = PipelineOptionsFactory.create();
        Pipeline pipeline = Pipeline.create(pipelineOptions);

    }
}
package com.hsbc.trds.beam.pipeline;

import org.apache.beam.sdk.options.Default;
import org.apache.beam.sdk.options.PipelineOptions;

public interface EcsOptions extends PipelineOptions {

    @Default.String("uk-dsl-ss-1-s3-nonprod")
    String getEcsBucket();
    void setEcsBucket(String ecsBucket);

    @Default.String("uk-dsl-ss-1-s3-nonprod-u")
    String getEcsUser();
    void setEcsUser(String ecsUser);

    @Default.String("2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0")
    String getEcsSecretKey();
    void setEcsSecretKey(String ecsSecretKey);

    @Default.String("http://ecs-storage.it.global.hsbc:9020")
    String getEcsServiceEndpoint();
    void setEcsServiceEndpoint(String ecsServiceEndpoint);
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.beam.sdk.values.KV;

import com.hsbc.trds.beam.function.FormatAsTextFn;
import com.hsbc.trds.beam.options.WordCountOptions;
import com.hsbc.trds.beam.transformer.LinesToWords;

import lombok.extern.slf4j.Slf4j;

public class FileBeam {

  public void runFilePipeline(String filePath) {
    WordCountOptions pipelineOptions = PipelineOptionsFactory.fromArgs(argumentBuilder(filePath)).as(WordCountOptions.class);
    Pipeline pipeline = Pipeline.create(pipelineOptions);
    pipeline.apply("ReadLines", TextIO.read().from(filePath))
        .apply(new LinesToWords())
        .apply(MapElements.via(new FormatAsTextFn()))
        .apply("WriteCounts", TextIO.write().to("word-count"));

    pipeline.run().waitUntilFinish();
  }

  private String[] argumentBuilder(String filePath) {
    String[] args = new String[1];
    args[0] = "--inputFile=" + filePath;
    return args;
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import com.google.auth.oauth2.GoogleCredentials;
import com.hsbc.trds.beam.constants.GcpService;
import com.hsbc.trds.beam.function.FormatAsTextFn;
import com.hsbc.trds.beam.gcp.CredentialsHelper;
import com.hsbc.trds.beam.transformer.LinesToWords;
import com.hsbc.trds.beam.transformer.WordsToGCS;
import org.apache.beam.runners.spark.SparkContextOptions;
import org.apache.beam.runners.spark.SparkRunner;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.coders.StringUtf8Coder;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;

public class GcsBeam {

  public void runGcsPipeline(String filePath) {
//      System.setProperty("https.proxyHost", "googleapis-dev.gcp.cloud.uk.hsbc");
//      System.setProperty("https.proxyPort", "3128");
      TrdsPipeLineOptions trdsPipeLineOptions = PipelineOptionsFactory.as(TrdsPipeLineOptions.class);

      GoogleCredentials gcpCredentials = CredentialsHelper.getCredentials(GcpService.CLOUD_STORAGE);
      //GcsOptions gcsOptions = PipelineOptionsFactory.as(GcsOptions.class);
      trdsPipeLineOptions.setGcpCredential(gcpCredentials);
      trdsPipeLineOptions.setRunner(SparkRunner.class);
      trdsPipeLineOptions.setJobName("GcsBeam-runner");
      trdsPipeLineOptions.setSparkMaster("yarn");

      SparkConf sparkConf = new SparkConf()
              .setAppName("GCSBeam-Spark-runner")
              .setMaster("yarn");

//      sparkConf.set("https.proxyHost","googleapis-dev.gcp.cloud.uk.hsbc");
//      sparkConf.set("https.proxyPort","3128");


      SparkSession sparkSession = SparkSession.builder().config(sparkConf).getOrCreate();
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.project.id", "hsbc-11007943-trds-dev");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.enable", "true");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.enable", "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key.id","23ba198aad06dfaf039323ceda2104468dd690ee");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key.id","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");
      String proxyhost = System.getProperty("https.proxyHost");
      String proxyPort = System.getProperty("https.proxyPort");
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.https.proxyHost", proxyhost);
      sparkSession.sparkContext().hadoopConfiguration().set("fs.gs.https.proxyPort", proxyPort);

      String keyFile = System.getProperty("keyFile");
      sparkSession.sparkContext().hadoopConfiguration().set("google.cloud.auth.service.account.json.keyfile", keyFile);

      //JavaSparkContext jsc = new JavaSparkContext(sparkConf);
      JavaSparkContext jsc = new JavaSparkContext(sparkSession.sparkContext());
      trdsPipeLineOptions.as(SparkContextOptions.class).setProvidedSparkContext(jsc);
      trdsPipeLineOptions.as(SparkContextOptions.class).setUsesProvidedSparkContext(true);

      System.setProperty("https.proxyHost", "googleapis-dev.gcp.cloud.uk.hsbc");
      System.setProperty("https.proxyPort", "3128");
      Pipeline pipeline = Pipeline.create(trdsPipeLineOptions);
      pipeline.apply("ReadLines", TextIO.read().from(filePath))
        .apply(new LinesToWords())
        .apply(MapElements.via(new FormatAsTextFn()))
        .setCoder(StringUtf8Coder.of())
        .apply(new WordsToGCS());

      pipeline.run().waitUntilFinish();
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.extensions.gcp.auth.GcpCredentialFactory;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
import org.apache.beam.sdk.io.gcp.pubsub.PubsubOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.MapElements;
import com.google.auth.Credentials;
import com.google.auth.oauth2.GoogleCredentials;

import com.hsbc.trds.beam.constants.GcpService;
import com.hsbc.trds.beam.function.FormatAsTextFn;
import com.hsbc.trds.beam.gcp.CredentialsHelper;
import com.hsbc.trds.beam.transformer.LinesToWords;

public class PubSubBeam {

  public void runPubSubPipeline(String filePath) {
    PubsubOptions pubsubOptions = PipelineOptionsFactory.as(PubsubOptions.class);
    GoogleCredentials credentials = CredentialsHelper.getCredentials(GcpService.PUBSUB);
    pubsubOptions.setGcpCredential(credentials);
    Pipeline pipeline = Pipeline.create(pubsubOptions);

    System.setProperty("https.proxyHost", "googleapis-dev.gcp.cloud.uk.hsbc");
    System.setProperty("https.proxyPort", "3128");

    pipeline.apply("ReadLines", TextIO.read().from(filePath))
        .apply(new LinesToWords())
        .apply(MapElements.via(new FormatAsTextFn()))
        .apply(PubsubIO.writeStrings().to("projects/hsbc-11007943-trds-dev/topics/data-lake-topic"));

    pipeline.run().waitUntilFinish();
  }
}
package com.hsbc.trds.beam.pipeline;

import org.apache.beam.runners.spark.SparkPipelineOptions;
import org.apache.beam.sdk.extensions.gcp.options.GcpOptions;

public interface TrdsPipeLineOptions extends SparkPipelineOptions, GcpOptions {
}
package com.hsbc.trds.beam.pipeline;

import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.hsbc.trds.beam.config.SolaceConfig;
import com.hsbc.trds.beam.properties.SolaceProperties;
import com.solace.connector.beam.SolaceIO;
import com.solace.connector.beam.examples.common.SolaceTextRecord;
import com.solacesystems.jcsmp.JCSMPProperties;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.aws.options.AwsOptions;
import org.apache.beam.sdk.io.aws.options.S3ClientBuilderFactory;
import org.apache.beam.sdk.io.aws.options.S3Options;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.WithTimestamps;
import org.apache.beam.sdk.transforms.windowing.AfterPane;
import org.apache.beam.sdk.transforms.windowing.AfterWatermark;
import org.apache.beam.sdk.transforms.windowing.FixedWindows;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.apache.beam.sdk.values.PCollection;
import org.joda.time.Duration;
import org.joda.time.Instant;

import java.io.Serializable;

public class XmlExtractToEcsBeam implements Serializable{

    private Duration WINDOW_TIME = Duration.standardMinutes(1);
    private Duration ALLOWED_LATENESS = Duration.standardMinutes(1);

    public void runEcsBeam(String args[]) {

        EcsOptions options = PipelineOptionsFactory.fromArgs(args).as(EcsOptions.class);

        AWSCredentials creds = new BasicAWSCredentials(options.getEcsUser(),options.getEcsSecretKey());
        options.as(AwsOptions.class).setAwsCredentialsProvider(new AWSStaticCredentialsProvider(creds));
        options.as(S3Options.class).setS3ClientFactoryClass(S3ClientBuilderFactory.class);
        options.as(S3Options.class).setAwsServiceEndpoint(options.getEcsServiceEndpoint());

        Pipeline pipeline = Pipeline.create(options);

        PCollection<SolaceTextRecord> windowedRead = readSolace(pipeline);

        //required to process the SolaceTextRecords and store directly to ECS





    }

    private PCollection<SolaceTextRecord> readSolace(Pipeline pipeline) {
        SolaceConfig solaceConfig = new SolaceConfig();
        SolaceProperties solaceProperties = solaceConfig.getSolaceProperties();
        JCSMPProperties jcsmpProperties = solaceConfig.getJcsmpProperties();

        PCollection<SolaceTextRecord> readFromSolace = pipeline.apply("Read from solace", SolaceIO.read(jcsmpProperties,
                solaceProperties.getQueues(),
                SolaceTextRecord.getCoder(),
                SolaceTextRecord.getMapper()));

        PCollection<SolaceTextRecord> windowedRead = readFromSolace
                .apply("append event time for PCollection records", WithTimestamps.of((SolaceTextRecord rec) -> new Instant()))
                .apply(
                        Window.<SolaceTextRecord>into(FixedWindows.of(WINDOW_TIME))
                                .withAllowedLateness(ALLOWED_LATENESS)
                                .triggering(
                                        AfterWatermark.pastEndOfWindow()
                                                .withLateFirings(AfterPane.elementCountAtLeast(1))
                                )
                                .accumulatingFiredPanes()
                );
        return windowedRead;
    }


}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import java.io.Serializable;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.FormatStyle;

import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.coders.VarLongCoder;
import org.apache.beam.sdk.extensions.gcp.options.GcsOptions;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.Combine;
import org.apache.beam.sdk.transforms.Count;
import org.apache.beam.sdk.transforms.MapElements;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.SimpleFunction;
import org.apache.beam.sdk.transforms.WithTimestamps;
import org.apache.beam.sdk.transforms.windowing.AfterPane;
import org.apache.beam.sdk.transforms.windowing.AfterWatermark;
import org.apache.beam.sdk.transforms.windowing.FixedWindows;
import org.apache.beam.sdk.transforms.windowing.SlidingWindows;
import org.apache.beam.sdk.transforms.windowing.Window;
import org.apache.beam.sdk.values.PCollection;
import org.joda.time.Duration;
import org.joda.time.Instant;
import com.google.auth.oauth2.GoogleCredentials;
import com.solace.connector.beam.SolaceIO;
import com.solace.connector.beam.examples.common.SolaceTextRecord;
import com.solacesystems.jcsmp.JCSMPProperties;

import com.hsbc.trds.beam.coder.DataLakeModelCoder;
import com.hsbc.trds.beam.config.GcsSparkConfig;
import com.hsbc.trds.beam.config.SolaceConfig;
import com.hsbc.trds.beam.constants.GcpService;
import com.hsbc.trds.beam.function.ExtractPayloadFn;
import com.hsbc.trds.beam.gcp.CredentialsHelper;
import com.hsbc.trds.beam.model.DataLakeModel;
import com.hsbc.trds.beam.options.GcsSparkOptions;
import com.hsbc.trds.beam.properties.SolaceProperties;
import com.hsbc.trds.beam.transformer.DataToGCS;

public class XmlExtractToGcsBeam implements Serializable {

  private static final long serialVersionUID = 6857648557104729747L;
  private Duration WINDOW_TIME = Duration.standardMinutes(1);
  private Duration ALLOWED_LATENESS = Duration.standardMinutes(1);
  private static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL)
      .withZone(ZoneId.systemDefault());

  public void runDataLakeBeam(String[] args) {
    GcsSparkOptions gcsSparkOptions = new GcsSparkConfig().gcsSparkOptions();
    Pipeline pipeline = Pipeline.create(gcsSparkOptions);

    SolaceConfig solaceConfig = new SolaceConfig();
    SolaceProperties solaceProperties = solaceConfig.getSolaceProperties();
    JCSMPProperties jcsmpProperties = solaceConfig.getJcsmpProperties();

    PCollection<SolaceTextRecord> readFromSolace = pipeline.apply("Read from solace", SolaceIO.read(jcsmpProperties,
        solaceProperties.getQueues(),
        SolaceTextRecord.getCoder(),
        SolaceTextRecord.getMapper()));



    PCollection<SolaceTextRecord> windowedRead = readFromSolace
        .apply("append event time for PCollection records", WithTimestamps.of((SolaceTextRecord rec) -> new Instant()))
        .apply(
            Window.<SolaceTextRecord>into(FixedWindows.of(WINDOW_TIME))
                .withAllowedLateness(ALLOWED_LATENESS)
                .triggering(
                    AfterWatermark.pastEndOfWindow()
                        .withLateFirings(AfterPane.elementCountAtLeast(1))
                )
                .accumulatingFiredPanes()
        );

    windowedRead.apply(ParDo.of(new ExtractPayloadFn()))
        .setCoder(DataLakeModelCoder.of())
        .apply(new DataToGCS())
        .apply(Combine.globally(Count.<DataLakeModel>combineFn()).withoutDefaults())
        .apply(MapElements.via(new SimpleFunction<Long, DataLakeModel>() {
          @Override
          public DataLakeModel apply(Long input) {
            System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) +": Count = " + input);
            return DataLakeModel.builder().build();
          }
        }));

    pipeline.run().waitUntilFinish();
  }
}
package com.hsbc.trds.beam.pipeline;

import org.apache.spark.SparkFiles;

import java.net.URISyntaxException;
import java.nio.file.Path;
import java.nio.file.Paths;

public class XmlGCSBeamExtractor {
    public static void main(String args[]) throws URISyntaxException {
        GcsBeam gcsBeam = new GcsBeam();
        //Path path = Paths.get(Thread.currentThread().getContextClassLoader().getResource("input.txt").toURI());
       // gcsBeam.runGcsPipeline("file://" + path.toFile().getAbsolutePath());

        gcsBeam.runGcsPipeline("gs://kv_bucket/my_test_blob");
    }
}

cat: ./src/main/java/com/hsbc/trds/beam/properties: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.properties;

import java.io.Serializable;
import java.util.List;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class SolaceProperties implements Serializable {

  private static final long serialVersionUID = 8238974059323023194L;
  private String host;
  private int port;
  private String vpn;
  private String username;
  private String password;
  private List<String> queues;
}
cat: ./src/main/java/com/hsbc/trds/beam/transformer: Is a directory
package com.hsbc.trds.beam.transformer;


import com.hsbc.trds.beam.model.DslXmlDbModel;
import com.hsbc.trds.beam.partition.ByBusinessDateForEcs;
import com.hsbc.trds.beam.partition.ByDslObjectTypeForEcs;
import com.hsbc.trds.beam.pipeline.EcsOptions;
import org.apache.beam.sdk.io.FileIO;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.Contextful;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.SerializableFunction;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;

import java.util.Objects;

public class DataToECS extends PTransform<PCollection<DslXmlDbModel>, PDone> {
    @Override
    public PDone expand(PCollection<DslXmlDbModel> input) {
        EcsOptions ecsOptions = PipelineOptionsFactory.as(EcsOptions.class);
        input.apply(
                FileIO.<String,DslXmlDbModel>writeDynamic()
                        .by(new ByBusinessDateForEcs())
                        .by(new ByDslObjectTypeForEcs())
                        .via(Contextful.fn
                                ((SerializableFunction<DslXmlDbModel,String>) record -> {
                                    if (Objects.nonNull(record)) {
                                        return record.getPayload();
                                    }
                                    return "";
                                }),
                                TextIO.sink())
                .to("s3://"+ecsOptions.getEcsBucket()+"/")


        );


        return null;
    }

}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.transformer;

import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.FormatStyle;
import java.util.Objects;

import org.apache.beam.sdk.coders.StringUtf8Coder;
import org.apache.beam.sdk.io.FileIO;
import org.apache.beam.sdk.io.FileIO.Write;
import org.apache.beam.sdk.io.FileIO.Write.FileNaming;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.WriteFilesResult;
import org.apache.beam.sdk.transforms.Contextful;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.SerializableFunction;
import org.apache.beam.sdk.values.PCollection;

import com.hsbc.trds.beam.filenaming.XmlFileNaming;
import com.hsbc.trds.beam.model.DataLakeModel;
import com.hsbc.trds.beam.partition.ByBusinessDate;
import com.hsbc.trds.beam.partition.ByDslObjectType;
import com.hsbc.trds.beam.partition.PartitionProvider;

import lombok.AllArgsConstructor;

@AllArgsConstructor
public class DataToGCS extends PTransform<PCollection<DataLakeModel>, PCollection<DataLakeModel>> {

  private static final String GCS_SCHEME = "gs://";
  private static final String DATA_LAKE_BUCKET = "data-lake-0001";
  private static final String FILE_PREFIX = "/data";
  public static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL)
      .withZone(ZoneId.systemDefault());

  @Override
  public PCollection<DataLakeModel> expand(PCollection<DataLakeModel> input) {
    System.setProperty("https.proxyHost", "googleapis-dev.gcp.cloud.uk.hsbc");
    System.setProperty("https.proxyPort", "3128");

//    PCollection<KV<DataLakeModel, Long>> count = input.apply(Count.perElement());

    WriteFilesResult<String> writeFilesResult = input.apply(
        FileIO.<String, DataLakeModel>writeDynamic()
//            .by(new ByBusinessDate())
            .by(new ByDslObjectType())
            .via(
                Contextful.fn(
                    (SerializableFunction<DataLakeModel, String>) record -> {
                      if (Objects.nonNull(record)) {
                        return record.getPayload();
                      }
                      return "";
                    }
                ),
                TextIO.sink())
            .to(GCS_SCHEME + DATA_LAKE_BUCKET + FILE_PREFIX)
            .withNaming(key -> {
              String[] split = key.split("@@@");
//              String path = split[0] + "/" + split[1] + "/" + split[2];
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": key = " + key);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[0] = " + split[0]);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[1] = " + split[1]);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[2] = " + split[2]);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[3] = " + split[3]);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[4] = " + split[4]);
//              System.out.println("" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": split[5] = " + split[5]);
//              System.out.println("split[3] = " + split[3]);

//              System.out.println(
//                  "" + DATE_TIME_FORMATTER.format(LocalDateTime.now()) + ": file = " + split[2] + "_" + split[3] + "_" + split[4] + "_" + split[2] + "_"
//                      + split[5] + ".xml");

              FileNaming fileNaming = Write.relativeFileNaming(new PartitionProvider(GCS_SCHEME + DATA_LAKE_BUCKET + FILE_PREFIX + "/" + split[0]),
                  Write.relativeFileNaming(new PartitionProvider(GCS_SCHEME + DATA_LAKE_BUCKET + FILE_PREFIX + "/" + split[0] + "/" + split[1]),
                      new XmlFileNaming(split[2], split[3], split[4], split[5])));

              return fileNaming;
            })
            .withDestinationCoder(StringUtf8Coder.of())
            .withTempDirectory(
                String.format(GCS_SCHEME + DATA_LAKE_BUCKET + "/tmp/%s", DATE_TIME_FORMATTER.format(LocalDateTime.now())))
            .withNumShards(5)

    );

//    PCollection<BigInteger> count = input.apply(Combine.globally(new TotalRecordsFn()))
//        .setCoder(BigIntegerCoder.of());

    return input;
//    return PDone.in(writeFilesResult.getPipeline());
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.transformer;

import org.apache.beam.sdk.transforms.Count;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;

import com.hsbc.trds.beam.function.ExtractWordsFn;

public class LinesToWords extends PTransform<PCollection<String>, PCollection<KV<String, Long>>> {

  @Override
  public PCollection<KV<String, Long>> expand(PCollection<String> input) {

    PCollection<String> words = input.apply(ParDo.of(new ExtractWordsFn()));

    PCollection<KV<String, Long>> wordCounts = words.apply(Count.perElement());

    return wordCounts;
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.transformer;

import java.util.Objects;
import javax.jms.Connection;
import javax.jms.JMSException;
import javax.jms.Message;
import javax.jms.MessageConsumer;
import javax.jms.MessageListener;
import javax.jms.Session;
import javax.jms.TextMessage;

import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import com.solacesystems.jms.SolJmsUtility;

import com.hsbc.trds.beam.config.SolaceConfig;
import com.hsbc.trds.beam.function.ExtractMessageFn;

public class SolaceMessageListener extends PTransform<PCollection<String>, PCollection<String>> {

  private Connection connection;

  public SolaceMessageListener() {
    SolaceConfig solaceConfig = new SolaceConfig();
    connection = solaceConfig.createConnection();
  }

  @Override
  public PCollection<String> expand(PCollection<String> input) {
    //    CountDownLatch latch = new CountDownLatch(1);
    MessageConsumer messageConsumer = null;
    Session session = null;
    try {
      connection.start();
      session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);

      String queueName = "FLARE_POC_Accounting_Business_Event";

      messageConsumer = getMessageConsumer(session, queueName);

      System.out.println("recieving message");
      System.out.println("Awaiting message...");
//      latch.await();

      Message receivedMessage = messageConsumer.receive();

      String message = extractMessage(receivedMessage);

      PCollection<String> messages = input.apply(ParDo.of(new ExtractMessageFn()));

      return messages;

    } catch (Exception e) {

      e.printStackTrace();
    } finally {
      stopConnection();
      closeMessageConsumer(messageConsumer);
      closeSession(session);
      closeConnection();
    }
    return null;
  }

  private void closeConnection() {
    if (Objects.nonNull(connection)) {
      try {
        connection.close();
      } catch (JMSException e) {
        e.printStackTrace();
      }
    }
  }

  private void closeSession(Session session) {
    if (Objects.nonNull(session)) {
      try {
        session.close();
      } catch (JMSException e) {
        e.printStackTrace();
      }
    }
  }

  private void closeMessageConsumer(MessageConsumer messageConsumer) {
    if (Objects.nonNull(messageConsumer)) {
      try {
        messageConsumer.close();
      } catch (JMSException e) {
        e.printStackTrace();
      }
    }
  }

  private void stopConnection() {
    if (Objects.nonNull(connection)) {
      try {
        connection.stop();
      } catch (JMSException e) {
        e.printStackTrace();
      }
    }
  }

  private String extractMessage(Message receivedMessage) throws JMSException {
    if (receivedMessage instanceof TextMessage) {
      return ((TextMessage) receivedMessage).getText();
    }
    return SolJmsUtility.dumpMessage(receivedMessage);
  }

  private MessageConsumer getMessageConsumer(Session session,
      String queueName) throws JMSException {
    MessageConsumer messageConsumer = session.createConsumer(SolJmsUtility.createQueue(queueName));
    messageConsumer.setMessageListener(new MessageListener() {

      int counte = 0;

      public void onMessage(final Message message) {
        counte++;

        try {
          if (message instanceof TextMessage) {
            System.out.printf("TextMessage received: '%s'%n", ((TextMessage) message).getText());
          } else {
            System.out.println("Message received. " + counte);
          }
          System.out.printf("Message Content:%n%s%n", SolJmsUtility.dumpMessage(message));

//            latch.countDown(); // unblock the main thread
        } catch (JMSException ex) {
          System.out.println("Error processing incoming message.");
          ex.printStackTrace();
        }
      }
    });
    return messageConsumer;
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.transformer;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.nio.channels.Channels;
import java.nio.channels.ReadableByteChannel;

import org.apache.beam.sdk.coders.StringUtf8Coder;
import org.apache.beam.sdk.io.FileSystems;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.fs.MatchResult;
import org.apache.beam.sdk.io.fs.ResourceId;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;
import org.apache.commons.io.FileUtils;

import lombok.AllArgsConstructor;

@AllArgsConstructor
public class WordsToGCS extends PTransform<PCollection<String>, PDone> {

  private static final String GCS_SCHEME = "gs://";
  private static final String DATA_LAKE_BUCKET = "data-lake-0001";
  private static final String WORD_COUNT_PREFIX = "/word-count.txt";
  private static final String TRDS_BUCKET_OBJECT = GCS_SCHEME + "trds_1/tradeIdHash=0/part-00000-b70bf5aa-e07b-403d-a4ec-dc69b7a9d064.c000.snappy.parquet";

  @Override
  public PDone expand(PCollection<String> input) {
    System.setProperty("https.proxyHost", "googleapis-dev.gcp.cloud.uk.hsbc");
    System.setProperty("https.proxyPort", "3128");
    // read from gcs bucket object
    ReadableByteChannel readChannel = getReadableByteChannel();
    try (InputStream stream = Channels.newInputStream(readChannel)) {
      File targetFile = new File("targetFile.parquet");
      FileUtils.copyInputStreamToFile(stream, targetFile);
    } catch (IOException e) {
      e.printStackTrace();
    }
    // write to gcs
    ResourceId resourceId = FileSystems.matchNewResource(GCS_SCHEME + DATA_LAKE_BUCKET + WORD_COUNT_PREFIX, false);
    PDone done = input.apply(TextIO.write().to(resourceId));
//    readAllFiles();
    return done;
  }

  private ReadableByteChannel getReadableByteChannel() {
    try {
      return FileSystems.open(FileSystems.matchNewResource(TRDS_BUCKET_OBJECT, false));
    } catch (IOException e) {
      e.printStackTrace();
    }
    return null;
  }

  private void readAllFiles() {
    try {
      MatchResult listResult = FileSystems.match(GCS_SCHEME + DATA_LAKE_BUCKET + "/**/*");
      listResult
          .metadata()
          .forEach(
              metadata -> {
                ResourceId resourceId = metadata.resourceId();
                System.out.println(resourceId.toString());
              });
    } catch (IOException e) {
      e.printStackTrace();
    }
  }


}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam;

import com.hsbc.trds.beam.pipeline.XmlExtractToGcsBeam;

public class XmlExtractorToGcs {

  public static void main(String[] args) {
    XmlExtractToGcsBeam xmlExtractToGcsBeam = new XmlExtractToGcsBeam();
    xmlExtractToGcsBeam.runDataLakeBeam(args);
  }
}
cat: ./src/main/java/com/hsbc/trds/core: Is a directory
cat: ./src/main/java/com/hsbc/trds/core/resource: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.core.resource;

import java.io.IOException;
import java.io.InputStream;
import java.util.Locale;
import java.util.Properties;
import java.util.ResourceBundle;

import org.springframework.core.io.DefaultResourceLoader;
import org.springframework.core.io.ResourceLoader;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class ResourceHandler {

  private static final String SYSTEM_NAME = "TRDS";
  private ResourceLoader resourceLoader;

  protected ResourceHandler() {
    resourceLoader = new DefaultResourceLoader();
  }

  protected ResourceBundle getResourceBundle() {
    Locale locale = new Locale(SYSTEM_NAME.toLowerCase());
    return ResourceBundle.getBundle(SYSTEM_NAME.toLowerCase(), locale);
  }

  protected InputStream loadResource(String jsonSchemaPath) throws IOException {
    return resourceLoader.getResource("classpath:" + jsonSchemaPath).getInputStream();
  }

  protected Properties loadProperties(String key) {
    ResourceBundle resourceBundle = getResourceBundle();
    String propertiesPath = resourceBundle.getString(key);
    Properties properties = new Properties();
    try (InputStream inputStream = loadResource(propertiesPath)) {
      properties.load(inputStream);
    } catch (IOException ex) {
      log.error("Error while loading the properties");
      throw new RuntimeException("Error while loading the properties", ex);
    }
    return properties;
  }
}
cat: ./src/main/java/com/hsbc/trds/dao: Is a directory
package com.hsbc.trds.dao;

import java.io.IOException;
import java.util.Date;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

import com.hsbc.trds.scala.driver.CftcTrade;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestHighLevelClient;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.base.Stopwatch;

import com.hsbc.trds.es.config.ElasticsearchPropertiesConfig;
import com.hsbc.trds.es.config.RestHighLevelClientConfig;
import com.hsbc.trds.es.model.Trade;
import com.hsbc.trds.es.properties.ElasticsearchProperties;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class ElasticService {

  private RestHighLevelClient client;
  private ElasticsearchProperties elasticsearchProperties;
  private ObjectMapper objectMapper;

  public ElasticService() {
    elasticsearchProperties = new ElasticsearchPropertiesConfig().elasticsearchProperties();
    client = new RestHighLevelClientConfig(elasticsearchProperties).restHighLevelClient();
    objectMapper = new ObjectMapper();
  }

  //public void ingest(List<Trade> trades) {
  public void ingest(List<CftcTrade> trades) {
    Stopwatch stopwatch = Stopwatch.createStarted();
   // Stopwatch stopwatch = new Stopwatch().start();
    BulkRequest bulkRequest = prepareBulkRequest(trades);
    try {
      client.bulk(bulkRequest, RequestOptions.DEFAULT);
      log.info("Time taken to bulk load movies = {} ms", stopwatch.elapsed(TimeUnit.MILLISECONDS));
      //log.info("Time taken to bulk load movies = {} ms", stopwatch.elapsedTime(TimeUnit.MILLISECONDS));
    } catch (IOException e) {
      log.error("Something went wrong while bulk loading movies", e);
      e.printStackTrace();
    }
  }

  private BulkRequest prepareBulkRequest(List<CftcTrade> trades) {
    List<IndexRequest> indexRequests = trades.stream()
        .map(this::newIndexRequest)
        .collect(Collectors.toList());
    BulkRequest bulkRequest = new BulkRequest();
    indexRequests.forEach(bulkRequest::add);
    return bulkRequest;
  }

  private IndexRequest newIndexRequest(CftcTrade cftcTrade) {
    Trade trade = new Trade(cftcTrade.utiValue(),cftcTrade.tradeId(),cftcTrade.executionVenueType(),
            cftcTrade.clearingStatus(),cftcTrade.productId(),cftcTrade.creationTimeStamp(),
            cftcTrade.executionDateTime(),cftcTrade.primaryAssetClass(),cftcTrade.party1Lei(),cftcTrade.party2Lei(),
            cftcTrade.transactionType(), "", new Date(), "");
    return new IndexRequest(elasticsearchProperties.getIndex().getTrds())
        .source(objectMapper.convertValue(trade, Map.class));
  }
}
cat: ./src/main/java/com/hsbc/trds/domain: Is a directory
package com.hsbc.trds.domain;

/**
 * Captures the concept of com.hsbc.trds.domain.Jurisdiction
 */
public enum Jurisdiction {
    CFTC, MIFID
}
package com.hsbc.trds.domain;

public class ObjectType {
}
cat: ./src/main/java/com/hsbc/trds/es: Is a directory
cat: ./src/main/java/com/hsbc/trds/es/config: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.es.config;

import java.util.Arrays;
import java.util.Properties;

import com.hsbc.trds.core.resource.ResourceHandler;
import com.hsbc.trds.es.properties.ElasticsearchProperties;
import com.hsbc.trds.es.properties.Index;
import com.hsbc.trds.es.properties.Rest;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class ElasticsearchPropertiesConfig extends ResourceHandler {

  public ElasticsearchPropertiesConfig() {
    super();
  }

  public ElasticsearchProperties elasticsearchProperties() {
    final String key = "trds.elasticsearch.properties.file";
    Properties properties = loadProperties(key);
    String uris = properties.getProperty("elasticsearch.rest.uris");
    String[] restUris = uris.split(",");
    return ElasticsearchProperties.builder()
        .index(Index.builder()
            .trds(properties.getProperty("elasticsearch.index.trds"))
            .build())
        .rest(Rest.builder()
            .uris(Arrays.asList(restUris))
            .build())
        .build();
  }


}
package com.hsbc.trds.es.config;

import org.apache.http.HttpHost;
import org.elasticsearch.client.RestClient;
import org.elasticsearch.client.RestClientBuilder;
import org.elasticsearch.client.RestHighLevelClient;

import com.hsbc.trds.es.properties.ElasticsearchProperties;

import lombok.RequiredArgsConstructor;

@RequiredArgsConstructor
public class RestHighLevelClientConfig {

  private final ElasticsearchProperties elasticsearchProperties;

  public RestHighLevelClient restHighLevelClient() {
    HttpHost[] httpHosts = elasticsearchProperties.getRest().getUris()
        .stream()
        .map(HttpHost::create)
        .toArray(HttpHost[]::new);
    RestClientBuilder builder = RestClient.builder(httpHosts);
    return new RestHighLevelClient(builder);
  }
}
cat: ./src/main/java/com/hsbc/trds/es/model: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.es.model;

import java.util.Date;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class Trade {

  private String utiValue;
  private String tradeId;
  private String executionVenueType;
  private String clearingStatus;
  private String productId;
  private Date creationTimestamp;
  private Date executionDateTime;
  private String primaryAssetClass;
  private String party1Lei;
  private String party2Lei;
  private String transactionType;
  private String dslObjectType;
  private Date businessDate;
  private String submissionReport;

  @Override
  public String toString() {
    return utiValue + "|"
        + tradeId + "|"
        + executionVenueType + "|"
        + clearingStatus + "|"
        + productId + "|"
        + creationTimestamp.getTime() + "|"
        + executionDateTime.getTime() + "|"
        + primaryAssetClass + "|"
        + party1Lei + "|"
        + party2Lei + "|"
        + transactionType + "|"
        + dslObjectType + "|"
        + businessDate.getTime() + "|"
        + submissionReport + "|";
  }
}
cat: ./src/main/java/com/hsbc/trds/es/properties: Is a directory
package com.hsbc.trds.es.properties;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class ElasticsearchProperties {

  private Rest rest;
  private Index index;
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.es.properties;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class Index {

  private String trds;
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.es.properties;

import java.util.List;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class Rest {
  private List<String> uris;
}
cat: ./src/main/java/com/hsbc/trds/extract: Is a directory
package com.hsbc.trds.extract;

import java.time.LocalDate;

/**
 * XML Extractor to pull data from EXA
 */
public interface DSLXMLExtractor<T> {
    T extract(LocalDate createDate, String objectType);
}package com.hsbc.trds.extract;

public class ECSL2Extractor {

}
cat: ./src/main/java/com/hsbc/trds/generator: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.generator;

import static org.apache.commons.lang.RandomStringUtils.randomAlphabetic;
import static org.apache.commons.lang.RandomStringUtils.randomAlphanumeric;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.lang.reflect.Field;
import java.text.DecimalFormat;
import java.text.DecimalFormatSymbols;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.time.format.FormatStyle;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Date;
import java.util.List;
import java.util.Random;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import java.util.stream.IntStream;
import java.util.stream.LongStream;

import org.apache.commons.lang.StringUtils;
import com.google.common.base.Stopwatch;
import com.opencsv.CSVWriter;

import com.hsbc.trds.es.model.Trade;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class DataGenerator {

  private static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.FULL)
      .withZone(ZoneId.systemDefault());
  private ExecutorService prepareExecutorService;
  private ExecutorService mainExecutorService;
  private ExecutorService writeExecutorService;
  private ExecutorService objectGeneratorExecutorService;
  private DataGeneratorProperties dataGeneratorProperties;

  public DataGenerator() {
    prepareExecutorService = new ThreadPool().threadPoolExecutor(200);
    mainExecutorService = new ThreadPool().threadPoolExecutor(100);
    writeExecutorService = new ThreadPool().threadPoolExecutor(20);
    objectGeneratorExecutorService = new ThreadPool().threadPoolExecutor(200);
    dataGeneratorProperties = new DataGeneratorPropertiesConfig().dataGeneratorProperties();
  }

  public void generate(long numberOfRecords,
      String exportCsvToPath,
      boolean addHeader,
      int repeat,
      String fileGenerationTime) {
    Stopwatch stopwatch = Stopwatch.createStarted();
//    long thousand = 1000L;
//    List<CompletableFuture<String>> futures = IntStream.range(0, 10000)
//        .mapToObj(index -> CompletableFuture.supplyAsync(
//            () -> doWrite(numberOfRecords, exportCsvToPath, addHeader, repeat, fileGenerationTime, index),
//            mainExecutorService))
//        .collect(Collectors.toList());
//
//
//    futures.forEach(future -> {
//      try {
//        future.get();
//      } catch (InterruptedException e) {
//        e.printStackTrace();
//      } catch (ExecutionException e) {
//        e.printStackTrace();
//      }
//    });
    for (int i = 0; i < repeat; i++) {
      doWrite(numberOfRecords, exportCsvToPath, addHeader, repeat, fileGenerationTime, i);
    }
    System.out.println("Total records = " + stopwatch.elapsed(TimeUnit.MILLISECONDS) + " ms");
  }

  private String doWrite(long numberOfRecords,
      String exportCsvToPath,
      boolean addHeader,
      int repeat,
      String fileGenerationTime,
      int futureIndex) {
    Stopwatch iterationStopWatch = Stopwatch.createStarted();
    List<CompletableFuture<List<String[]>>> preparedRecordsFutures = IntStream.range(0, repeat)
        .mapToObj(index -> CompletableFuture.supplyAsync(
            () -> prepare(numberOfRecords, addHeader),
            prepareExecutorService))
        .collect(Collectors.toList());
    List<String[]> recordsPerIndex = new ArrayList<>();
    preparedRecordsFutures.forEach(preparedRecords -> {
      try {
        List<String[]> records = preparedRecords.get();
//        recordsPerIndex.addAll(records);
//        writeRecords(exportCsvToPath, records, String.valueOf(futureIndex % 2 == 0 ? 0 : 1));
        writeRecords(exportCsvToPath, records, fileGenerationTime);
      } catch (InterruptedException e) {
        e.printStackTrace();
      } catch (ExecutionException e) {
        e.printStackTrace();
      }
    });

    String totalRecords = getTotalRecords(numberOfRecords, repeat);
    System.out.println(StringUtils.rightPad(String.valueOf(futureIndex + 1), 8)
        + DATE_TIME_FORMATTER.format(LocalDateTime.now())
        + ": Time taken to write " + totalRecords
        + " records = "
        + iterationStopWatch.elapsed(TimeUnit.MILLISECONDS)
        + " ms");
    return totalRecords;
  }

  private int writeRecords(String exportCsvToPath,
      List<String[]> preparedRecords,
      String fileGenerationTime) {
//    Stopwatch stopwatch = Stopwatch.createStarted();
    try (CSVWriter csvWriter = new CSVWriter(
        new FileWriter(new File(exportCsvToPath + File.separator + "unique-data_" + fileGenerationTime + ".csv"), true))) {
      csvWriter.writeAll(preparedRecords);
//      System.out.println(fileGenerationTime + " - Time taken to write all records = " + stopwatch.elapsed(TimeUnit.MILLISECONDS) + " ms");
      return preparedRecords.size();
    } catch (IOException e) {
      System.out.println("Error while writing the records" + e.getMessage());
      e.printStackTrace();
    }
//    System.out.println("Time taken to write 0 records = " + stopwatch.elapsed(TimeUnit.MILLISECONDS) + " ms");
    return 0;
  }

  private List<String[]> prepare(long numberOfRecords,
      boolean addHeader) {
//    Stopwatch stopwatch = Stopwatch.createStarted();
    List<String[]> records = new ArrayList<>();
    if (addHeader) {
      String[] header = Arrays.stream(Trade.class.getDeclaredFields())
          .map(Field::getName).toArray(String[]::new);
      records.add(header);
    }

    List<CompletableFuture<String[]>> objectFutures = LongStream.range(0, numberOfRecords)
        .mapToObj(index -> CompletableFuture.supplyAsync(
            () -> getTradeAndConvertToArray(index),
            objectGeneratorExecutorService))
        .collect(Collectors.toList());

    objectFutures.forEach(future -> {
      try {
        String[] record = future.get();
        records.add(record);
      } catch (InterruptedException e) {
        e.printStackTrace();
      } catch (ExecutionException e) {
        e.printStackTrace();
      }
    });

//    records.addAll(LongStream.range(0, numberOfRecords)
//        .mapToObj(this::getTradeAndConvertToArray)
//        .collect(Collectors.toList()));
//    System.out.println(Thread.currentThread().getName() + ": Time taken to prepare " + numberOfRecords + " records = " + stopwatch.elapsed(TimeUnit.MILLISECONDS) + " ms");
    return records;
  }

  private String[] getTradeAndConvertToArray(long index) {
    Trade trade = randomTradeFixedDomainValues();
    String plainStr = trade.toString();
    return plainStr.split("\\|");
  }

  private Trade trade() {
    return Trade.builder()
        .clearingStatus("Uncleared")
        .creationTimestamp(new Date())
        .executionDateTime(new Date())
        .executionVenueType("OffFacility")
        .party1Lei("MP6I5ZYZBEU3UXPYFY54")
        .party2Lei("7H6GLXDRUGQFU57RNE97")
        .primaryAssetClass("Commodity")
        .productId("Commodity:Energy:Coal:Option:Cash")
        .tradeId("TH14f1acae")
        .transactionType("Trade")
        .utiValue("APO2017112300000000000TH14f1acae")
        .build();
  }

  private Trade randomTrade() {
    return Trade.builder()
        .clearingStatus(randomAlphabetic(9))
        .creationTimestamp(new Date())
        .executionDateTime(new Date())
        .executionVenueType(randomAlphabetic(11))
        .party1Lei(randomAlphabetic(20))
        .party2Lei(randomAlphabetic(20))
        .primaryAssetClass(randomAlphabetic(9))
        .productId(
            randomAlphabetic(9) + ":"
                + randomAlphabetic(6) + ":"
                + randomAlphabetic(4) + ":"
                + randomAlphabetic(6) + ":"
                + randomAlphabetic(4))
        .tradeId(randomAlphabetic(10))
        .transactionType(randomAlphabetic(5))
        .utiValue(randomAlphabetic(32))
        .build();
  }

  private Trade randomTradeFixedDomainValues() {
    String assetClass = getValue(dataGeneratorProperties.getAssetClasses());
    Date date = new Date();
    return Trade.builder()
        .clearingStatus(getValue(dataGeneratorProperties.getClearingStatuses()))
        .creationTimestamp(date)
        .executionDateTime(date)
        .executionVenueType(getValue(dataGeneratorProperties.getExecutionVenueTypes()))
        .party1Lei(getValue(dataGeneratorProperties.getLegalEntityIds()))
        .party2Lei(getValue(dataGeneratorProperties.getLegalEntityIds()))
        .primaryAssetClass(assetClass)
        .productId(
            assetClass + ":"
                + randomAlphabetic(6) + ":"
                + randomAlphabetic(4) + ":"
                + randomAlphabetic(6) + ":"
                + randomAlphabetic(4))
        .tradeId(randomAlphanumeric(10))
        .transactionType(getValue(dataGeneratorProperties.getTransactionTypes()))
        .utiValue(randomAlphanumeric(200))
        .dslObjectType(getValue(dataGeneratorProperties.getDslObjectTypes()))
        .businessDate(date)
        .submissionReport(getValue(dataGeneratorProperties.getSubmissionReports()))
        .build();
  }

  private int getRandomIndex(int max) {
    Random random = new Random();
    return random.nextInt(max);
  }

  private String getValue(List<String> list) {
    int randomIndex = getRandomIndex(list.size());
    return list.get(randomIndex);
  }

  private String getTotalRecords(long numberOfRecords,
      int repeat) {
    DecimalFormatSymbols symbols = DecimalFormatSymbols.getInstance();
    symbols.setGroupingSeparator(',');
    DecimalFormat formatter = new DecimalFormat("###,###", symbols);
    return formatter.format(numberOfRecords * repeat);
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.generator;

import java.util.List;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class DataGeneratorProperties {

  private List<String> dslObjectTypes;
  private List<String> clearingStatuses;
  private List<String> assetClasses;
  private List<String> transactionTypes;
  private List<String> submissionReports;
  private List<String> executionVenueTypes;
  private List<String> legalEntityIds;
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.generator;

import static org.apache.commons.lang.RandomStringUtils.randomAlphabetic;
import static org.apache.commons.lang.RandomStringUtils.randomAlphanumeric;

import java.util.Arrays;
import java.util.List;
import java.util.Properties;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

import com.google.common.base.Stopwatch;

import com.hsbc.trds.core.resource.ResourceHandler;
import com.hsbc.trds.es.properties.ElasticsearchProperties;
import com.hsbc.trds.es.properties.Index;
import com.hsbc.trds.es.properties.Rest;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class DataGeneratorPropertiesConfig extends ResourceHandler {

  public static final String CLEARING_STATUSES = "clearing-statuses";
  public static final String ASSET_CLASSES = "asset-classes";
  public static final String TRANSACTION_TYPES = "transaction-types";
  public static final String SUBMISSION_REPORTS = "submission-reports";
  public static final String EXEC_VENUE_TYPES = "execution-venue-types";
  public static final String DSL_OBJECT_TYPES = "dsl-object-types";

  public DataGeneratorPropertiesConfig() {
    super();
  }

  public DataGeneratorProperties dataGeneratorProperties() {
    final String key = "trds.data.generator.properties";
    Properties properties = loadProperties(key);
    Stopwatch stopwatch = Stopwatch.createStarted();
    List<String> legalEntityIds = IntStream.range(0, 1000000)
        .mapToObj(index -> randomAlphanumeric(20))
        .collect(Collectors.toList());
    System.out.println("1 million LEI generated in " + stopwatch.elapsed(TimeUnit.MILLISECONDS) + " ms");
    return DataGeneratorProperties.builder()
        .clearingStatuses(extract(properties, CLEARING_STATUSES))
        .assetClasses(extract(properties, ASSET_CLASSES))
        .executionVenueTypes(extract(properties, EXEC_VENUE_TYPES))
        .transactionTypes(extract(properties, TRANSACTION_TYPES))
        .submissionReports(extract(properties, SUBMISSION_REPORTS))
        .dslObjectTypes(extract(properties, DSL_OBJECT_TYPES))
        .legalEntityIds(legalEntityIds)
        .build();
  }


  private List<String> extract(Properties properties, String key) {
    String value = properties.getProperty(key);
    return Arrays.asList(value.split(","));
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.generator;

import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

public class ThreadPool {

  public ExecutorService threadPoolExecutor(int threads) {
    return Executors.newFixedThreadPool(threads, threadPoolTaskExecutor());
  }

  private ThreadPoolTaskExecutor threadPoolTaskExecutor() {
    ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();
    threadPoolTaskExecutor.setThreadGroupName("data-generator");
    threadPoolTaskExecutor.setCorePoolSize(2000);
    threadPoolTaskExecutor.setMaxPoolSize(2000);
    threadPoolTaskExecutor.setQueueCapacity(5000);
    return threadPoolTaskExecutor;
  }
}
cat: ./src/main/java/com/hsbc/trds/load: Is a directory
cat: ./src/main/java/com/hsbc/trds/load/def: Is a directory
package com.hsbc.trds.load.def;

import com.hsbc.trds.domain.Jurisdiction;

public class JurisdictionChecker {
    public static Jurisdiction belongs(String objectType) {
        return Jurisdiction.CFTC;
    }
}
package com.hsbc.trds.load.def;

import java.time.LocalDate;

public interface SearchPrepWriteAdaptor<D> {
    boolean write(String objectType, LocalDate creationDate, D data) throws Exception;
}
package com.hsbc.trds.load.def;

import com.hsbc.trds.domain.Jurisdiction;
import com.hsbc.trds.load.L2ElasticSearchPrepWriteAdaptor;

public class SearchSLAChecker {
    public static SearchPrepWriteAdaptor getSearchPrepWriteAdaptor(String objectType) {
        Jurisdiction j = JurisdictionChecker.belongs(objectType);
        //adaptor based on Jurisdiciton
        switch (j){
            case CFTC: return new L2ElasticSearchPrepWriteAdaptor();
            default: return null;
        }
    }
}
package com.hsbc.trds.load;

import com.hsbc.trds.load.def.SearchPrepWriteAdaptor;
import org.apache.spark.sql.Dataset;

import java.time.LocalDate;

public class L2ECSWriteAdaptor implements SearchPrepWriteAdaptor<Dataset> {

    @Override
    public boolean write(String objectType, LocalDate creationDate, Dataset data) throws Exception {
        return false;
    }
}
package com.hsbc.trds.load;
import com.hsbc.trds.load.def.SearchPrepWriteAdaptor;
import org.apache.spark.sql.Dataset;

import java.time.LocalDate;

public class L2ElasticSearchPrepWriteAdaptor implements SearchPrepWriteAdaptor<Dataset> {

    @Override
    public boolean write(String objectType, LocalDate creationDate, Dataset data) throws Exception {
        return false;
    }
}
package com.hsbc.trds.load;

import com.hsbc.trds.load.def.SearchPrepWriteAdaptor;
import com.hsbc.trds.load.def.SearchSLAChecker;
import org.apache.spark.sql.Dataset;

import java.time.LocalDate;

public class L2SearchPrepWriteAdaptor implements SearchPrepWriteAdaptor<Dataset> {

    @Override
    public boolean write(String objectType, LocalDate creationDate, Dataset data) throws Exception {
        SearchPrepWriteAdaptor srchPrepWriteAdaptor = SearchSLAChecker.getSearchPrepWriteAdaptor(objectType);
        return srchPrepWriteAdaptor.write(objectType, creationDate, data);
    }

    public static void main(){
        //Get DataSet from L2 - READ the Parquet rep of search fields
       // ECSL2Extractor
    }
}
cat: ./src/main/java/com/hsbc/trds/search: Is a directory
package com.hsbc.trds.search;

public interface SearchAdaptor {

}
cat: ./src/main/java/com/hsbc/trds/solace: Is a directory
package com.hsbc.trds.solace;

import com.solacesystems.jms.SolConnectionFactory;
import com.solacesystems.jms.SolJmsUtility;
import com.solacesystems.jms.SupportedProperty;

import javax.jms.*;
import java.util.concurrent.CountDownLatch;

public class QueueConsumer {
    final String QUEUE_NAME = "Q_DATA_LAKE_TEST";

   // final CountDownLatch latch = new CountDownLatch(1);

    public void run1() throws Exception {

        //String[] split = args[1].split("@");

        String host = "gbwgdcsolacetest.systems.uk.hsbc:55555";
        String vpnName = "FIN_SM_NP_SSGU";
        String username = "GB-SVC-TRDS";
        String password = "7533-8d2cEF6F8";

        System.out.printf("QueueConsumer is connecting to Solace messaging at %s...%n", host);

        SolConnectionFactory connectionFactory = SolJmsUtility.createConnectionFactory();
        connectionFactory.setHost(host);
        connectionFactory.setVPN(vpnName);
        connectionFactory.setUsername(username);
        connectionFactory.setPassword(password);

        connectionFactory.setDynamicDurables(true);

        Connection connection = connectionFactory.createConnection();

        Session session = connection.createSession(false, SupportedProperty.SOL_CLIENT_ACKNOWLEDGE);

        System.out.printf("Connected to the Solace Message VPN '%s' with client username '%s'.%n", vpnName,
                username);

       // Queue queue = session.createQueue(QUEUE_NAME);

        //MessageConsumer messageConsumer = session.createConsumer(queue);

        MessageConsumer messageConsumer = getMessageConsumer(session,QUEUE_NAME);

        messageConsumer.setMessageListener(new MessageListener() {

            @Override
            public void onMessage(Message message) {
                try {
                    if (message instanceof TextMessage) {
                        System.out.printf("TextMessage received: '%s'%n", ((TextMessage) message).getText());
                    } else{
                        System.out.println("Message received.");
                    }
                    System.out.printf("Message Content: %n%s%n", SolJmsUtility.dumpMessage(message));

                    message.acknowledge();
                  //  latch.countDown();

                } catch(JMSException ex) {
                    System.out.println("Error processing incoming message.");
                    ex.printStackTrace();
                }
            }
        });

        //start receiving messages
        connection.start();
        System.out.println("Awaiting message..");

       // latch.wait();

        connection.stop();
        messageConsumer.close();
        session.close();
        connection.close();
    }

    private MessageConsumer getMessageConsumer(Session session,
                                               String queueName) throws JMSException {
        MessageConsumer messageConsumer = session.createConsumer(SolJmsUtility.createQueue(queueName));
        messageConsumer.setMessageListener(new MessageListener() {

            int counte = 0;

            public void onMessage(final Message message) {
                counte++;

                try {
                    if (message instanceof TextMessage) {
                        System.out.printf("TextMessage received: '%s'%n", ((TextMessage) message).getText());
                    } else {
                        System.out.println("Message received. " + counte);
                    }
                    System.out.printf("Message Content:%n%s%n", SolJmsUtility.dumpMessage(message));

//            latch.countDown(); // unblock the main thread
                } catch (JMSException ex) {
                    System.out.println("Error processing incoming message.");
                    ex.printStackTrace();
                }
            }
        });
        return messageConsumer;
    }

    public static void main(String... args) throws Exception {
        new QueueConsumer().run1();
    }
}
package com.hsbc.trds.solace;


import com.solacesystems.jcsmp.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class SolaceConnectionManager {
    private static final Logger logger = LoggerFactory.getLogger(SolaceConnectionManager.class);
    public static final String HOST = "tcp://128.10.3.107:55555";
    public static final String VPN_NAME = "FIN_SM_NP_SSGU";
    public static final String USERNAME = "GB-SVC-TRDS";
    public static final String PASSWORD = "7533-8d2cEF6F8";

    private JCSMPSession session;

    public void connect() throws Exception {
        //establish solace session
        JCSMPProperties properties = new JCSMPProperties();
        properties.setProperty(JCSMPProperties.HOST, HOST);
        properties.setProperty(JCSMPProperties.VPN_NAME, VPN_NAME);
        properties.setProperty(JCSMPProperties.USERNAME, USERNAME);
        properties.setProperty(JCSMPProperties.PASSWORD, PASSWORD);
        //properties.setProperty(JCSMPProperties.PUB_ACK_TIME, 10000);
        session = JCSMPFactory.onlyInstance().createSession(properties);

        if (HOST.contains(",")) {
            //ha - automatic reconnects
            JCSMPChannelProperties channelProperties = (JCSMPChannelProperties)properties.getProperty(JCSMPProperties.CLIENT_CHANNEL_PROPERTIES);
            channelProperties.setConnectRetries(3);
            channelProperties.setReconnectRetries(3);
            channelProperties.setReconnectRetryWaitInMillis(2000);
            channelProperties.setConnectRetriesPerHost(3); //Solace recommendeds 3 or more
        }
    }

    public JCSMPSession getSession() {
        return session;
    }

    public XMLMessageProducer createProducer() throws Exception {
        return session.getMessageProducer(new JCSMPStreamingPublishEventHandler() {
            @Override
            public void handleError(String s, JCSMPException e, long l) {
                logger.error("error connecting to solace: {} error code: {}", s, l, e);
            }

            @Override
            public void responseReceived(String s) {
                logger.debug("response received from solace: {}", s);
            }
        });
    }

    public void disconnect() {
        if(!session.isClosed()) {
            session.closeSession();
        }
    }
}
cat: ./src/main/java/com/hsbc/trds/transform: Is a directory
package com.hsbc.trds.transform;

public class DSLSearchFieldExtractor {

}
cat: ./src/main/java/com/hsbc/trds/utils: Is a directory
package com.hsbc.trds.utils;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.Closeable;
import java.io.IOException;
import java.util.zip.GZIPInputStream;
import java.util.zip.GZIPOutputStream;

public class CompressionUtils {

    private static ThreadLocal<byte[]> threadLocalDecompresionBuffers = ThreadLocal.withInitial(() -> new byte[10240]);

    public static byte[] compress(String payload) {

        if(payload == null) return null;
        ByteArrayOutputStream boas = new ByteArrayOutputStream();
        GZIPOutputStream gos = null;

        try {
            gos = new GZIPOutputStream(boas);
            gos.write(payload.getBytes());
            gos.close();
        } catch (IOException var7) {
            throw new RuntimeException("Failed to gzip document content", var7);
        } finally {
            closeQuietly(gos, boas);
        }

        return boas.toByteArray();

    }


    public static String decompress(byte[] packedContent) {
        if(packedContent == null || packedContent.length == 0) return null;
        ByteArrayOutputStream boas = new ByteArrayOutputStream();
        ByteArrayInputStream bois = new ByteArrayInputStream(packedContent);
        GZIPInputStream gos = null;
        byte[] buffer = (byte[])threadLocalDecompresionBuffers.get();

        try {
            gos = new GZIPInputStream(bois);

            int read;
            while((read = gos.read(buffer)) != -1) {
                boas.write(buffer, 0, read);
            }
        } catch (IOException var9) {
            throw new RuntimeException("Failed to gzip document content", var9);
        } finally {
            closeQuietly(gos, boas, bois);
        }

        byte[] decompressed = boas.toByteArray();
        boas = null;
        gos = null;
        bois = null;
        return new String(decompressed);
    }

    private static void closeQuietly(Closeable... closables) {
        Closeable[] arr = closables;
        int len = closables.length;

        for(int i = 0; i < len; ++i) {
            Closeable closeable = arr[i];
            if (closeable != null) {
                try {
                    closeable.close();
                } catch (IOException e) {
                    ;
                }
            }
        }

    }

}
package com.hsbc.trds.utils;

import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.TimeZone;

public class DateUtil {
    public static void main(String args[]) {
        Date date = new Date(1550534400000L);
        //DateFormat format = new SimpleDateFormat("dd/MM/yyyy HH:mm:ss");
        DateFormat format = new SimpleDateFormat("yyyyMMdd");
        format.setTimeZone(TimeZone.getTimeZone("Etc/UTC"));
        String formatted = format.format(date);
        System.out.println(formatted);
        format.setTimeZone(TimeZone.getTimeZone("Australia/Sydney"));
        formatted = format.format(date);
        System.out.println(formatted);
    }
}
package com.hsbc.trds.utils;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.parquet.hadoop.ParquetWriter;

import java.io.*;
import java.net.URI;
import java.net.URISyntaxException;

public class XmlUtil {
    public static void main(String args[]) {
        try {
            String xmlString ="<DSLObject businessDate=\"2021-01-13T11:11:07Z\">\n" +
                    "   <DslUser>dsloat</DslUser>\n" +
                    "   <SourceSystem>APO</SourceSystem>\n" +
                    "   <Region>HBUS</Region>\n" +
                    "   <TradeId>APOTRHBUS20210113110900001210</TradeId>\n" +
                    "   <Version>1</Version>\n" +
                    "   <TargetSystem>DSL</TargetSystem>\n" +
                    "   <DSLObjectType>UC_TRADE</DSLObjectType>\n" +
                    "   <DSLID>0x00000176FB74337E80A12927DE891C17E402743C3D22F8E2CB36A4CC5DCC9987</DSLID>\n" +
                    "   <ExternalID>TRADE_SOAK_1</ExternalID>\n" +
                    "   <AssetClass>Commodity</AssetClass>\n" +
                    "</DSLObject>";

            File file = new File("s3a://uk-dsl-ss-1-s3-nonprod/TRDS_LAKE/temp/"+"a.xml");
            FileWriter fr= new FileWriter(file);
            Writer br= new BufferedWriter(fr);
            br.write(xmlString);
            br.close();

        } catch(Exception ex) {
            System.out.println("write File to ECS is failed");
        }
    }


}
cat: ./src/main/resources: Is a directory
appName = dsl-trds-L0-extractor
mode = yarn

dev{
  ecs {
    uid1 = "uk-dsl-ss-1-s3-nonprod-u"
    host1 = "http://ecs-storage.it.global.hsbc:9020"
    secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
    bucketName1 = "uk-dsl-ss-1-s3-nonprod"
    s3bucketName1 = "s3a://uk-dsl-ss-1-s3-nonprod/"
    ecsLocation = "uk-dsl-ss-1-s3-nonprod/TRDS_LAKE/DSL_STREAM_09MAR"
    checkPointDir = "/TRDS_LAKE/DSL_STREAM_L0_CP"
  }
  solace {
      host = "gbwgdcsolacetest.systems.uk.hsbc,gbsygdcsolacetest.systems.uk.hsbc"
      vpn = "XA_SM_NP_XDS_SPLIT"
      username = "DSL-SOL-APP-SUB-TEST"
      password = "baRh5sGAwFZm05"
      queue = "TEST/Q-TRDS/DATA-LAKE"
  }
  checkpoint {
      enabled = true
  }
  partitions {
    columns = "dslObjectType,businessDate"
  }
}

guest{
  ecs {
    uid1 = "uk-dsl-ss-1-s3-nonprod-u"
    host1 = "http://ecs-storage.it.global.hsbc:9020"
    secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
    bucketName1 = "uk-dsl-ss-1-s3-nonprod"
    s3bucketName1 = "s3a://uk-dsl-ss-1-s3-nonprod/"
    ecsLocation = "uk-dsl-ss-1-s3-nonprod/TRDS_LAKE/DSL_STREAM_09MAR"
    checkPointDir = "/TRDS_LAKE/DSL_STREAM_L0_CP"
  }
  solace {
    host = "gbwgdcsolacetest.systems.uk.hsbc:55555"
    vpn = "FIN_SM_NP_SSGU"
    username = "GB-SVC-TRDS"
    password = "7533-8d2cEF6F8"
  }
  checkpoint {
    enabled = "true"
  }
  partitions {
    columns = "dslObjectType,businessDate"
  }
}elasticsearch.rest.uris=http://gbl17451.systems.uk.hsbc:9200
elasticsearch.index.trds=test_trdsappName = trds-prototype

dev{
  ecs {
    uid = "uslab-middle-office-s3-nonprod-u"
    host="http://ecs-walden-lab.us.hsbc:9020"
    secret="LOMWus6RzNGZINZVHAfwhm6SCQt4XkV5BrnLBzhz"
    bucketName="uslab-middle-office-s3-nonprod"
  }
  group1{
    host = ""

  }
}

sit{

  group2{

  }
  group3{

  }
}host=gbwgdcsolacetest.systems.uk.hsbc
port=55555
vpn=FIN_SM_NP_SSGU
username=GB-SVC-TRDS
password=7533-8d2cEF6F8
queues=Q_DATA_LAKE_TEST{
  "type": "service_account",
  "project_id": "hsbc-11007943-trds-dev",
  "private_key_id": "7d578f7053333010557337201dc76539f368ae4f",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDCbleC2W/em4q6\nBIytcIdNSqOUeqfp/lLKKC1X/Ql9qGijYo89FBodyFvOZFF7dkcB5AsMiaVy52sk\nGWqMZD0dh3CfKOUaGlvtslBTkIFtaT3IR/wM8qhDkGs6Wr3RhYaf7pOmMmQEGQNw\nfCfSP+SOVWfkKBkWo7Sc/8KHXVrfEF2ORqb14UBP4j5jAbJWJB0xNoytzVcuE8bZ\n4HzS5NIDOHqDCE/5D25ZgFbniYTdIHjSxKfTv2lXZpzvu+tBW9aSYpWrH0LWA7Vz\ngMbuN+zoFYjaaNgv4pNB/MCcqur9tf6j6aheFpi2Ugt/+kOb3Xdnc++DeulcSEpK\n9oAgzUyHAgMBAAECggEACe+Dj083NUPzid/zpGVMtLlstIi0DfQOHQCCmK14j7BM\ncJT3xuJBF2aGsnxT4aBArqsNmX1rM/z5b6XQ2Q3CRcEHE0sOsg/IxNPI4QxQYNvw\n6UwsaCj6SkGWCSxb6NoO8BtL9kE69S6mC1h83HShZFie73zRE2QYNbGoKiyuGRnx\nG29ot6W1w0HIfQQTjHNwIO7z18/Sw6xM3rE8nu3sT6O3t7t+Mk0Lj8XhbGnq13qJ\n7Y+ebwRv0rxZr/kKMKEYhMH8qt7gpZaw0CGyorH/zUUDz1GlcHHSBvTN4dU7R9lG\n7nHGSrAHKoTJzsukCn9g4CSByYWAq9dC3awVREamyQKBgQDy+yyxhry1+VBGF5Hh\n4UM68Ps7NiNipE2nsvGGfBjp9dKmZkVsaOQkNZACIiU/CKVileAx3wntYXbz9rV0\nFGqRtdR6RNxqC2I5f9T+WSPs8orvhQBvPw+ZdEvoifHjTj3PwpkNGY33zTASVikd\nkWYdyDvdnw3oejrYrAr5sNnlewKBgQDM2TwKJTEfK7i+rHksX9u3zEPR+3kkWiV6\nbEHM0avvk7OxwY0oEd7cANGWQh3tgryQw8S8/0SEWyFj/565F78c0VsqPWZtvSbE\nIB7YOP62GMtqJOjZTTibHEjloYFfW1ePSw4L+4IkbGKKbrtzsa+8HT2UOLe/bGpX\ne69LZcpZZQKBgQDWNt62PCVPAVfmE42HaINPCXlUQGx94ICd4kLQ38NPMGvgS7XU\n17yat6YYW2Ye5k54Vc3r6cjkwNKQTua2SfGSOI30pZT04MheDPcIringEgyvEcDK\nUvw7u63YexUH4sjXy6YHSIpC92D0KYXiDXfLteYhgsJOPMR4lWSvsVzI1QKBgCVH\nt6SFBj6M/288BkVsIJl/hCw33uEdwspX31W+JtNQBxjnh5/uOKfDQmFdIQMzksup\nxSk8L6UCzL3dLG61AqPk+fli+Twpe1+gFNz26mwLcSGdG/9IDXA1IpMf5GKNRg8e\n8qx9lj64tupIora4dKoAjE9oTxB4U5YePVW0bbH9AoGBAM7PCrZ76ipKqOoATB0E\nXSns17zX3nSviTFqeXNuJpmsphn7XAz/8ROM73RS1ZAA5fdpg+R/4bvUsV2aYw3E\nYi8ugKoQAUGndCr9QqkT5qCT4WsVp9x8IdKodJJeSc9Bf5ggR7lCNji9BdSb3ZkU\nZwboGN8xfJ8FtnR6R+K/6Bdq\n-----END PRIVATE KEY-----\n",
  "client_email": "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com",
  "client_id": "104365758285129631244",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/storage-admin%40hsbc-11007943-trds-dev.iam.gserviceaccount.com"
}
{
  "type": "service_account",
  "project_id": "hsbc-11007943-trds-dev",
  "private_key_id": "a3dc58a39c43e620e49656825c46546cc9facab9",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCxaEqFGQ6mmOzK\nAk59g32yXCG/Go6u+icNG0KWG8KUc1SMzKYRt3dkByEZVh4mFeADtfxAZ/QR2OCy\n9yFdqacxLKqYO0IryCeHdOHgt/uVZ38QmCkwgwop5A4OIXQVuJqNQLHcVdPoMxgy\npBbPrdIjlI8KfIHlq+9wf5omv6khx676b8Jep+GBprbDkJ0V4RhP+KkbS3TLmQq8\nTJLyrNsTb4kW2Rb2BeR61cx+GTc4ml0A7krfVwkBZlKoeqrryYWSW5WobMjfxhAJ\noUCZs0XnMdmNLhsb8HLb7Mzg566N4zr34msxaRxJQsjIGy6M19JZHb7HddbikWvq\nmybRTeHLAgMBAAECggEACCG7qLYpUcGYuPm6F067qD1I3rWc7LBSsesZ9j93f855\nhaqrwgwi0Nby0XJ9F0nRZtokod8h9N94DTv+r4hjPYHKoAe3VMJr7W2SHPbR9Vs1\nG9GdsbeP87OcYVFvPL+ldewNOwhEGlSdO1UCJqrVAwSAOf7TBXAIHaP7sOMAIagt\n1V/plOsqJjKsVTvoh+LAFRQYxmNXTrepDOajpAE31ewcUMdx9M0wsl/houdL/XZS\nsNLeCoHYzdpgdaa6UZpaKmjgVnZw9D0EGwtQ7r+jWEPZw9S+raGWBd4ojX613btL\nWbw0dLUpSOF314cxLn6ehWlWaLIW7xtZYF7yiQl2wQKBgQDvjpF4y+S6zQfysu2/\nLoEsA1KDWi++f7yRwhQC+Z2Jaa7LoyRWXnmxZehelk43v+WiL+98X+NcdmeeM0ZP\npOugEXJDEDtS+VE2iRhbGNgGaIQ2e5LBUnO9nr2h9DCQpR5MHs0TkQsg592jh8EB\nzNH6bFsgeEXEa0uGPxd9s4G/aQKBgQC9laXKWstpZtRot6VxhZqOkoOPjk6zDZF/\nv0Z+BZljdkjmn1vkBbloowZQAvvWgFlcdzYgMQ8sZeixC/E0urVaBOnFL6W/+8OQ\nNT/zTa0NDZM3YNFWLQBRETIHgLgxpH6ep1pandfYxxVRexDTS8zUg3DSoaNSG7kS\nsJmR8B6lEwKBgHYSwSTW6mAgGqDHDGPE2ioFYTAYzZuJfjohfJeSzNEj4+G/AXQI\nkNadMhEc6GSWEusD0XhuErRKL/xLrYYn9XMp5jWj1HmrJRpKLFUKQ7+02CW6drUO\nnLmDelhO8Xj5yZsfO1k/jkqjvhySFtF6UKyrp6azYT/U2p7KjhuI1JSxAoGBAKUE\nIK6loPA5hSEw2FybghAfD0xg/bd0U/TxtJX0obKlPocepokvQlcFr3TtY+8tAJsv\nWHlkumRWv9d7IZRX/4o/RItSEd+tcGeRMfzA26PE8SL+rrdnrCLj6LwHGkx7dYr6\n9/Zv9XTGtJObnuWL0NoZHpb4AoRxhQHznXgdnU7pAoGAf2TqwCFi4ChWiVrufP96\nMfHELVDx57Bm/qWIAYDxbC1v/SSbK/F8vbyoudwf+2A0+b9izAvLu96H9ysGCr3R\nTv8O83QfPdzEifo7Lw2PszpLGDYRXkC4wfuICKFDAKZaWkH8DoX3jUdScg1XRE1u\nKRMYfrEMTHU66YCyswsJGYQ=\n-----END PRIVATE KEY-----\n",
  "client_email": "pub-sub-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com",
  "client_id": "113919012707230698168",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/pub-sub-admin%40hsbc-11007943-trds-dev.iam.gserviceaccount.com"
}
# Set everything to be logged to the file bagel/target/unit-tests.log
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Ignore messages below warning level from Jetty, because it's a bit verbose
# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.spark=WARN
log4j.logger.parquet=ERROR
log4j.logger.org.apache.spark.sql.execution.datasources.parquet=ERROR
log4j.logger.org.apache.spark.sql.execution.datasources.FileScanRDD=ERROR
log4j.logger.org.apache.hadoop.io.compress.CodecPool=ERROR#!/bin/bash

BASEDIR=$(cd "$( dirname "${BASH_SOURCE[0]}" )"/../ && pwd )
echo "BaseDir: , " $BASEDIR

LIB_JARS=$(cd ${BASEDIR}; ls lib/ | awk -v prefix="${BASEDIR}/lib/" '{print prefix $1}' | paste -sd ',')
EXEC_JARS=$(cd ${BASEDIR}; ls lib/ | awk -v prefix="${BASEDIR}/lib/" '{print prefix $1}' | paste -sd ':')

cp -rf $BASEDIR/conf/* .

spark-submit --conf spark.driver.extraJavaOptions="-Denv=dev -Dconfig.file=./default.conf" \
             --conf spark.executor.extraJavaOptions="-Denv=dev -Dconfig.file=./default.conf" \
             --conf spark.driver.extraClassPath="$EXEC_JARS" \
             --conf spark.executor.extraClassPath="$EXEC_JARS" \
             --conf spark.executor.memoryOverhead="4096" \
             --conf  spark.hadoop.fs.s3a.multiobjectdelete.enable="false" \
             --conf  spark.hadoop.fs.s3a.fast.upload="true" \
             --conf  spark.sql.parquet.filterPushdown="true" \
             --conf  spark.sql.parquet.mergeSchema="false" \
             --conf  spark.sql.parquet.fs.optimized.committer.optimization-enabled="true" \
             --conf  spark.speculation="false" \
             --conf spark.yarn.submit.waitAppCompletion="false" \
            --conf spark.cleaner.referenceTracking="false" \
            --conf spark.cleaner.referenceTracking.blocking="false" \
            --conf spark.cleaner.referenceTracking.blocking.shuffle="false" \
            --conf spark.cleaner.referenceTracking.cleanCheckpoints="false" \
            --files ./trds.properties,./es.properties \
             --master yarn \
             --deploy-mode cluster \
             --driver-memory 4G \
             --executor-memory 8G \
             --driver-cores 4 \
             --num-executors 4 \
             --executor-cores 2 \
             --class com.hsbc.trds.scala.driver.XmlExtractor \
             --jars $LIB_JARS \
             --name "ecs-extractor-datalake" \
             /dsl/app/dslt12/suneel/trds_lake-1.0-SNAPSHOT/trds_lake-1.0-SNAPSHOT.jar $@
trds.elasticsearch.properties.file=es.properties
trds.guest.solace.properties=guest.solace.propertiescat: ./src/main/scala: Is a directory
cat: ./src/main/scala/com: Is a directory
cat: ./src/main/scala/com/hsbc: Is a directory
cat: ./src/main/scala/com/hsbc/trds: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala/configuration: Is a directory
package com.hsbc.trds.scala.configuration

import org.apache.spark.internal.Logging

case class StreamConfiguration(checkpointingEnabled: Boolean = false,
                               checkpointPath: String = "",
                               checkpointPathIntervalMs: Int = 0,
                               repartition: Int = 2,
                               endpoint: String = "",
                               batchInterval: Int = 0,
                               checkpointInterval: Int = 0,
                               awaitTerminationTimeout: Int = 0
                              ) extends Serializable {
}

object StreamConfiguration extends Logging {
  val CONFIG_FILE: String  = "stream.conf"


}

package com.hsbc.trds.scala.configuration

import java.io.File
import java.util.Properties

import com.typesafe.config.{Config, ConfigFactory}

class TrdsConfig(fileNameOption: Option[String] = None) extends java.io.Serializable{

  def baseConfig: Config = {
    ConfigFactory.load()
  }

  def parseFile(file: File): Config = {
    ConfigFactory.parseFile(file).withFallback(baseConfig)
  }

  def parseResources(fileName: String): Config = {
    ConfigFactory.parseResources(fileName).withFallback(baseConfig)
  }

  val config: Config = {
    if (fileNameOption.getOrElse("").isEmpty)
      ConfigFactory.load
    else
      ConfigFactory
        .systemProperties
      .withFallback(ConfigFactory.systemEnvironment.withFallback
        (ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseFile(new File(fileNameOption.getOrElse("")))
        .resolve
        )))
  }

  /**
    * returns Config object for specified path.
    *
    * This object contains all the original APIs
    *
    * @param path - json path to a specific node
    * @return - a Config object
    */

   def getConfig(path: String): Config = {
     config.getConfig(path)
   }

  def getString(path: String): String = {
    config.getString(path)
  }

  /**
    * returns a string object based on the json path
    *
    * @param path
    * @return
    */
  def getProperties(path: String): Properties = {
    import scala.collection.JavaConversions._

    val properties: Properties = new Properties()

    config.getConfig(path)
      .entrySet()
      .map({
        entry => properties.setProperty(entry.getKey, entry.getValue.unwrapped().toString)
      })
    properties
  }


}
cat: ./src/main/scala/com/hsbc/trds/scala/driver: Is a directory
package com.hsbc.trds.scala.driver

import java.util.Date

case class CftcTrade(
                    utiValue: String,
                    tradeId: String,
                    executionVenueType: String,
                    clearingStatus: String,
                    productId: String,
                    creationTimeStamp: Date,
                    executionDateTime: Date,
                    primaryAssetClass: String,
                    party1Lei: String,
                    party2Lei: String,
                    transactionType: String
                    ) {

  //def this(utiValue:String, tradeId:String, executionVenueType:String) = this(utiValue,tradeId,executionVenueType,)



}
package com.hsbc.trds.scala.driver

import java.text.SimpleDateFormat
import java.util.{Date, Random}

import com.hsbc.trds.scala.endpoint.database.DslXmlDbReader
import com.hsbc.trds.solace.SolaceConnectionManager
import com.hsbc.trds.utils.CompressionUtils
import com.solacesystems.jcsmp.impl.sdt.MapImpl
import com.solacesystems.jcsmp._
import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.sql.{Row, SparkSession}

object DSLXmlDbSolacePublisher extends Logging {

  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val basePublishTopic = "DATA_LAKE_TEST"


  def main(args: Array[String]): Unit = {
    val toString = udf((payload: Array[Byte]) => decodeBinary(payload))
    val sparkConf = new SparkConf().setAppName("dsl-xml-db-spark-solace-publisher").setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    primeSparkSessionForECSConnection(sparkSession)

    val query = "(select UUID,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,BUSINESS_DATE,ORA_HASH(UUID) as UUID_HASH,DSL_OBJECT_TYPE,AUDIT_DATE,XML_DOCUMENT from DSL_XML.T_XML_DATA)"
    val result = DslXmlDbReader.pullDslXmlDataForObjectType(sparkSession,query)

    val xmlDS = result.withColumn("XML_STRING",toString(col("XML_DOCUMENT"))).filter(row => isCorrectBusinessType(row))

    //log.info("xmlDs count: " + xmlDS.count)

    //xmlDS.foreachPartition(itrPar => uploadSolaceMultiple(itrPar.toList))

    xmlDS.take(4000).foreach(row => uploadSolace(row))
   // uploadSolace(xmlDS.first)
  }


  def iteratorToJavaList[T](iterator: Iterator[T]): java.util.List[T] = {
    import collection.JavaConverters._
    iterator.toList.asJava
  }

  def uploadSolaceMultiple(rows: List[Row]): Unit = {
    val solaceConnectionManager = new SolaceConnectionManager
    solaceConnectionManager.connect()
    val len = rows.length
    val producer: XMLMessageProducer = solaceConnectionManager.createProducer()
    val topic: Topic = JCSMPFactory.onlyInstance().createTopic(basePublishTopic)
    rows.iterator.foreach(row => {
      val payloadMessage: TextMessage = generateMessage(DeliveryMode.DIRECT, row)
      sendSolace(producer, payloadMessage, topic)
    })
    solaceConnectionManager.disconnect()
  }
  //def uploadSolace(payload: Array[Byte]): String = {
  def uploadSolace(row: Row): Unit = {
    val solaceConnectionManager = new SolaceConnectionManager
    solaceConnectionManager.connect()
    val topic:Topic = JCSMPFactory.onlyInstance().createTopic(basePublishTopic)
    //val start: Long = System.nanoTime()
    val producer:XMLMessageProducer = solaceConnectionManager.createProducer()
    //val payloadMessage: TextMessage = generateMessage(DeliveryMode.DIRECT, row)
    //sendSolace(producer, payloadMessage, topic)

    val payloadMessage: BytesXMLMessage = generateDslMssage(DeliveryMode.DIRECT, row)
    sendSolaceDsl(producer, payloadMessage, topic)
    solaceConnectionManager.disconnect()
  }

  def sendSolace(producer: XMLMessageProducer, payloadMessage: TextMessage, topic: Topic): Unit = {
      producer.send(payloadMessage, topic)
  }

  def sendSolaceDsl(producer: XMLMessageProducer, payloadMessage: BytesXMLMessage, topic: Topic): Unit = {
    producer.send(payloadMessage, topic)
  }

  def generateMessage(deliveryMode: DeliveryMode, row: Row): TextMessage = {
    val payloadMessage: TextMessage = JCSMPFactory.onlyInstance().createMessage(classOf[TextMessage])
    payloadMessage.setDeliveryMode(deliveryMode)
    val payload = row.getAs[String]("XML_STRING")
    payloadMessage.setText(payload)
    setSolaceHeaders(row, payloadMessage)
    payloadMessage
  }

  def generateDslMssage(deliveryMode: DeliveryMode, row: Row): BytesXMLMessage ={
      val factory = JCSMPFactory.onlyInstance();
      val message = factory.createMessage(classOf[BytesXMLMessage])
      val payload = row.getAs[Array[Byte]]("XML_DOCUMENT")
      //val payload = row.getAs[String]("XML_STRING")
      message.writeBytes(payload)

      val properties = factory.createMap()
      properties.putString("id", row.getAs[String]("UUID"))
      properties.putString("context", "context123")
      properties.putString("businessDate",convertDateToString(row.getAs[Date]("BUSINESS_DATE")))
      properties.putDouble("tradeVersion", row.getAs[java.math.BigDecimal]("TRADE_VERSION").doubleValue())
      properties.putString("dslObjectType", row.getAs[String]("DSL_OBJECT_TYPE"))
      properties.putString("dslUser", "dsluser123")
      properties.putString("region", row.getAs[String]("REGION"))
      properties.putString("sourceSystem", row.getAs[String]("SOURCE_SYSTEM"))

      message.setProperties(properties)
      message.setDeliveryMode(deliveryMode)
      message
  }

  def setSolaceHeaders(row: Row, payloadMessage: TextMessage): TextMessage = {
    val messageProperties:SDTMap =  new MapImpl()
    messageProperties.putString("UUID", row.getAs[String]("UUID"))
    messageProperties.putString("TRADE_ID", row.getAs[String]("TRADE_ID"))
    messageProperties.putDouble("TRADE_VERSION", row.getAs[java.math.BigDecimal]("TRADE_VERSION").doubleValue())
    messageProperties.putString("SOURCE_SYSTEM", row.getAs[String]("SOURCE_SYSTEM"))
    messageProperties.putString("REGION", row.getAs[String]("REGION"))
    messageProperties.putString("BUSINESS_DATE", convertDateToString(row.getAs[Date]("BUSINESS_DATE")))
    messageProperties.putString("DSL_OBJECT_TYPE", row.getAs[String]("DSL_OBJECT_TYPE"))
    messageProperties.putString("AUDIT_DATE", convertDateToString(row.getAs[Date]("AUDIT_DATE")))
    val now: Date = new Date()
    messageProperties.putString("TIME_STAMP",String.valueOf(now.getTime))
    payloadMessage.setProperties(messageProperties)
    payloadMessage
  }





  def convertDateToString(date: Date) : String = {
    val formatter = new SimpleDateFormat("yyyyMMdd")
    formatter.format(date)
  }

  def decodeBinary(bytes: Array[Byte]): String = {
    CompressionUtils.decompress(bytes)
  }

  def isCorrectBusinessType(row: Row) : Boolean = {
    val businessType = row.getAs[String]("DSL_OBJECT_TYPE")
    val result: Boolean = businessType match {
//      case "TR_CFTC_RT" | "TR_CFTC_VALUATION" | "TR_CSA_CFTC_RT"
//           | "TR_CFTC_VALUATION" | "TR_CFTC_DOCUMENT" | "TR_CFTC_SNAPSHOT" => true
      case "UC_TRADE"  | "RAW" => true
      case _ => false
    }
    result
  }

  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    // ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", uid1)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret1)
    // end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", host1)
  }

}
package com.hsbc.trds.scala.driver

import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{Dataset, SparkSession}
//import org.elasticsearch.hadoop.cfg.ConfigurationOptions
import org.apache.spark.sql.functions.col

object EcsExtractor extends Logging{

  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val bucketName1 = "uk-dsl-ss-1-s3-nonprod"
  val s3bucketName1: String = "s3a://" + bucketName1 + "/"

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("ecs-extractor-datalake").setMaster("yarn")

    sparkConf.set("spark.testing.memory", "2147480000")
//    sparkConf.set(ConfigurationOptions.ES_NODES, "gbl17451.systems.uk.hsbc:9200")
//    sparkConf.set(ConfigurationOptions.ES_BATCH_SIZE_BYTES, "30000")
//    sparkConf.set(ConfigurationOptions.ES_BATCH_SIZE_ENTRIES, "10000")
//    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
//    sparkConf.set(ConfigurationOptions.ES_HTTP_TIMEOUT, "10m")
//    sparkConf.set(ConfigurationOptions.ES_HTTP_RETRIES, "50")
//    sparkConf.set(ConfigurationOptions.ES_HEART_BEAT_LEAD, "50")
//    sparkConf.set(ConfigurationOptions.ES_SCROLL_SIZE,"500")
//    sparkConf.set(ConfigurationOptions.ES_INDEX_READ_MISSING_AS_EMPTY,"true")

    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()

    primeSparkSessionForECSConnection(sparkSession)
    execute(sparkSession)
  }

  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    // ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", uid1)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret1)
    // end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", host1)
  }

  def execute(sparkSession: SparkSession ): Unit = {
    log.info("Extracting data from ECS")
    //val filePath = s3bucketName1.concat("TRDS_LAKE/L2/Searchable/DSL_OBJECT_TYPE=REALTIME_PET/tradeIdHash=0/purexml")

    val filePath = s3bucketName1.concat("HCE_PCF_YARN_TEST/p_20200120/*")
    val data = sparkSession.read.parquet(filePath)

    log.info("data schema: ")
    data.printSchema


    val data1 = data.drop(col("TRADE_VERSION"))
                    .drop(col("IMPL_VERSION"))
                    .drop(col("THASH"))
                    .drop(col("TRADEIDHASH"))
                    .drop(col("POF_BINARY"))

    //log.info("data1 schema: ")
   // data1.printSchema

    data1.toDF().write.format("org.elasticsearch.spark.sql")
        .option("es.mapping.id","UUID")
        .option("es.nodes","gbl17451.systems.uk.hsbc:9200")
        .option("es.port","9200")
        .option("es.write.operation", "upsert")
        .mode("append")
        .save("hce-index-es")

    //data1.foreach(row => println("datarow: " + row))
  }


}
package com.hsbc.trds.scala.driver

import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.SparkSession
import scala.reflect.ClassTag

class MyTrade(val i: Int)

case class NormalPerson(name: String, age: Int) {
  def _name = name
  def _age = age
}

case class ReversePerson(name: Int, age: String){
  def _name = name
  def _age = age
}

case class Trade1(utiValue: String,tradeId: String)

object MyTrade extends Logging{



  def main(args: Array[String]): Unit = {

    implicit def kryoEncoder[A](implicit ct: ClassTag[A]) = org.apache.spark.sql.Encoders.kryo[A](ct)

    val sparkConf = new SparkConf().setAppName("ecs-MyTrade-datalake").setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()

    val d1 = sparkSession.createDataset(Seq(new MyTrade(1),new MyTrade(2),new MyTrade(3)))

    val d2 = d1.map(d => (d.i+1,d)).alias("d2")
    val d3 = d1.map(d => (d.i,d)).alias("d3")

    d2.printSchema()
    d3.printSchema()



     println("end")


  }

}
package com.hsbc.trds.scala.driver

import java.io._
import java.text.SimpleDateFormat
import java.util.Date

import com.hsbc.trds.scala.endpoint.database.DslXmlDbReader
import com.hsbc.trds.scala.util.ECSS3API
import com.hsbc.trds.scala.util.ECSS3API.ECSS3Config
import com.hsbc.trds.utils.CompressionUtils
import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.functions.{col, udf}

object XmlBlobExtractor extends Logging {

  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val bucketName1 = "uk-dsl-ss-1-s3-nonprod"
  val s3bucketName1: String = "s3a://" + bucketName1 + "/"
  val ecsLocation: String = "s3a://uk-dsl-ss-1-s3-nonprod/TRDS_LAKE/temp/"

  def main(args: Array[String]): Unit = {
    val toString = udf((payload: Array[Byte]) => decodeBinary(payload))
    val sparkConf = new SparkConf().setAppName("dsl-xml-db-extractor-with-spark") //.concat(context.getPartitionTblName)
      .setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    primeSparkSessionForECSConnection(sparkSession)

    val query = "(select UUID,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,BUSINESS_DATE,ORA_HASH(UUID) as UUID_HASH,DSL_OBJECT_TYPE,AUDIT_DATE,XML_DOCUMENT from DSL_XML.T_XML_DATA)"
  // val query = "(select UUID,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,DSL_OBJECT_TYPE,XML_DOCUMENT from DSL_XML.T_XML_DATA)"
    val result = DslXmlDbReader.pullDslXmlDataForObjectType(sparkSession,query)

    result.printSchema()


    val xmlDS = result.withColumn("xmlstring",toString(col("XML_DOCUMENT"))).filter(row => isCorrectBusinessType(row))
//    xmlDS.take(30).foreach(row => {
//      println("dsl xml row: " + row.toString())
//    })
    xmlDS.take(10).foreach(row => {
      //println("dsl xml row: " + row.toString())
      val tradeId = row.getAs[String]("TRADE_ID")
      val region = row.getAs[String]("REGION")
      val uuid = row.getAs[String]("UUID")
      val timeInLong = System.currentTimeMillis().toLong
      val xmlContent = row.getAs[String]("xmlstring")
      val businessDate =  convertDateToString(row.getAs[Date]("BUSINESS_DATE"))
      log.info("the businessDate is: " + businessDate)
      val fileName  = s"${tradeId}_${uuid}_${region}_${tradeId}_${timeInLong}.xml"
      saveToECS1(fileName, xmlContent, s"TRDS_LAKE/temp/$businessDate")
    })
    xmlDS.printSchema()
    //log.info("result size: " + xmlDS.count())
  }


  def convertDateToString(date: Date) : String = {
    val formatter = new SimpleDateFormat("yyyyMMdd")
    formatter.format(date)
  }

  def isCorrectBusinessType(row: Row) : Boolean = {
    val businessType = row.getAs[String]("DSL_OBJECT_TYPE")
    val result: Boolean = businessType match {
      case "TR_CFTC_RT" | "TR_CFTC_VALUATION" | "TR_CSA_CFTC_RT"
           | "TR_CFTC_VALUATION" | "TR_CFTC_DOCUMENT" | "TR_CFTC_SNAPSHOT" => true
      case _ => false
    }
    result
  }

  def decodeBinary(bytes: Array[Byte]): String = {
    CompressionUtils.decompress(bytes)
  }

  def saveToECS(fileName: String, xmlString: String) = {
    log.info("filename is: " + fileName)
    val file: File = new File(fileName)
    val fileWriter: FileWriter = new FileWriter(file)
    val bufferedWriter: Writer = new BufferedWriter(fileWriter)
    bufferedWriter.write(xmlString)
    bufferedWriter.close
  }

  def saveToECS1(fileName: String, xmlString: String, subDir: String) = {
    log.info("filename-ecs1 is: " + fileName)
    val config =  new ECSS3Config(true, false, s"$bucketName1/$subDir","","")
    val inputStream = new ByteArrayInputStream(xmlString.getBytes())
    val amazonS3Client = ECSS3API.getAmazonS3Client
    ECSS3API.putS3ObjectAsOutputStream(config, fileName, inputStream,amazonS3Client,false)

  }



  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    // ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", uid1)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret1)
    // end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", host1)
  }

}
package com.hsbc.trds.scala.driver

import java.text.SimpleDateFormat

import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{Encoders, Row, SparkSession}
import java.util.Date

import com.hsbc.trds.dao.ElasticService

object XmlExtractor extends Logging{

  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val bucketName1 = "uk-dsl-ss-1-s3-nonprod"
  val s3bucketName1: String = "s3a://" + bucketName1 + "/"

//  implicit def single[A](implicit c: ClassTag[A]): Encoder[A] = Encoders.kryo[A](c)
//  implicit def tuple2[A1, A2](implicit e1: Encoder[A1],e2: Encoder[A2]): Encoder[(A1,A2)] = Encoders.tuple[A1,A2](e1, e2)
//  implicit val encoder1: Encoder[GenericRecord] = org.apache.spark.sql.Encoders.javaSerialization(classOf[GenericRecord])
  implicit val cftcTradeEncoder = Encoders.kryo[CftcTrade]
  implicit val rowEncoder = Encoders.kryo[Row]

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("ecs-extractor-datalake").setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    primeSparkSessionForECSConnection(sparkSession)
    execute(sparkSession)

  }

  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    // ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", uid1)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret1)
    // end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", host1)
  }

  def iteratorToJavaList[T](iterator: Iterator[T]): java.util.List[T] = {
    import collection.JavaConverters._
    iterator.toList.asJava
  }

//  def mapEmployeesToBulkIndexRequest(employees: Iterator[Employee], indexName: String): String =
//    employees.map { employee =&gt;
//      s"""{ "index" : { "_index" : "${indexName}", "_type" : "employee", "_id" : "${employee.id.toString}" }}
//							{"id": "${employee.id.toString}", "name": "${employee.name}"\n""".stripMargin}.reduce(_ + _) + "\n" //A bulk request must be terminated with a new line as per ES Guide
//  //Use above method to form a bulk index request of Employees as below:
//  spark.read.jdbc(jdbcURL, "employee","id", lowerBound, upperBound, numPartitions, connectionProperties)
//    .select("id", "name").as[Employee]
//    .foreachPartition { emp =>
//      val client = new ElasticSearchClientProvider(esURL).getClient
//      val response = client.bulk(Option(indexName), Option("employee"), mapEmployeesToBulkIndexRequest(emp, indexName))
//      Await.result(response, Duration(indexRequestTimeout, "millis"))
//    }

  def execute(sparkSession: SparkSession ): Unit = {
    log.info("Extracting data from ECS")
    val filePath = s3bucketName1.concat("TRDS_LAKE/L0/APO/TR_CFTC_SNAPSHOT/")
    val rawXmlDS = sparkSession.read.format("com.databricks.spark.xml")
      .option("rowTag", "nonpublicExecutionReport")
      .load(filePath)



   // rawXmlDS.foreach(row => println(row))

    //rawXmlDS.printSchema()

    val df = rawXmlDS.mapPartitions(partitions => {
      partitions.map(row => {
        //val tradeData = new Trade

        val header = row.getAs[Row]("header")
        val party = row.getAs[Seq[Row]]("party")
        val transactionType = row.getAs[String]("originatingEvent")
        val trade = row.getAs[Row]("trade")
        val partyTradeIdentifier = trade.getAs[Row]("tradeHeader").getAs[Seq[Row]]("partyTradeIdentifier")

        var utiValue = ""
        var tradeId = ""
        for (partyTI <- partyTradeIdentifier) {
          if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("unique-transaction-identifier")) {
            utiValue = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
           // tradeData.setUtiValue(utiValue)
            println("utiValue is : " + utiValue)
          }
          if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("internal_Referenceid")) {
            tradeId = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
            //tradeData.setTradeId(tradeId)
            println("tradeId : " + tradeId)
          }
        }

        val partyTradeInformation = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation")
        val executionVenueType = partyTradeInformation.getAs[String]("executionVenueType")
        val clearingStatus = partyTradeInformation.getAs[String]("clearingStatus")
        val creationTimeStamp = header.getAs[String]("creationTimestamp")

        val commodityOption = trade.getAs[Row]("commodityOption")
        val executionDateTime = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation").getAs[String]("executionDateTime")
        val primaryAssetClass = commodityOption.getAs[String]("primaryAssetClass")
        val productId = commodityOption.getAs[Row]("productId").getAs[String]("_VALUE")

        val party1Lei = party(0).getAs[Row]("partyId").getAs[String]("_VALUE")
        val party2Lei = party(1).getAs[Row]("partyId").getAs[String]("_VALUE")

        println(s"cftc values: tradeId= $tradeId  executionVenueType=$executionVenueType clearingStatus=$clearingStatus" +
          s" productId=$productId creationTimeStamp=$creationTimeStamp executionDateTime=$executionDateTime party1Lei=$party1Lei" +
          s" party2Lei=$party2Lei transactionType=$transactionType primaryAssetClass=$primaryAssetClass")

        val cftcTrade = new CftcTrade(utiValue,
                      tradeId,
                      executionVenueType,
                      clearingStatus,
                      productId,
                      new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'").parse(creationTimeStamp),
                      new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'").parse(executionDateTime),
                      primaryAssetClass,
                      party1Lei,
                      party2Lei,
                      transactionType
                      )
        cftcTrade
      })
    })

    df.cache()

    df.foreachPartition(rowsItr => {
      val elasticService = new ElasticService()
      elasticService.ingest(iteratorToJavaList(rowsItr))
    })

//    df.printSchema()
//
//    df.show(10,false)
//    val totalXmls = df.count()
//    log.info("totalXmls : " + totalXmls)

  }
}
cat: ./src/main/scala/com/hsbc/trds/scala/endpoint: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala/endpoint/database: Is a directory
package com.hsbc.trds.scala.endpoint.database

import java.sql.{Connection, DriverManager, SQLException}
import java.util.Properties

import org.apache.spark.sql.{DataFrameReader, Dataset, Row, SparkSession}

object DslXmlDbReader {

  def getConnection() : Connection = DriverManager.getConnection("jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSL_XML_OAT.systems.uk.hsbc", "DSL_XML_RO", "quattr0s1")

  def closeConnection(con: Connection) = {
    try {
      if (con == null && con.isClosed)
        con.close
    } catch {
      case e1: SQLException => {
        e1.printStackTrace()
      }
    }
  }

  def registerHistoricControlEventView(sparkSession: SparkSession, historicControlEventViewAlias: String, tblName: String): Unit = {
    val connectionProps = getConnectionProperties()
    val jdbcDF = getHistoricControlEventRowDataset(sparkSession, connectionProps, tblName)
    jdbcDF.createOrReplaceGlobalTempView(historicControlEventViewAlias)
  }

  def getHistoricControlEventRowDataset(sparkSession: SparkSession, connectionProps: Properties, tblName: String): Dataset[Row] = {
    val datFrameReader = getEXADataFrameReader(sparkSession, connectionProps)
    datFrameReader.option("dbtable", tblName).load()

  }

  def pullDslXmlDataForObjectType(spark: SparkSession, query: String): Dataset[Row] = {
   // val lowerBound = 0l
    //val upperBound = 99l


    val result = getEXADataFrameReader(spark, getConnectionProperties())
      .option("dbtable", query)
      .option("partitionColumn",  "UUID_HASH")
      .option("lowerBound", 9677L)
      .option("upperBound", 4294963654L)
      .option("numPartitions", 100)
//      .option("fetchSize", 1000)
//      .option("oracle.jdbc.mapDateToTimestamp", "false")
//      .option("sessionInitStatement", "ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD'")
      .load()

    result

  }

  private def getEXADataFrameReader(sparkSession: SparkSession, connectionProps: Properties): DataFrameReader = {
    sparkSession.read
      .format("jdbc")
      .option("url", connectionProps.getProperty("url"))
      .option("password", connectionProps.getProperty("password"))
      .option("driver", connectionProps.getProperty("driver"))
      .option("user", connectionProps.getProperty("user"))

  }

  def getConnectionProperties(): Properties = {
    val connectionProps = new Properties()
    connectionProps.setProperty("user","DSL_XML_RO")
    connectionProps.setProperty("password","quattr0s1")
    connectionProps.setProperty("driver","oracle.jdbc.OracleDriver")
    connectionProps.setProperty("url","jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSL_XML_OAT.systems.uk.hsbc")
    connectionProps
  }

}
cat: ./src/main/scala/com/hsbc/trds/scala/stream: Is a directory
package com.hsbc.trds.scala.stream

import java.io.ByteArrayInputStream
import java.util
import java.util.{Date, Properties}

import com.amazonaws.services.s3.model.ObjectMetadata
import com.hsbc.trds.scala.util.ECSS3API.ECSS3Config
import com.hsbc.trds.scala.util._
import com.hsbc.trds.utils.CompressionUtils
import com.solacesystems.jcsmp.{BytesXMLMessage, JCSMPProperties}
import org.apache.spark.internal.Logging
import org.apache.spark.streaming.jms.SolaceMessageConsumerFactory
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

case class ReceivedDocument(id: String,
                            context: String,
                            businessDate: Date,
                            tradeVersion: Long,
                            dslObjectType: String,
                            dslUser: String,
                            region: String,
                            sourceSystem: String,
                            docVersion: Long,
                            payload: Array[Byte])

object DslXmlSparkSolaceStream extends Logging{
//
//  def main(args: Array[String]): Unit = {
//    test();
//  }

  def main(args: Array[String]): Unit = {
    //val fileName = System.getProperty("config.file")
    val fileName = "dsl-trds-L0.conf"
    val environment = System.getProperty("env")

    val config = new ConfigLoader(Option(fileName))
    val solaceProperties = config.getProperties(environment+".solace")

    val jcsmpProperties = new JCSMPProperties()

    jcsmpProperties.setProperty(JCSMPProperties.HOST, solaceProperties.getProperty("host"))
    jcsmpProperties.setProperty(JCSMPProperties.VPN_NAME, solaceProperties.getProperty("vpn"))
    jcsmpProperties.setProperty(JCSMPProperties.USERNAME, solaceProperties.getProperty("username"))
    jcsmpProperties.setProperty(JCSMPProperties.PASSWORD, solaceProperties.getProperty("password"))

    receiveStreams(config,environment)
  }

  def sparkStreamingContext(config: ConfigLoader, env: String) : StreamingContext = {
    val appName = config.getString("appName")
    val mode = config.getString("mode")
    val sparkConf = new SparkConf().setAppName(appName) //.concat(context.getPartitionTblName)
      .setMaster(mode)

    val ecsProperties = config.getProperties(env+".ecs")

    val sparkContext = new SparkContext(sparkConf)
    val hadoopConf = sparkContext.hadoopConfiguration

    hadoopConf.set("fs.s3a.access.key", ecsProperties.getProperty("uid1"))
    hadoopConf.set("fs.s3a.secret.key", ecsProperties.getProperty("secret1"))
    hadoopConf.set("fs.s3a.endpoint", ecsProperties.getProperty("host1"))
    //primeSparkSessionForECSConnection(sparkContext)

    val ssc = new StreamingContext(sparkContext, Seconds(120))
    val checkPointEnabled: Boolean =
      if (config.getString(env+".checkpoint.enabled").equals("true")) true
      else false
    if (checkPointEnabled) {
      ssc.checkpoint(ecsProperties.getProperty("checkPointDir"))
    }

   // primeSparkSessionForECSConnection(ssc.sparkContext)
    ssc
  }

  def receiveStreams(config: ConfigLoader, env: String) = {
    val solaceProperties = config.getProperties(env+".solace")

    val jcsmpProperties = new JCSMPProperties()

    jcsmpProperties.setProperty(JCSMPProperties.HOST, solaceProperties.getProperty("host"))
    jcsmpProperties.setProperty(JCSMPProperties.VPN_NAME, solaceProperties.getProperty("vpn"))
    jcsmpProperties.setProperty(JCSMPProperties.USERNAME, solaceProperties.getProperty("username"))
    jcsmpProperties.setProperty(JCSMPProperties.PASSWORD, solaceProperties.getProperty("password"))

    val ecsProperties = config.getProperties(env+".ecs")

    val queueName = solaceProperties.getProperty("queue")
    val solaceDestinationInfo: QueueSolaceDestinationInfo = QueueSolaceDestinationInfo(queueName)

    val bytesXmlConverter:BytesXMLMessage => Option[ReceivedDocument] = {
      case msg: BytesXMLMessage =>
        val props = msg.getProperties

        /**
          * Properties keys are as below.
          *
          *  context, id, streamCount, region, tradeVersion, dslUser, businessDate, trouxId, streamId,
          *  dslObjectType, sourceSystem
          */

        val id = props.getString("id")
        val context = props.getString("context")
        val businessDate = new Date(props.getLong("businessDate"))
        val tradeVersion = props.getLong("tradeVersion")
        val dslObjectType = props.getString("dslObjectType")
        val dslUser = props.getString("dslUser")
        val region = props.getString("region")
        val sourceSystem = props.getString("sourceSystem")
        val docVersion = props.getLong("docVersion")

        val document = ReceivedDocument(id,
          context,
          businessDate,
          tradeVersion,
          dslObjectType,
          dslUser,
          region,
          sourceSystem,
          docVersion,
          msg.getBytes)
        Some(document)
      case _ => None
    }

    val ssc = sparkStreamingContext(config,env)

    //    val stream = SolaceStreamUtils.createSynchronousSolaceQueueStream(ssc,
    //      //JndiMessageConsumerFactory(props,jmsDestinationInfo),
    //      SolaceMessageConsumerFactory(props,solaceDestinationInfo),
    //      bytesXmlConverter,
    //      1000,
    //      10.second,
    //      20.seconds)

    val stream = SolaceStreamUtils.createAsynchronousSolaceQueueStream(ssc,
      SolaceMessageConsumerFactory(jcsmpProperties, solaceDestinationInfo),
      bytesXmlConverter)

    stream.foreachRDD{
      rdd => {
        rdd.foreach {
          (message:ReceivedDocument) => {
//            log.info(s" id = ${message.id}" +
//              s" dslObjectType = ${message.dslObjectType}" +
//              s" context = ${message.context}" +
//              s" dslUser = ${message.dslUser}" +
//              s" businessDate = ${message.businessDate}" +
//              s" sourceSystem = ${message.sourceSystem}" +
//              s" tradeVersion = ${message.tradeVersion}" +
//              s" region = ${message.region}" +
//              s" sourceSystem = ${message.sourceSystem}" +
//              s" docVersion = ${message.docVersion}"
//            )

            saveToECS(message,ecsProperties)

            //log.info("record is : " + record)
          }
        }
        println("the count of batches: " + rdd.count)
      }
    }

    stream.start()
    ssc.start
    ssc.awaitTermination()

  }


  def saveToECS(message: ReceivedDocument,properties: Properties) = {

    val uid1 = properties.getProperty("uid1")
    val secret1 = properties.getProperty("secret1")
    val host1 = properties.getProperty("host1")
    val ecsLoc = properties.getProperty("ecsLocation")

    val metadata = new ObjectMetadata()
    metadata.setContentType("application/xml")
    metadata.setContentLength(message.payload.length)

    val userMetaData = new util.HashMap[String,String]()
    userMetaData.put("id", message.id)
    userMetaData.put("context", message.context)
    userMetaData.put("dslUser", message.dslUser)
    userMetaData.put("sourceSystem", message.sourceSystem)
    userMetaData.put("region", message.region)
    userMetaData.put("tradeVersion", ""+message.tradeVersion)
    userMetaData.put("docVersion", ""+message.docVersion)

    metadata.setUserMetadata(userMetaData)

    val tradeId = message.id.split("_")(0)
    //val timeInLong = System.currentTimeMillis()
    val fileName = s"${message.id}_${message.region}_${tradeId}_${message.docVersion}.xml"
    val businessDateYYYYMMDD = DateUtils.convertEpochLongDateIntoString(message.businessDate)
    val bucketName = s"${ecsLoc}/DSL_OBJECT_TYPE=${message.dslObjectType}/BUSINESS_DATE=${businessDateYYYYMMDD}"
    //log.info("the bucket path is : " + bucketName)
    val config = new ECSS3Config(true, false,bucketName,"","")

    val record: String = CompressionUtils.decompress(message.payload)
    val inputStream = new ByteArrayInputStream(record.getBytes)
    val amazonS3Client = ECSS3API.getAmazonS3Client(uid1,secret1,host1)

    ECSS3API.putS3ObjectAsOutputStream(config, fileName, inputStream, amazonS3Client, false, metadata)
  }

}
package com.hsbc.trds.scala.stream

import java.util

import com.solacesystems.jms.SupportedProperty
import javax.jms.ConnectionFactory


object SolaceCon {
  def main(args: Array[String]): Unit = {
    val env = new util.Hashtable[String,String]()

    import javax.naming.Context
    import javax.naming.InitialContext
    env.put(Context.INITIAL_CONTEXT_FACTORY, "com.solacesystems.jndi.SolJNDIInitialContextFactory")
    env.put(Context.PROVIDER_URL, "smf://gbwgdcsolacetest.systems.uk.hsbc:55555")
    env.put(Context.REFERRAL, "throw")
    env.put(Context.SECURITY_PRINCIPAL, "GB-SVC-TRDS")
    env.put(Context.SECURITY_CREDENTIALS,"7533-8d2cEF6F8")
    env.put(SupportedProperty.SOLACE_JMS_VPN, "FIN_SM_NP_SSGU")
    val context = new InitialContext(env)

    val cf = context.lookup("Q_DATA_LAKE_TEST")

    println("success")


  }
}
cat: ./src/main/scala/com/hsbc/trds/scala/util: Is a directory
package com.hsbc.trds.scala.util

import java.io.File
import java.util.Properties

import com.typesafe.config.{Config, ConfigFactory}

class ConfigLoader (fileNameOption: Option[String] = None) extends java.io.Serializable{

  def baseConfig: Config = {
    ConfigFactory.load()
  }

  def parseFile(file: File): Config = {
    ConfigFactory.parseFile(file).withFallback(baseConfig)
  }

  def parseResources(fileName: String): Config = {
    ConfigFactory.parseResources(fileName).withFallback(baseConfig)
  }

  val config: Config = {
    if (fileNameOption.getOrElse("").isEmpty)
      ConfigFactory.load
    else
      ConfigFactory
        .systemProperties
        .withFallback(ConfigFactory.systemEnvironment.withFallback
        //(ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseFile(new File(fileNameOption.getOrElse("")))
          (ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseResources(fileNameOption.getOrElse(""))
          .resolve
        )))
  }

  /**
    * returns Config object for specified path.
    *
    * This object contains all the original APIs
    *
    * @param path - json path to a specific node
    * @return - a Config object
    */

  def getConfig(path: String): Config = {
    config.getConfig(path)
  }

  def getString(path: String): String = {
    config.getString(path)
  }

  /**
    * returns a string object based on the json path
    *
    * @param path
    * @return
    */
  def getProperties(path: String): Properties = {
    import scala.collection.JavaConversions._

    val properties: Properties = new Properties()

    config.getConfig(path)
      .entrySet()
      .map({
        entry => properties.setProperty(entry.getKey, entry.getValue.unwrapped().toString)
      })
    properties
  }


}package com.hsbc.trds.scala.util

import java.io.File

import com.hsbc.trds.scala.util.ECSS3API.ECSS3Config
import com.typesafe.config.{Config, ConfigFactory}

object ConfigObject {

  def getConfig(fileNameOption: Option[String] = None): Config = {
    if (fileNameOption.getOrElse("").isEmpty)
      ConfigFactory.load
    else
      ConfigFactory
        .systemProperties
        .withFallback(ConfigFactory.systemEnvironment.withFallback
        (ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseFile(new File(fileNameOption.getOrElse("")))
          .resolve
        )))

  }

  val config = getConfig()

  val ecsS3Config = initEcsS3Config

  val initEcsS3Config: ECSS3Config = {
    val isMd5Validation = config.getString("aws_env.s3_disable_get_object_md5_validation")
    val bucketName = config.getString("aws_env.bucket_name")
    val s3_root = config.getString("aws_env.s3_root")
    val isS3Storage = config.getString("aws_env.is_s3_storage")
    //log("isS3Storage = " + isS3Storage)("INFO")
    val s3_persistence_bucket_name = config.getString("aws_env.s3_persistence_bucket_name")
    //log("s3_persistence_bucket_name = " + s3_persistence_bucket_name)("INFO")
    new ECSS3Config(if ("true" == isS3Storage) true else false,
      if ("true" == isMd5Validation) true else false, bucketName, s3_root, s3_persistence_bucket_name)
  }





}
package com.hsbc.trds.scala.util

import java.text.SimpleDateFormat
import java.time.LocalDateTime
import java.time.format.DateTimeFormatter
import java.util.{Date, Locale, TimeZone}

object DateUtils {

  def main(args: Array[String]): Unit = {
    val dat = new Date
    println(convertDateToString(dat))

    val datStr = "2021-01-13"
    val dateFormat = new SimpleDateFormat("yyyy-MM-dd")
    val inputDate = dateFormat.parse(datStr)
    println(convertDateToString(inputDate))

//    val datStr1 = "Tue Feb 19 00:00:00 GMT 2019"
//    val dateFormat1 = new SimpleDateFormat("yyyy-MM-dd")
//    val inputDate1 = dateFormat.parse(datStr1)
//    println(convertDateToString(inputDate))

    longToDate(1552003200000L)


  }
  def convertDateToString(date: Date) : String = {
    val formatter = new SimpleDateFormat("yyyyMMdd")
    formatter.format(date)
  }

  def longToDate(timeInLong: Long) : String = {
    val uuuuMmDdHhMmSs = DateTimeFormatter.ofPattern("uuuuMMddHHmmss")
    //val longDate = 20120720162145L
    val dateTime = LocalDateTime.parse(String.valueOf(timeInLong), uuuuMmDdHhMmSs)

    val humanReadableFormatter = DateTimeFormatter.ofPattern("uuuu-MM-dd h:mm a", Locale.ENGLISH);
    val formattedDateTime = dateTime.format(humanReadableFormatter)
    printf("formattedDateTime: " + formattedDateTime)
    formattedDateTime
  }

  def longToDateEasy(timeInLong: Long) : String = {

    val inputDF = new SimpleDateFormat("yyyyMMddHHmmss")
    val outputDF = new SimpleDateFormat("yyyy-MM-dd K:mm a")
    val date = inputDF.parse(""+timeInLong)
    println("dateinstring : " + outputDF.format(date))
   // println("date is : " + date)
    ""

  }

  def convertEpochLongDateIntoString(date: Date): String = {
   // val date = new Date(timeInLong)
    //DateFormat format = new SimpleDateFormat("dd/MM/yyyy HH:mm:ss");
    val format = new SimpleDateFormat("yyyyMMdd")
    format.setTimeZone(TimeZone.getTimeZone("Etc/UTC"))
    format.format(date)
  }

}
package com.hsbc.trds.scala.util

import java.io.{ByteArrayInputStream, File, IOException, InputStream}

import com.amazonaws.auth.{AWSCredentials, AWSStaticCredentialsProvider, BasicAWSCredentials}
import com.amazonaws.client.builder.AwsClientBuilder
import com.amazonaws.regions.Regions
import com.amazonaws.services.s3.{AmazonS3, AmazonS3ClientBuilder}
import com.amazonaws.services.s3.model._
import com.amazonaws.util.IOUtils
import com.amazonaws.{ClientConfiguration, Protocol}
import com.hsbc.trds.scala.util.ConfigObject.ecsS3Config

import scala.collection.JavaConversions._

object ECSS3API {
  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val bucketName1 = "uk-dsl-ss-1-s3-nonprod"
  val s3bucketName1: String = "s3a://" + bucketName1 + "/"
  class ECSS3Config(var is_ecs_storage: Boolean, var ecs_disable_get_object_md5_validation: Boolean,
                    var bucket_name: String, var root: String, var persistance_bucket_name: String)

  def getIdList(prefix: String, extension: String, checkPersisted: Boolean = false): List[String] = {
    System.setProperty("com.amazonaws.services.s3.disableGetObjectMD5Validation", if (ecsS3Config.is_ecs_storage) "true" else "false")
    val bucketName = if (!checkPersisted) ecsS3Config.bucket_name else ecsS3Config.persistance_bucket_name

    val s3Client: AmazonS3 = getAmazonS3Client()

    val request = new ListObjectsRequest().withBucketName(bucketName).withPrefix(prefix)
    var allResultRetrieved = false

    var keyList = List[String]()

    do {
      val response = s3Client.listObjects(request)
      val summaries = response.getObjectSummaries

      keyList ::: {
        val newSummaries = summaries.map(summary => summary.getKey)
        if (!extension.isEmpty)
          newSummaries.filter(key => {key.endsWith(extension)}).toList
        else
          newSummaries.toList
      }

      if (response.isTruncated) {
        request.setMarker(response.getNextMarker)
      } else {
        allResultRetrieved = true
      }

    } while(!allResultRetrieved)
    keyList
  }

  def isExistingS3Location(S3String: String, hostName: String= ""): Boolean = {
    val amazonS3Client = getAmazonS3Client()
    try{
      val result = checkObjectExists(ecsS3Config, S3String+ ( if (hostName.isEmpty) "" else "/" + hostName), amazonS3Client, true)
      result
    }catch {
      case _: Throwable => false
    }
  }

  def getS3ObjectAsString(config: ECSS3Config, id: String, amazonS3Client: AmazonS3, checkPersisted: Boolean) = {
    val is = getS3ObjectAsInputStream(config, id, amazonS3Client, checkPersisted)
    val resultString = convertStreamToString(is)
    try {
      is.close
    }catch {
      case ex: IOException => throw new RuntimeException("Cannot close the input stream for id: " + id, ex)
    }
    resultString
  }

  def putS3ObjectAsOutputStream(config: ECSS3Config, id: String, is: InputStream, amazonS3Client: AmazonS3, checkPersisted: Boolean) = {
    val bytes:Array[Byte] = IOUtils.toByteArray(is)
    val metadata = new ObjectMetadata()
    metadata.setContentType("application/xml")
    metadata.setContentLength(bytes.length)

    val byteArrayInputStream = new ByteArrayInputStream(bytes)
    val request = new PutObjectRequest(if (checkPersisted) config.persistance_bucket_name else config.bucket_name, id, byteArrayInputStream,new ObjectMetadata)
    try {
      amazonS3Client.putObject(request)
    }catch {
      case ex: Exception => { println(ex.getMessage); false}
    }
    true
  }

  def putS3ObjectAsOutputStream(config: ECSS3Config, id: String, is: InputStream, amazonS3Client: AmazonS3, checkPersisted: Boolean, metadata: ObjectMetadata) = {
    val bytes:Array[Byte] = IOUtils.toByteArray(is)
    metadata.setContentType("application/xml")
    metadata.setContentLength(bytes.length)
    val byteArrayInputStream = new ByteArrayInputStream(bytes)
    val request = new PutObjectRequest(if (checkPersisted) config.persistance_bucket_name else config.bucket_name, id, byteArrayInputStream,metadata)
    try {
      amazonS3Client.putObject(request)
    }catch {
      case ex: Exception => { println(ex.getMessage); false}
    }
    true
  }

  def putS3ObjectAsFile(config: ECSS3Config, id: String, file: File, amazonS3Client: AmazonS3, checkPersisted: Boolean) : Boolean = {
    val request = new PutObjectRequest(if (checkPersisted) config.persistance_bucket_name else config.bucket_name, id, file)
    try {
      amazonS3Client.putObject(request)
    }catch {
      case ex: Exception => { println(ex.getMessage); false}
    }
    true
  }


  def getS3ObjectAsInputStream(config: ECSS3Config, id: String, amazonS3Client: AmazonS3, checkPersisted: Boolean) =
    getS3ObjectInputStream(config, id, amazonS3Client, checkPersisted)


  def getS3ObjectInputStream(config: ECSS3Config, id: String, amazonS3Client: AmazonS3, checkPersisted: Boolean) = {
    val rangeObjectRequest = new GetObjectRequest(if (checkPersisted) config.persistance_bucket_name else config.bucket_name, id)
    val objectPortion = amazonS3Client.getObject(rangeObjectRequest)
    val metaMap = objectPortion.getObjectMetadata.getUserMetadata
    println("metaMap Size is: " + metaMap.size())
    for ((k,v) <- metaMap)
      println(s"key: $k, value: $v")
    objectPortion.getObjectContent
  }

  def getAmazonS3Client():AmazonS3 = {
    val credentials:AWSCredentials = new BasicAWSCredentials(uid1,secret1)
    val clientConfig = new ClientConfiguration()
    clientConfig.setProtocol(Protocol.HTTP)
    AmazonS3ClientBuilder.standard().withClientConfiguration(clientConfig)
      .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(host1,Regions.DEFAULT_REGION.getName))
      .withCredentials(new AWSStaticCredentialsProvider(credentials)).withPathStyleAccessEnabled(true)
      .build()
  }

  def getAmazonS3Client(uid: String, secret: String, host: String ):AmazonS3 = {
    val credentials:AWSCredentials = new BasicAWSCredentials(uid,secret)
    val clientConfig = new ClientConfiguration()
    clientConfig.setProtocol(Protocol.HTTP)
    val result = AmazonS3ClientBuilder.standard().withClientConfiguration(clientConfig)
      .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(host,Regions.DEFAULT_REGION.getName))
      .withCredentials(new AWSStaticCredentialsProvider(credentials)).withPathStyleAccessEnabled(true)
      .build()

    result
  }

  def checkObjectExists(config: ECSS3Config, id: String, s3Client: AmazonS3, checkPersisted: Boolean): Boolean = {
    var obj: S3Object = null
    try {
      obj = s3Client.getObject(if (checkPersisted) {config.persistance_bucket_name} else {config.bucket_name}, id)
    } catch {
      case ex: Exception => false
    }
    val result = (obj != null)
    if (obj != null) {
      try {
        obj.getObjectContent.close()
      } catch {
        case ex: IOException => throw new RuntimeException(ex)
      }
    }
    result
  }

  def convertStreamToString(inputStream: InputStream) = {
    val s = new java.util.Scanner(inputStream).useDelimiter("\\A")
    if (s.hasNext) s.next else ""
  }

}
package com.hsbc.trds.scala.util

import com.solacesystems.jcsmp.BytesXMLMessage
import javax.jms._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.jms.{AsynchronousJmsReceiver, MessageConsumerFactory, SynchronousJmsReceiver}

import scala.concurrent.duration._
import scala.reflect.ClassTag


object JmsStreamUtils {

  /**
    * Reliable Receiver to use for a Jms provider that does not support an individual acknowledgement mode.
    *
    * @param ssc
    * @param consumerFactory     Implementation specific factory for building MessageConsumer.
    *                            Use JndiMessageConsumerFactory to setup via JNDI
    * @param messageConverter    Function to map from Message type to T. Return None to filter out message
    * @param batchSize           Howmany messages to read off JMS source before submitting to streaming. Every batch
    *                            is a new task so reasonably high to avoid excessive task creation
    * @param maxWait             Max time to wait for messages before submitting a batch to streaming.
    * @param maxBatchSize        Max age of a batch before it is submitting. Used to cater for the case of a slow trickle
    *                            of messages
    * @param storageLevel
    * @tparam T
    * @return
    */
  def createSynchronousJmsQueueStream[T: ClassTag](ssc: StreamingContext,
                                                   consumerFactory: MessageConsumerFactory,
                                                   messageConverter: (Message) => Option[T],
                                                   batchSize: Int = 1000,
                                                   maxWait: Duration = 1.second,
                                                   maxBatchSize: Duration = 10.seconds,
                                                   storageLevel: StorageLevel =
                                                     StorageLevel.MEMORY_AND_DISK_SER_2):
                                              ReceiverInputDStream[T] = {
    ssc.receiverStream(new SynchronousJmsReceiver[T](
      consumerFactory,
      messageConverter,
      batchSize,
      maxWait,
      maxBatchSize,
      storageLevel
    ))
  }

  /**
    * Jms receiver that support asynchronous acknowledgement. If used with an individual acknowledgement mode can be
    * considered "Reliable". Individual acknowledgment mode is currently part of JMS spec but is supported by some vendors
    * such as  ActiveMQ and Solace
    *
    * @param ssc
    * @param consumerFactory
    * @param messageConverter
    * @param acknowledgementMode
    * @param storageLevel
    * @tparam T
    * @return
    */
  def createAsynchronousJmsQueueStream[T: ClassTag](ssc: StreamingContext,
                                                    consumerFactory: MessageConsumerFactory,
                                                    messageConverter: (Message) => Option[T],
                                                    acknowledgementMode: Int,
                                                    storageLevel: StorageLevel =
                                                    StorageLevel.MEMORY_AND_DISK_SER_2):
                                      ReceiverInputDStream[T] = {
    ssc.receiverStream(new AsynchronousJmsReceiver[T](consumerFactory,
      messageConverter,
      acknowledgementMode,
      storageLevel
      ))
  }
}

/**
  *
  *
 */
 sealed trait JmsDestinationInfo {

 }

/**
  * @param queueName
  *
  */
 case class QueueJmsDestinationInfo(queueName: String) extends JmsDestinationInfo

/**
  *
  * @param topicName
  * @param subscriptionName
  */
 case class DurableTopicJmsDestinationInfo(topicName: String, subscriptionName: String) extends JmsDestinationInfo










import java.util.Date

import com.google.common.base.CaseFormat
import com.hsbc.trds.scala.stream.ReceivedDocument

def extract(p: ReceivedDocument, fieldName: String): String = {
  fieldName match {
    case "dslObjectType" => p.dslObjectType
    case "businessDate" => p.businessDate.toString
  }
}

val field = CaseFormat.LOWER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, "DSL_OBJECT_TYPE")

println(field)


val message = ReceivedDocument("FXOHBEU20180405080000000301_0x00000162949C922780A4D46262C2042C03452C14339A7B38CFA8B8CA3418B02F","gbl17456_Cache_3",new Date(),1l, "RAW", "dslt12", "HBEU","FXO",1615564141619L,null)

extract(message,"businessDate")
extract(message,"dslObjectType")


package com.hsbc.trds.scala.util

import com.solacesystems.jcsmp.BytesXMLMessage
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.jms.{AsynchronousSolaceReceiver, MessageConsumerFactory, SynchronousJmsReceiver, SynchronousSolaceReceiver}

import scala.concurrent.duration.Duration
import scala.reflect.ClassTag
import scala.concurrent.duration._


object SolaceStreamUtils {

  def createSynchronousSolaceQueueStream[T: ClassTag](ssc: StreamingContext,
                                                      consumerFactory: MessageConsumerFactory,
                                                      bytesMessageConverter: (BytesXMLMessage) => Option[T],
                                                      batchSize: Int = 1000,
                                                      maxWait: Duration = 1.second,
                                                      maxBatchSize: Duration = 10.seconds,
                                                      storageLevel: StorageLevel =
                                                      StorageLevel.MEMORY_AND_DISK_SER_2):
      ReceiverInputDStream[T] = {
        ssc.receiverStream(new SynchronousSolaceReceiver[T](
          consumerFactory,
          bytesMessageConverter,
          batchSize,
          maxWait,
          maxBatchSize,
          storageLevel
        ))
  }

  def createAsynchronousSolaceQueueStream[T: ClassTag](ssc: StreamingContext,
                                                    consumerFactory: MessageConsumerFactory,
                                                       bytesMessageConverter: (BytesXMLMessage) => Option[T],
                                                    storageLevel: StorageLevel =
                                                    StorageLevel.MEMORY_AND_DISK_SER_2):
        ReceiverInputDStream[T] = {
          ssc.receiverStream(new AsynchronousSolaceReceiver[T](consumerFactory,
            bytesMessageConverter,
            storageLevel
          ))
        }
  }



/**
  *
  */
sealed trait SolaceDestinationInfo {

}


/**
  * @param queueName
  *
  */
case class QueueSolaceDestinationInfo(queueName: String) extends SolaceDestinationInfo

/**
  *
  */
case class DurableTopicSolaceDestinationInfo(topicName: String, subscriptionName: String) extends SolaceDestinationInfo


cat: ./src/main/scala/org: Is a directory
cat: ./src/main/scala/org/apache: Is a directory
cat: ./src/main/scala/org/apache/spark: Is a directory
cat: ./src/main/scala/org/apache/spark/streaming: Is a directory
cat: ./src/main/scala/org/apache/spark/streaming/jms: Is a directory
package org.apache.spark.streaming.jms

import java.util.concurrent.Executors

import javax.jms._
import org.apache.spark.storage.{StorageLevel, StreamBlockId}
import org.apache.spark.streaming.receiver.BlockGeneratorListener

import scala.collection.mutable

/**
  * Jms receiver that support asynchronous acknowledgement. If used with an individual
  * acknowledgement mode can be considered "Reliable". Individual acknowledgement mode is not
  * currently part of JMS spec but is supported by some vendors such as ActiveMQ and
  * Solace
  *
  * @tparam T
  */

class AsynchronousJmsReceiver[T] (override val consumerFactory: MessageConsumerFactory,
                                  override val messageConverter: (Message) => Option[T],
                                  val acknowledgementMode: Int = Session.AUTO_ACKNOWLEDGE,
                                  override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
            extends BaseJmsReceiver[T](consumerFactory, messageConverter, storageLevel) {

  override def onStart(): Unit = {
     running = true
     receiverThread = Some(Executors.newSingleThreadExecutor())
     blockGenerator = supervisor.createBlockGenerator(new AsyncGeneratedBlockHandler)

     blockGenerator.start()

     receiverThread.get.execute {
       new Runnable {
         override def run(): Unit = {
            try {
              val consumer = consumerFactory.newConsumer(acknowledgementMode)
              while (running) {
                val message = consumer.receive()
                blockGenerator.addData(message)
              }
            }catch {
              case e: Throwable =>
                  logError(e.getLocalizedMessage, e)
                  restart(e.getLocalizedMessage, e)
            }
         }
       }
     }
  }

  /** Class to handle blocks generated by the block generator. */

  private final class AsyncGeneratedBlockHandler extends BlockGeneratorListener {
    override def onAddData(data: Any, metadata: Any): Unit = {}
    override def onGenerateBlock(blockId:  StreamBlockId): Unit = {}

    def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {
      val messages = arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Message]]
      store(messages.flatMap(messageConverter(_)))
      messages.foreach(_.acknowledge())
    }

    def onError(message: String, throwable: Throwable) = {
      reportError(message,throwable)
    }
  }

}



package org.apache.spark.streaming.jms

import java.util.concurrent.Executors

import com.solacesystems.jcsmp.BytesXMLMessage
import javax.jms.{Message, Session}
import org.apache.spark.storage.{StorageLevel, StreamBlockId}
import org.apache.spark.streaming.receiver.BlockGeneratorListener

import scala.collection.mutable

class AsynchronousSolaceReceiver[T] (override val consumerFactory: MessageConsumerFactory,
                                     override val bytesMessageConverter: (BytesXMLMessage) => Option[T],
                                     //val acknowledgementMode: Int = Session.AUTO_ACKNOWLEDGE,
                                     override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
  extends BaseSolaceReceiver[T](consumerFactory, bytesMessageConverter, storageLevel) {

  override def onStart(): Unit = {
    running = true
    receiverThread = Some(Executors.newSingleThreadExecutor())
    blockGenerator = supervisor.createBlockGenerator(new AsyncGeneratedBlockHandler)

    blockGenerator.start()

    receiverThread.get.execute {
      new Runnable {
        override def run(): Unit = {
          try {
            val consumer = consumerFactory.newSolaceConsumer()
            while (running) {
              val message = consumer.receive()
              blockGenerator.addData(message)
            }
          }catch {
            case e: Throwable =>
              logError(e.getLocalizedMessage, e)
              restart(e.getLocalizedMessage, e)
          }
        }
      }
    }
  }

  /** Class to handle blocks generated by the block generator. */

  private final class AsyncGeneratedBlockHandler extends BlockGeneratorListener {
    override def onAddData(data: Any, metadata: Any): Unit = {}
    override def onGenerateBlock(blockId:  StreamBlockId): Unit = {}

    def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {
      val messages = arrayBuffer.asInstanceOf[mutable.ArrayBuffer[BytesXMLMessage]]
      store(messages.flatMap(bytesMessageConverter(_)))
      messages.foreach(_.ackMessage())
    }

    def onError(message: String, throwable: Throwable) = {
      reportError(message,throwable)
    }
  }
}
package org.apache.spark.streaming.jms

import java.util.concurrent.{ExecutorService, Executors, TimeUnit}

import com.solacesystems.jcsmp.BytesXMLMessage
import javax.jms._
import org.apache.spark.internal.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.{BlockGenerator, Receiver}


/**
  * Reliable receiver for a JMS source
  *
  * @param consumerFactory Implementation specific factory for building MessageConsumer.
  *                        Use JndiMessageConsumerFactory to setup via JNDI
  * @param messageConverter Function to map from Message type to T. Return None to filter out message
  * @param storageLevel
  * @tparam T
  */
abstract class BaseJmsReceiver[T](val consumerFactory: MessageConsumerFactory,
                                  val messageConverter: (Message) => Option[T],
                                  override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
                            extends Receiver[T](storageLevel) with Logging{

  @volatile
  @transient
  var receiverThread: Option[ExecutorService] = None

  @volatile
  @transient
  var running = false

  @volatile
  @transient
  var blockGenerator: BlockGenerator = _

  val stopWaitTime = 1000L

  override def onStop(): Unit = {
    running = false
    consumerFactory.stopConnection()
    receiverThread.foreach(_.shutdown())
    receiverThread.foreach{
      ex => if (!ex.awaitTermination(stopWaitTime, TimeUnit.MILLISECONDS)) {
        ex.shutdownNow()
      }
    }
    if (blockGenerator != null) {
      blockGenerator.stop()
      blockGenerator = null
    }

  }
}

package org.apache.spark.streaming.jms

import java.util.concurrent.{ExecutorService, TimeUnit}

import com.solacesystems.jcsmp.BytesXMLMessage
import org.apache.spark.internal.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.{BlockGenerator, Receiver}

abstract class BaseSolaceReceiver[T](val consumerFactory: MessageConsumerFactory,
                            val bytesMessageConverter: (BytesXMLMessage) => Option[T],
                            override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
  extends Receiver[T](storageLevel) with Logging {

  @volatile
  @transient
  var receiverThread: Option[ExecutorService] = None

  @volatile
  @transient
  var running = false

  @volatile
  @transient
  var blockGenerator: BlockGenerator = _

  val stopWaitTime = 1000L

  override def onStop(): Unit = {
    running = false
    consumerFactory.stopConnection()
    receiverThread.foreach(_.shutdown())
    receiverThread.foreach{
      ex => if (!ex.awaitTermination(stopWaitTime, TimeUnit.MILLISECONDS)) {
        ex.shutdownNow()
      }
    }
    if (blockGenerator != null) {
      blockGenerator.stop()
      blockGenerator = null
    }

  }
}
package org.apache.spark.streaming.jms

import java.util
import java.util.Properties

import com.hsbc.trds.scala.util.{DurableTopicJmsDestinationInfo, JmsDestinationInfo, QueueJmsDestinationInfo}
import com.solacesystems.jcsmp.Consumer
import com.solacesystems.jms.{SolConnectionFactory, SolJmsUtility, SupportedProperty}
import javax.jms.{Connection, Destination, MessageConsumer, Session, Topic}
import javax.naming.{Context, InitialContext}
import org.apache.spark.internal.Logging

import scala.collection.JavaConverters._

case class JndiMessageConsumerFactory(jndiProperties: Properties,
                                      destinationInfo: JmsDestinationInfo,
                                      connectionFactoryName: String = "ConnectionFactory",
                                      messageSelector: String = "")
  extends MessageConsumerFactory with Logging {

  @volatile
  @transient
  var initialContext: InitialContext = _

  override def makeConsumer(session: Session): MessageConsumer = {
   // log.info("askr in makeConsumer -- ")
    destinationInfo match {
      case DurableTopicJmsDestinationInfo(topicName, subscriptionName) =>
        val destination = initialContext.lookup(topicName).asInstanceOf[Topic]
        session.createDurableSubscriber(destination,
          subscriptionName,
          messageSelector,
          false)
      case QueueJmsDestinationInfo(queueName: String) =>
        //log.info("askr in QueueJmsDestinationInfo -- " + queueName)
        val destination = initialContext.lookup(queueName).asInstanceOf[Destination]
        session.createConsumer(destination, messageSelector)
    }
  }

  /*
  override def makeConnection: Connection = {
    if (initialContext == null) {
      initialContext = new InitialContext(jndiProperties)
    }

    val connectionFactory = initialContext.lookup(connectionFactoryName).asInstanceOf[ConnectionFactory]

    val props = jndiProperties.asScala
    val username = props.get(Context.SECURITY_PRINCIPAL)
    val password = props.get(Context.SECURITY_CREDENTIALS)

    val createConnection: Connection = (username, password) match {
      case (Some(username), Some(password)) =>
         connectionFactory.createConnection(username, password)
      case _ =>
        connectionFactory.createConnection()
    }
    createConnection
  }
  */

  /**
    *
    * @return
    */

  override def makeConnection: Connection = {

    log.info("Creating solace connection")
    val props = jndiProperties.asScala

    val env = new util.Hashtable[String,String]()
    env.put(Context.INITIAL_CONTEXT_FACTORY, props.getOrElse(Context.INITIAL_CONTEXT_FACTORY, ""))
    env.put(Context.PROVIDER_URL,props.getOrElse(Context.PROVIDER_URL,""))
    env.put(Context.SECURITY_PRINCIPAL,props.getOrElse(Context.SECURITY_PRINCIPAL,""))
    env.put(Context.SECURITY_CREDENTIALS,props.getOrElse(Context.SECURITY_CREDENTIALS,""))
    env.put(SupportedProperty.SOLACE_JMS_VPN,props.getOrElse(SupportedProperty.SOLACE_JMS_VPN,""))

    val connectionFactory: SolConnectionFactory = SolJmsUtility.createConnectionFactory(env)

    if (initialContext == null) {
      initialContext = new InitialContext(env)
    }
    log.info("askr in makeConnection111 --")
    connectionFactory.createConnection()
  }

  override def makeSolaceConsumer: Consumer = ???

  //   private def findFreePort(): Int = {
  //     val candidatePort = RandomUtils.nextInt(1024, 65536)
  //     Utils.startServiceOnPort(candidatePort, (trailPort: Int) => {
  //        val socket = new ServerSocket(trailPort)
  //        socket.close()
  //       (null, trailPort)
  //     }, new SparkConf()())._2
  //   }
}package org.apache.spark.streaming.jms

import com.solacesystems.jcsmp.Consumer
import javax.jms._
import org.apache.spark.internal.Logging

/**
  * Implement to setup Jms consumer programmatically. Must serializable i.e. Don't put Jms object in
  * non transient fields.
  */
trait MessageConsumerFactory extends Serializable with Logging {

  @volatile
  @transient
  var connection: Connection = _

  @volatile
  @transient
  var consumer: Consumer = _

  def newConsumer(acknowledgeMode: Int): MessageConsumer = {
    log.info("askr in newConsumer : ")
    stopConnection()
    connection = makeConnection
    val session  = makeSession(acknowledgeMode)
    val consumer = makeConsumer(session)

    connection.start()
    consumer
  }

  def newSolaceConsumer(): Consumer = {
    log.info("askr in newSolaceConsumer : ")
    consumer = makeSolaceConsumer
    consumer.start
    consumer
  }

  def stopSolaceConsumer(): Unit = {

  }

  def stopConnection(): Unit = {
    try {
      if (connection != null) {
        connection.close()
      }
    } finally {
      connection = null
    }
    try {
      if (consumer != null) {
        consumer.stop()
      }
    } finally  {
      consumer = null
    }
  }

  private def makeSession(acknowledgeMode: Int): Session = {
    connection.createSession(false, acknowledgeMode)
  }

  /**
    * Implement to make new Connection
    *
    * @return
    */
   def makeConnection: Connection

  /**
    * Build new consumer
    *
    * @param session
    * @return
    */
   def makeConsumer(session: Session): MessageConsumer

  /**
    * Implement to make a new solace consumer
    *
    * @return
    */
  def makeSolaceConsumer: Consumer

}
package org.apache.spark.streaming.jms

import com.hsbc.trds.scala.util.{DurableTopicSolaceDestinationInfo, QueueSolaceDestinationInfo, SolaceDestinationInfo}
import com.solacesystems.jcsmp._
import javax.jms.{Connection, MessageConsumer, Session}
import org.apache.spark.internal.Logging

case class SolaceMessageConsumerFactory(jcsmpProperties: JCSMPProperties,
                                        destinationInfo: SolaceDestinationInfo,
                                        connectionFactoryName: String = "ConnectionFactory",
                                        messageSelector: String = "")
  //extends MessageConsumerFactory with XMLMessageListener with Logging {
  extends MessageConsumerFactory with Logging {

  override def makeConsumer(session: Session): MessageConsumer =  ???

  override def makeSolaceConsumer(): Consumer = {

    destinationInfo match {
      case DurableTopicSolaceDestinationInfo(topicName, subscriptionName) =>
        log.info(s"$topicName")
        //Not a valid code for topic case match - ignore - starts
        //val destinationTopic = JCSMPFactory.onlyInstance().createTopic(topicName)
        val consumerSession = JCSMPFactory.onlyInstance().createSession(jcsmpProperties)
        consumerSession.connect()
        val flowProperties = new ConsumerFlowProperties
        //flowProperties.setEndpoint(destinationTopic)
        flowProperties.setAckMode(JCSMPProperties.SUPPORTED_MESSAGE_ACK_CLIENT)
        val endPointProperties = new EndpointProperties()
        endPointProperties.setAccessType(EndpointProperties.ACCESSTYPE_EXCLUSIVE)
        val consumer = consumerSession.createFlow(null, flowProperties, endPointProperties)
        consumer
      //Not a valid code for topic case match - ignore - ends

      case QueueSolaceDestinationInfo(queueName: String) =>
        val destinationQueue =JCSMPFactory.onlyInstance().createQueue(queueName)
        val consumerSession = JCSMPFactory.onlyInstance().createSession(jcsmpProperties)
        consumerSession.connect()

        val flowProperties = new ConsumerFlowProperties
        flowProperties.setEndpoint(destinationQueue)
        flowProperties.setAckMode(JCSMPProperties.SUPPORTED_MESSAGE_ACK_CLIENT)

        val endPointProperties = new EndpointProperties()
        endPointProperties.setAccessType(EndpointProperties.ACCESSTYPE_EXCLUSIVE)

        val consumer = consumerSession.createFlow(null, flowProperties, endPointProperties)
        consumer
    }
  }

  /**
    * Implement to make new Connection
    *
    * @return
    */
  override def makeConnection: Connection = ???
}package org.apache.spark.streaming.jms

import java.util.concurrent._

import com.google.common.base.Stopwatch
import javax.jms._
import org.apache.spark.storage.{StorageLevel, StreamBlockId}
import org.apache.spark.streaming.receiver.BlockGeneratorListener

import scala.collection.mutable.ArrayBuffer
import scala.concurrent.duration._

class SynchronousJmsReceiver[T] (override val consumerFactory: MessageConsumerFactory,
                                 override val messageConverter: (Message) => Option[T],
                                 val batchSize: Int = 10000,
                                 val maxWait: Duration = 1.second,
                                 val maxBatchSize: Duration = 1.seconds,
                                 override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
                                extends BaseJmsReceiver[T](consumerFactory, messageConverter, storageLevel) {
  override def onStart(): Unit = {
    //log.info("askr in onStart : ")
    running = true
    receiverThread = Some(Executors.newSingleThreadExecutor())

    //We are just using the blockGenerator for access to rate limiter
    blockGenerator = supervisor.createBlockGenerator(new GeneratedBlockHandler)
    receiverThread.get.execute {
      new Runnable {
        val buffer = ArrayBuffer[Message]()
       // val stopWatch = new Stopwatch().start()
        val stopWatch = Stopwatch.createUnstarted().start()

        override def run(): Unit = {
          try {
            val consumer = consumerFactory.newConsumer(Session.CLIENT_ACKNOWLEDGE)
            //val consumer = consumerFactory.newSolaceConsumer()
            while(running) {
              if (buffer.size >= batchSize ||
                  stopWatch.elapsed(TimeUnit.MILLISECONDS) >= maxBatchSize.toMillis) {
                //stopWatch.elapsedTime(TimeUnit.MILLISECONDS) >= maxBatchSize.toMillis) {
                storeBuffer()
              }

              val message = if (maxWait.toMillis > 0) {
                consumer.receive(maxWait.toMillis.toInt)
              } else {
                consumer.receiveNoWait()
              }

              if (message == null && running) {
                storeBuffer()
              } else {
                blockGenerator.waitToPush() //Use rate limiter
                buffer += (message)
              }
            }
          } catch {
            case e: Throwable =>
               logError(e.getLocalizedMessage, e)
               restart(e.getLocalizedMessage, e)
          }
        }

        def storeBuffer() = {
          if (buffer.nonEmpty) {
            store(buffer.flatMap(x => messageConverter(x)))
            buffer.last.acknowledge()
            buffer.clear()
          }
          stopWatch.reset()
          stopWatch.start()
        }
      }
    }
  }

  class GeneratedBlockHandler extends BlockGeneratorListener {
    override def onAddData(data: Any, metadata: Any): Unit = {}

    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: ArrayBuffer[_]): Unit = {}

    override def onError(message: String, throwable: Throwable): Unit = {}

    override def onGenerateBlock(blockId: StreamBlockId): Unit = {}
  }

}
package org.apache.spark.streaming.jms

import java.util.concurrent._

import com.google.common.base.Stopwatch
import com.solacesystems.jcsmp.BytesXMLMessage
import org.apache.spark.storage.{StorageLevel, StreamBlockId}
import org.apache.spark.streaming.receiver.BlockGeneratorListener

import scala.collection.mutable.ArrayBuffer
import scala.concurrent.duration.{Duration, _}

class SynchronousSolaceReceiver[T] (override val consumerFactory: MessageConsumerFactory,
                                    override val bytesMessageConverter: (BytesXMLMessage) => Option[T],
                                    val batchSize: Int = 10000,
                                    val maxWait: Duration = 1.second,
                                    val maxBatchSize: Duration = 1.seconds,
                                    override val storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2)
  extends BaseSolaceReceiver[T](consumerFactory, bytesMessageConverter, storageLevel) {

  override def onStart(): Unit = {
    //log.info("askr in onStart : ")
    running = true
    receiverThread = Some(Executors.newSingleThreadExecutor())

    //We are just using the blockGenerator for access to rate limiter
    blockGenerator = supervisor.createBlockGenerator(new GeneratedBlockHandler)
    receiverThread.get.execute {
      new Runnable {
        val buffer = ArrayBuffer[BytesXMLMessage]()
        // val stopWatch = new Stopwatch().start()
        val stopWatch = Stopwatch.createUnstarted().start()

        override def run(): Unit = {
          try {
            //val consumer = consumerFactory.newConsumer(Session.CLIENT_ACKNOWLEDGE)
            val consumer = consumerFactory.newSolaceConsumer()
            while(running) {
              if (buffer.size >= batchSize ||
                stopWatch.elapsed(TimeUnit.MILLISECONDS) >= maxBatchSize.toMillis) {
                //stopWatch.elapsedTime(TimeUnit.MILLISECONDS) >= maxBatchSize.toMillis) {
                storeBuffer()
              }

              val message = if (maxWait.toMillis > 0) {
                consumer.receive(maxWait.toMillis.toInt)
              } else {
                consumer.receiveNoWait()
              }

              if (message == null && running) {
                storeBuffer()
              } else {
                blockGenerator.waitToPush() //Use rate limiter
                buffer += (message)
              }
            }
          } catch {
            case e: Throwable =>
              logError(e.getLocalizedMessage, e)
              restart(e.getLocalizedMessage, e)
          }
        }

        def storeBuffer() = {
          if (buffer.nonEmpty) {
            store(buffer.flatMap(x => bytesMessageConverter(x)))
            buffer.last.ackMessage()
            buffer.clear()
          }
          stopWatch.reset()
          stopWatch.start()
        }
      }
    }
  }

  class GeneratedBlockHandler extends BlockGeneratorListener {
    override def onAddData(data: Any, metadata: Any): Unit = {}

    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: ArrayBuffer[_]): Unit = {}

    override def onError(message: String, throwable: Throwable): Unit = {}

    override def onGenerateBlock(blockId: StreamBlockId): Unit = {}
  }

}
cat: ./src/test: Is a directory
cat: ./src/test/java: Is a directory
cat: ./src/test/java/com: Is a directory
cat: ./src/test/java/com/hsbc: Is a directory
cat: ./src/test/java/com/hsbc/trds: Is a directory
cat: ./src/test/java/com/hsbc/trds/beam: Is a directory
cat: ./src/test/java/com/hsbc/trds/beam/pipeline: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import java.net.URISyntaxException;
import java.nio.file.Path;
import java.nio.file.Paths;

import org.junit.Before;
import org.junit.Test;

public class FileBeamTest {

  private FileBeam fileBeam;

  @Before
  public void setUp() {
    fileBeam = new FileBeam();
  }

  @Test
  public void run_() throws URISyntaxException {
    Path path = Paths.get(Thread.currentThread().getContextClassLoader().getResource("files/input.txt").toURI());
    fileBeam.runFilePipeline("file://" + path.toFile().getAbsolutePath());
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import java.net.URISyntaxException;
import java.nio.file.Path;
import java.nio.file.Paths;

import org.junit.Before;
import org.junit.Test;

public class GcsBeamTest {

  private GcsBeam gcsBeam;

  @Before
  public void setUp() {
    gcsBeam = new GcsBeam();
  }

  @Test
  public void run_givenTextFile_expectWordCountLoadedToGcs() throws URISyntaxException {
    Path path = Paths.get(Thread.currentThread().getContextClassLoader().getResource("files/input.txt").toURI());

    gcsBeam.runGcsPipeline("file://" + path.toFile().getAbsolutePath());

  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import java.net.URISyntaxException;
import java.nio.file.Path;
import java.nio.file.Paths;

import org.junit.Before;
import org.junit.Test;

public class PubSubBeamTest {

  private PubSubBeam pubSubBeam;

  @Before
  public void setUp() {
    pubSubBeam = new PubSubBeam();
  }

  @Test
  public void run_givenTextFile_expectWordCountSentToPubSub() throws URISyntaxException {
    Path path = Paths.get(Thread.currentThread().getContextClassLoader().getResource("files/input.txt").toURI());
    pubSubBeam.runPubSubPipeline("file://" + path.toFile().getAbsolutePath());
  }
}
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2021. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.beam.pipeline;

import org.junit.Before;
import org.junit.Test;

public class XmlExtractToGcsBeamTest {

  private XmlExtractToGcsBeam xmlExtractToGcsBeam;

  @Before
  public void setUp() {
    xmlExtractToGcsBeam = new XmlExtractToGcsBeam();
  }

  @Test
  public void run_givenSolaceSubscription_expectDataSentToGcs() {
    xmlExtractToGcsBeam.runDataLakeBeam(new String[]{});
  }
}
cat: ./src/test/java/com/hsbc/trds/dao: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.dao;

import java.util.Arrays;
import java.util.Date;
import java.util.UUID;

import com.hsbc.trds.scala.driver.CftcTrade;
import org.junit.Before;
import org.junit.Test;

import com.hsbc.trds.es.model.Trade;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class ElasticServiceTest {

  private ElasticService elasticService;

  @Before
  public void setUp() {
    elasticService = new ElasticService();
  }

  @Test
  public void ingest_givenOneRecord_shouldIngestSuccessfully() {
    elasticService.ingest(Arrays.asList(cftcTrade()));
  }

  private Trade trade() {
    return Trade.builder()
        .clearingStatus("clearing")
        .creationTimestamp(new Date())
        .executionDateTime(new Date())
        .executionVenueType("exec type 1")
        .party1Lei("L1")
        .party2Lei("L2")
        .primaryAssetClass("P class")
        .productId("P001")
        .tradeId("A123456")
        .transactionType("TYPE 1")
        .utiValue(UUID.randomUUID().toString())
        .build();
  }

  private CftcTrade cftcTrade() {
    return new CftcTrade("APO2017112300000000000TH14f1acae","TH14f1acae","OffFacility",
            "Uncleared","Commodity:Energy:Coal:Option:Cash",new Date(),new Date(),
            "Commodity","MP6I5ZYZBEU3UXPYFY54","7H6GLXDRUGQFU57RNE97","Trade");
  }
}
cat: ./src/test/java/com/hsbc/trds/generator: Is a directory
/*
 * COPYRIGHT. HSBC HOLDINGS PLC 2020. ALL RIGHTS RESERVED.
 *
 * This software is only to be used for the purpose for which it has been
 * provided. No part of it is to be reproduced, disassembled, transmitted,
 * stored in a retrieval system nor translated in any human or computer
 * language in any way or for any other purposes whatsoever without the prior
 * written consent of HSBC Holdings plc.
 */

package com.hsbc.trds.generator;

import java.io.File;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;

import org.junit.Before;
import org.junit.Test;

public class DataGeneratorTest {

  private DataGenerator dataGenerator;
  private String fileGenerationTime;
  private String userDirectory;

  @Before
  public void setUp() {
    dataGenerator = new DataGenerator();
    userDirectory = System.getProperty("user.dir") + File.separator + "target" + File.separator;
    if (fileGenerationTime == null) {
      fileGenerationTime = DateTimeFormatter.ISO_DATE.format(LocalDate.now());
    }
  }

  @Test
  public void generate_100MillionRecords_1() {
    long thousand = 100L;
    int repeat = 1000;
    dataGenerator.generate(thousand, userDirectory, true, repeat, "100_01");
    System.out.println("100 million records generated");
  }
}
cat: ./src/test/java/com/hsbc/trds/solace: Is a directory
package com.hsbc.trds.solace;

import org.junit.Test;

import static org.junit.Assert.*;

public class QueueConsumerTest {

    @Test
    public void run1() throws Exception {
        String args[] = new String[2];
        new QueueConsumer().run1();
    }
}cat: ./src/test/resources: Is a directory
<?xml version="1.0"?>
<valuationReport xmlns="http://www.fpml.org/FpML-5/recordkeeping" fpmlVersion="5-5"
                 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                 xsi:schemaLocation="http://www.fpml.org/FpML-5/recordkeeping ">
    <header>
        <messageId messageIdScheme="http://www.sef.com/msg_id">
            6485170999_0x000001752025345B80A4D46A712B3DA04C9EBE8AD72358F3C5D3CDC8756A1D08
        </messageId>
        <sentBy messageAddressScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</sentBy>
        <sendTo>DTCCUS</sendTo>
        <creationTimestamp>2020-10-13T04:09:29Z</creationTimestamp>
    </header>
    <onBehalfOf>
        <partyReference href="party1"/>
    </onBehalfOf>
    <reportContents>
        <primaryAssetClass>ForeignExchange</primaryAssetClass>
    </reportContents>
    <party id="party1">
        <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</partyId>
        <classification
                industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">
            FinancialEntity
        </classification>
        <country countryScheme="http://www.fpml.org/ext/iso3166">NonUSA</country>
        <organizationType>SD</organizationType>
    </party>
    <party id="party2">
        <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">E57ODZWZ7FF32TWEFA76</partyId>
        <classification
                industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">
            FinancialEntity
        </classification>
        <country>USA</country>
        <organizationType>SD</organizationType>
    </party>
    <tradeValuationItem>
        <trade>
            <tradeHeader>
                <partyTradeIdentifier>
                    <issuer issuerIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">1030209445</issuer>
                    <tradeId tradeIdScheme="http://www.fpml.org/coding-scheme/external/unique-transaction-identifier">
                        RODS20190110000000006485170999
                    </tradeId>
                </partyTradeIdentifier>
                <partyTradeIdentifier>
                    <partyReference href="party1"/>
                    <tradeId tradeIdScheme="http://www.dtcc.com/internal_Referenceid">6485170999</tradeId>
                </partyTradeIdentifier>
                <partyTradeInformation>
                    <partyReference href="party1"/>
                    <reportingRegime>
                        <reportingRole>ReportingParty</reportingRole>
                        <reportingPurpose>Valuation</reportingPurpose>
                    </reportingRegime>
                </partyTradeInformation>
            </tradeHeader>
        </trade>
        <valuationSet>
            <assetValuation>
                <quote>
                    <value>100</value>
                    <measureType>MarkToMarket</measureType>
                    <currency>USD</currency>
                    <pricingModel>MarkToModel</pricingModel>
                    <time>2020-10-13T03:12:42Z</time>
                    <valuationDate>2020-10-13Z</valuationDate>
                </quote>
            </assetValuation>
        </valuationSet>
    </tradeValuationItem>
</valuationReport>
cat: ./src/test/resources/apo: Is a directory
cat: ./src/test/resources/apo/TR_CFTC_SNAPSHOT: Is a directory
<nonpublicExecutionReport xmlns="http://www.fpml.org/FpML-5/recordkeeping" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" fpmlVersion="5-5" xsi:schemaLocation="http://www.fpml.org/FpML-5/recordkeeping ../fpml-main-5-5.xsd http://www.w3.org/2000/09/xmldsig# xmldsig-core-schema.xsd">
         <header>
            <messageId messageIdScheme="http://www.sef.com/msg_id">TH14f1acae_0x000001753A4F9E1580A4D46B531722B52A2C987FB1ECE2E4C26F0788D009DBC1</messageId>
            <sentBy messageAddressScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</sentBy>
            <sendTo>DTCCUS</sendTo>
            <creationTimestamp>2020-10-18T06:05:30Z</creationTimestamp>
         </header>
         <isCorrection>false</isCorrection>
         <onBehalfOf>
            <partyReference href="party1"/>
         </onBehalfOf>
         <asOfDate>2020-10-18</asOfDate>
         <asOfTime>06:05:30</asOfTime>
         <originatingEvent>Trade</originatingEvent>
         <trade>
            <tradeHeader>
               <partyTradeIdentifier>
                  <issuer issuerIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">1030209445</issuer>
                  <tradeId tradeIdScheme="http://www.fpml.org/coding-scheme/external/unique-transaction-identifier">APO2017112300000000000TH14f1acae</tradeId>
               </partyTradeIdentifier>
               <partyTradeIdentifier>
                  <partyReference href="party1"/>
                  <tradeId tradeIdScheme="http://www.dtcc.com/internal_Referenceid">TH14f1acae</tradeId>
               </partyTradeIdentifier>
               <partyTradeInformation>
                  <partyReference href="party1"/>
                  <relatedParty>
                     <partyReference href="party1"/>
                     <role>ReportingParty</role>
                  </relatedParty>
                  <relatedParty>
                     <partyReference href="party2"/>
                     <role>Counterparty</role>
                  </relatedParty>
                  <executionDateTime>2017-11-23T11:50:36Z</executionDateTime>
                  <intentToClear>false</intentToClear>
                  <clearingStatus>Uncleared</clearingStatus>
                  <collateralizationType collateralTypeScheme="http://www.fpml.org/coding-scheme/collateral-type">Partially</collateralizationType>
                  <reportingRegime>
                     <name>CFTC</name>
                     <supervisorRegistration>
                        <supervisoryBody>CFTC</supervisoryBody>
                     </supervisorRegistration>
                     <reportingRole>ReportingParty</reportingRole>
                     <reportingPurpose>Snapshot</reportingPurpose>
                  </reportingRegime>
                  <reportingRegime>
                     <reportingRole>ReportingParty</reportingRole>
                     <reportingPurpose>Snapshot</reportingPurpose>
                     <mandatorilyClearable>false</mandatorilyClearable>
                  </reportingRegime>
                  <nonStandardTerms>false</nonStandardTerms>
                  <offMarketPrice>false</offMarketPrice>
                  <largeSizeTrade>false</largeSizeTrade>
                  <executionVenueType>OffFacility</executionVenueType>
                  <verificationMethod>Unverified</verificationMethod>
                  <confirmationMethod>NonElectronic</confirmationMethod>
               </partyTradeInformation>
               <tradeDate>2017-11-23</tradeDate>
            </tradeHeader>
            <commodityOption id="_S1_">
               <primaryAssetClass>Commodity</primaryAssetClass>
               <productType>CommodityOption</productType>
               <productId productIdScheme="http://www.fpml.org/coding-scheme/product-taxonomy">Commodity:Energy:Coal:Option:Cash</productId>
               <buyerPartyReference href="party2"/>
               <sellerPartyReference href="party1"/>
               <optionType>Call</optionType>
               <commodity>
                  <instrumentId instrumentIdScheme="http://www.fpml.org/coding-scheme/commodity-reference-price-1-0.xml">COAL-API2-ARGUS COAL</instrumentId>
                  <commodityBase>Coal</commodityBase>
                  <unit>MT</unit>
                  <currency>USD</currency>
                  <specifiedPrice>MeanOfHighAndLow</specifiedPrice>
                  <deliveryDates>CalculationPeriod</deliveryDates>
               </commodity>
               <effectiveDate>
                  <adjustableDate>
                     <unadjustedDate>2017-12-01</unadjustedDate>
                     <dateAdjustments>
                        <businessDayConvention>NotApplicable</businessDayConvention>
                     </dateAdjustments>
                  </adjustableDate>
               </effectiveDate>
               <calculationPeriodsSchedule id="_S1_asianCalcPeriods">
                  <periodMultiplier>1</periodMultiplier>
                  <period>M</period>
                  <balanceOfFirstPeriod>false</balanceOfFirstPeriod>
               </calculationPeriodsSchedule>
               <pricingDates>
                  <calculationPeriodsScheduleReference href="_S1_asianCalcPeriods"/>
                  <dayType>CommodityBusiness</dayType>
                  <businessCalendar>ARGUS-MCCLOSKEYS-COAL-REPORT</businessCalendar>
               </pricingDates>
               <averagingMethod>Unweighted</averagingMethod>
               <notionalQuantity>
                  <quantityUnit>MT</quantityUnit>
                  <quantityFrequency>PerCalculationPeriod</quantityFrequency>
                  <quantity>1</quantity>
               </notionalQuantity>
               <totalNotionalQuantity>3</totalNotionalQuantity>
               <exercise>
                  <europeanExercise>
                     <expirationDate>
                        <adjustableDate>
                           <unadjustedDate>2018-02-28</unadjustedDate>
                           <dateAdjustments>
                              <businessDayConvention>NotApplicable</businessDayConvention>
                           </dateAdjustments>
                        </adjustableDate>
                     </expirationDate>
                     <expirationDate>
                        <adjustableDate>
                           <unadjustedDate>2018-01-31</unadjustedDate>
                           <dateAdjustments>
                              <businessDayConvention>NotApplicable</businessDayConvention>
                           </dateAdjustments>
                        </adjustableDate>
                     </expirationDate>
                     <expirationDate>
                        <adjustableDate>
                           <unadjustedDate>2017-12-31</unadjustedDate>
                           <dateAdjustments>
                              <businessDayConvention>NotApplicable</businessDayConvention>
                           </dateAdjustments>
                        </adjustableDate>
                     </expirationDate>
                     <exerciseFrequency>
                        <periodMultiplier>1</periodMultiplier>
                        <period>M</period>
                     </exerciseFrequency>
                  </europeanExercise>
                  <settlementCurrency>USD</settlementCurrency>
                  <relativePaymentDates>
                     <payRelativeTo>CalculationPeriodEndDate</payRelativeTo>
                     <calculationPeriodsReference href="_S1_asianCalcPeriods"/>
                     <paymentDaysOffset>
                        <periodMultiplier>5</periodMultiplier>
                        <period>D</period>
                        <dayType>Business</dayType>
                        <businessDayConvention>NONE</businessDayConvention>
                     </paymentDaysOffset>
                     <businessCenters>
                        <businessCenter>USNY</businessCenter>
                     </businessCenters>
                  </relativePaymentDates>
               </exercise>
               <strikePricePerUnit>
                  <currency>USD</currency>
                  <amount>3.6</amount>
               </strikePricePerUnit>
               <premium>
                  <paymentDate>
                     <adjustableDate>
                        <unadjustedDate>2017-11-27</unadjustedDate>
                        <dateAdjustments>
                           <businessDayConvention>NotApplicable</businessDayConvention>
                        </dateAdjustments>
                     </adjustableDate>
                  </paymentDate>
                  <paymentAmount>
                     <currency>USD</currency>
                     <amount>21</amount>
                  </paymentAmount>
                  <premiumPerUnit>
                     <currency>USD</currency>
                     <amount>7</amount>
                  </premiumPerUnit>
               </premium>
               <rounding>
                  <roundingDirection>Nearest</roundingDirection>
                  <precision>2</precision>
               </rounding>
            </commodityOption>
            <documentation>
               <masterAgreement>
                  <masterAgreementType>ISDA</masterAgreementType>
                  <masterAgreementVersion>1992</masterAgreementVersion>
               </masterAgreement>
            </documentation>
         </trade>
         <tradingEvent>
            <eventType>Modify</eventType>
         </tradingEvent>
         <quote>
            <value>0.07760000</value>
            <measureType>PriceNotation</measureType>
            <quoteUnits>Percentage</quoteUnits>
         </quote>
         <party id="party1">
            <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</partyId>
            <classification industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">FinancialEntity</classification>
            <country countryScheme="http://www.fpml.org/ext/iso3166">NonUSA</country>
            <organizationType>SD</organizationType>
         </party>
         <party id="party2">
            <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">7H6GLXDRUGQFU57RNE97</partyId>
            <classification industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">FinancialEntity</classification>
            <country>USA</country>
            <organizationType>SD</organizationType>
         </party>
      </nonpublicExecutionReport>
<!--@DSL
TREventReason=TRADE
IsStandardizedTrade=False
TRDecompLevel=Trade
TR_EVENT_TYPE=Trade:Amendment:Economic
TRProductType=Energy:Coal:Option:Cash
DSL@--><nonpublicExecutionReport xmlns="http://www.fpml.org/FpML-5/recordkeeping" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" fpmlVersion="5-5" xsi:schemaLocation="http://www.fpml.org/FpML-5/recordkeeping ../fpml-main-5-5.xsd http://www.w3.org/2000/09/xmldsig# xmldsig-core-schema.xsd">
         <header>
            <messageId messageIdScheme="http://www.sef.com/msg_id">TH2db83f3d_0x000001754D493F1B80A4D46C2B0E6FBCCEBD08FDCFED599EC1F5DC231FB6A830</messageId>
            <sentBy messageAddressScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</sentBy>
            <sendTo>DTCCUS</sendTo>
            <creationTimestamp>2020-10-21T22:31:30Z</creationTimestamp>
         </header>
         <isCorrection>false</isCorrection>
         <onBehalfOf>
            <partyReference href="party1"/>
         </onBehalfOf>
         <asOfDate>2020-10-21</asOfDate>
         <asOfTime>22:31:30</asOfTime>
         <originatingEvent>Trade</originatingEvent>
         <trade>
            <tradeHeader>
               <partyTradeIdentifier>
                  <issuer issuerIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">1030209445</issuer>
                  <tradeId tradeIdScheme="http://www.fpml.org/coding-scheme/external/unique-transaction-identifier">APO2019022000000000000TH2db83f3d</tradeId>
               </partyTradeIdentifier>
               <partyTradeIdentifier>
                  <partyReference href="party1"/>
                  <tradeId tradeIdScheme="http://www.dtcc.com/internal_Referenceid">TH2db83f3d</tradeId>
               </partyTradeIdentifier>
               <partyTradeInformation>
                  <partyReference href="party1"/>
                  <relatedParty>
                     <partyReference href="party1"/>
                     <role>ReportingParty</role>
                  </relatedParty>
                  <relatedParty>
                     <partyReference href="party2"/>
                     <role>Counterparty</role>
                  </relatedParty>
                  <executionDateTime>2019-02-20T06:29:08Z</executionDateTime>
                  <intentToClear>false</intentToClear>
                  <clearingStatus>Uncleared</clearingStatus>
                  <collateralizationType collateralTypeScheme="http://www.fpml.org/coding-scheme/collateral-type">Partially</collateralizationType>
                  <reportingRegime>
                     <name>CFTC</name>
                     <supervisorRegistration>
                        <supervisoryBody>CFTC</supervisoryBody>
                     </supervisorRegistration>
                     <reportingRole>ReportingParty</reportingRole>
                     <reportingPurpose>Snapshot</reportingPurpose>
                  </reportingRegime>
                  <reportingRegime>
                     <reportingRole>ReportingParty</reportingRole>
                     <reportingPurpose>Snapshot</reportingPurpose>
                     <mandatorilyClearable>false</mandatorilyClearable>
                  </reportingRegime>
                  <nonStandardTerms>false</nonStandardTerms>
                  <executionVenueType>OffFacility</executionVenueType>
                  <verificationMethod>Unverified</verificationMethod>
                  <confirmationMethod>Electronic</confirmationMethod>
               </partyTradeInformation>
               <tradeDate>2019-02-20</tradeDate>
            </tradeHeader>
            <commodityForward>
               <primaryAssetClass>Commodity</primaryAssetClass>
               <productId productIdScheme="http://www.fpml.org/coding-scheme/external/unique-product-identifier">Commodity:Metals:Precious:SpotFwd:Physical</productId>
               <valueDate>
                  <adjustableDate>
                     <unadjustedDate>2019-08-22</unadjustedDate>
                  </adjustableDate>
               </valueDate>
               <fixedLeg>
                  <payerPartyReference href="party1"/>
                  <receiverPartyReference href="party2"/>
                  <fixedPrice>
                     <price>1297.25092865</price>
                     <priceCurrency>USD</priceCurrency>
                     <priceUnit>Price</priceUnit>
                  </fixedPrice>
                  <totalPrice>
                     <currency>USD</currency>
                     <amount>66159797.36115</amount>
                  </totalPrice>
                  <paymentDates>
                     <adjustableDates>
                        <adjustedDate>2019-08-22</adjustedDate>
                     </adjustableDates>
                  </paymentDates>
               </fixedLeg>
               <bullionPhysicalLeg>
                  <payerPartyReference href="party2"/>
                  <receiverPartyReference href="party1"/>
                  <bullionType>Gold</bullionType>
                  <deliveryLocation>LONDON</deliveryLocation>
                  <physicalQuantity>
                     <quantityUnit>ozt</quantityUnit>
                     <quantityFrequency>Term</quantityFrequency>
                     <quantity>51000</quantity>
                  </physicalQuantity>
                  <totalPhysicalQuantity>
                     <quantityUnit>ozt</quantityUnit>
                     <quantity>51000</quantity>
                  </totalPhysicalQuantity>
                  <settlementDate>
                     <adjustableDate>
                        <unadjustedDate>2019-08-22</unadjustedDate>
                     </adjustableDate>
                  </settlementDate>
               </bullionPhysicalLeg>
            </commodityForward>
         </trade>
         <tradingEvent>
            <eventType>Amendment</eventType>
         </tradingEvent>
         <quote>
            <value>0.07760000</value>
            <measureType>PriceNotation</measureType>
            <quoteUnits>Percentage</quoteUnits>
         </quote>
         <party id="party1">
            <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">MP6I5ZYZBEU3UXPYFY54</partyId>
            <classification industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">FinancialEntity</classification>
            <country countryScheme="http://www.fpml.org/ext/iso3166">NonUSA</country>
            <organizationType>SD</organizationType>
         </party>
         <party id="party2">
            <partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso17442">VB7RXNZGO6KVDABYB880</partyId>
            <classification industryClassificationScheme="http://www.dtcc.org/coding-scheme/external/cftc-industrial-classification">FinancialEntity</classification>
            <country>USA</country>
            <organizationType>non-SD/MSP</organizationType>
         </party>
      </nonpublicExecutionReport>
<!--@DSL
TREventReason=TRADE
IsStandardizedTrade=False
TRDecompLevel=Trade
TR_EVENT_TYPE=Trade:Amendment
TRProductType=Metals:Precious:SpotFwd:Cash
DSL@--><?xml version="1.0"?>
<ROWSET>
    <ROW>
        <year>2012<!--A comment within tags--></year>
        <make>Tesla</make>
        <model>S</model>
        <comment>No comment</comment>
    </ROW>
    <ROW>
        <year>1997</year><!--A comment within elements-->
        <make>Ford</make>
        <model>E350</model>
        <comment><!--A comment before a value-->Go get one now they are going fast</comment>
    </ROW>
    <ROW>
        <year>2015</year>
        <make>Chevy</make>
        <model>Volt</model>
        <comment>No</comment>
    </ROW>
</ROWSET>clearing-statuses=Cleared,Uncleared
asset-classes=Equity,Interest Rates,Credit,Commodities,FX
transaction-types=Trade,Amendment,Novation,Termination,Increase,Novation-Trade,Exercise,CorporateAction,Exit,Historical,HistoricalExpired,Backload,Verified,Disputed
submission-reports=RT,PET,Confirm,Snapshot,RT-PET,PET-Confirm,RT-PET-Confirm,Valuation,Verification,Document
execution-venue-types=SEF,DCM,OffFacility,LEI
dsl-object-types=RECORDKEEPING_TRADEVIEW,PET,TRANSPARENCY,RAW,RECORDKEEPING,SNAPSHOT,SNAPSHOT_SHORT,REALTIME,RESPONSE_PET,RAW_VALUATION,VALUATION,RESPONSE_REALTIME,REALTIME_PET,RAW_SNAPSHOT,CONFIRM,RESPONSE_REALTIME_PET,DOCUMENT,STATIC_USI,RESPONSE_CONFIRM,RESPONSE_DOCUMENT,STATIC_DATA,REQUEST_FILE,PET_HKTR,RESPONSE_FILE,RESPONSE_PET_HKTR,RESPONSE_SNAPSHOT_EMIR,SNAPSHOT_EMIR,RESPONSE_SNAPSHOT_EMIR_DEL_REP,SNAPSHOT_EMIR_DEL_REP,RESPONSE_SNAPSHOT_SHORT_EMIR,DEL_REP_FILE_METADATA,SNAPSHOT_SHORT_EMIR,RAW_UTI,RAW_CLIENT_VALIDATION,RESPONSE_SNAP_EMIR_DEL_REP_SHORT,SNAP_EMIR_DEL_REP_SHORT,SNAP_EMIR_DEL_REP_CPTY,RAW_CONFIRM,RESPONSE_VALUATION,RESPONSE_NOREF,RESPONSE_SNAPSHOT_SHORT,RESPONSE_SNAPSHOT,RESPONSE_SNAP_EMIR_DEL_REP_CPTY,UC_TRADE,FI_TRADE_POST,REALTIME_PET_CONFIRM,SPARC_ETD,SPARC_VALUATION,RESPONSE_REALTIME_PET_CONFIRM,RESPONSE_VALUATION_EMIR,RESPONSE_VALUATION_EMIR_DEL_REP,CONTROL_META,VALUATION_EMIR_DEL_REP,RAW_COLLATERAL,VALUATION_EMIR,PET_CONFIRM,RAW_COLLATERAL_POSITION,RAW_LCH_CCP_TRADE,RAW_LCH_CCP_VALUATION,RESPONSE_PET_CONFIRM,RESPONSE_SPARC,RESPONSE_SPARC_EODMARKER,RESPONSE_OMRC_EODMARKER,RESPONSE_OMRC_VALUATION,RESPONSE_OMRC_TRADE,RESPONSE_OMRC_NOTIFICATION,RESPONSE_DTCC_VALUATION,FI_TRADE_BLOOMBERG,RAW_RESPONSE_BLOOMBERG,FI_TRADE_BLOOMBERG_US,UC_VALUATION,RESPONSE_HS_UC_TRADE,RESPONSE_HS_UC_VALUATION,RESPONSE_HS_EODMARKER,RESPONSE_HS_NOTIFICATION,FI_TRADE_RISK_DML,UC_POSITION,REALTIME_PET_MASASIC,PET_MASASIC,RESPONSE_SNAPSHOT_MASASIC,RESPONSE_PET_MASASIC,SNAPSHOT_MASASIC,RESPONSE_REALTIME_PET_MASASIC,VALUATION_MASASIC,RESPONSE_SNAP_SHORT_MASASIC,RESPONSE_VALUATION_MASASIC,SNAP_SHORT_MASASIC,RESPONSE_DOCUMENT_MASASIC,CONFIRM_MASASIC,RESPONSE_CONFIRM_MASASIC,DOCUMENT_MASASIC,SCP_UC_TRADE,SCP_UC_VALUATION,EXCEPTION_ERRORS,RESPONSE_SNAP_EMIR_DEL_REP_IND,RESPONSE_DTCC_COLLATERAL_LINK,RESPONSE_SPARC_UC_TRADE,RESPONSE_VAL_EMIR_DEL_REP_IND,VAL_EMIR_DEL_REP_IND,RESPONSE_HS_POSITION_NOTIFICATION,RESPONSE_HS_UC_POSITION,RESPONSE_FILE_MMSR_OIS,RESPONSE_TR_MMSR_UNSECURED,REQUEST_FILE_MMSR_OTCD,TR_MMSR_OIS,REQUEST_FILE_MMSR_UNSECURED,REQUEST_FILE_MMSR_OIS,RESPONSE_FILE_MMSR_OTCD,RESPONSE_FILE_MMSR_SECURED,TR_MMSR_OTCD,RESPONSE_TR_MMSR_OTCD,TR_MMSR_SECURED,RESPONSE_TR_MMSR_OIS,REQUEST_FILE_MMSR_SECURED,RESPONSE_FILE_MMSR_UNSECURED,TR_MMSR_UNSECURED,RESPONSE_TR_MMSR_SECURED,HS_UC_VALUATION,OMRC_VALUATION,UC_CONFIRM,FI_TRADE_RIMS,FXBSM_UC_TRADE,FI_RIMS_POSITION_MARKER,FI_RIMS_POSITION,SPARC_RIMS_SETTLE,RESPONSE_SPARC_RIMS_EODMARKER,SPARC_RIMS_EODMARKER,RESPONSE_SPARC_RIMS_TRADE,FI_TRADE_TREATS,SPARC_RIMS_TRADE,SPARC_RIMS_CORP,RESPONSE_SPARC_RIMS_CORP,FI_TRADE_RTCM,RESPONSE_SPARC_RIMS_SETTLE,FI_TRADE_MARTINI,FI_TRADE_MERIT,FI_TRADE_NCOMS,SPARC_BBG,SPARC_MARTINI,RESPONSE_SPARC_BBG,RESPONSE_SPARC_MARTINI,FI_TRADE_MARTINI_ACK,RESPONSE_SPARC_MARTINI_EODMARKER,SPARC_MARTINI_EODMARKER,FI_TRADE_TREATS_NOSTRO,FI_TRADE_RTCM_NOSTRO,SNAP_EMIR_DEL_REP_IND,UC_CASHFLOW,PMC_UC_TRADE,UC_CASHFLOW_PREDECOMPOSE,UC_SETTLEMENT_DATA,TR_BOE_SECURED,UC_NOTIFICATION,RAW_SETTLEMENT_DATA,UC_ID_COLLECTION_CONFIRM,UC_ID_COLLECTION_PTS,TR_BOE_UNSECURED,RB_UC_TRADE,RESPONSE_TR_BOE_UNSECURED,RESPONSE_TR_BOE_SECURED,RESPONSE_FILE_BOE_UNSECURED,REQUEST_FILE_BOE_UNSECURED,REQUEST_FILE_BOE_SECURED,RESPONSE_FILE_BOE_SECURED,MODSOUT_UC_TRADE,MODSOUT_UC_META,CTP_UC_TRADE,TR_STD_TRADE,UC_ID_COLLECTION_OPERATION,IMPACT_UC_TRADE_TP,IMPACT_UC_TRADE_NON_TP,TR_HKTR_VALUATION,RESPONSE_TR_HKTR_PET,TR_HKTR_TRADE_EQ_OTH,REQUEST_FILE_HKTR_PET,RESPONSE_FILE_HKTR_PET,TR_HKTR_TRADE_EQFXIR_STD,TR_STD_HKTR_TRADE,TR_STD_REEVALUATION_TRADE_FILE,TR_HKTR_TRADE_EQFXIR_OTH_WTHDRWL,TR_HKTR_TRADE_FX_OTH,TR_HKTR_TRADE_CDCM_STD,TR_STD_REVALUATION_TRADE,TR_HKTR_TRADE_CD_OTH,TR_HKTR_TRADE_IR_OTH,TR_HKTR_TRADE_CDCM_OTH_WTHDRWL,OTP_TRADE,FI_TRADE_MIFID,TR_HKTR_TRADE_CM_OTH,UC_CONFIRM_PRE_DECOMPOSE,TREATS_B2B_HBAPMTN_UC_TRADE,UC_DATA_VALIDATION,PUBSUB_JSON_TRADE,MoSS_HBEU_TRADE,VSS_UC_TRADE,VSX_HBEU_UC_TRADE,BOSS_REPO_UC_TRADE,SPARC_HBUK_REPO_UC_TRADE,XARA_HBUK_UC_TRADE,SPARC_HBUK_UC_TRADE,FI_TRADE_POST_HBUK,FI_TRADE_RISK_DML_HBUK_REPO,XCEPT_HBUK_UC_TRADE,TR_STD_CONFIRM,FI_TRADE_RIMS_HBUK,RISK_DML_VB_UC_TRADE,SPARC_HBUK_BOND_UC_TRADE,DTP_HBUK_UC_TRADE,DTP_UC_CONFIRM,RISK_DML_HBUK_UC_TRADE,SPARC_XFOS_EODMARKER,FI_CALL_ACC_DML,RESPONSE_TR_HKTR_VALUATION,TR_HKMA_STD_TRADE,RESPONSE_TR_MIFIDTXN_STD_TRADE,TR_MIFIDTXN_STD_TRADE,RAW_CONFIRM_DOCUMENT,RESPONSE_TR_MIFIDPTT_STD_TRADE,TR_MIFIDPTT_STD_TRADE,TR_STD_VALUATION,UC_CONFIRM_DOCUMENT,FI_EFTA_MAR_UC_TRADE,OMRC_TRADE,XARA_HBAP_UC_TRADE,SPARC,OMRC_NOTIFICATION,MI360_UC_TRADE,SPARC_UC_TRADE,OMRC_EODMARKER,XARA_HBEU_UC_TRADE,HS_NOTIFICATION,HS_UC_POSITION,SPARC_EODMARKER,RB_UC_POSITION,HS_UC_CONFIRM,HS_POSITION_NOTIFICATION,HS_EODMARKER,TR_EMIR_SNAPSHOT,TR_EMIR_VALUATION,TR_STD_COLL_VALUE,RESPONSE_TR_EMIR_VALUATION,TR_STD_COLL_LINK,RESPONSE_TR_EMIR_SNAPSHOT,TR_EMIR_COLL_VALUE,TR_EMIR_COLL_LINK,REQUEST_FILE_EMIR_COLL_VALUE,REQUEST_FILE_EMIR_COLL_LINK,RESPONSE_FILE_EMIR_COLL_VALUE,RESPONSE_TR_EMIR_COLL_VALUE,RESPONSE_TR_EMIR_COLL_LINK,RESPONSE_FILE_EMIR_COLL_LINK,RAW_COLLATERAL_MOVEMENT,COLLATERAL_UC_MOVEMENT,RAW_ICE_CCP_VALUATION,MIFID_FIRDS_TOTV_FILE,MIFID_FIRDS_TOTV,PUBSUB_TRADE,RAW_EUREX_CCP_VALUATION,RAW_COLLATERAL_VALUATION,COLLATERAL_UC_VALUATION,COLLATERAL_UC_POSITION,RESPONSE_ASIC_COLLATERAL_LINK,SPARC_HGHQ_EOD_MARKER,SPARC_HGHQ_BOND_UC_TRADE,TR_STD_COLLATERAL_POSITION,RAW_CME_CCP_VALUATION,RAW_LCH_CDSCLEAR_VALUATION,TradedRisk_UC_TRADE,SPARC_HGHQ_IRD_UC_TRADE,XFOS_XCEPTOR_UC_TRADE,RESPONSE_TR_SFF_STD_TRADE,TR_SFF_STD_TRADE,TR_SFTR_REPORT,TR_SFTR_VALUATION,TR_SFTR_COLLVAL,RAW_COLLATERAL_POSITION_FINISH,RAW_COLLATERAL_POSITION_START,RESPONSE_TR_SFTR_REPORT,RB_POSITION_NOTIFICATION,RESPONSE_TR_SFTR_VALUATION,TR_STD_UC_ID_COLLECTION,RESPONSE_TR_SFTR_COLLVAL,TR_SFTR_PREAG_COLLATERAL_REPORT,TR_SFTR_COLLATERAL_REPORT,TR_SFTR_COLLATERAL_POS_FINISH,RESPONSE_TR_SFTR_COLL_REP,RAW_POSITION,PSDW_MARGINLENDINGPOS_FILE,RESPONSE_TR_SFTR_MLENDCLPS,TR_SFTR_DEL_REPORT,TR_STD_ML_POSITION,RESPONSE_TR_SFTR_DEL_REPORT,TR_SFTR_DEL_VALUATION,TR_SFTR_MLENDCLPS,TR_SFTR_PREAG_MLENDCLPSPOS,TR_STD_ML_COLLATERAL,TR_SFTR_PREAG_MLENDCLPSCOLL,TR_SFTR_DEL_COLLVAL,RESPONSE_TR_SFTR_DEL_COLLVAL,PSDW_MARGINLENDINGCOLL_FILE,UC_ID_COLLECTION_NON_PTS,TR_SFTR_PREAG_MLENDCOLL,RESPONSE_TR_SFTR_MLENDCOLL,RESPONSE_TR_SFTR_MLEND,TR_SFTR_MLENDCOLL,TR_SFTR_MLEND,TR_SFTR_PREAG_MLEND,RESPONSE_TR_SFTR_DEL_VALUATION,TR_SFTR_COLLREUSE,RESPONSE_TR_SFTR_COLLREUSE,TR_SFTR_CCPMARGIN_LCH,RESPONSE_TR_SFTR_CCPMARGIN_LCH,TR_SFTR_PREAG_DEL_MLENDCLPSCOLL,TR_SFTR_PREAG_DEL_MLENDCLPSPOS,RESPONSE_TR_SFTR_DEL_MLENDCLPS,TR_SFTR_DEL_MLENDCLPS,TR_SFTR_CCPMARGIN_EUREX,RESPONSE_TR_SFTR_CCPMARGIN_EUREX,TR_CFTC_RT,TR_CSA_CFTC_SNAPSHOT,TR_CSA_CFTC_RT,TR_CSA_RT,TR_CSA_SNAPSHOT,TR_CFTC_SNAPSHOT,RESPONSE_TR_CFTC_RT,TR_CFTC_VALUATION,RESPONSE_TR_CSA_SNAPSHOT,RESPONSE_TR_CFTC_VALUATION,RESPONSE_TR_CFTC_SNAPSHOT,RESPONSE_TR_CSA_CFTC_SNAPSHOT,RESPONSE_TR_CSA_RT,TR_CFTC_STD_TRADE,TR_STD_CONFIRM_DOCUMENT,RESPONSE_TR_CFTC_DOCUMENT,TR_CFTC_DOCUMENT,TR_SFTR_PREAG_COLLATERAL_DEL_REPORT,RESPONSE_TR_SFTR_COLL_DEL_REP,TR_SFTR_COLLATERAL_DEL_REPORT,RESPONSE_TR_CSA_CFTC_RT,TR_SFTR_PREAG_DEL_MLEND,RESPONSE_TR_SFTR_DEL_MLENDCOLL,RESPONSE_TR_SFTR_DEL_MLEND,TR_SFTR_DEL_MLENDCOLL,TR_SFTR_DEL_MLEND,TR_SFTR_PREAG_DEL_MLENDCOLL,TR_STD_CCP_VALUATION,TR_STD_CCP_TRADE,MIFID_FIRDS_TOTV_FILE_UK,MIFID_FIRDS_TOTV_UK,TR_UKSFTR_COLLATERAL_POS_FINISH,RESPONSE_TR_UKSFTR_MLEND,TR_UKSFTR_PREAG_MLENDCLPSPOS,TR_UKSFTR_PREAG_MLENDCOLL,RESPONSE_TR_UKSFTR_MLENDCOLL,TR_UKSFTR_PREAG_COLLATERAL_REPORT,TR_UKSFTR_PREAG_MLENDCLPSCOLL,TR_UKSFTR_PREAG_MLEND,TR_UKSFTR_PREAG_COLLATERAL_DEL_REP,RESPONSE_VALUATION_FCA_DEL_REP,TR_UKSFTR_COLLATERAL_DEL_REPORT,TR_MIFIDTXN_STD_TRADE_EU,RESPONSE_TR_UKSFTR_DEL_COLLVAL,TR_UKSFTR_VALUATION,TR_UKSFTR_DEL_COLLVAL,RESPONSE_TR_UKSFTR_MLENDCLPS,RESPONSE_VAL_FCA_DEL_REP_IND,RESPONSE_TR_UKSFTR_COLL_REPORT,TR_MIFIDTXN_STD_TRADE_VENUE,TR_UKSFTR_MLEND,TR_UKSFTR_MLENDCLPS,TR_UKSFTR_MLENDCOLL,RESPONSE_TR_UKSFTR_COLL_DEL_REPORT,RSPN_TR_MIFIDTXN_STD_TRD_VENUE,RESPONSE_TR_UKSFTR_REPORT,TR_UKSFTR_COLLATERAL_REPORT,RESPONSE_TR_UKSFTR_CCPMARGIN_EUREX,RESPONSE_SNAPSHOT_FCA_DEL_REP,RSPN_TR_MIFIDTXN_STD_TRD_EU,TR_UKSFTR_CCPMARGIN_EUREX,TR_UKSFTR_COLLREUSE,RESPONSE_TR_UKSFTR_COLLREUSE,RESPONSE_TR_UKSFTR_COLLVAL,RESPONSE_TR_UKSFTR_CCPMARGIN_LCH,RESPONSE_TR_UKSFTR_VALUATION,RESPONSE_SNAP_FCA_DEL_REP_IND,RESPONSE_SNAPSHOT_FCA,TR_UKSFTR_CCPMARGIN_LCH,TR_UKSFTR_REPORT,RESPONSE_VALUATION_FCA,TR_UKSFTR_COLLVAL,VALUATION_FCA,SNAPSHOT_FCA_DEL_REP,VAL_FCA_DEL_REP_IND,VALUATION_FCA_DEL_REP,SNAPSHOT_FCA,SNAP_FCA_DEL_REP_IND,RESPONSE_TR_UKSFTR_DEL_REPORT,TR_UKSFTR_DEL_REPORT,RESPONSE_TR_UKSFTR_DEL_VALUATION,TR_UKSFTR_DEL_VALUATIONcat: ./src/test/resources/elasticsearch: Is a directory
elasticsearch.rest.uris=http://gbl17451.systems.uk.hsbc:9200
#elasticsearch.rest.uris=http://localhost:9200
elasticsearch.index.trds=test_trdscat: ./src/test/resources/files: Is a directory
* Y o u   h a v e   r e c e i v e d   t h e   s o f t w a r e   a s   p a r t   o f   t h e   V i s u a l   S t u d i o   9 . 0   B e t a   P r o g r a m .   
 
 T h e   t e r m s   a n d   c o n d i t i o n s   o f   t h e   V i s u a l   S t u d i o   9 . 0 ,   B e t a   1   l i c e n s e   a g r e e m e n t   
 
 a p p l i e s   t o   y o u r   u s e   o f   t h e   s o f t w a r e . * *   M I C R O S O F T   P R E - R E L E A S E   S O F T W A R E   L I C E N S E   
 
 T E R M S   M I C R O S O F T   V I S U A L   S T U D I O   9 . 0   P R O F E S S I O N A L ,   B E T A   1   T h e s e   l i c e n s e   t e r m s   a r e   
 
 a n   a g r e e m e n t   b e t w e e n   M i c r o s o f t   C o r p o r a t i o n   ( o r   b a s e d   o n   w h e r e   y o u   l i v e ,   o n e   o f   
 
 i t s   a f f i l i a t e s )   a n d   y o u .   P l e a s e   r e a d   t h e m .   T h e y   a p p l y   t o   t h e   p r e - r e l e a s e   
 
 s o f t w a r e   n a m e d   a b o v e ,   w h i c h   i n c l u d e s   t h e   m e d i a   o n   w h i c h   y o u   r e c e i v e d   i t ,   i f   a n y .   
 
 T h e   t e r m s   a l s o   a p p l y   t o   a n y   M i c r o s o f t   "   u p d a t e s ,   "   s u p p l e m e n t s ,   "   I n t e r n e t - b a s e d   
 
 s e r v i c e s ,   a n d   "   s u p p o r t   s e r v i c e s   f o r   t h i s   s o f t w a r e ,   u n l e s s   o t h e r   t e r m s   a c c o m p a n y   
 
 t h o s e   i t e m s .   I f   s o ,   t h o s e   t e r m s   a p p l y .   B Y   U S I N G   T H E   S O F T W A R E ,   Y O U   A C C E P T   T H E S E   
 
 T E R M S .   I F   Y O U   D O   N O T   A C C E P T   T H E M ,   D O   N O T   U S E   T H E   S O F T W A R E .   I f   y o u   c o m p l y   w i t h   
 
 t h e s e   l i c e n s e   t e r m s ,   y o u   h a v e   t h e   r i g h t s   b e l o w .   1 .   I N S T A L L A T I O N   A N D   U S E   R I G H T S .   
 
 "   G e n e r a l .   Y o u   m a y   i n s t a l l   a n d   u s e   o n e   c o p y   o f   t h e   s o f t w a r e   o n   y o u r   p r e m i s e s   t o   
 
 d e s i g n ,   d e v e l o p   a n d   t e s t   y o u r   p r o g r a m s   f o r   u s e   w i t h   t h e   s o f t w a r e .   "   E v a l u a t i o n   
 
 O n l y .   T h e   s o f t w a r e   i s   f o r   e v a l u a t i o n   p u r p o s e s   o n l y .   Y o u   m a y   n o t   d i s t r i b u t e   a n y   
 
 p r o g r a m   y o u   d e v e l o p   w i t h   t h e   s o f t w a r e .   "   I n c l u d e d   M i c r o s o f t   P r o g r a m s .   T h e s e   
 
 l i c e n s e   t e r m s   a p p l y   t o   a l l   M i c r o s o f t   p r o g r a m s   i n c l u d e d   w i t h   t h e   s o f t w a r e .   "   
 
 T h i r d   P a r t y   P r o g r a m s .   T h e   s o f t w a r e   c o n t a i n s   t h i r d   p a r t y   p r o g r a m s .   T h e   l i c e n s e   
 
 t e r m s   w i t h   t h o s e   p r o g r a m s   a p p l y   t o   y o u r   u s e   o f   t h e m .   2 .   I N T E R N E T - B A S E D   S E R V I C E S .   
 
 M i c r o s o f t   p r o v i d e s   I n t e r n e t - b a s e d   s e r v i c e s   w i t h   t h e   s o f t w a r e .   I t   m a y   c h a n g e   o r   
 
 c a n c e l   t h e m   a t   a n y   t i m e .   3 .   T I M E - S E N S I T I V E   S O F T W A R E .   T h e   s o f t w a r e   w i l l   s t o p   
 
 r u n n i n g   o n   1 5   M a r c h   2 0 0 8 .   Y o u   w i l l   n o t   r e c e i v e   a n y   o t h e r   n o t i c e .   Y o u   m a y   n o t   b e   
 
 a b l e   t o   a c c e s s   d a t a   u s e d   w i t h   t h e   s o f t w a r e   w h e n   i t   s t o p s   r u n n i n g .   4 .   P R E - R E L E A S E   
 
 S O F T W A R E .   T h i s   s o f t w a r e   i s   a   p r e - r e l e a s e   v e r s i o n .   I t   m a y   n o t   w o r k   t h e   w a y   a   
 
 f i n a l   v e r s i o n   o f   t h e   s o f t w a r e   w i l l .   W e   m a y   c h a n g e   i t   f o r   t h e   f i n a l ,   c o m m e r c i a l   
 
 v e r s i o n .   W e   a l s o   m a y   n o t   r e l e a s e   a   c o m m e r c i a l   v e r s i o n .   5 .   F E E D B A C K .   I f   y o u   g i v e   
 
 f e e d b a c k   a b o u t   t h e   s o f t w a r e   t o   M i c r o s o f t ,   y o u   g i v e   t o   M i c r o s o f t ,   w i t h o u t   c h a r g e ,   
 
 t h e   r i g h t   t o   u s e ,   s h a r e   a n d   c o m m e r c i a l i z e   y o u r   f e e d b a c k   i n   a n y   w a y   a n d   f o r   a n y   
 
 p u r p o s e .   Y o u   a l s o   g i v e   t o   t h i r d   p a r t i e s ,   w i t h o u t   c h a r g e ,   a n y   p a t e n t   r i g h t s   
 
 n e e d e d   f o r   t h e i r   p r o d u c t s ,   t e c h n o l o g i e s   a n d   s e r v i c e s   t o   u s e   o r   i n t e r f a c e   w i t h   
 
 a n y   s p e c i f i c   p a r t s   o f   a   M i c r o s o f t   s o f t w a r e   o r   s e r v i c e   t h a t   i n c l u d e s   t h e   
 
 f e e d b a c k .   Y o u   w i l l   n o t   g i v e   f e e d b a c k   t h a t   i s   s u b j e c t   t o   a   l i c e n s e   t h a t   r e q u i r e s   
 
 M i c r o s o f t   t o   l i c e n s e   i t s   s o f t w a r e   o r   d o c u m e n t a t i o n   t o   t h i r d   p a r t i e s   b e c a u s e   w e   
 
 i n c l u d e   y o u r   f e e d b a c k   i n   t h e m .   T h e s e   r i g h t s   s u r v i v e   t h i s   a g r e e m e n t .   6 .   S C O P E   O F   
 
 L I C E N S E .   T h e   s o f t w a r e   i s   l i c e n s e d ,   n o t   s o l d .   T h i s   a g r e e m e n t   o n l y   g i v e s   y o u   s o m e   
 
 r i g h t s   t o   u s e   t h e   s o f t w a r e .   M i c r o s o f t   r e s e r v e s   a l l   o t h e r   r i g h t s .   U n l e s s   
 
 a p p l i c a b l e   l a w   g i v e s   y o u   m o r e   r i g h t s   d e s p i t e   t h i s   l i m i t a t i o n ,   y o u   m a y   u s e   t h e   
 
 s o f t w a r e   o n l y   a s   e x p r e s s l y   p e r m i t t e d   i n   t h i s   a g r e e m e n t .   I n   d o i n g   s o ,   y o u   m u s t   
 
 c o m p l y   w i t h   a n y   t e c h n i c a l   l i m i t a t i o n s   i n   t h e   s o f t w a r e   t h a t   o n l y   a l l o w   y o u   t o   u s e   
 
 i t   i n   c e r t a i n   w a y s .   Y o u   m a y   n o t   "   d i s c l o s e   t h e   r e s u l t s   o f   a n y   b e n c h m a r k   t e s t s   o f   
 
 t h e   s o f t w a r e   t o   a n y   t h i r d   p a r t y   w i t h o u t   M i c r o s o f t  s   p r i o r   w r i t t e n   a p p r o v a l ;   "   
 
 w o r k   a r o u n d   a n y   t e c h n i c a l   l i m i t a t i o n s   i n   t h e   s o f t w a r e ;   "   r e v e r s e   e n g i n e e r ,   
 
 d e c o m p i l e   o r   d i s a s s e m b l e   t h e   s o f t w a r e ,   e x c e p t   a n d   o n l y   t o   t h e   e x t e n t   t h a t   
 
 a p p l i c a b l e   l a w   e x p r e s s l y   p e r m i t s ,   d e s p i t e   t h i s   l i m i t a t i o n ;   "   m a k e   m o r e   c o p i e s   o f   
 
 t h e   s o f t w a r e   t h a n   s p e c i f i e d   i n   t h i s   a g r e e m e n t   o r   a l l o w e d   b y   a p p l i c a b l e   l a w ,   
 
 d e s p i t e   t h i s   l i m i t a t i o n ;   "   p u b l i s h   t h e   s o f t w a r e   f o r   o t h e r s   t o   c o p y ;   "   r e n t ,   
 
 l e a s e   o r   l e n d   t h e   s o f t w a r e ;   "   t r a n s f e r   t h e   s o f t w a r e   o r   t h i s   a g r e e m e n t   t o   a n y   
 
 t h i r d   p a r t y ;   o r   "   u s e   t h e   s o f t w a r e   f o r   c o m m e r c i a l   s o f t w a r e   h o s t i n g   s e r v i c e s .   7 .   
 
 E X P O R T   R E S T R I C T I O N S .   T h e   s o f t w a r e   i s   s u b j e c t   t o   U n i t e d   S t a t e s   e x p o r t   l a w s   a n d   
 
 r e g u l a t i o n s .   Y o u   m u s t   c o m p l y   w i t h   a l l   d o m e s t i c   a n d   i n t e r n a t i o n a l   e x p o r t   l a w s   a n d   
 
 r e g u l a t i o n s   t h a t   a p p l y   t o   t h e   s o f t w a r e .   T h e s e   l a w s   i n c l u d e   r e s t r i c t i o n s   o n   
 
 d e s t i n a t i o n s ,   e n d   u s e r s   a n d   e n d   u s e .   F o r   a d d i t i o n a l   i n f o r m a t i o n ,   s e e   
 
 w w w . m i c r o s o f t . c o m / e x p o r t i n g .   8 .   S U P P O R T   S E R V I C E S .   B e c a u s e   t h i s   s o f t w a r e   i s    a s   
 
 i s ,    w e   m a y   n o t   p r o v i d e   s u p p o r t   s e r v i c e s   f o r   i t .   9 .   E N T I R E   A G R E E M E N T .   T h i s   
 
 a g r e e m e n t ,   a n d   t h e   t e r m s   f o r   s u p p l e m e n t s ,   u p d a t e s ,   I n t e r n e t - b a s e d   s e r v i c e s   a n d   
 
 s u p p o r t   s e r v i c e s   t h a t   y o u   u s e ,   a r e   t h e   e n t i r e   a g r e e m e n t   f o r   t h e   s o f t w a r e   a n d   
 
 s u p p o r t   s e r v i c e s .   1 0 .   A P P L I C A B L E   L A W .   a .   U n i t e d   S t a t e s .   I f   y o u   a c q u i r e d   t h e   
 
 s o f t w a r e   i n   t h e   U n i t e d   S t a t e s ,   W a s h i n g t o n   s t a t e   l a w   g o v e r n s   t h e   i n t e r p r e t a t i o n   
 
 o f   t h i s   a g r e e m e n t   a n d   a p p l i e s   t o   c l a i m s   f o r   b r e a c h   o f   i t ,   r e g a r d l e s s   o f   c o n f l i c t   
 
 o f   l a w s   p r i n c i p l e s .   T h e   l a w s   o f   t h e   s t a t e   w h e r e   y o u   l i v e   g o v e r n   a l l   o t h e r   
 
 c l a i m s ,   i n c l u d i n g   c l a i m s   u n d e r   s t a t e   c o n s u m e r   p r o t e c t i o n   l a w s ,   u n f a i r   
 
 c o m p e t i t i o n   l a w s ,   a n d   i n   t o r t .   b .   O u t s i d e   t h e   U n i t e d   S t a t e s .   I f   y o u   a c q u i r e d   t h e   
 
 s o f t w a r e   i n   a n y   o t h e r   c o u n t r y ,   t h e   l a w s   o f   t h a t   c o u n t r y   a p p l y .   1 1 .   L E G A L   E F F E C T .   
 
 T h i s   a g r e e m e n t   d e s c r i b e s   c e r t a i n   l e g a l   r i g h t s .   Y o u   m a y   h a v e   o t h e r   r i g h t s   u n d e r   
 
 t h e   l a w s   o f   y o u r   c o u n t r y .   Y o u   m a y   a l s o   h a v e   r i g h t s   w i t h   r e s p e c t   t o   t h e   p a r t y   
 
 f r o m   w h o m   y o u   a c q u i r e d   t h e   s o f t w a r e .   T h i s   a g r e e m e n t   d o e s   n o t   c h a n g e   y o u r   r i g h t s   
 
 u n d e r   t h e   l a w s   o f   y o u r   c o u n t r y   i f   t h e   l a w s   o f   y o u r   c o u n t r y   d o   n o t   p e r m i t   i t   t o   
 
 d o   s o .   1 2 .   D I S C L A I M E R   O F   W A R R A N T Y .   T H E   S O F T W A R E   I S   L I C E N S E D    A S - I S .    Y O U   B E A R   
 
 T H E   R I S K   O F   U S I N G   I T .   M I C R O S O F T   G I V E S   N O   E X P R E S S   W A R R A N T I E S ,   G U A R A N T E E S   O R   
 
 C O N D I T I O N S .   Y O U   M A Y   H A V E   A D D I T I O N A L   C O N S U M E R   R I G H T S   U N D E R   Y O U R   L O C A L   L A W S   W H I C H   
 
 T H I S   A G R E E M E N T   C A N N O T   C H A N G E .   T O   T H E   E X T E N T   P E R M I T T E D   U N D E R   Y O U R   L O C A L   L A W S ,   
 
 M I C R O S O F T   E X C L U D E S   T H E   I M P L I E D   W A R R A N T I E S   O F   M E R C H A N T A B I L I T Y ,   F I T N E S S   F O R   A   
 
 P A R T I C U L A R   P U R P O S E   A N D   N O N - I N F R I N G E M E N T .   1 3 .   L I M I T A T I O N   O N   A N D   E X C L U S I O N   O F   
 
 R E M E D I E S   A N D   D A M A G E S .   Y O U   C A N   R E C O V E R   F R O M   M I C R O S O F T   A N D   I T S   S U P P L I E R S   O N L Y   
 
 D I R E C T   D A M A G E S   U P   T O   U . S .   $ 5 . 0 0 .   Y O U   C A N N O T   R E C O V E R   A N Y   O T H E R   D A M A G E S ,   I N C L U D I N G   
 
 C O N S E Q U E N T I A L ,   L O S T   P R O F I T S ,   S P E C I A L ,   I N D I R E C T   O R   I N C I D E N T A L   D A M A G E S .   T h i s   
 
 l i m i t a t i o n   a p p l i e s   t o   "   a n y t h i n g   r e l a t e d   t o   t h e   s o f t w a r e ,   s e r v i c e s ,   c o n t e n t   
 
 ( i n c l u d i n g   c o d e )   o n   t h i r d   p a r t y   I n t e r n e t   s i t e s ,   o r   t h i r d   p a r t y   p r o g r a m s ;   a n d   "   
 
 c l a i m s   f o r   b r e a c h   o f   c o n t r a c t ,   b r e a c h   o f   w a r r a n t y ,   g u a r a n t e e   o r   c o n d i t i o n ,   
 
 s t r i c t   l i a b i l i t y ,   n e g l i g e n c e ,   o r   o t h e r   t o r t   t o   t h e   e x t e n t   p e r m i t t e d   b y   
 
 a p p l i c a b l e   l a w .   I t   a l s o   a p p l i e s   e v e n   i f   M i c r o s o f t   k n e w   o r   s h o u l d   h a v e   k n o w n   
 
 a b o u t   t h e   p o s s i b i l i t y   o f   t h e   d a m a g e s .   T h e   a b o v e   l i m i t a t i o n   o r   e x c l u s i o n   m a y   n o t   
 
 a p p l y   t o   y o u   b e c a u s e   y o u r   c o u n t r y   m a y   n o t   a l l o w   t h e   e x c l u s i o n   o r   l i m i t a t i o n   o f   
 
 i n c i d e n t a l ,   c o n s e q u e n t i a l   o r   o t h e r   d a m a g e s .   P l e a s e   n o t e :   A s   t h i s   s o f t w a r e   i s   
 
 d i s t r i b u t e d   i n   Q u e b e c ,   C a n a d a ,   s o m e   o f   t h e   c l a u s e s   i n   t h i s   a g r e e m e n t   a r e   
 
 p r o v i d e d   b e l o w   i n   F r e n c h .   R e m a r q u e   :   C e   l o g i c i e l    a n t   d i s t r i b u  a u   Q u  e c ,   
 
 C a n a d a ,   c e r t a i n e s   d e s   c l a u s e s   d a n s   c e   c o n t r a t   s o n t   f o u r n i e s   c i - d e s s o u s   e n   
 
 f r a n  i s .   E X O N R A T I O N   D E   G A R A N T I E .   L e   l o g i c i e l   v i s  p a r   u n e   l i c e n c e   e s t   o f f e r t     
 
 t e l   q u e l   .   T o u t e   u t i l i s a t i o n   d e   c e   l o g i c i e l   e s t    v o t r e   s e u l e   r i s q u e   e t   p  i l .   
 
 M i c r o s o f t   n  a c c o r d e   a u c u n e   a u t r e   g a r a n t i e   e x p r e s s e .   V o u s   p o u v e z   b   i c i e r   d e   
 
 d r o i t s   a d d i t i o n n e l s   e n   v e r t u   d u   d r o i t   l o c a l   s u r   l a   p r o t e c t i o n   d e s   c o n s o m m a t e u r s ,   
 
 q u e   c e   c o n t r a t   n e   p e u t   m o d i f i e r .   L a   o u   e l l e s   s o n t   p e r m i s e s   p a r   l e   d r o i t   l o c a l e ,   
 
 l e s   g a r a n t i e s   i m p l i c i t e s   d e   q u a l i t  m a r c h a n d e ,   d  a d  u a t i o n    u n   u s a g e   
 
 p a r t i c u l i e r   e t   d  a b s e n c e   d e   c o n t r e f a  n   s o n t   e x c l u e s .   L I M I T A T I O N   D E S   
 
 D O M M A G E S - I N T R T S   E T   E X C L U S I O N   D E   R E S P O N S A B I L I T   P O U R   L E S   D O M M A G E S .   V o u s   p o u v e z   
 
 o b t e n i r   d e   M i c r o s o f t   e t   d e   s e s   f o u r n i s s e u r s   u n e   i n d e m n i s a t i o n   e n   c a s   d e   d o m m a g e s   
 
 d i r e c t s   u n i q u e m e n t    h a u t e u r   d e   5 , 0 0   $   U S .   V o u s   n e   p o u v e z   p r  e n d r e    a u c u n e   
 
 i n d e m n i s a t i o n   p o u r   l e s   a u t r e s   d o m m a g e s ,   y   c o m p r i s   l e s   d o m m a g e s   s p  i a u x ,   
 
 i n d i r e c t s   o u   a c c e s s o i r e s   e t   p e r t e s   d e   b   i c e s .   C e t t e   l i m i t a t i o n   c o n c e r n e   :   "   
 
 t o u t   c e   q u i   e s t   r e l i  a u   l o g i c i e l ,   a u x   s e r v i c e s   o u   a u   c o n t e n u   ( y   c o m p r i s   l e   
 
 c o d e )   f i g u r a n t   s u r   d e s   s i t e s   I n t e r n e t   t i e r s   o u   d a n s   d e s   p r o g r a m m e s   t i e r s   ;   e t   "   
 
 l e s   r  l a m a t i o n s   a u   t i t r e   d e   v i o l a t i o n   d e   c o n t r a t   o u   d e   g a r a n t i e ,   o u   a u   t i t r e   d e   
 
 r e s p o n s a b i l i t  s t r i c t e ,   d e   n  l i g e n c e   o u   d  u n e   a u t r e   f a u t e   d a n s   l a   l i m i t e   
 
 a u t o r i s    p a r   l a   l o i   e n   v i g u e u r .   E l l e   s  a p p l i q u e    a l e m e n t ,   m  e   s i   M i c r o s o f t   
 
 c o n n a i s s a i t   o u   d e v r a i t   c o n n a  r e   l   e n t u a l i t  d  u n   t e l   d o m m a g e .   S i   v o t r e   p a y s   
 
 n  a u t o r i s e   p a s   l  e x c l u s i o n   o u   l a   l i m i t a t i o n   d e   r e s p o n s a b i l i t  p o u r   l e s   d o m m a g e s   
 
 i n d i r e c t s ,   a c c e s s o i r e s   o u   d e   q u e l q u e   n a t u r e   q u e   c e   s o i t ,   i l   s e   p e u t   q u e   l a   
 
 l i m i t a t i o n   o u   l  e x c l u s i o n   c i - d e s s u s   n e   s  a p p l i q u e r a   p a s    v o t r e    a r d .   E F F E T   
 
 J U R I D I Q U E .   L e   p r  e n t   c o n t r a t   d  r i t   c e r t a i n s   d r o i t s   j u r i d i q u e s .   V o u s   p o u r r i e z   
 
 a v o i r   d  a u t r e s   d r o i t s   p r  u s   p a r   l e s   l o i s   d e   v o t r e   p a y s .   L e   p r  e n t   c o n t r a t   n e   
 
 m o d i f i e   p a s   l e s   d r o i t s   q u e   v o u s   c o n f  e n t   l e s   l o i s   d e   v o t r e   p a y s   s i   c e l l e s - c i   n e   
 
 l e   p e r m e t t e n t   p a s .   host=gbwgdcsolacetest.systems.uk.hsbc
port=55555
vpn=FIN_SM_NP_SSGU
username=GB-SVC-TRDS
password=7533-8d2cEF6F8
queues=Q_DATA_LAKE_TEST
#host=gbwgdcsolacetest01-mgt.systems.uk.hsbc
#port=80
#vpn=GDIS_SM_NP_DDF
#queues=FLARE_POC_Accounting_Business_Event
#queues=DDF_EBCCtrds.elasticsearch.properties.file=/elasticsearch/es.properties
trds.guest.solace.properties=guest.solace.properties
trds.data.generator.properties=data-generator-constants.properties<?xml version="1.0" encoding="UTF-8"?>
<tradeEventMessage xmlns:fpml="http://www.fpml.org/FpML-5/recordkeeping" xmlns="urn:hsbc:uc"
                   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                   xmlns:hsbc="urn:hsbc:fpml-5-ext"
                   xmlns:ip="urn:hsbc:ipt">
<!--fidessa-uc-cash-equity-main.xslt Version: 1.3.11.8| fidessa-common.xslt Version: 1.3.11.8| fidessa-lib.xslt Version: 1.3.11.8| uc-common-functions.xslt Version: 0.0.0.2 generated on : 2020-12-10T11:19:47.319Z-->
<header>
      <messageId messageIdScheme="urn:hsbc:message-id:DSL:control-event">21081425237TRLO1_1_0x000001764C60E57580A4D46C29395ED5F1E028D3BD493F55C489D3EC55E1E410</messageId>
      <sourceSystem>FIDESSA</sourceSystem>
      <sourceSystemTrouxId>000000245007</sourceSystemTrouxId>
      <originatingSystem>XTP_SOR</originatingSystem>
      <timestamp>2020-12-10T11:19:22.809066+00:00</timestamp>
   </header>
   <tradeEvent>
      <eventType>Trade:Inception:NewTrade</eventType>
      <onBehalfOf>
         <partyReference href="party1"/>
         <accountReference href="party1Book"/>
      </onBehalfOf>
      <counterparty>
         <partyReference href="party2"/>
      </counterparty>
      <details>
         <executionDateTime>2020-12-10T11:19:22.809066Z</executionDateTime>
      </details>
      <trade xsi:type="hsbc:Trade">
         <fpml:tradeHeader>
            <fpml:partyTradeIdentifier>
               <fpml:partyReference href="party1"/>
               <fpml:versionedTradeId>
                  <fpml:tradeId tradeIdScheme="urn:hsbc:trade-id:FIDESSA">21081425237TRLO1_1</fpml:tradeId>
                  <fpml:version>1</fpml:version>
               </fpml:versionedTradeId>
               <fpml:versionedTradeId>
                  <fpml:tradeId tradeIdScheme="urn:hsbc:trade-id:HTI">00000024500721081425237TRLO1_1</fpml:tradeId>
                  <fpml:version>1</fpml:version>
               </fpml:versionedTradeId>
               <fpml:tradeId tradeIdScheme="urn:hsbc:trade-id:FIDESSA:EXCHANGE-TRADE-CODE">00001528062XENA1</fpml:tradeId>
               <fpml:linkId linkIdScheme="urn:hsbc:trade-id:SOPR">FQGGCjxJE200001528062XENA1</fpml:linkId>
               <fpml:linkId linkIdScheme="urn:hsbc:order-id:FIDESSA">21082304810ORLO1</fpml:linkId>
               <fpml:linkId linkIdScheme="urn:hsbc:parent-order-id:FIDESSA">21082304809ORLO1</fpml:linkId>
            </fpml:partyTradeIdentifier>
            <fpml:partyTradeIdentifier>
               <fpml:partyReference href="party3"/>
               <fpml:tradeId tradeIdScheme="urn:external:trade-id"/>
            </fpml:partyTradeIdentifier>
            <fpml:partyTradeInformation>
               <fpml:partyReference href="party1"/>
               <fpml:relatedParty>
                  <fpml:partyReference href="party3"/>
                  <fpml:role partyRoleScheme="http://www.fpml.org/coding-scheme/party-role">ExecutionFacility</fpml:role>
               </fpml:relatedParty>
               <fpml:relatedPerson>
                  <fpml:personReference href="Clerk"/>
                  <fpml:role personRoleScheme="urn:hsbc:person-role">EnteredUser</fpml:role>
               </fpml:relatedPerson>
               <fpml:relatedPerson>
                  <fpml:personReference href="Orderer"/>
                  <fpml:role personRoleScheme="urn:hsbc:person-role">Orderer</fpml:role>
               </fpml:relatedPerson>
               <fpml:relatedPerson>
                  <fpml:personReference href="Trader"/>
                  <fpml:role>Trader</fpml:role>
               </fpml:relatedPerson>
               <fpml:relatedPerson>
                  <fpml:personReference href="InvestmentDecisionMaker"/>
                  <fpml:role>InvestmentDecisionMaker</fpml:role>
               </fpml:relatedPerson>
               <fpml:relatedPerson>
                  <fpml:personReference href="ExecutionWithinFirm"/>
                  <fpml:role>ExecutionWithinFirm</fpml:role>
               </fpml:relatedPerson>
               <fpml:category>Principal</fpml:category>
               <fpml:executionDateTime>2020-12-10T11:19:21.956Z</fpml:executionDateTime>
               <fpml:timestamps>
                  <fpml:timestamp>
                     <fpml:type>TradeEntry</fpml:type>
                     <fpml:value>2020-12-10T11:19:22.805411+00:00</fpml:value>
                  </fpml:timestamp>
                  <fpml:timestamp>
                     <fpml:type timestampScheme="urn:hsbc:timestamp">TradeTime</fpml:type>
                     <fpml:value>2020-12-10T11:19:21.956Z</fpml:value>
                  </fpml:timestamp>
               </fpml:timestamps>
            </fpml:partyTradeInformation>
            <fpml:tradeDate>2020-12-10</fpml:tradeDate>
         </fpml:tradeHeader>
         <hsbc:equityTradeDetails>
            <fpml:primaryAssetClass>Equity</fpml:primaryAssetClass>
            <fpml:productType productTypeScheme="urn:hsbc:product-type:FIDESSA">EQ</fpml:productType>
            <fpml:productType productTypeScheme="urn:hsbc:product-type:isda-plus">Equity:Cash:Shares:Common</fpml:productType>
            <fpml:productType productTypeScheme="urn:hsbc:product-type:FIDESSA:INSTRUMENT_TYPE_QUALIFIER">EQ</fpml:productType>
            <fpml:buyerPartyReference href="party2"/>
            <fpml:sellerPartyReference href="party1"/>
            <fpml:sellerAccountReference href="party1Book"/>
            <hsbc:exchangeId>XNGS</hsbc:exchangeId>
            <fpml:equity>
               <fpml:instrumentId instrumentIdScheme="urn:hsbc:instrument-id:FIDESSA">46534</fpml:instrumentId>
               <fpml:instrumentId instrumentIdScheme="http://www.fpml.org/coding-scheme/external/instrument-id-Reuters-RIC">INTC.O</fpml:instrumentId>
               <fpml:instrumentId instrumentIdScheme="urn:hsbc:instrument-id:GRDS:LIST">700000231245</fpml:instrumentId>
               <fpml:instrumentId instrumentIdScheme="http://www.fpml.org/coding-scheme/external/instrument-id-ISIN">US4581401001</fpml:instrumentId>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:exchangeId>NAS</fpml:exchangeId>
            </fpml:equity>
            <hsbc:numberOfUnits>100</hsbc:numberOfUnits>
            <hsbc:unitPrice>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>50.06</fpml:amount>
            </hsbc:unitPrice>
            <hsbc:principalAmount>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>50.06</fpml:amount>
            </hsbc:principalAmount>
            <hsbc:grossPrice>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>50.06</fpml:amount>
            </hsbc:grossPrice>
            <hsbc:grossConsideration>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>5006</fpml:amount>
            </hsbc:grossConsideration>
            <hsbc:netPrice>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>50.06</fpml:amount>
            </hsbc:netPrice>
            <hsbc:netConsideration>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>5006</fpml:amount>
            </hsbc:netConsideration>
            <hsbc:netNetPrice>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>50.06</fpml:amount>
            </hsbc:netNetPrice>
            <hsbc:netNetConsideration>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>5006</fpml:amount>
            </hsbc:netNetConsideration>
            <hsbc:settlementDate>
               <fpml:unadjustedDate>2020-12-14</fpml:unadjustedDate>
            </hsbc:settlementDate>
            <hsbc:settlementAmount>
               <fpml:currency currencyScheme="urn:hsbc:currency-id">USD</fpml:currency>
               <fpml:amount>5006</fpml:amount>
            </hsbc:settlementAmount>
         </hsbc:equityTradeDetails>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">BUSINESS_TRANSACTION</hsbc:type>
            <hsbc:text>BE</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">BUSINESS_TRANSACTION_DESCRIPTION</hsbc:type>
            <hsbc:text>Broker Execution</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">MARKET_ID</hsbc:type>
            <hsbc:text>NAS-GSM</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">REPORT_DESTINATION_ID</hsbc:type>
            <hsbc:text>NAS</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">CLIENT_GROSS_BASIS</hsbc:type>
            <hsbc:text>G</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">DEALT_PRICE_TYPE</hsbc:type>
            <hsbc:text>G</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">FLAGS</hsbc:type>
            <hsbc:text>AT</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">FLAGS_DESCRIPTION</hsbc:type>
            <hsbc:text>Automatic Execution</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">EXECUTION_VENUE</hsbc:type>
            <hsbc:text>XNGS</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">COUNTERPARTY_TYPE</hsbc:type>
            <hsbc:text>Market-side</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">COUNTERPARTY_TYPE_QUALIFIER</hsbc:type>
            <hsbc:text>Broker Dealer</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">CONTRIBUTOR_ID</hsbc:type>
            <hsbc:text>US_HSBC_XTP_DMA</hsbc:text>
         </hsbc:narrative>
         <hsbc:narrative>
            <hsbc:type narrativeTypeScheme="urn:hsbc:narrative-type:FIDESSA">ORDER_FLOW_TYPE</hsbc:type>
            <hsbc:text>HOUSE</hsbc:text>
         </hsbc:narrative>
         <hsbc:internalProductType>
            <ip:productType productName="DELTA ONE - SHARE" underlyingType="SINGLE NAME"/>
         </hsbc:internalProductType>
      </trade>
      <regulatoryCharacteristics>
         <mifid2>
            <tradingCapacity>DEAL</tradingCapacity>
            <dividendReinvestment>false</dividendReinvestment>
            <reportableTrade>true</reportableTrade>
            <transactionReportingVenue>XOFF</transactionReportingVenue>
         </mifid2>
      </regulatoryCharacteristics>
   </tradeEvent>
   <party id="party1">
      <fpml:partyId partyIdScheme="urn:hsbc:party-id:FIDESSA">HSI</fpml:partyId>
      <fpml:partyId partyIdScheme="urn:hsbc:party-id:GRID_ID">14152</fpml:partyId>
      <fpml:person id="Clerk">
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA">-4886</fpml:personId>
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA-USER">XTP</fpml:personId>
      </fpml:person>
      <fpml:person id="Trader">
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA">-4886</fpml:personId>
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA-USER">XTP</fpml:personId>
      </fpml:person>
      <fpml:person id="Orderer">
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA">44</fpml:personId>
         <fpml:personId personIdScheme="urn:hsbc:person-id:FIDESSA-USER">UMcintyreL</fpml:personId>
         <fpml:personId personIdScheme="urn:hsbc:person-id:PEOPLESOFT">43289053</fpml:personId>
      </fpml:person>
      <fpml:person id="InvestmentDecisionMaker">
         <fpml:personId personIdScheme="urn:hsbc:person-id:PEOPLESOFT">43289053</fpml:personId>
      </fpml:person>
      <fpml:person id="ExecutionWithinFirm">
         <fpml:personId personIdScheme="urn:hsbc:person-id:PEOPLESOFT">43289053</fpml:personId>
      </fpml:person>
   </party>
   <!--[party-block-id:party2][is-this-hsbc-block:false]--><!--[derived-tr-grid-id:][derived-lo-grid-id:]--><party id="party2">
      <fpml:partyId partyIdScheme="urn:hsbc:party-id:GRID_ID">2617697</fpml:partyId>
      <fpml:partyId partyIdScheme="urn:hsbc:party-id:FIDESSA">1018811</fpml:partyId>
      <fpml:partyId partyIdScheme="urn:hsbc:party-id:VIEW">NSDQ</fpml:partyId>
      <!--[ber-id-by-treats-and-po:][ber-id-by-paris-short-name-and-po:][ber-id-by-impact-code-and-po:]--><fpml:partyName partyNameScheme="urn:hsbc:party-name:FIDESSA">Client ID is - 1018811</fpml:partyName>
      <fpml:country countryScheme="http://www.fpml.org/coding-scheme/external/iso3166">US</fpml:country>
   </party>
   <!--[party-block-id:party3][is-this-hsbc-block:false]--><!--[derived-tr-grid-id:][derived-lo-grid-id:]--><party id="party3">
      <fpml:partyId partyIdScheme="http://www.fpml.org/coding-scheme/external/iso10383">XOFF</fpml:partyId>
      <!--[ber-id-by-treats-and-po:][ber-id-by-paris-short-name-and-po:][ber-id-by-impact-code-and-po:]--><!--[coast:][treats:][frontarena:][htde-pts:][internal:]--></party>
   <account id="party1Book" xsi:type="hsbc:Account">
      <fpml:accountId accountIdScheme="urn:hsbc:book-id:FIDESSA">NYUF</fpml:accountId>
      <fpml:accountId accountIdScheme="urn:hsbc:book-id:HMS">FO0090840</fpml:accountId>
      <fpml:accountBeneficiary href="party1"/>
      <hsbc:complianceLocation>NY</hsbc:complianceLocation>
   </account>
</tradeEventMessage>cat: ./src/test/scala: Is a directory
cat: ./src/test/scala/com: Is a directory
cat: ./src/test/scala/com/hsbc: Is a directory
cat: ./src/test/scala/com/hsbc/trds: Is a directory
cat: ./src/test/scala/com/hsbc/trds/scala: Is a directory
cat: ./src/test/scala/com/hsbc/trds/scala/driver: Is a directory
package com.hsbc.trds.scala.driver

import java.nio.file.{Files, Path}
import java.text.SimpleDateFormat

import com.hsbc.trds.es.model.Trade
import com.hsbc.trds.solace.QueueConsumer
import org.apache.spark.sql.{Encoder, Encoders, Row, SparkSession}
import org.scalatest.BeforeAndAfterAll
import org.scalatest.funsuite.AnyFunSuite

import scala.reflect.ClassTag

//case class Rec(productId: String, party1Lei: String)  //extends java.io.Serializable

//class Trade1 {
//  private[this] var tradeId: String = _
//  private[this] var productId: String = _
//  def getTradeId =  tradeId
//  def tradeId_=(trdId: String): Unit = {
//    tradeId = trdId
//  }
//  def getProductId = productId
//  def prod
//}

class XmlExtractorTest extends AnyFunSuite with BeforeAndAfterAll {



  private val resDir = "src/test/resources/"

  private val trdsFile = resDir + "6485170999_0x000001752025345B80A4D46A712B3DA04C9EBE8AD72358F3C5D3CDC8756A1D08_HBEU_6485170999_1602543600000.xml"

  private val aopFile = resDir + "apo/TR_CFTC_SNAPSHOT/"

  private val uctrdsFile = resDir + "UC_TRADE.xml"

  private val carsFile = resDir + "cars.xml"

  private lazy val spark: SparkSession = {
    // It is intentionally a val to allow import implicits.
    SparkSession.builder().
      master("local[2]").
      appName("XmlExtractor").
      config("spark.ui.enabled", false).
      getOrCreate()
  }

  private var tempDir: Path = _


  override protected def beforeAll(): Unit = {
    super.beforeAll()
    spark  // Initialize Spark session
    tempDir = Files.createTempDirectory("XmlExtractor")
    tempDir.toFile.deleteOnExit()
  }

  override protected def afterAll(): Unit = {
    try {
      spark.stop()
    } finally {
      super.afterAll()
    }
  }

  private def getEmptyTempDir(): Path = {
    Files.createTempDirectory(tempDir, "test")
  }

  def printList(args: List[_]): Unit = {
    args.foreach(println)
  }

  test("solace test") {
    new QueueConsumer().run1()
  }

  def argsList(): java.util.List[String] = {
    val args = Array("aa","bb")
    import collection.JavaConverters._
    args.toList.asJava
  }
  //tests

  test("FPML test") {
    val results = spark.read.format("com.databricks.spark.xml")
      .option("rootTag","tradeEventMessage")
      .option("rowTag","tradeEventMessage")
      .load(uctrdsFile)


     results.printSchema()
     val finalResult = results.select("party")
      .collect()

    printList(finalResult.toList)

    assert(finalResult.length === 1)
  }

  //test to extract multi values
  test("FPML test party") {
    val results = spark.read.format("com.databricks.spark.xml")
      .option("rootTag","tradeEventMessage")
      .option("rowTag","tradeEventMessage")
      .load(uctrdsFile)

      val arrayRes = results.select("party").collect()

    arrayRes.foreach(row => println(row))


    results.printSchema()

    val _id = results.select("account").collect()(0).getStruct(0).getAs[String]("_id")
    println("_id is: " + _id)
    val finalResult = results.select("party")
      .collect()

    printList(finalResult.toList)

    assert(finalResult.length === 1)
  }

//  def extractData(row: Row): Trade1 = {
//    val trade  = row.getAs[Row]("trade")
//
//    val tradeData = new Trade1(trade,)
//  }

  def extractXmlData(row : Row): Trade = {
   val tradeData = new Trade

    val header = row.getAs[Row]("header")
    val party = row.getAs[Seq[Row]]("party")
    val transactionType = row.getAs[String]("originatingEvent")
    val trade  = row.getAs[Row]("trade")
    val partyTradeIdentifier = trade.getAs[Row]("tradeHeader").getAs[Seq[Row]]("partyTradeIdentifier")

    for (partyTI <- partyTradeIdentifier) {
      if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("unique-transaction-identifier")) {
        val utiValue = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
        tradeData.setUtiValue(utiValue)
        println("utiValue is : " + utiValue)
      }
      if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("internal_Referenceid")) {
        val tradeId = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
        tradeData.setTradeId(tradeId)
        println("tradeId : " + tradeId)
      }
    }

    val partyTradeInformation = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation")
    val executionVenueType = partyTradeInformation.getAs[String]("executionVenueType")
    val clearingStatus = partyTradeInformation.getAs[String]("clearingStatus")
    val creationTimeStamp = header.getAs[String]("creationTimestamp")

    val commodityOption = trade.getAs[Row]("commodityOption")
    val executionDateTime = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation").getAs[String]("executionDateTime")
    val primaryAssetClass = commodityOption.getAs[String]("primaryAssetClass")
    val productId = commodityOption.getAs[Row]("productId").getAs[String]("_VALUE")

    val party1Lei = party(0).getAs[Row]("partyId").getAs[String]("_VALUE")
    val party2Lei = party(1).getAs[Row]("partyId").getAs[String]("_VALUE")

//    tradeData.setExecutionVenueType(executionVenueType)
//    tradeData.setClearingStatus(clearingStatus)
//    tradeData.setProductId(productId)
//    tradeData.setExecutionDateTime(ZonedDateTime.parse(executionDateTime))
//    tradeData.setCreationTimestamp(ZonedDateTime.parse(creationTimeStamp))
    tradeData.setPrimaryAssetClass(primaryAssetClass)
//    tradeData.setParty1Lei(party1Lei)
//    tradeData.setParty2Lei(party2Lei)
//    tradeData.setTransactionType(transactionType)

  tradeData


  }

//  def printDetails(rows: Iterator[Rec]) = {
//    rows.foreach(row => println(row))
//  }

  test ("es test") {

    val results = spark.read.format("com.databricks.spark.xml")
      .option("rootTag","nonpublicExecutionReport")
      .option("rowTag","nonpublicExecutionReport")
      .load(aopFile)

   //implicit val myObjEncoder: org.apache.spark.sql.Encoder[Trade] = Encoders.javaSerialization(classOf[Trade])

    implicit def single[A](implicit c: ClassTag[A]): Encoder[A] = Encoders.kryo[A](c)
    //type MyObjEncoded = (String, String)
    //implicit conversions
    //implicit def toEncoded(trade: Trade): MyObjEncoded = (trade.getTradeId,trade.getUtiValue)
    //implicit def fromEncoded(e: MyObjEncoded): Trade  = new Trade(e._1,e._2)
    implicit def tuple2[A1, A2](implicit e1: Encoder[A1],e2: Encoder[A2]): Encoder[(A1,A2)] = Encoders.tuple[A1,A2](e1, e2)
    import spark.implicits._

    val df = results.mapPartitions(partitions => {
          partitions.map(row => {
            val tradeData = new Trade
            val trade  = row.getAs[Row]("trade")
            val partyTradeIdentifier = trade.getAs[Row]("tradeHeader").getAs[Seq[Row]]("partyTradeIdentifier")
            for (partyTI <- partyTradeIdentifier) {
              if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("unique-transaction-identifier")) {
                val utiValue = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
                tradeData.setUtiValue(utiValue)
                println("utiValue is : " + utiValue)
              }
              if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("internal_Referenceid")) {
                val tradeId = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
                tradeData.setTradeId(tradeId)
                println("tradeId : " + tradeId)
              }
            }
            (tradeData.getTradeId,tradeData.getUtiValue)
          })
    }).toDF("tradeId","utiValue").as[(String,String)]

    df.printSchema()

    df.foreachPartition(rows => {
      val result: List[Trade] = rowToTrade(rows)
      println(result.toString)
    })

 //   {
   //   parts.foreach(row => {
//        val tradeData = new Trade
//        tradeData.setTradeId(row.getAs[String]("tradeId"))
//        tradeData.setUtiValue(row.getAs[String]("utiValue"))
//        tradeData
     //   rowToTrade(row)
      // println(row._1,row._2)
      //})
    //})

//    res.mapPartitions((data: Iterator[Trade]) => {
//      data.map(r => {
//        println(r.toString)
//      })
//    })

    df.printSchema()

//    df.foreachPartition(recItr => {
//     recItr.foreach(rec => println(rec.toString()))
//    })

//    df.select(col("productId"),col("party1Lei")).as[Rec]
//      .foreachPartition(recItr => {
//        recItr.foreach(rec => println(rec.toString))
//      })

    //df.printSchema

    //val finalDS = results.mapPartitions(rowItr => {rowItr.map(row => extractXmlData(row))})(myObjEncoder)
//    results.foreachPartition(rowItr => {
//      rowItr.map(row => extractXmlData(row)).foreach(trade => println(trade))
//    })
    //finalDS.printSchema
  }

  def rowToTrade(rows: Iterator[(String,String)]) : List[Trade] = {
    rows.map(row => {
      val trade = new Trade
      trade.setTradeId(row._1)//._1row.getAs[String]("tradeId"))
      trade.setTradeId(row._2) //getAs[String]("utiValue"))
      trade
    }).toList
  }

  test("Encoders test") {

  }

  test("apo test") {
    val results = spark.read.format("com.databricks.spark.xml")
      .option("rootTag","nonpublicExecutionReport")
      .option("rowTag","nonpublicExecutionReport")
      .load(aopFile)

   // results.printSchema()
    val schemaLoc = results.first.getAs[String]("_schemaLocation")
    val header = results.first.getAs[Row]("header")
    val party = results.first.getAs[Seq[Row]]("party")
    val originatingEvent = results.first.getAs[String]("originatingEvent")


    val trade  = results.first.getAs[Row]("trade")
    val partyTradeIdentifier = trade.getAs[Row]("tradeHeader").getAs[Seq[Row]]("partyTradeIdentifier")
    for (partyTI <- partyTradeIdentifier) {
      if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("unique-transaction-identifier")) {
        val utiValue = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
        println("utiValue is : " + utiValue)
      }
      if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("internal_Referenceid")) {
        val tradeId = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
        println("tradeId : " + tradeId)
      }
    }

    val partyTradeInformation = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation")
    val executionVenueType = partyTradeInformation.getAs[String]("executionVenueType")
    val clearingStatus = partyTradeInformation.getAs[String]("clearingStatus")

    println("executionVenueType: " + executionVenueType)
    println("clearingStatus: " + clearingStatus)

    val commodityOption = trade.getAs[Row]("commodityOption")

    val executionDateTime = trade.getAs[Row]("tradeHeader").getAs[Row]("partyTradeInformation").getAs[String]("executionDateTime")
    val primaryAssetClass = commodityOption.getAs[String]("primaryAssetClass")
    val productId = commodityOption.getAs[Row]("productId").getAs[String]("_VALUE")
    println("productId: " + productId)


    val party1Lei = party(0).getAs[Row]("partyId").getAs[String]("_VALUE")
    val party2Lei = party(1).getAs[Row]("partyId").getAs[String]("_VALUE")

    println("creationTimestamp: " +  header.getAs[String]("creationTimestamp"))

    println("schemaLoc" + schemaLoc)
    println("executionDateTime: " + executionDateTime)
    println("primaryAssetClass: " + primaryAssetClass)
    println("party1Lei: " + party1Lei)
    println("party2Lei: " + party2Lei)
    println("originatingEvent: "+originatingEvent)

   // results.foreach(row => println(row.get(0)) )


    assert(results.count === 2)
  }


  test("kryo-encoders") {
    implicit val normalPersonKryoEncoder = Encoders.kryo[NormalPerson]
    implicit val reversePersonKryoEncoder = Encoders.kryo[ReversePerson]

    val normalPersons = Seq(NormalPerson("Superman",25), NormalPerson("SpiderMan",17), NormalPerson("Ironman",29))

    val ds3 = spark.createDataset(normalPersons)

    ds3.printSchema()

    val ds4 = ds3.map(np => ReversePerson(np.age, np.name))

    ds3.show
    ds4.show

    ds4.printSchema()

    ds3.foreach(p => {
      println("age: " + p._age + "name: " + p._name)
    })

    ds4.foreach(p => {
      println("age: " + p._age + "name: " + p._name)
    })

  }

  test ("kryo-trade-encoders") {
    val results = spark.read.format("com.databricks.spark.xml")
      .option("rootTag","nonpublicExecutionReport")
      .option("rowTag","nonpublicExecutionReport")
      .load(aopFile)

    implicit val tradeEncoder = Encoders.kryo[Trade1]
    implicit val rowEncoder = Encoders.kryo[Row]

    val df = results.mapPartitions(partitions => {
      partitions.map(row => {
        //val tradeData = new Trade1
        val trade  = row.getAs[Row]("trade")
        val partyTradeIdentifier = trade.getAs[Row]("tradeHeader").getAs[Seq[Row]]("partyTradeIdentifier")
        var utiValue = ""
        var tradeId = ""
        for (partyTI <- partyTradeIdentifier) {
          if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("unique-transaction-identifier")) {
             utiValue = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
            //tradeData.setUtiValue(utiValue)
            println("utiValue is : " + utiValue)
          }
          if (partyTI.getAs[Row]("tradeId").getAs[String]("_tradeIdScheme").contains("internal_Referenceid")) {
            tradeId = partyTI.getAs[Row]("tradeId").getAs[String]("_VALUE")
            //tradeData.setTradeId(tradeId)
            println("tradeId : " + tradeId)
          }
        }
        val tradeData = new Trade1(utiValue, tradeId)
        tradeData
      })
    })

    df.show

    df.foreachPartition(rows => {
      rows.foreach(row => println(row.tradeId + "--" + row.utiValue))
    })

  }

  test ("datetest") {
    val date = "2020-10-18T06:05:30Z"

    val parsedDate = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'").parse(date)
    println(parsedDate)


  }


}
cat: ./src/test/scala/com/hsbc/trds/scala/util: Is a directory
package com.hsbc.trds.scala.util

import java.io.StringWriter
import java.nio.charset.StandardCharsets

import com.hsbc.trds.scala.driver.XmlBlobExtractor.bucketName1
import com.hsbc.trds.scala.util.ECSS3API.ECSS3Config
import org.apache.commons.io.IOUtils
import org.scalatest.BeforeAndAfter
import org.scalatest.funsuite.AnyFunSuite


//@RunWith(classOf[JUnitRunner])
class ECSS3APITest extends AnyFunSuite with BeforeAndAfter{

  var config: ECSS3Config =  _

  before{
    config = new ECSS3Config(true, false,
      s"$bucketName1/TRDS_LAKE/DSL_STREAM/DSL_OBJECT_TYPE=RAW/BUSINESS_DATE=20190219","","")
  }

//
  test("getS3ObjectAsInpuStream") {
    val amazonS3Client = ECSS3API.getAmazonS3Client()
    val is = ECSS3API.getS3ObjectInputStream(config,
      "FXOHBFR20190219213400005905_0x0000016907B67FDC80A4D46325E84FDC77FF2D1882B9D0A0C91DD545CF17860C_HBFR_1615548752362.xml",amazonS3Client,false)
    val writer = new StringWriter
    IOUtils.copy(is, writer,StandardCharsets.UTF_8)
    val resultString = writer.toString
    println("resultString: " + resultString)
    assert(true)
  }

  test("getIDList") {
    val amazonS3Client = ECSS3API.getAmazonS3Client()

  }


}
<?xml version="1.0" encoding="UTF-8"?>
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">
  <component name="NewModuleRootManager" LANGUAGE_LEVEL="JDK_1_8">
    <output url="file://$MODULE_DIR$/target/classes" />
    <output-test url="file://$MODULE_DIR$/target/test-classes" />
    <content url="file://$MODULE_DIR$">
      <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />
      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
      <sourceFolder url="file://$MODULE_DIR$/src/test/resources" type="java-test-resource" />
      <excludeFolder url="file://$MODULE_DIR$/target" />
    </content>
    <orderEntry type="inheritedJdk" />
    <orderEntry type="sourceFolder" forTests="false" />
  </component>
</module>
