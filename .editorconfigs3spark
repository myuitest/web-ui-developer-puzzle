cat: .: Is a directory
/trds.iml
/target/
/.idea/
cat: ./.idea: Is a directory
trds<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CompilerConfiguration">
    <annotationProcessing>
      <profile name="Maven default annotation processors profile" enabled="true">
        <sourceOutputDir name="target/generated-sources/annotations" />
        <sourceTestOutputDir name="target/generated-test-sources/test-annotations" />
        <outputRelativeToContentRoot value="true" />
        <module name="trds" />
      </profile>
    </annotationProcessing>
    <bytecodeTargetLevel>
      <module name="trds" target="1.8" />
    </bytecodeTargetLevel>
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="HydraSettings">
    <option name="hydraStorePath" value="C:\Users\45143309\Downloads\TRDS_PROTOTYPING-master\TRDS_PROTOTYPING-master\.hydra\idea" />
    <option name="noOfCores" value="2" />
    <option name="projectRoot" value="C:\Users\45143309\Downloads\TRDS_PROTOTYPING-master\TRDS_PROTOTYPING-master" />
    <option name="sourcePartitioner" value="auto" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="MavenProjectsManager">
    <option name="originalFiles">
      <list>
        <option value="$PROJECT_DIR$/pom.xml" />
      </list>
    </option>
  </component>
  <component name="ProjectRootManager" version="2" languageLevel="JDK_1_8" project-jdk-name="1.8" project-jdk-type="JavaSDK">
    <output url="file://$PROJECT_DIR$/classes" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ProjectModuleManager">
    <modules>
      <module fileurl="file://$PROJECT_DIR$/trds.iml" filepath="$PROJECT_DIR$/trds.iml" />
    </modules>
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ScalaSbtSettings">
    <option name="customVMPath" />
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="GradleLocalSettings">
    <option name="projectSyncType">
      <map>
        <entry key="$USER_HOME$/IdeaProjects" value="PREVIEW" />
        <entry key="$USER_HOME$/IdeaProjects/emf-spark" value="PREVIEW" />
        <entry key="$USER_HOME$/IdeaProjects/emf-spark1" value="PREVIEW" />
      </map>
    </option>
  </component>
  <component name="ProjectFrameBounds" extendedState="6">
    <option name="x" value="345" />
    <option name="width" value="720" />
    <option name="height" value="572" />
  </component>
  <component name="PropertiesComponent">
    <property name="last_opened_file_path" value="$PROJECT_DIR$" />
    <property name="settings.editor.selected.configurable" value="preferences.pathVariables" />
  </component>
  <component name="RunManager">
    <configuration default="true" type="tests" factoryName="Nosetests">
      <option name="INTERPRETER_OPTIONS" value="" />
      <option name="PARENT_ENVS" value="true" />
      <option name="SDK_HOME" value="" />
      <option name="WORKING_DIRECTORY" value="" />
      <option name="IS_MODULE_SDK" value="false" />
      <option name="ADD_CONTENT_ROOTS" value="true" />
      <option name="ADD_SOURCE_ROOTS" value="true" />
      <module name="" />
      <option name="_new_regexPattern" value="&quot;&quot;" />
      <option name="_new_additionalArguments" value="&quot;&quot;" />
      <option name="_new_target" value="&quot;&quot;" />
      <option name="_new_targetType" value="&quot;PATH&quot;" />
      <method v="2" />
    </configuration>
    <configuration default="true" type="tests" factoryName="Twisted Trial">
      <option name="INTERPRETER_OPTIONS" value="" />
      <option name="PARENT_ENVS" value="true" />
      <option name="SDK_HOME" value="" />
      <option name="WORKING_DIRECTORY" value="" />
      <option name="IS_MODULE_SDK" value="false" />
      <option name="ADD_CONTENT_ROOTS" value="true" />
      <option name="ADD_SOURCE_ROOTS" value="true" />
      <module name="" />
      <option name="_new_additionalArguments" value="&quot;&quot;" />
      <option name="_new_target" value="&quot;&quot;" />
      <option name="_new_targetType" value="&quot;PATH&quot;" />
      <method v="2" />
    </configuration>
    <configuration default="true" type="tests" factoryName="Unittests">
      <option name="INTERPRETER_OPTIONS" value="" />
      <option name="PARENT_ENVS" value="true" />
      <option name="SDK_HOME" value="" />
      <option name="WORKING_DIRECTORY" value="" />
      <option name="IS_MODULE_SDK" value="false" />
      <option name="ADD_CONTENT_ROOTS" value="true" />
      <option name="ADD_SOURCE_ROOTS" value="true" />
      <module name="" />
      <option name="_new_additionalArguments" value="&quot;&quot;" />
      <option name="_new_target" value="&quot;&quot;" />
      <option name="_new_targetType" value="&quot;PATH&quot;" />
      <method v="2" />
    </configuration>
    <configuration default="true" type="tests" factoryName="py.test">
      <option name="INTERPRETER_OPTIONS" value="" />
      <option name="PARENT_ENVS" value="true" />
      <option name="SDK_HOME" value="" />
      <option name="WORKING_DIRECTORY" value="" />
      <option name="IS_MODULE_SDK" value="false" />
      <option name="ADD_CONTENT_ROOTS" value="true" />
      <option name="ADD_SOURCE_ROOTS" value="true" />
      <module name="" />
      <option name="_new_keywords" value="&quot;&quot;" />
      <option name="_new_additionalArguments" value="&quot;&quot;" />
      <option name="_new_target" value="&quot;&quot;" />
      <option name="_new_targetType" value="&quot;PATH&quot;" />
      <method v="2" />
    </configuration>
  </component>
  <component name="SbtLocalSettings">
    <option name="projectSyncType">
      <map>
        <entry key="$PROJECT_DIR$/../../configexample-master/configexample-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../exercises-pureconfig-main/exercises-pureconfig-main" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../pure-spark-2.2-master/pure-spark-2.2-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../spark-config-example-master/spark-config-example-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../spark-seed.g8-master/spark-seed.g8-master" value="PREVIEW" />
        <entry key="$PROJECT_DIR$/../../typesafe-config-example-master/typesafe-config-example-master" value="PREVIEW" />
      </map>
    </option>
  </component>
  <component name="VcsContentAnnotationSettings">
    <option name="myLimit" value="2678400000" />
  </component>
  <component name="masterDetails">
    <states>
      <state key="ProjectJDKs.UI">
        <settings>
          <last-edited>1.8</last-edited>
          <splitter-proportions>
            <option name="proportions">
              <list>
                <option value="0.2" />
              </list>
            </option>
          </splitter-proportions>
        </settings>
      </state>
    </states>
  </component>
</project><?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>trds_main</groupId>
  <artifactId>trds</artifactId>
  <version>1.0-SNAPSHOT</version>
  <build>
    <plugins>
      <plugin>
        <artifactId>maven-jar-plugin</artifactId>
        <version>2.4</version>
        <configuration>
          <excludes>
            <exclude>**/*.sh</exclude>
          </excludes>
        </configuration>
      </plugin>
      <plugin>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.6.1</version>
        <executions>
          <execution>
            <id>default-compile</id>
            <phase>none</phase>
          </execution>
        </executions>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest-maven-plugin</artifactId>
        <version>2.0.0</version>
        <executions>
          <execution>
            <id>test</id>
          </execution>
        </executions>
        <configuration>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
          <junitxml>.</junitxml>
          <filereports>TestSuite.txt</filereports>
        </configuration>
      </plugin>
      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <version>3.2.2</version>
        <executions>
          <execution>
            <goals>
              <goal>compile</goal>
              <goal>testCompile</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <recompileMode>incremental</recompileMode>
        </configuration>
      </plugin>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>3.3.0</version>
        <executions>
          <execution>
            <id>assemble-all</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
      </plugin>
      <plugin>
        <artifactId>maven-shade-plugin</artifactId>
        <version>3.2.4</version>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <relocations>
            <relocation>
              <pattern>org.apache.avro</pattern>
              <shadedPattern>shaded.org.apache.avro</shadedPattern>
            </relocation>
            <relocation>
              <pattern>com.google.guava</pattern>
              <shadedPattern>shaded.com.google.guava</shadedPattern>
            </relocation>
          </relocations>
          <artifactSet>
            <excludes>
              <exclude>net.sourceforge.saxon:saxon:jar:</exclude>
            </excludes>
          </artifactSet>
          <filters>
            <filter>
              <artifact>*:*</artifact>
              <excludes>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
                <exclude>net.sourceforge.saxon:saxon</exclude>
                <exclude>**/*.conf</exclude>
              </excludes>
            </filter>
          </filters>
          <transformers>
            <transformer>
              <manifestEntries>
                <Main-Class>${app.main.class}</Main-Class>
              </manifestEntries>
            </transformer>
          </transformers>
        </configuration>
      </plugin>
    </plugins>
  </build>
  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.13</version>
      <scope>test</scope>
      <exclusions>
        <exclusion>
          <artifactId>hamcrest-core</artifactId>
          <groupId>org.hamcrest</groupId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.projectlombok</groupId>
      <artifactId>lombok</artifactId>
      <version>1.18.10</version>
      <scope>provided</scope>
    </dependency>
  </dependencies>
  <properties>
    <avro.version>1.8.2</avro.version>
    <lombok.version>1.18.10</lombok.version>
    <slf4j.version>1.7.25</slf4j.version>
    <scalatest.version>3.0.3</scalatest.version>
    <dsl-version>471.212.12</dsl-version>
    <java.version>1.8</java.version>
    <aws-s3.version>1.11.319</aws-s3.version>
    <coherence.version>12.2.1.0.3.1</coherence.version>
    <app.main.class>com.hsbc.trds.scala.driver</app.main.class>
    <spark.binary.version>2.3.0</spark.binary.version>
    <maven.compiler.target>1.8</maven.compiler.target>
    <scala.binary.version>2.11</scala.binary.version>
    <dsl.common.api.version>471.212.5</dsl.common.api.version>
    <scala.version>2.11.12</scala.version>
    <spark.version>2.3.0</spark.version>
    <maven.compiler.source>1.8</maven.compiler.source>
  </properties>
</project>
#!groovy
@Library('tr-jenkins-lib') _

simpleBuild()
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<groupId>com.hsbc.trds_main</groupId>
	<artifactId>trds</artifactId>
	<version>1.0.258-SNAPSHOT</version>

	<scm>
		<connection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_PROTOTYPING.git
		</connection>
		<developerConnection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_PROTOTYPING.git
		</developerConnection>
		<url>https://alm-github.systems.uk.hsbc/CATS/TRDS_PROTOTYPING.git</url>
		<tag>HEAD</tag>
	</scm>

	<distributionManagement>
		<!-- use the following if you're not using a release version. -->
		<repository>
			<id>dsnexus</id>
			<name>
			</name>
			<url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases
			</url>
		</repository>
		<snapshotRepository>
			<id>dsnexus-snapshots</id>
			<name>
			</name>
			<url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots
			</url>
		</snapshotRepository>
	</distributionManagement>

	<properties>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
		<java.version>1.8</java.version>
		<scala.version>2.11.12</scala.version>
		<scala.binary.version>2.11</scala.binary.version>
		<scalatest.version>3.0.3</scalatest.version>
		<spark.version>2.4.5</spark.version>
		<spark.binary.version>2.4.5</spark.binary.version>
		<slf4j.version>1.7.25</slf4j.version>
		<avro.version>1.7.7</avro.version>
		<aws-s3.version>1.11.319</aws-s3.version>
		<dsl.common.api.version>473.216.8</dsl.common.api.version>
		<dsl-version>473.216.16</dsl-version>
		<coherence.version>12.2.1.0.3.1</coherence.version>
		<lombok.version>1.18.10</lombok.version>
		<maven.javadoc.plugin.version>3.2.0</maven.javadoc.plugin.version>
		<app.main.class>com.hsbc.trds.driver</app.main.class>

	</properties>

	<dependencies>

		<dependency>
			<groupId>com.amazonaws</groupId>
			<artifactId>aws-java-sdk-s3</artifactId>
			<version>${aws-s3.version}</version>
			<exclusions>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>log4j-over-slf4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>ch.qos.logback</groupId>
					<artifactId>logback-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>ch.qos.logback</groupId>
					<artifactId>logback-classic</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.binary.version}</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_${scala.binary.version}</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>4.13</version>
			<scope>test</scope>
		</dependency>

		<!-- https://mvnrepository.com/artifact/com.oracle.database.jdbc/ojdbc8 -->
		<dependency>
			<groupId>com.oracle.database.jdbc</groupId>
			<artifactId>ojdbc8</artifactId>
			<version>19.8.0.0</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>


		<dependency>
			<groupId>org.apache.avro</groupId>
			<artifactId>avro</artifactId>
			<version>${avro.version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-aws</artifactId>
			<version>2.10.0</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>2.10.0</version>
			<exclusions>
				<exclusion>
					<artifactId>com.google.guava</artifactId>
					<groupId>guava</groupId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
			<version>2.10.0</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.scalatest</groupId>
			<artifactId>scalatest_${scala.binary.version}</artifactId>
			<version>${scalatest.version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>com.typesafe</groupId>
			<artifactId>config</artifactId>
			<version>1.3.3</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>com.oracle.coherence</groupId>
			<artifactId>coherence</artifactId>
			<version>${coherence.version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.projectlombok</groupId>
			<artifactId>lombok</artifactId>
			<version>${lombok.version}</version>
			<scope>provided</scope>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-beans</artifactId>
			<version>5.2.7.RELEASE</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.jasypt</groupId>
			<artifactId>jasypt</artifactId>
			<version>1.9.3</version>
		</dependency>
		<dependency>
			<groupId>com.ibm.icu</groupId>
			<artifactId>icu4j</artifactId>
			<version>67.1</version>
			<scope>provided</scope>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-context</artifactId>
			<version>5.2.7.RELEASE</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-core</artifactId>
			<version>5.2.7.RELEASE</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-expression</artifactId>
			<version>5.2.7.RELEASE</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>com.hsbc.gbm.dsl</groupId>
			<artifactId>dsl-common-api</artifactId>
			<version>${dsl.common.api.version}</version>
			<exclusions>
				<exclusion>
					<groupId>org.springframework</groupId>
					<artifactId>spring-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.springframework</groupId>
					<artifactId>spring-context</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>


		<dependency>
			<groupId>com.hsbc.gbm.dsl</groupId>
			<artifactId>DSLCore</artifactId>
			<version>${dsl-version}</version>
			<exclusions>
				<exclusion>
					<groupId>*</groupId>
					<artifactId>*</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>com.hsbc.gbm.dsl</groupId>
			<artifactId>DSLDomain</artifactId>
			<version>${dsl-version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>


		<dependency>
			<groupId>com.hsbc.gbm.dsl</groupId>
			<artifactId>DSLRepository</artifactId>
			<version>${dsl-version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>com.hsbc.gbm.dsl</groupId>
			<artifactId>DSLServices</artifactId>
			<version>${dsl-version}</version>
			<exclusions>
				<exclusion>
					<groupId>com.hsbc.efx.uui.microservices</groupId>
					<artifactId>isin-service-client</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>


		<!-- GCP Dependency -->
		<dependency>
			<groupId>com.google.cloud.bigdataoss</groupId>
			<artifactId>gcs-connector</artifactId>
			<version>hadoop2-1.9.17</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<!--parquet writer -->
		<dependency>
			<groupId>org.apache.parquet</groupId>
			<artifactId>parquet-avro</artifactId>
			<version>1.8.1</version>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>com.google.guava</groupId>
			<artifactId>guava</artifactId>
			<version>29.0-jre</version>
		</dependency>

		<dependency>
			<groupId>com.hsbc.efx.uui.microservices</groupId>
			<artifactId>isin-service-client</artifactId>
			<version>1.0.22</version>
		</dependency>


	</dependencies>


	<dependencyManagement>
		<dependencies>
			<dependency>
				<groupId>com.hsbc.efx.uui.microservices</groupId>
				<artifactId>isin-service-client</artifactId>
				<version>1.0.22</version>
			</dependency>
		</dependencies>
	</dependencyManagement>


	<build>
		<plugins>
			<!-- <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId>
				<version>2.4</version> <configuration> <excludes> <exclude>**/*.properties</exclude>
				<exclude>**/*.conf</exclude> <exclude>**/*.sh</exclude> <exclude>**/*.json</exclude>
				<exclude>**/*.avsc</exclude> </excludes> </configuration> </plugin> -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.6.1</version>
				<executions>
					<execution>
						<id>default-compile</id>
						<phase>none</phase>
					</execution>
				</executions>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-javadoc-plugin</artifactId>
				<version>${maven.javadoc.plugin.version}</version>
				<executions>
					<execution>
						<id>attach-javadocs</id>
						<goals>
							<goal>jar</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<failOnError>false</failOnError>
					<doclint>none</doclint>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-release-plugin</artifactId>
				<version>2.5.3</version>
				<configuration>
					<providerImplementations>
						<git>git</git>
					</providerImplementations>
				</configuration>
				<dependencies>
					<dependency>
						<groupId>org.apache.maven.scm</groupId>
						<artifactId>maven-scm-provider-gitexe</artifactId>
						<version>1.9.5</version>
					</dependency>
					<dependency>
						<groupId>org.apache.maven.scm</groupId>
						<artifactId>maven-scm-provider-git-commons</artifactId>
						<version>1.9.5</version>
					</dependency>
					<dependency>
						<groupId>org.apache.maven.release</groupId>
						<artifactId>maven-release-manager</artifactId>
						<version>2.5.3</version>
					</dependency>
				</dependencies>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-scm-plugin</artifactId>
				<version>1.9.5</version>
				<configuration>
					<providerImplementations>
						<git>git</git>
					</providerImplementations>
				</configuration>
				<dependencies>
					<dependency>
						<groupId>org.apache.maven.scm</groupId>
						<artifactId>maven-scm-provider-gitexe</artifactId>
						<version>1.9.5</version>
					</dependency>
					<dependency>
						<groupId>org.apache.maven.scm</groupId>
						<artifactId>maven-scm-provider-git-commons</artifactId>
						<version>1.9.5</version>
					</dependency>
				</dependencies>
			</plugin>
			<plugin>
				<groupId>org.scalatest</groupId>
				<artifactId>scalatest-maven-plugin</artifactId>
				<version>2.0.0</version>
				<configuration>
					<reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
					<junitxml>.</junitxml>
					<filereports>TestSuite.txt</filereports>
				</configuration>
				<executions>
					<execution>
						<id>test</id>
						<goals>

						</goals>
					</execution>
				</executions>
			</plugin>

			<plugin>
				<groupId>net.alchim31.maven</groupId>
				<artifactId>scala-maven-plugin</artifactId>
				<version>3.2.2</version>
				<executions>
					<execution>
						<goals>
							<goal>compile</goal>
							<goal>testCompile</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<recompileMode>incremental</recompileMode>
				</configuration>
			</plugin>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-assembly-plugin</artifactId>
				<version>3.3.0</version>

				<configuration>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
				</configuration>
				<executions>
					<execution>
						<id>assemble-all</id>
						<phase>package</phase>
						<goals>
							<goal>single</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<version>3.2.4</version>
				<configuration>
					<shadedArtifactAttached>false</shadedArtifactAttached>
					<relocations combine.children="append">
						<relocation>
							<pattern>org.apache.avro</pattern>
							<shadedPattern>shaded.org.apache.avro</shadedPattern>
						</relocation>
						<relocation>
							<pattern>com.google.</pattern>
							<shadedPattern>com.shaded_pkg.google.</shadedPattern>
						</relocation>
					</relocations>
					<artifactSet>
						<excludes>
							<exclude>net.sourceforge.saxon:saxon:jar:</exclude>
						</excludes>
					</artifactSet>
					<filters>
						<filter>
							<artifact>*:*</artifact>

							<excludes>
								<exclude>META-INF/*.SF</exclude>
								<exclude>META-INF/*.DSA</exclude>
								<exclude>META-INF/*.RSA</exclude>
								<exclude>net.sourceforge.saxon:saxon</exclude>
								<exclude>**/*.conf</exclude>
							</excludes>

						</filter>
					</filters>
					<transformers>
						<transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
							<manifestEntries>
								<Main-Class>${app.main.class}</Main-Class>
							</manifestEntries>
						</transformer>
					</transformers>
					<createDependencyReducedPom>false</createDependencyReducedPom>
				</configuration>
				<executions>
					<execution>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
					</execution>
				</executions>

			</plugin>


		</plugins>


	</build>
</project>
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.hsbc.trds_main</groupId>
    <artifactId>trds</artifactId>
    <version>1.0.5-SNAPSHOT</version>

    <scm>
        <connection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_PROTOTYPING.git
        </connection>
        <developerConnection>scm:git:git@alm-github.systems.uk.hsbc:CATS/TRDS_PROTOTYPING.git
        </developerConnection>
        <url>https://alm-github.systems.uk.hsbc/CATS/TRDS_PROTOTYPING.git</url>
        <tag>HEAD</tag>
    </scm>

    <distributionManagement>
        <!-- use the following if you're not using a release version. -->
        <repository>
            <id>dsnexus</id>
            <name>
            </name>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/releases
            </url>
        </repository>
        <snapshotRepository>
            <id>dsnexus-snapshots</id>
            <name>
            </name>
            <url>https://dsnexus.uk.hibm.hsbc:8081/nexus/content/repositories/snapshots
            </url>
        </snapshotRepository>
    </distributionManagement>

    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <java.version>1.8</java.version>
        <scala.version>2.11.12</scala.version>
        <scala.binary.version>2.11</scala.binary.version>
        <scalatest.version>3.0.3</scalatest.version>
        <spark.version>2.3.0</spark.version>
        <spark.binary.version>2.3.0</spark.binary.version>
        <slf4j.version>1.7.25</slf4j.version>
        <avro.version>1.7.7</avro.version>
        <aws-s3.version>1.11.319</aws-s3.version>
        <dsl.common.api.version>473.216.8</dsl.common.api.version>
        <dsl-version>473.216.16</dsl-version>
        <coherence.version>12.2.1.0.3.1</coherence.version>
        <lombok.version>1.18.10</lombok.version>
        <app.main.class>com.hsbc.trds.scala.driver</app.main.class>
    </properties>

    <dependencies>

        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk-s3</artifactId>
            <version>${aws-s3.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>log4j-over-slf4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>ch.qos.logback</groupId>
                    <artifactId>logback-core</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>ch.qos.logback</groupId>
                    <artifactId>logback-classic</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13</version>
            <scope>test</scope>
        </dependency>

        <!-- https://mvnrepository.com/artifact/com.oracle.database.jdbc/ojdbc8 -->
        <dependency>
            <groupId>com.oracle.database.jdbc</groupId>
            <artifactId>ojdbc8</artifactId>
            <version>19.8.0.0</version>
        </dependency>


        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>${avro.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-aws</artifactId>
            <version>2.8.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.8.5</version>
            <exclusions>
                <exclusion>
                    <artifactId>com.google.guava</artifactId>
                    <groupId>guava</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.8.5</version>
        </dependency>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <version>${scalatest.version}</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe</groupId>
            <artifactId>config</artifactId>
            <version>1.3.3</version>
        </dependency>
        <dependency>
            <groupId>com.oracle.coherence</groupId>
            <artifactId>coherence</artifactId>
            <version>${coherence.version}</version>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>${lombok.version}</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
            <version>5.2.7.RELEASE</version>
        </dependency>

        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
            <version>5.2.7.RELEASE</version>
        </dependency>

        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-core</artifactId>
            <version>5.2.7.RELEASE</version>
        </dependency>

        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-expression</artifactId>
            <version>5.2.7.RELEASE</version>
        </dependency>

        <dependency>
            <groupId>com.hsbc.gbm.dsl</groupId>
            <artifactId>dsl-common-api</artifactId>
            <version>${dsl.common.api.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.springframework</groupId>
                    <artifactId>spring-core</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.springframework</groupId>
                    <artifactId>spring-context</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.google.guava</groupId>
                    <artifactId>guava</artifactId>
                </exclusion>
            </exclusions>
        </dependency>


        <dependency>
            <groupId>com.hsbc.gbm.dsl</groupId>
            <artifactId>DSLCore</artifactId>
            <version>${dsl-version}</version>
            <exclusions>
                <exclusion>
                    <groupId>*</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>com.hsbc.gbm.dsl</groupId>
            <artifactId>DSLDomain</artifactId>
            <version>${dsl-version}</version>
        </dependency>


        <dependency>
            <groupId>com.hsbc.gbm.dsl</groupId>
            <artifactId>DSLRepository</artifactId>
            <version>${dsl-version}</version>
        </dependency>

        <dependency>
            <groupId>com.hsbc.gbm.dsl</groupId>
            <artifactId>DSLServices</artifactId>
            <version>${dsl-version}</version>
            <!--<exclusions>-->
                <!--<exclusion>-->
                    <!--<groupId>com.hsbc.efx.uui.microservices</groupId>-->
                    <!--<artifactId>isin-service-client</artifactId>-->
                <!--</exclusion>-->
            <!--</exclusions>-->
        </dependency>

        <!-- GCP Dependency-->
        <dependency>
            <groupId>com.google.cloud.bigdataoss</groupId>
            <artifactId>gcs-connector</artifactId>
            <version>hadoop2-1.9.17</version>
        </dependency>

        <!--parquet writer-->
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-avro</artifactId>
            <version>1.8.1</version>
        </dependency>

        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
            <version>29.0-jre</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>2.4</version>
                <configuration>
                    <excludes>
                        <exclude>**/*.properties</exclude>
                        <exclude>**/*.conf</exclude>
                        <exclude>**/*.sh</exclude>
                        <exclude>**/*.json</exclude>
                    </excludes>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.1</version>
                <executions>
                    <execution>
                        <id>default-compile</id>
                        <phase>none</phase>
                    </execution>
                </executions>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <version>2.0.0</version>
                <configuration>
                    <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                    <junitxml>.</junitxml>
                    <filereports>TestSuite.txt</filereports>
                </configuration>
                <executions>
                    <execution>
                        <id>test</id>
                        <goals>

                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <recompileMode>incremental</recompileMode>
                </configuration>
            </plugin>


            <!--

                               <plugin>
                                   <groupId>org.apache.avro</groupId>
                                   <artifactId>avro-maven-plugin</artifactId>
                                   <version>${avro.version}</version>
                                   <executions>
                                       <execution>
                                           <phase>generate-resources</phase>
                                           <goals>
                                               <goal>schema</goal>
                                           </goals>
                                           <configuration>
                                               <sourceDirectory>${project.basedir}/src/main/resources/avro/</sourceDirectory>
                                               <outputDirectory>${project.basedir}/target/generated-sources/</outputDirectory>
                                           </configuration>
                                       </execution>
                                   </executions>
                               </plugin>

-->



            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.2.0</version>
                <configuration>
                    <tarLongFileMode>posix</tarLongFileMode>
                    <descriptors>
                        <descriptor>src/main/assembly/assembly.xml</descriptor>
                    </descriptors>
                </configuration>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <id>make-assembly</id>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

<!--

                      <plugin>
                          <artifactId>maven-assembly-plugin</artifactId>
                          <version>3.3.0</version>
                          <configuration>
                              <descriptorRefs>
                                  <descriptorRef>jar-with-dependencies</descriptorRef>
                              </descriptorRefs>
                          </configuration>
                          <executions>
                              <execution>
                                  <id>assemble-all</id>
                                  <phase>package</phase>
                                  <goals>
                                      <goal>single</goal>
                                  </goals>
                              </execution>
                          </executions>
                      </plugin>





            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <configuration>
                    <relocations>
                        <relocation>
                            <pattern>org.apache.avro</pattern>
                            <shadedPattern>shaded.org.apache.avro</shadedPattern>
                        </relocation>
                        <relocation>
                            <pattern>com.google.guava</pattern>
                            <shadedPattern>shaded.com.google.guava</shadedPattern>
                        </relocation>
                    </relocations>
                    <artifactSet>
                        <excludes>
                            <exclude>net.sourceforge.saxon:saxon:jar:</exclude>
                        </excludes>
                    </artifactSet>
                    <filters>
                        <filter>
                             <artifact>*:*</artifact>

                            <excludes>
                                <exclude>META-INF/*.SF</exclude>
                                <exclude>META-INF/*.DSA</exclude>
                                <exclude>META-INF/*.RSA</exclude>
                                <exclude>net.sourceforge.saxon:saxon</exclude>
                                <exclude>**/*.conf</exclude>
                            </excludes>

                        </filter>
                    </filters>
                    <transformers>
                        <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                            <manifestEntries>
                                <Main-Class>${app.main.class}</Main-Class>
                            </manifestEntries>
                        </transformer>
                    </transformers>
                </configuration>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                    </execution>
                </executions>

            </plugin>

-->

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-release-plugin</artifactId>
                <version>2.5.3</version>
                <configuration>
                    <providerImplementations>
                        <git>git</git>
                    </providerImplementations>
                </configuration>
                <dependencies>
                    <dependency>
                        <groupId>org.apache.maven.scm</groupId>
                        <artifactId>maven-scm-provider-gitexe</artifactId>
                        <version>1.9.5</version>
                    </dependency>
                    <dependency>
                        <groupId>org.apache.maven.scm</groupId>
                        <artifactId>maven-scm-provider-git-commons</artifactId>
                        <version>1.9.5</version>
                    </dependency>
                    <dependency>
                        <groupId>org.apache.maven.release</groupId>
                        <artifactId>maven-release-manager</artifactId>
                        <version>2.5.3</version>
                    </dependency>
                </dependencies>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-scm-plugin</artifactId>
                <version>1.9.5</version>
                <configuration>
                    <providerImplementations>
                        <git>git</git>
                    </providerImplementations>
                </configuration>
                <dependencies>
                    <dependency>
                        <groupId>org.apache.maven.scm</groupId>
                        <artifactId>maven-scm-provider-gitexe</artifactId>
                        <version>1.9.5</version>
                    </dependency>
                    <dependency>
                        <groupId>org.apache.maven.scm</groupId>
                        <artifactId>maven-scm-provider-git-commons</artifactId>
                        <version>1.9.5</version>
                    </dependency>
                </dependencies>
            </plugin>
        </plugins>
    </build>

</project>
# TRDS_PROTOTYPING
For all TRDS related POC and PROTOTYPING work
cat: ./s3spark.txt: input file is output file
cat: ./src: Is a directory
cat: ./src/main: Is a directory
cat: ./src/main/assembly: Is a directory
<assembly
        xmlns="http://maven.apache.org/ASSEMBLY/2.0.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd http://maven.apache.org/ASSEMBLY/2.0.0 ">
    <id>bundle</id>
    <formats>
        <format>tar.gz</format>
    </formats>
    <fileSets>
        <fileSet>
            <directory>${project.build.directory}</directory>
            <outputDirectory>\</outputDirectory>
            <includes>
                <include>*.jar</include>
            </includes>
            <excludes>
                <exclude>**/*.xml</exclude>
                <exclude>**/*.properties</exclude>
                <exclude>**/*.conf</exclude>
                <exclude>**.*.sh</exclude>
            </excludes>
        </fileSet>

        <fileSet>
            <directory>${project.basedir}/src/main/resources</directory>
            <outputDirectory>bin</outputDirectory>
            <includes>
                <include>**/*.sh</include>
            </includes>
            <fileMode>0755</fileMode>
            <directoryMode>0755</directoryMode>
            <lineEnding>unix</lineEnding>
            <filtered>false</filtered>
        </fileSet>
        <fileSet>
            <directory>${project.basedir}/src/main/resources/avro</directory>
            <outputDirectory>schema</outputDirectory>
            <includes>
                <include>**/*.avsc</include>
            </includes>
            <filtered>false</filtered>
        </fileSet>
        <fileSet>
            <directory>${project.basedir}/src/main/resources</directory>
            <outputDirectory>conf</outputDirectory>
            <includes>
                <include>**/*.conf</include>
                <include>**/*.json</include>
                <include>**/*.properties</include>
            </includes>
            <filtered>false</filtered>
        </fileSet>
    </fileSets>
    <dependencySets>
        <dependencySet>
            <outputDirectory>/lib</outputDirectory>
            <unpack>false</unpack>
            <includes>
                <include>com.oracle.database.jdbc:ojdbc8:jar:19.8.0.0</include>
                <include>org.apache.avro:avro:jar:${avro.version}</include>
                <include>org.apache.spark:spark-avro_2.11:jar:2.4.0</include>
                <include>org.apache.avro:avro-mapred:jar:${avro.version}</include>
                <include>org.springframework:spring-beans:jar:5.2.7.RELEASE</include>
                <include>org.springframework:spring-context:jar:5.2.7.RELEASE</include>
                <include>org.springframework:spring-core:jar:5.2.7.RELEASE</include>
                <include>org.springframework:spring-expression:jar:5.2.7.RELEASE</include>
                <include>com.hsbc.gbm.dsl:DSLDomain:jar:473.216.16</include>
                <include>com.hsbc.gbm.dsl:DSLRepository:jar:473.216.16</include>
                <include>com.hsbc.gbm.dsl:DSLServices:jar:473.216.16</include>
                <include>com.hsbc.gbm.dsl:dsl-common-api:jar:473.216.8</include>
                <include>com.hsbc.gbm.dsl:DSLCore:jar:473.216.16</include>
                <include>org.apache.parquet:parquet-avro:jar:1.8.1</include>
                <include>com.oracle.coherence:coherence:jar:12.2.1.0.3.1</include>
                <include>com.google.cloud.bigdataoss:gcs-connector:jar:hadoop2-1.9.17</include>
                <include>com.google.api-client:google-api-client-java6:jar:1.27.0</include>
                <include>com.google.api-client:google-api-client:jar:1.27.0</include>
                <include>com.google.api-client:google-api-client-jackson2:jar:1.27.0</include>
                <include>com.google.http-client:google-http-client-jackson2:jar:1.27.0</include>
                <include>com.google.apis:google-api-services-storage:jar:v1-rev20181109-1.27.0</include>
                <include>com.google.oauth-client:google-oauth-client:jar:1.27.0</include>
                <include>com.google.http-client:google-http-client:jar:1.27.0</include>
                <include>com.google.oauth-client:google-oauth-client-java6:jar:1.27.0</include>
                <include>com.google.cloud.bigdataoss:util:jar:1.9.17</include>
                <include>com.google.auto.value:auto-value-annotations:jar:1.6.3</include>
                <include>com.google.cloud.bigdataoss:util-hadoop:jar:hadoop2-1.9.17</include>
                <include>com.google.cloud.bigdataoss:gcsio:jar:1.9.17</include>
                <include>com.google.flogger:google-extensions:jar:0.3.1</include>
                <include>com.google.flogger:flogger:jar:0.3.1</include>
                <include>com.google.flogger:flogger-system-backend:jar:0.3.1</include>
                <include>com.google.flogger:flogger-log4j-backend:jar:0.3.1</include>
                <include>org.apache.spark:spark-avro_2.11:jar:2.4.0</include>
                <include>com.typesafe:config:jar:1.3.3</include>
            </includes>
        </dependencySet>
    </dependencySets>


</assembly>cat: ./src/main/java: Is a directory
cat: ./src/main/java/com: Is a directory
cat: ./src/main/java/com/hsbc: Is a directory
cat: ./src/main/java/com/hsbc/trds: Is a directory
cat: ./src/main/java/com/hsbc/trds/context: Is a directory
package com.hsbc.trds.context;

import java.io.Serializable;
import java.time.LocalDateTime;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

public class TRDSContext implements Serializable {
    //Made up of TradeId_UUID
    private String partitionTblName;
    private String objectStorageRootLocation;
    private String controlEventIds[];
    private String uuids[];
    private String tradeIds[];
    private String objectTypes[];
    private String partitionColumns[];
    private Map<String,String> extraParameters = new HashMap<>();
    private boolean notInitialized = true;
    private boolean noUuids = true;
    private LocalDateTime startTime;
    public void setPartitionColumns(String[] partitionColumns) {
        this.partitionColumns = partitionColumns;
    }

    private long exaConnectionCount;
    private long exaMaxFetchRows;
    private long sinkRecordCount;

    public long getSinkRecordCount() {
        return sinkRecordCount;
    }

    public void setSinkRecordCount(long sinkRecordCount) {
        this.sinkRecordCount = sinkRecordCount;
    }

    public String[] getUUIDs(){
        return uuids;
    }

    private void unpack(String tradid_uuids) {
        controlEventIds = tradid_uuids.split(",");
        uuids = new String[controlEventIds.length];
        tradeIds = new String[controlEventIds.length];
        for(int c=0; c< controlEventIds.length; c++) {
            String[] unpacked = controlEventIds[c].split("_");
            tradeIds[c] = unpacked[0];
            if(unpacked.length==2) {
                uuids[c] = unpacked[1];
                noUuids = false;
            }
        }
    }
    public LocalDateTime getStartTime() { return startTime; }

    public void setStartTime(LocalDateTime startTime) {
        this.startTime = startTime;
    }
    /**
     * Could be passed in along with the other Exa parameters at some point where we have finished testing max
     * fetch rows allowed.
     * @param exaMaxFetchRows
     * @return
     */
    public TRDSContext setExaMaxFetchRows(long exaMaxFetchRows){
        this.exaMaxFetchRows = exaMaxFetchRows;
        return this;
    }

    public long getExaMaxFetchRows() {
        return exaMaxFetchRows;
    }

    /**
     * Could be passed in along with the other Exa parameters at some point where we have finished testing max
     * connections allowed.
     * @param connectionCount
     * @return
     */
    public TRDSContext setExaConnectionCount(long connectionCount){
        this.exaConnectionCount = connectionCount;
        return this;
    }

    public long getExaConnectionCount() {
        return exaConnectionCount;
    }

    public String getObjectStorageRootLocation() {
        return objectStorageRootLocation;
    }

    public TRDSContext setObjectStorageRootLocation(String objectStorageRootLocation) {
        this.objectStorageRootLocation = objectStorageRootLocation;
        return this;
    }



    public TRDSContext setPartitionTblName(String partitionTblName) {
        this.partitionTblName = partitionTblName;
        return this;
    }

    public String getPartitionTblName() {
        return partitionTblName;
    }

    public String[] getControlEventIds() {
        return controlEventIds;
    }

    public String[] getPartitionColumns() {
        return partitionColumns;
    }

    public String[] getTradeIds(){
        return tradeIds;
    }

    public String[] getObjectTypesInScope() {  return objectTypes; }

    public TRDSContext setControlEventIds(String tradid_uuid) {
        if(notInitialized){
            unpack(tradid_uuid);
            notInitialized = false;

        }else{
            System.out.println("Object:"+this+"  will not overwritten with "+tradid_uuid + " existing value:"+tradid_uuid + " will be retained!");
        }
       return this;
    }



    public boolean noUuIDS() {
        return noUuids;
    }



    /**
     * Sets the ObjectType scope - within which any search will function
     * @param objectTypesInScope
     * @return
     */
    public TRDSContext setObjectTypesInScope(String objectTypesInScope) {
        objectTypes = objectTypesInScope.split(",");
        return  this;
    }

    public Map<String, String> getExtraParameters() {
        return extraParameters;
    }

    public void setExtraParameters(Map<String, String> extraParameters) {
        this.extraParameters = extraParameters;
    }

}
cat: ./src/main/java/com/hsbc/trds/driver: Is a directory
package com.hsbc.trds.driver;

import com.hsbc.trds.*;
import com.hsbc.trds.context.TRDSContext;
import com.typesafe.config.Config;
import org.apache.avro.Schema;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.Properties;

/*
 * POF-NONPOF-ECS  -->
 * POF-ECS
 * NONPOF-ECS
 * NONPOF-GCS
 * POFECS-NONPOFGCS
 */

public interface HCEMigrationSparkDriver {
    boolean registerSQLModel(Dataset<Row> exaData, String tblName);
    Dataset<Row> addTradeIdHash(Dataset<Row> exaData);
    //Dataset<Row> extractExaData(SparkSession spark, TRDSContext context, String query);
    void writePofAndNonPofIntoECS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema, Properties prop, WrittenRecordsSparkListener listener,String env,String encKey);
    void writePofIntoECS(Dataset<Row> data,  TRDSContext context, String filePath, Schema schema,Properties prop, WrittenRecordsSparkListener listener,String env,String encKey);
    void writeNonPofIntoECS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema,Properties prop, WrittenRecordsSparkListener listener,String env,String encKey);
    void writeNonPofIntoGCS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema,Properties prop, WrittenRecordsSparkListener listener,String env,String encKey);
    void writePofIntoECSAndNonPofIntoGCS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema,Properties prop, WrittenRecordsSparkListener listener,String env,String encKey);
}
package com.hsbc.trds.driver;

import com.hsbc.trds.HCEWriteStat;
import com.hsbc.trds.context.TRDSContext;
import com.hsbc.trds.endpoints.EndPointWriter;

import com.hsbc.trds.model.ExaDataModel;
import com.sun.xml.bind.v2.TODO;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Properties;
import java.util.UUID;

import static com.hsbc.trds.endpoints.TRDSConstants.TRADE_ID_HASH;


public class HCEMigrationSparkDriverImpl implements HCEMigrationSparkDriver{

    private static final Logger log = LogManager.getLogger(HCEMigrationSparkDriverImpl.class);

    public static void main(String args[]){
        Properties prop = new Properties();
        try (InputStream input = HCEMigrationSparkDriverImpl.class.getClassLoader().getResourceAsStream("default-props.properties")) {
            if (input == null) {
                System.out.println("Sorry, unable to find config.properties");
                return;
            }

            //load a properties file from class path, inside static method
            prop.load(input);

        } catch (IOException ex) {
            ex.printStackTrace();
        }

        //String fileName = System.getProperty("config.file");
        String environment = args[5];
        String encKey = args[6];
        //Config defaultConfig = ConfigFactory.parseFile(new File(fileName)).resolve();
        //Config defaultConfig =ConfigFactory.systemProperties().withFallback(ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseFile(new File(fileName)).resolve()));

        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        HCEMigrationSparkDriver driver = new HCEMigrationSparkDriverImpl();
        WrittenRecordsSparkListener listener = new WrittenRecordsSparkListener();
        String[] argOptions = {"POF-NONPOF-ECS","POF-ECS","NONPOF-ECS","NONPOF-GCS","POFECS-NONPOFGCS", "NONPOF-GCS-AUG", "ECS-POF-BIN"};

        if (args.length < 7) {
            throw new IllegalArgumentException("Required number of arguments are not passed");
        }

        if (!Arrays.asList(argOptions).contains(args[0])) {
            throw new IllegalArgumentException("The first argument should be in the list of options " +
                    "POF-NONPOF-ECS|POF-ECS|NONPOF-ECS|NONPOF-GCS|NONPOF-GCS-AUG|POFECS-NONPOFGCS|ECS-POF-BIN");
        }
        Schema schema = helper.hceWriteStatAvroSchema();
        TRDSContext context = new TRDSContext();
        helper.fillContext(args,context);
        SparkSession sparkSession = helper.getSparkSession(context,prop,args[0],environment,encKey );
        sparkSession.sparkContext().addSparkListener(listener);
        //TODO - Java tradeIdHash is making the job running very slow. In POC used orahash for partitioning
        // Dataset<Row> df = driver.addTradeIdHash(driver.extractExaData(sparkSession, context));

        log.info("GCP bucket : "+prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME"));
        log.info("ECS bucket : "+prop.getProperty(environment+".ecs.s3bucketName1"));
        log.info("GCP Project : "+prop.getProperty(environment+".gcs.HSBC_11007943_TRDS_DEV"));

        if (args[0].equalsIgnoreCase("POF-NONPOF-ECS") || args[0].equalsIgnoreCase("ECS-POF-BIN")) {
            Dataset<Row> df = helper.getRowDatasetForPofNonPofEcs(context, sparkSession,prop,environment,encKey);
            String filePath = prop.getProperty(environment+".ecs.s3bucketName1").concat("ecs_stats_pof_nonpof/").concat(context.getPartitionTblName()).concat("_"+UUID.randomUUID().toString()+".parquet");
            driver.writePofAndNonPofIntoECS(df,context,filePath,schema,prop, listener,environment,encKey);
        } else if (args[0].equalsIgnoreCase("POF-ECS")) {
            Dataset<Row> df = helper.getRowDatasetForPofEcs(driver, context, sparkSession,prop,environment,encKey);
            String filePath = prop.getProperty(environment+".ecs.s3bucketName1").concat("ecs_stats_pof/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet");
            driver.writePofIntoECS(df,context,filePath,schema,prop, listener,environment,encKey);
        } else if (args[0].equalsIgnoreCase("NONPOF-ECS")) {
            Dataset<Row> df = helper.getRowDatasetForNonPofEcs(driver, context, sparkSession,prop,environment,encKey);
            String filePath = prop.getProperty(environment+".ecs.s3bucketName1").concat("ecs_stats_pof/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet");
            driver.writeNonPofIntoECS(df,context,filePath,schema,prop, listener,environment,encKey);
        }
        else if (args[0].equalsIgnoreCase(("NONPOF-GCS"))){
            Dataset<Row> df = helper.getRowDatasetForNonPofGcs(driver, context, sparkSession,prop,environment,encKey);
            String filePath = prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME").concat("gcs_stats_pof_1/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet");
            driver.writeNonPofIntoGCS(df,context,filePath,schema,prop,listener,environment,encKey);
        }
        else if (args[0].equalsIgnoreCase(("NONPOF-GCS-AUG"))){
            Dataset<Row> df = helper.getRowDatasetForNonPofGcsAug(driver, context, sparkSession,prop,environment,encKey);
            String filePath = prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME").concat("gcs_stats_pof_1/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet");
            driver.writeNonPofIntoGCS(df,context,filePath,schema,prop, listener,environment,encKey);
        }

    }

    @Override
    public boolean registerSQLModel(Dataset<Row> exaData, String tblName) {
        log.debug("Registering S sql view!");
        exaData.createOrReplaceTempView(tblName);
        return true;
    }

    @Override
    public Dataset<Row> addTradeIdHash(Dataset<Row> exaData) {
        log.debug("Adding hash!");
        String partitionTbl = "partitionTbl";
        registerSQLModel(exaData, partitionTbl);
        String sqlQuery = ExaDataModel.SELECT_HCE_MIG_COLUMLIST +
                ", DOCPATH" +
                ", abs(mod(HASH(trade_id),100)) as " + TRADE_ID_HASH +
                ", PARTITION_ID" +
                " from " + partitionTbl;

        return exaData.sparkSession().sql(sqlQuery);//.repartition(10);
    }

//
//    public Dataset<Row> extractExaData(SparkSession spark, TRDSContext context, String query) {
//        log.debug("Getting data from Exa!");
//        return ExaReader.getHistoricControlEventRowDatasetForPartition(spark, context);
//    }

    @Override
    public void writePofAndNonPofIntoECS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema,Properties prop,WrittenRecordsSparkListener listener,String env,String encKey) {
       // data.cache();
        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        EndPointWriter.write(data, EndPointWriter.ECS,context,prop,env);
        //TODO: Confirm with Suhas if 2 level partitions are needed for POF Binary as well
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        GenericData.Record ecsStasRecord = helper.setECSInitialStats(context,schema, prop,env,encKey);
        //ecsStasRecord.put("srcRecordCount",data.count());
        //String filePath = ECSConfig.s3bucketName1.concat(fileName);
        helper.writeToParquet(ecsStasRecord,schema,filePath,data.sparkSession().sparkContext().hadoopConfiguration());
    }

    @Override
    public void writePofIntoECS(Dataset<Row> data,TRDSContext context, String filePath, Schema schema,Properties prop,WrittenRecordsSparkListener listener,String env,String encKey) {
       // data.cache();
        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        //Removing the partitions.
        //TODO: Confirm with Suhas if 2 level partitions are needed for POF Binary as well
        EndPointWriter.write(data, EndPointWriter.ECS,context,prop,env);
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        GenericData.Record ecsStasRecord = helper.setECSInitialStats(context,schema, prop,env,encKey);
        //ecsStasRecord.put("srcRecordCount",data.count());
        helper.writeToParquet(ecsStasRecord,schema,filePath,data.sparkSession().sparkContext().hadoopConfiguration());
    }

    @Override
    public void writeNonPofIntoECS(Dataset<Row> data, TRDSContext context, String filePath, Schema schema,Properties prop,WrittenRecordsSparkListener listener,String env,String encKey) {
       // data.cache();
        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        EndPointWriter.write(data, context.getPartitionColumns(), EndPointWriter.ECS,context,prop,env);
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        GenericData.Record ecsStasRecord = helper.setECSInitialStats(context,schema, prop,env,encKey);
        //ecsStasRecord.put("srcRecordCount",data.count());
        helper.writeToParquet(ecsStasRecord,schema,filePath,data.sparkSession().sparkContext().hadoopConfiguration());
    }

    @Override
    public void writeNonPofIntoGCS(Dataset<Row> data,TRDSContext context, String filePath, Schema schema,Properties prop,WrittenRecordsSparkListener listener,String env,String encKey) {
       // data.cache();
        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        EndPointWriter.write(data, context.getPartitionColumns(), EndPointWriter.G_CS,context,prop,env);
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        GenericData.Record gcsStasRecord = helper.setGCSInitialStats(context,schema, prop,env,encKey);
        //gcsStasRecord.put("srcRecordCount",data.count());
        helper.writeToParquet(gcsStasRecord,schema,filePath,data.sparkSession().sparkContext().hadoopConfiguration());
    }

    @Override
    public void writePofIntoECSAndNonPofIntoGCS(Dataset<Row> data,TRDSContext context, String fileName, Schema schema,Properties prop,WrittenRecordsSparkListener listener,String env,String encKey) {
      //  data.cache();
        String environment = env;
        HCEMigrationSparkHelper helper = new HCEMigrationSparkHelper();
        EndPointWriter.write(data, context.getPartitionColumns(), EndPointWriter.ECS,context,prop,env);
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        String ecsFilePath = prop.getProperty(environment+".ecs.s3bucketName1").concat(context.getPartitionTblName()).concat("/ecs_stats_pof"+ fileName);
        GenericData.Record ecsStasRecord = helper.setECSInitialStats(context,schema, prop,env,encKey);
        //ecsStasRecord.put("srcRecordCount",data.count());
        //ecswrite
        helper.writeToParquet(ecsStasRecord,schema,ecsFilePath,data.sparkSession().sparkContext().hadoopConfiguration());


        EndPointWriter.write(data, context.getPartitionColumns(), EndPointWriter.G_CS,context,prop,env);
        context.setSinkRecordCount(listener.getOutputRecordsWritten());
        String gcsFilePath = prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME").concat(context.getPartitionTblName()).concat("/gcs_stats_nonpof"+ fileName);
        GenericData.Record gcsStasRecord = helper.setGCSInitialStats(context,schema, prop,env,encKey);
        gcsStasRecord.put("srcRecordCount",data.count());
        //gcswrite
        helper.writeToParquet(gcsStasRecord,schema,gcsFilePath,data.sparkSession().sparkContext().hadoopConfiguration());
    }
}
package com.hsbc.trds.driver;

import com.hsbc.gbm.dsl.domain.ControlEvent;
import com.hsbc.trds.context.TRDSContext;
import com.hsbc.trds.endpoints.TRDSConstants;

import com.hsbc.trds.endpoints.ExaReader;
import com.hsbc.trds.endpoints.exa.pof.converter.PofBytesConverterInput;
import com.hsbc.trds.endpoints.exa.pof.converter.PofBytesToControlEventConverter;

import com.hsbc.trds.model.ExaDataModel;
import com.typesafe.config.Config;
import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.generic.GenericData;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.expressions.UserDefinedFunction;
import org.jasypt.encryption.pbe.StandardPBEStringEncryptor;

import java.io.IOException;
import java.io.Serializable;
import java.net.URI;
import java.net.URISyntaxException;
import java.time.LocalDateTime;
import java.util.Properties;

import static com.hsbc.trds.endpoints.TRDSConstants.*;

import static com.hsbc.trds.endpoints.ExaReader.getConnectionProperties;
import static com.hsbc.trds.model.ExaDataModel.DSL_OBJECT_TYPE;
import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.udf;
import static org.apache.spark.sql.types.DataTypes.StringType;

public class HCEMigrationSparkHelper implements Serializable {

    private static final Logger log = LogManager.getLogger(HCEMigrationSparkHelper.class);

    public void writeToParquet(GenericData.Record record, Schema schema, String filePath, Configuration conf) {
        ParquetWriter<GenericData.Record> writer = null;
        try{
            URI uri = new URI(filePath);
            Path path = new Path(uri);
            writer  = AvroParquetWriter.<GenericData.Record>builder(path).withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withSchema(schema).withConf(conf).build();
            writer.write(record);
            writer.close();
        } catch(URISyntaxException uriSyntaxexception) {
            log.error("URISyntax exception: " + uriSyntaxexception.getMessage());
        } catch(IOException ioException) {
            log.error("Parquet Writer IoException: " + ioException.getMessage());
        }

        // Path path = new Path("s3a://uk-dsl-ss-1-s3-nonprod/"+ "ecs_stats_2.parquet");
    }

    public Schema hceWriteStatAvroSchema() {
        return SchemaBuilder.record("HCEWriteStat").fields()
                .name("partitionTableName").type().nullable().stringType().noDefault()
                .name("startTime").type().nullable().stringType().noDefault()
                .name("endTime").type().nullable().stringType().noDefault()
                .name("objectStorageFootPrintMB").type().nullable().floatType().floatDefault(0.0f)
                .name("objectStorageRootLocation").type().nullable().stringType().noDefault()
                .name("bucket").type().nullable().stringType().noDefault()
                .name("objectStorageEndpoint").type().nullable().stringType().noDefault()
                .name("exaEndPointUrl").type().nullable().stringType().noDefault()
                .name("exaParallelConnections").type().nullable().longType().longDefault(0l)
                .name("exaMaxRowsPerConnection").type().nullable().longType().longDefault(0l)
                .name("extraParameters").type().nullable().map().values().stringType().noDefault()
                .name("srcRecordCount").type().nullable().longType().longDefault(0l)
                .name("sinkRecordCount").type().nullable().longType().longDefault(0l)
                .endRecord();
    }

    public GenericData.Record setECSInitialStats(TRDSContext context, Schema schema, Properties prop,String env,String encKey) {
        String environment = env;
        GenericData.Record ecsStatRecord = new GenericData.Record(schema);

        ecsStatRecord.put("partitionTableName", context.getPartitionTblName());
        ecsStatRecord.put("startTime",context.getStartTime().toString());
        ecsStatRecord.put("endTime", LocalDateTime.now().toString());
        ecsStatRecord.put("objectStorageRootLocation", context.getObjectStorageRootLocation());
        ecsStatRecord.put("objectStorageEndpoint", prop.getProperty(environment+".ecs.host1"));
        ecsStatRecord.put("bucket", prop.getProperty(environment+".ecs.bucketName1"));
        ecsStatRecord.put("exaEndPointUrl", getConnectionProperties(env,encKey).getProperty("url"));
        ecsStatRecord.put("exaParallelConnections", context.getExaConnectionCount());
        ecsStatRecord.put("exaMaxRowsPerConnection", context.getExaMaxFetchRows());
        ecsStatRecord.put("extraParameters", context.getExtraParameters());
        ecsStatRecord.put("sinkRecordCount",context.getSinkRecordCount());
        return ecsStatRecord;

    }

    public GenericData.Record  setGCSInitialStats(TRDSContext context, Schema schema, Properties prop,String env,String encKey) {
        GenericData.Record gcsStatRecord = new GenericData.Record(schema);
        String environment = env;
        gcsStatRecord.put("partitionTableName", context.getPartitionTblName());
        gcsStatRecord.put("startTime",context.getStartTime().toString());
        gcsStatRecord.put("endTime", LocalDateTime.now().toString());
        gcsStatRecord.put("objectStorageRootLocation", context.getObjectStorageRootLocation());
        gcsStatRecord.put("objectStorageEndpoint", prop.getProperty(environment+".gcs.HSBC_11007943_TRDS_DEV"));
        gcsStatRecord.put("bucket", prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME"));
        gcsStatRecord.put("exaEndPointUrl", getConnectionProperties(env,encKey).getProperty("url"));
        gcsStatRecord.put("exaParallelConnections", context.getExaConnectionCount());
        gcsStatRecord.put("exaMaxRowsPerConnection", context.getExaMaxFetchRows());
        gcsStatRecord.put("extraParameters", context.getExtraParameters());
        gcsStatRecord.put("sinkRecordCount",context.getSinkRecordCount());
        return gcsStatRecord;
    }

    public Dataset<Row> getRowDatasetForNonPofGcsAug(HCEMigrationSparkDriver driver, TRDSContext context, SparkSession sparkSession,Properties prop,String env,String encKey) {
        String environment = env;
        String query = prop.getProperty(environment+".exa.query_NONPOF_GCS_AUG").replace("HCE_PARTITION_NAME",context.getPartitionTblName());
        Dataset<Row> udfDataSet = getGcsRowAugDataset(driver, context, sparkSession, query,env,encKey);
        log.debug("non-pof-gcs-aug-schema");
        udfDataSet.printSchema();
        return udfDataSet;
    }

    public Dataset<Row> getRowDatasetForNonPofGcs(HCEMigrationSparkDriver driver, TRDSContext context, SparkSession sparkSession,Properties prop,String env,String encKey) {
        String environment = env;
        String query = prop.getProperty(environment+".exa.query_NONPOF_GCS").replace("HCE_PARTITION_NAME",context.getPartitionTblName());
        Dataset<Row> udfDataSet = getRowDataset(context, sparkSession, query, "non-pof-gcs-schema : ",env,encKey);
        return udfDataSet;
    }


    public Dataset<Row> getRowDatasetForNonPofEcs(HCEMigrationSparkDriver driver, TRDSContext context, SparkSession sparkSession,Properties prop,String env,String encKey) {
        String environment = env;
        String query = prop.getProperty(environment+".exa.query_NONPOF_ECS").replace("HCE_PARTITION_NAME",context.getPartitionTblName());
        log.debug("exa-nonpof-ecs-query: " + query);
        Dataset<Row> udfDataSet = getRowDataset(context, sparkSession, query, "non-pof-ecs-schema : ",env,encKey);
        return udfDataSet;


    }

    public ControlEvent extractPof(String uuid, byte[] pofBinary){
        ControlEvent event = null;
        try {
            PofBytesToControlEventConverter converter = new PofBytesToControlEventConverter();
            PofBytesConverterInput input = new PofBytesConverterInput(uuid, pofBinary);
            event = converter.convert(input);
            return event;
        }catch(IOException ex) {
            log.error("Exception while parsing controlEvent: " + ex.getMessage());
        }
        return event;
    }

    private Dataset<Row> getGcsRowAugDataset(HCEMigrationSparkDriver driver, TRDSContext context, SparkSession sparkSession, String query,String env,String encKey) {
        Dataset<Row> df = extractExaData(sparkSession, context, query,env,encKey);
        UserDefinedFunction controlEvent = udf((String uuid, String tradeId) -> uuid.concat("_").concat(tradeId), StringType);
        UserDefinedFunction createOn = udf((String uuid, byte[] pofBinary) -> extractPof(uuid,pofBinary).getCreatedOn().toString(), StringType);
        UserDefinedFunction productType = udf((String uuid, byte[] pofBinary) -> extractPof(uuid,pofBinary).getEventParameters().getParameter("productType"), StringType);
        UserDefinedFunction assetClass = udf((String uuid, byte[] pofBinary) -> extractPof(uuid,pofBinary).getEventParameters().getAssetClass(), StringType);
        UserDefinedFunction externalId = udf((String uuid, byte[] pofBinary) -> extractPof(uuid,pofBinary).getEventParameters().getExternalId(), StringType);

        sparkSession.udf().register("controlEvent", controlEvent);
        sparkSession.udf().register("createdOn", createOn);
        sparkSession.udf().register("productType", productType);
        sparkSession.udf().register("assetClass", assetClass);
        sparkSession.udf().register("externalId", externalId);

        Dataset<Row> udfDataSet = df.withColumn("ControlEventID",controlEvent.apply(col("UUID"),col("TRADE_ID")))
                .withColumn("CreatedOn",createOn.apply(col("UUID"),col("POF_BINARY")))
                .withColumn("ProductType", productType.apply(col("UUID"),col("POF_BINARY")))
                .withColumn("AssetClass", assetClass.apply(col("UUID"),col("POF_BINARY")))
                .withColumn("ExternalId", externalId.apply(col("UUID"),col("POF_BINARY")));

        return udfDataSet;
    }

    private Dataset<Row> getRowDataset(TRDSContext context, SparkSession sparkSession, String query, String s,String env,String encKey) {
        Dataset<Row> df = extractExaData(sparkSession, context, query,env,encKey);
        UserDefinedFunction controlEvent = udf((String uuid, String tradeId) -> uuid.concat("_").concat(tradeId), StringType);
        sparkSession.udf().register("ControlEvent", controlEvent);
        log.debug(s);
        df.printSchema();
        return df.withColumn("ControlEventId", controlEvent.apply(col("UUID"), col("TRADE_ID")));
    }


    public Dataset<Row> extractExaData(SparkSession spark, TRDSContext context, String query,String env,String envKey) {
        log.debug("Getting data from Exa!");

        return ExaReader.getHistoricControlEventRowDatasetForPartition(spark, context, query,env,envKey);
    }


    public Dataset<Row> getRowDatasetForPofEcs(HCEMigrationSparkDriver driver, TRDSContext context, SparkSession sparkSession,Properties prop,String env,String encKey) {
        String environment = env;
        String query = prop.getProperty(environment+".exa.query_POF_ECS").replace("HCE_PARTITION_NAME",context.getPartitionTblName());
        log.debug("exa query: " + query);
        Dataset<Row> udfDataSet = getRowDataset(context, sparkSession, query, "pofEcs-schema: ",env, encKey);
        return udfDataSet;
    }

    public Dataset<Row> getRowDatasetForPofNonPofEcs(TRDSContext context, SparkSession sparkSession,Properties prop,String env,String encKey) {
        String environment = env;
        String query = prop.getProperty(environment+".exa.query_POFNONPOF_ECS").replace("HCE_PARTITION_NAME",context.getPartitionTblName());
        log.debug("exa query: " + query);
        Dataset<Row> udfDataSet = getRowDataset(context, sparkSession, query, "pof-non-pof-ecs-schema : ",env,encKey);
        return udfDataSet;
    }

    public SparkSession getSparkSession(TRDSContext context, Properties prop, String writeTypeDestination,String env,String encKey) {
        SparkConf sparkConf = new SparkConf()
                .setAppName("trds-proto_exa_"+writeTypeDestination+"_" + context.getPartitionTblName() + "->" + context.getObjectStorageRootLocation())
                .setMaster("yarn");

        SparkSession sparkSession = SparkSession.builder().config(sparkConf).getOrCreate();
        StandardPBEStringEncryptor encryptor = new StandardPBEStringEncryptor();
        encryptor.setPassword(encKey);
        encryptor.setAlgorithm(TRDSConstants.ENCRYPTION_ALGORITHM);
        primeSparkSessionForECSConnection(sparkSession,prop,env,encryptor); //ecs connection details
        primeSparkSessionForGCPConnection(sparkSession,prop,env,encryptor); //gcs connection details
        return sparkSession;
    }

    public void primeSparkSessionForECSConnection(SparkSession spark, Properties prop,String env,StandardPBEStringEncryptor encryptor) {
        String environment = env;
        // ECS S3 key
        spark.sparkContext().hadoopConfiguration().set("fs.s3a.access.key", prop.getProperty(environment+".ecs.uid1"));
        // Secret key
        spark.sparkContext().hadoopConfiguration().set("fs.s3a.secret.key", encryptor.decrypt(prop.getProperty(environment+".ecs.secret1")));
        // end point
        spark.sparkContext().hadoopConfiguration().set("fs.s3a.endpoint", prop.getProperty(environment+".ecs.host1"));
        //Style Access
        spark.sparkContext().hadoopConfiguration().set("fs.s3a.path.style.access", "true");
    }
    public static void primeSparkSessionForGCPConnection(SparkSession spark, Properties prop,String env,StandardPBEStringEncryptor encryptor) {
        String environment = env;
        spark.sparkContext().hadoopConfiguration().set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem");
        spark.sparkContext().hadoopConfiguration().set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS");
        spark.sparkContext().hadoopConfiguration().set("fs.gs.project.id", prop.getProperty(environment+".gcs.HSBC_11007943_TRDS_DEV"));
        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.enable", "true");
        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.email",prop.getProperty(environment+".gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL"));
        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key.id", prop.getProperty(environment+".gcs.GCP_GCS_PRIVATE_KEY_ID"));
        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key",encryptor.decrypt(prop.getProperty(environment+".gcs.GCP_GCS_SECRET_KEY")));
        spark.sparkContext().hadoopConfiguration().set("fs.gs.proxy.address",prop.getProperty(environment+".gcs.GCP_GCS_PROXY_ADDRESS"));


//        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key.id", "e3ae9cc24c214657eba0d8e703f39924a1334453");
//        spark.sparkContext().hadoopConfiguration().set("fs.gs.auth.service.account.private.key", "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC1EZGny943tKFy\nyM823QE7pR//5BEPexdWw6fMqXwNH+qzoL84iegTFmjbOFCy8lIQ+h02zMSSUDR0\nmYYGoR/SqYCH2LPVkSRtMxBDuzmhRGRap1Q+3I/IlSiAJK9cUdJTeVfFVUncDAMZ\ndGgGCPXctcG5mAp8qBPXRBBGZgpahHxXpIiYp2yCDdH98pQ2P/ol94php9D7KdyV\n224bEDsRo/RJF07zKpib44l00Iw4eO3acLWdBRAMDzDQAp3xe2yC2bGHBgXbkqd5\nwNfF97lDlsYbDJtXSp/KmtvAMLuSeWzcGZjsWZL1Q6WQ/VeMC5cdSaPrxPVwzU/3\nWeSQFObxAgMBAAECggEAEcZXP5seReaoAK4g1Vhtsg68LPEVEal6OYy8cLVklZ8/\n+YE6tFCpZRJKmdaV6XMrRjpfWs1QMLLJ63ZUt1ri1lnIYc2OndUBN+0O2C9NcXzW\nwyVw+jjJrp2h265+PnhTxtj/GkKMnRraxQlt2sXPaOq4yUDZKrKPPA8U7+w9cPQt\nNrKyFUNTnNnUzQjud9rINuQBl4pF7PRR+xZSn85aGoNsyu3GlBvVFFObgP0OXJlB\n/xFY5R+Ce8pxftuDjS3ywd8aaUDW3UHA5WdVtB9lC7rJHd8WXc91IQGKRcoKI80I\nhAiEMguPqi3SSD4ZMZaxzYZmss8HYoPSRguNTTSTLwKBgQDhEtWxKJrDPvlpj1J4\nr2U5/h8H4YpRewv44MkNsWyV/MIz9IdWN2vEpd93KZ7ZK0bhEPaJSN1xo+aTdHbz\nJ51Nqiy7jjXllUi0PmZTE6CxsUvvCHSmLK6582JR5O89XZCwkMORHN0KSRLClMk6\ne6S11HvspNQHWUmZuGjbf1E8CwKBgQDN8tIc+bFN6b/iYREs8bSHJasWlV6PEeJI\nuOZMgvzNQbTBK/jcvpgVUARvs3IwF0IP9Rhd2cd1NeJ9gyNRJmc+yVJS7luSSzz9\n6OBGIDiK2/kxkphs+73Iaor+6Rxewfa/33WdtBRj/rgzLs2xM4CrW53v5HKVag3g\njL3FF0uKcwKBgGmvA2JFU648QrbbUnlSunBpr75unBk4ct2xBxcD6Z3f6nyk5GuK\naHMVBAIbK/iDRqvl4C2EZl62/Eze0f/I6ScPsiN7WjlsSJBCAKAfxkPJoYMi8Esb\nDgxIiTE1E1U4Ovl9cCcSa4Qp3cI6RObOKgArPulWWCz/Mv0YRzxR4x4TAoGAa40g\nfzNCjc7Bf4ZzgsYjIeThCSUuQYb0ZkfxNQm+3a4vqCW/jAAYyiCEgJT0z/qFbHHx\nrKlbiXF/e05ttiZZCqf2TwrdVXPnQS5JWTXgcVvZMjM9WMTK4owJVIAGadHfe2pG\nMPpnH6VZKpmJn72mt1ZcD3h9AlxlLNwzCmdU8tMCgYEArETEkmQ8pQrvn0m/wlLL\n78UoZvGoi5PJqvJLX9durV6qlDW7nqMr6tKCQgqt7mKKrG5HDpZAwrJHFuwQlBiD\nvN4fQBK9+ntONWWGFGT8S9VCGuIkcsquoVFG4djmDEokFb7gvkignJSp7A78UsXa\nxcQVRk3F13mcBA1miwG4B8k=\n-----END PRIVATE KEY-----\n");
//        spark.sparkContext().hadoopConfiguration().set("fs.gs.proxy.address","googleapis-dev.gcp.cloud.uk.hsbc:3128");
    }

    public void fillContext(String[] args, TRDSContext context) {
        String writePartColumns[] = new String[]{DSL_OBJECT_TYPE, ExaDataModel.TRADE_ID_HASH};
        context.setPartitionColumns(writePartColumns);
        context.setPartitionTblName(args[1])
                .setObjectStorageRootLocation(args[2]+"/"+args[1]+"/")
                .setExaConnectionCount(Long.parseLong(args[3]))
                .setExaMaxFetchRows(Long.parseLong(args[4]));
        context.setStartTime(LocalDateTime.now());
    }
}
package com.hsbc.trds.driver;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.scheduler.SparkListener;
import org.apache.spark.scheduler.SparkListenerStageCompleted;

public class WrittenRecordsSparkListener extends SparkListener {

    private static final Logger log = LogManager.getLogger(WrittenRecordsSparkListener.class);
    public long getOutputRecordsWritten() {
        return outputRecordsWritten;
    }

    public void setOutputRecordsWritten(long outputRecordsWritten) {
        this.outputRecordsWritten = outputRecordsWritten;
    }

    long outputRecordsWritten;

    @Override
    public void onStageCompleted(SparkListenerStageCompleted stageCompleted){

         long outputRecordsWritten = stageCompleted.stageInfo().taskMetrics().outputMetrics().recordsWritten();
         log.info("Stage completed with " +outputRecordsWritten+" written");
         setOutputRecordsWritten(outputRecordsWritten);
    }
}
cat: ./src/main/java/com/hsbc/trds/endpoints: Is a directory
package com.hsbc.trds.endpoints;

public class ECSConfig {
    public static String uid1 = "uk-dsl-ss-1-s3-nonprod-u";
    public static String host1 = "http://ecs-storage.it.global.hsbc:9020";
    public static String secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0";
    public static String bucketName1 = "uk-dsl-ss-1-s3-nonprod";
    public static String s3bucketName1 = "s3a://" + bucketName1 + "/";
}package com.hsbc.trds.endpoints;

import com.hsbc.trds.context.TRDSContext;
import com.typesafe.config.Config;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;

import java.util.Arrays;
import java.util.Properties;

public class EndPointWriter {
    private static final Logger log = LogManager.getLogger(EndPointWriter.class);
    public final  static String ECS="ECG", G_CS ="G_CS" , GCS_HDFS ="GCS_HDFS";

    public static void write(Dataset<Row> df, String[] writePartColumns, String endPt, TRDSContext context, Properties prop,String env) {
        String endPtURL = getEndPointUrl(endPt,prop,env)
                .concat(context.getObjectStorageRootLocation());
                //.concat("/")
                //.concat(context.getObjectStorageRootLocation());

        log.debug("endptUrl:"+endPtURL);
        if(Arrays.asList(df.columns()).contains(TRDSConstants.POF_BINARY))
        {
            log.info("Dropping POF_BINARY column");
            df=df.drop(TRDSConstants.POF_BINARY);
        }
        df.write()
                /*.option("fs.s3a.committer.name", "partitioned")
                .option("fs.s3a.committer.staging.conflict-mode", "replace")*/
                .partitionBy(writePartColumns).mode(SaveMode.Overwrite)
                .parquet(endPtURL); //.concat("/").concat(context.getObjectStorageRootLocation()));
                //.parquet(endPtURL + context.getObjectStorageRootLocation());
        //data.write().partitionBy(partitionCols).mode(SaveMode.Append).parquet("s3a://"+ bucketName1 +"/"+ hce_migration_root_parquet_folder);
    }

    public static void write(Dataset<Row> df, String endPt, TRDSContext context, Properties prop,String env) {
        String endPtURL = getEndPointUrl(endPt,prop,env)
                .concat(context.getObjectStorageRootLocation());
        //.concat("/")
        //.concat(context.getObjectStorageRootLocation());

        log.debug("endptUrl:"+endPtURL);
        df.write()
                /*.option("fs.s3a.committer.name", "partitioned")
                .option("fs.s3a.committer.staging.conflict-mode", "replace")*/
                //.partitionBy(writePartColumns)
                .option(TRDSConstants.COMPRESSION,TRDSConstants.GZIP)
                .mode(SaveMode.Overwrite)
                .parquet(endPtURL); //.concat("/").concat(context.getObjectStorageRootLocation()));
        //.parquet(endPtURL + context.getObjectStorageRootLocation());
        //data.write().partitionBy(partitionCols).mode(SaveMode.Append).parquet("s3a://"+ bucketName1 +"/"+ hce_migration_root_parquet_folder);
    }


    private static String getEndPointUrl(String endPt, Properties prop,String env) {
        String environment = env;
        switch (endPt) {
            case ECS:
                return prop.getProperty(environment+".ecs.s3bucketName1");
            case G_CS:
                return prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME");
        }
        return null;
    }
}
cat: ./src/main/java/com/hsbc/trds/endpoints/exa: Is a directory
cat: ./src/main/java/com/hsbc/trds/endpoints/exa/pof: Is a directory
cat: ./src/main/java/com/hsbc/trds/endpoints/exa/pof/config: Is a directory
package com.hsbc.trds.endpoints.exa.pof.config;

import com.tangosol.io.ByteArrayReadBuffer;
import com.tangosol.io.Serializer;
import com.tangosol.io.pof.ConfigurablePofContext;
import java.io.IOException;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;


public class PofSerializerConfiguration {
    private static final String POF_CONFIG_FILENAME = "coherence/loader-pof-config.xml";


    public static Serializer pofSerializer()
            throws IOException {

        System.setProperty("coherence.log", "slf4j");
        System.setProperty("coherence.wka", "127.0.0.1");
        System.setProperty("coherence.ttl", "0");
        System.setProperty("coherence.tcmp.enabled", "false");

        Serializer serializer = new ConfigurablePofContext(POF_CONFIG_FILENAME);
        serializer.deserialize(new ByteArrayReadBuffer("this-will-eagerly-bootstrap-serializer".getBytes()).getBufferInput());

        return serializer;
    }
}
cat: ./src/main/java/com/hsbc/trds/endpoints/exa/pof/converter: Is a directory
package com.hsbc.trds.endpoints.exa.pof.converter;

import lombok.Getter;
import lombok.RequiredArgsConstructor;

//@RequiredArgsConstructor
//@Getter
public class PofBytesConverterInput {
    private String uuid;
    private byte[] bytes;

//    public void setUuid(String uuid) {
//        this.uuid = uuid;
//    }
//
//    public void setBytes(byte[] bytes) {
//        this.bytes = bytes;
//    }

    public String getUuid() {
        return uuid;
    }

    public byte[] getBytes() {
        return bytes;
    }

    public PofBytesConverterInput(String uuid, byte[] bytes) {
        this.uuid = uuid;
        this.bytes = bytes;
    }
}
package com.hsbc.trds.endpoints.exa.pof.converter;

import com.hsbc.gbm.dsl.domain.ControlEvent;
import com.hsbc.trds.endpoints.exa.pof.config.PofSerializerConfiguration;
import com.tangosol.io.ByteArrayReadBuffer;
import com.tangosol.io.Serializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.Serializable;
import java.util.Objects;

import static java.lang.String.format;


public class PofBytesToControlEventConverter implements Serializable {

    private static final Logger LOGGER = LoggerFactory.getLogger(PofBytesToControlEventConverter.class);
    private Serializer serializer;

    public PofBytesToControlEventConverter() throws IOException {
        serializer = PofSerializerConfiguration.pofSerializer();
    }


    public ControlEvent convert(PofBytesConverterInput input) {
        String uuid = input.getUuid();
        byte[] bytes = input.getBytes();

        if (bytes == null || bytes.length == 0) {
            String message = format("No bytes - cannot deserialize POF bytes for uuid: '%s'", uuid);
            LOGGER.error(message);

            throw new IllegalStateException(message);
        }

        try {
            ControlEvent controlEvent = (ControlEvent) serializer.deserialize(
                    new ByteArrayReadBuffer(Objects.requireNonNull(bytes)).getBufferInput());
            return controlEvent;
        } catch (IOException e) {
            String message = format("Cannot deserialize POF bytes for uuid: '%s'.  Exception: %s", uuid, e);
            LOGGER.error(message, e);

            throw new IllegalStateException(message, e);
        }
    }


}
package com.hsbc.trds.endpoints;

import com.hsbc.trds.context.TRDSContext;
import com.hsbc.trds.driver.HCEMigrationSparkDriverImpl;
import com.typesafe.config.ConfigFactory;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.sql.DataFrameReader;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import com.typesafe.config.Config;
import org.jasypt.encryption.pbe.StandardPBEStringEncryptor;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.util.Properties;

public class ExaReader {

    private static final Logger log = LogManager.getLogger(ExaReader.class);

    public static String historicArchiveTblName;//= "P_20200523";
    public static String tradeIdHashColumnName = "tradeIdHash";
    public static long exaConnectionCount = 10; //default
    public static long exaMaxFetchRows = 100;//default

    //normal jdbc
    public static Connection getConnection() throws SQLException {
        return DriverManager.getConnection("jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSLXDS_003.systems.uk.hsbc", "DSL_PROD", "S4nC4rl0");
    }
    public static void closeConnection(Connection con){
        try {
            if(con!=null && !con.isClosed())
                con.close();
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }


    public static Dataset<Row> getHistoricControlEventRowDataset(SparkSession spark, Properties connectionProps, String tblName) {
        DataFrameReader d = getExaDataFrameReader(spark, connectionProps);
        //'dbtable' or 'query'
        return d.option("dbtable", tblName).load();
    }


    /**
     *@Todo To Suneel: please move these options out into a prop/conf file and pass it as a -file from SparkSubmit
     *https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html
     * @param spark
     * @param query
     * @return
     */
    public static Dataset<Row> getHistoricControlEventRowDatasetFromQuery(SparkSession spark,  String query,String env,String encKey){
        long lowerBound = 0, upperBound =99;

        //options must all be specified if any of them is specified. In addition, numPartitions must be specified.
        // They describe how to partition the table when reading in parallel from multiple workers.
        // partitionColumn must be a numeric, date, or timestamp column from the table in question.
        // Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.
        //partitionColumn, lowerBound, upperBound	These

        //The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing.
        //--numPartitions

        //The JDBC fetch size, which determines how many rows to fetch per round trip.
        // This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).
        // This option applies only to reading.
        //--fetchsize = 1000000

        System.out.println("*********Query->: "+query);
        return getExaDataFrameReader(spark,getConnectionProperties(env,encKey))
                .option("dbtable",  "("+query+")")
                .option("partitionColumn",  tradeIdHashColumnName)
                .option("lowerBound",  lowerBound)
                .option("upperBound",  upperBound)
                .option("numPartitions", exaConnectionCount )
                .option("fetchsize",exaMaxFetchRows )
                .load();

    }

    public static Dataset<Row> getHistoricControlEventRowDatasetForPartition(SparkSession spark, TRDSContext context, String query,String env,String encKey){
        long lowerBound = 0, upperBound =99;


        System.out.println("query is: " + query);

        //options must all be specified if any of them is specified. In addition, numPartitions must be specified.
        // They describe how to partition the table when reading in parallel from multiple workers.
        // partitionColumn must be a numeric, date, or timestamp column from the table in question.
        // Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.
        //partitionColumn, lowerBound, upperBound	These

        //The maximum number of partitions that can be used for parallelism in table reading and writing.
        // This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit,
        // we decrease it to this limit by calling coalesce(numPartitions) before writing.
        //--numPartitions

        //The JDBC fetch size, which determines how many rows to fetch per round trip.
        // This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).
        // This option applies only to reading.
        //--fetchsize = 1000000
        return getExaDataFrameReader(spark,getConnectionProperties(env,encKey))
                //.option("dbtable",  "("+ SELECT_HCE_MIG_COLUMLIST+ ","+ PARALLEL_LOAD_AID_COLUMN +" thash, '" + context.getPartitionTblName() + "' as PARTITION_ID from "+ schemaName+"." + context.getPartitionTblName() +")")
                .option("dbtable",  "("+ query +")")
                .option("partitionColumn",  "thash")
                .option("lowerBound",  lowerBound)
                .option("upperBound",  upperBound)
                .option("numPartitions", context.getExaConnectionCount() )
                .option("fetchsize" , context.getExaMaxFetchRows())
                .load();
    }


    private static DataFrameReader getExaDataFrameReader(SparkSession spark, Properties connectionProps) {
        return spark.read()
                .format("jdbc")
                .option("url", connectionProps.getProperty("url"))
                .option("password", connectionProps.getProperty("password"))
                .option("driver", connectionProps.getProperty("driver"))
                .option("user", connectionProps.getProperty("user"));
    }

    public static Properties getConnectionProperties(String env,String encKey) {
        Properties connectionProps = new Properties();//, ""
        String environment = env;
        log.info("Env is: "+env);
        StandardPBEStringEncryptor encryptor = new StandardPBEStringEncryptor();
        encryptor.setPassword(encKey);
        encryptor.setAlgorithm(TRDSConstants.ENCRYPTION_ALGORITHM);
        Properties prop = new Properties();
        try (InputStream input = HCEMigrationSparkDriverImpl.class.getClassLoader().getResourceAsStream("default-props.properties")) {
            if (input == null) {
                log.error("Unable to find config.properties");
                return connectionProps;
            }

            //load a properties file from class path, inside static method
            prop.load(input);
        } catch (IOException ex) {
            ex.printStackTrace();
        }
        log.info("GCP bucket : "+prop.getProperty(environment+".gcs.GCP_GS_BUCKET_NAME"));
        log.info("ECS bucket : "+prop.getProperty(environment+".ecs.s3bucketName1"));
        log.info("GCP Project : "+prop.getProperty(environment+".gcs.HSBC_11007943_TRDS_DEV"));

        connectionProps.setProperty("user",prop.getProperty(environment+".exa.user"));
        connectionProps.setProperty("password",encryptor.decrypt(prop.getProperty(environment+".exa.password")));
        connectionProps.setProperty("driver",prop.getProperty(environment+".exa.driver"));
        connectionProps.setProperty("url",prop.getProperty(environment+".exa.url"));

        return connectionProps;
    }


}
package com.hsbc.trds.endpoints;

import com.typesafe.config.Config;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;

public class GCPConfig {
    public static final String HSBC_11007943_TRDS_DEV = "hsbc-11007943-trds-dev";
    public static final String GCP_GCS_SERVICE_ACCOUNT_EMAIL = "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com";
    //public static final String GCP_BUCKET_NAME = "control-event-bucket-uk";
    public static final String GCP_BUCKET_NAME = "control-event-bucket-uk-1";
    public static final String GCP_GS_BUCKET_NAME = "gs://"+GCP_BUCKET_NAME +"/";
    

}
package com.hsbc.trds.endpoints;

public class TRDSConstants {

    //General - constants
    public static String COMMA = ",";
    public static String AS = " as ";
    public static String FROM = " from ";
    public static String DOT = ".";


    //ECS - constants
    public static String ECS = "ECG";
    public static String POF_NONPOF_ECS = "POF-NONPOF-ECS";
    public static String POF_ECS = "POF-ECS";
    public static String NONPOF_ECS  = "NONOF-ECS";
    public static String POFECS_NONPOFGCS = "POFECS-NONPOFGCS";
    public static String ECS_POF_BIN = "ECS-POF-BIN";
    public static String POF_BINARY = "POF_BINARY";

    //GCS - constants

    public static String G_CS =  "G_CS";

    public static String TRADE_ID_HASH = "tradeIdHash";
    public static String T_HASH = "thash";
    public static String SCHEMA_NAME = "DSL_PROD";
    public static String PARALLEL_LOAD_AID_COLUMN = " cast(mod(ORA_HASH(trade_id),100) as INTEGER)  ";

    //Queries - Model

    public static String SELECT_HCE_MIG_COLUMNLIST_POF = "select UUID, TRADE_ID, TRADE_VERSION, POF_BINARY ";
    public static String SELECT_HCE_MIG_COLUMLIST = "select UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID," +
        "TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT ";
    public static String DOCPATH = "'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH";
    public static String PARTITION_ID = "PARTITION_ID";

    //JASYPT Constants
    public static String ENCRYPTION_ALGORITHM = "PBEWithMD5AndDES";

    //Compression Constants
    public static String COMPRESSION = "compression";
    public static String GZIP = "gzip";
}
package com.hsbc.trds;

import java.io.Serializable;

/**
 * To store down write stats
 * One per sink
 */
public class HCEWriteStat implements Serializable {
//    private static final long serialVersionUID = 1234567L;
//    private String partitionTableName;
//    private LocalDateTime startTime;
//    private LocalDateTime endTime;
//    private float objectStorageFootPrintMB;
//    private String objectStorageRootLocation;
//    private String bucket;
//    private String objectStorageEndpoint;
//    private String exaEndPointUrl;
//    private long   exaParallelConnections;
//
//    private long   exaMaxRowsPerConnection;
//    private HashMap<String,Object>  extraParameters = new HashMap<>();
//    private long srcRecordCount ;
//    private long sinkRecordCount;
//
//    public HCEWriteStat(String partitionTableName, LocalDateTime startTime, LocalDateTime endTime, float objectStorageFootPrintMB,
//                        String objectStorageRootLocation, String bucket, String objectStorageEndpoint, String exaEndPointUrl,
//                        long exaParallelConnections, long exaMaxRowsPerConnection, HashMap<String, Object> extraParameters,
//                        long srcRecordCount, long sinkRecordCount) {
//        this.partitionTableName = partitionTableName;
//        this.startTime = startTime;
//        this.endTime = endTime;
//        this.objectStorageFootPrintMB = objectStorageFootPrintMB;
//        this.objectStorageRootLocation = objectStorageRootLocation;
//        this.bucket = bucket;
//        this.objectStorageEndpoint = objectStorageEndpoint;
//        this.exaEndPointUrl = exaEndPointUrl;
//        this.exaParallelConnections = exaParallelConnections;
//        this.exaMaxRowsPerConnection = exaMaxRowsPerConnection;
//        this.extraParameters = extraParameters;
//        this.srcRecordCount = srcRecordCount;
//        this.sinkRecordCount = sinkRecordCount;
//    }
//
//    public String getPartitionTableName() {
//        return partitionTableName;
//    }
//
//    public void setPartitionTableName(String partitionTableName) {
//        this.partitionTableName = partitionTableName;
//    }
//
//    public LocalDateTime getStartTime() {
//        return startTime;
//    }
//
//    public void setStartTime(LocalDateTime startTime) {
//        this.startTime = startTime;
//    }
//
//    public LocalDateTime getEndTime() {
//        return endTime;
//    }
//
//    public void setEndTime(LocalDateTime endTime) {
//        this.endTime = endTime;
//    }
//
//    public float getObjectStorageFootPrintMB() {
//        return objectStorageFootPrintMB;
//    }
//
//    public void setObjectStorageFootPrintMB(float objectStorageFootPrintMB) {
//        this.objectStorageFootPrintMB = objectStorageFootPrintMB;
//    }
//
//    public String getObjectStorageRootLocation() {
//        return objectStorageRootLocation;
//    }
//
//    public void setObjectStorageRootLocation(String objectStorageRootLocation) {
//        this.objectStorageRootLocation = objectStorageRootLocation;
//    }
//
//    public String getBucket() {
//        return bucket;
//    }
//
//    public void setBucket(String bucket) {
//        this.bucket = bucket;
//    }
//
//    public String getObjectStorageEndpoint() {
//        return objectStorageEndpoint;
//    }
//
//    public void setObjectStorageEndpoint(String objectStorageEndpoint) {
//        this.objectStorageEndpoint = objectStorageEndpoint;
//    }
//
//    public String getExaEndPointUrl() {
//        return exaEndPointUrl;
//    }
//
//    public void setExaEndPointUrl(String exaEndPointUrl) {
//        this.exaEndPointUrl = exaEndPointUrl;
//    }
//
//    public long getExaParallelConnections() {
//        return exaParallelConnections;
//    }
//
//    public void setExaParallelConnections(long exaParallelConnections) {
//        this.exaParallelConnections = exaParallelConnections;
//    }
//
//    public long getExaMaxRowsPerConnection() {
//        return exaMaxRowsPerConnection;
//    }
//
//    public void setExaMaxRowsPerConnection(long exaMaxRowsPerConnection) {
//        this.exaMaxRowsPerConnection = exaMaxRowsPerConnection;
//    }
//
//    public HashMap<String, Object> getExtraParameters() {
//        return extraParameters;
//    }
//
//    public void setExtraParameters(HashMap<String, Object> extraParameters) {
//        this.extraParameters = extraParameters;
//    }
//
//    public long getSrcRecordCount() {
//        return srcRecordCount;
//    }
//
//    public void setSrcRecordCount(long srcRecordCount) {
//        this.srcRecordCount = srcRecordCount;
//    }
//
//    public long getSinkRecordCount() {
//        return sinkRecordCount;
//    }
//
//    public void setSinkRecordCount(long sinkRecordCount) {
//        this.sinkRecordCount = sinkRecordCount;
//    }
//
//    @Override
//    public String toString() {
//        return "HCEWriteStat{" +
//                "partitionTableName='" + partitionTableName + '\'' +
//                ", startTime=" + startTime +
//                ", endTime=" + endTime +
//                ", objectStorageFootPrintMB=" + objectStorageFootPrintMB +
//                ", objectStorageRootLocation='" + objectStorageRootLocation + '\'' +
//                ", bucket='" + bucket + '\'' +
//                ", objectStorageEndpoint='" + objectStorageEndpoint + '\'' +
//                ", exaEndPointUrl='" + exaEndPointUrl + '\'' +
//                ", exaParallelConnections=" + exaParallelConnections +
//                ", exaMaxRowsPerConnection=" + exaMaxRowsPerConnection +
//                ", extraParameters=" + extraParameters +
//                ", srcRecordCount=" + srcRecordCount +
//                ", sinkRecordCount=" + sinkRecordCount +
//                '}';
//    }








}
cat: ./src/main/java/com/hsbc/trds/model: Is a directory
package com.hsbc.trds.model;

public class ExaDataModel {
    public static final String DSL_OBJECT_TYPE = "DSL_OBJECT_TYPE";
    public static final String TRADE_ID = "TRADE_ID";
    public static String TRADE_ID_HASH = "tradeIdHash";
    public static String schemaName = "DSL_PROD";
    public static String PARALLEL_LOAD_AID_COLUMN = " cast(mod(ORA_HASH(trade_id),100) as INTEGER)  " ;
    public static final String SELECT_HCE_MIG_COLUMLIST = "select  " +
            "UUID, CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE," +
            "DSL_OBJECT_TYPE, TRADE_ID , TRADE_VERSION ,SOURCE_SYSTEM ," +
            "REGION , EXTERNAL_ID , CACHE_STATUS, AUDIT_DATE , PARENT_ID," +
            "LINKED_PARENT_ID, EVICTION_DATE, IMPL_VERSION , XML_DOCUMENT " ;
        //"'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/'|| source_system ||'/'||region ||'/'|| trade_id||'/'||trade_version||'/DSL/'||dsl_object_type as DOCPATH ";


    public static final String SELECT_HCE_MIG_COLUMNLIST_POF = "select UUID, TRADE_ID, POF_BINARY " ;

    public static final String SELECT_HCE_MIG_COLUMLIST_POF_NONPOF = "select  " +
            "UUID, CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE," +
            "DSL_OBJECT_TYPE, TRADE_ID , TRADE_VERSION ,SOURCE_SYSTEM ," +
            "REGION , EXTERNAL_ID , CACHE_STATUS, AUDIT_DATE , PARENT_ID," +
            "LINKED_PARENT_ID, EVICTION_DATE, IMPL_VERSION , XML_DOCUMENT, POF_BINARY ";
}
cat: ./src/main/resources: Is a directory
cat: ./src/main/resources/avro: Is a directory
{
    "type": "record",
    "name": "GenericControlEvent",
    "namespace": "com.hsbc.trds.avro",

    "fields": [
        {"name": "Id", "type": "string"},
        {"name": "pofString", "type": "string"}
    ]
}{
    "type": "record",
    "name": "HCEWriteStat",
    "namespace": "com.hsbc.trds",
    "fields": [
        {"name": "partitionTableName", "type": ["null", "string"]},
        {"name": "startTime", "type": ["null", "string"]},
        {"name": "endTime", "type": ["null", "string"]},
        {"name": "objectStorageFootPrintMB", "type": ["null", "float"]},
        {"name": "objectStorageRootLocation", "type": ["null", "string"]},
        {"name": "bucket", "type": ["null", "string"]},
        {"name": "objectStorageEndpoint", "type": ["null", "string"]},
        {"name": "exaEndPointUrl", "type": ["null", "string"]},
        {"name": "exaParallelConnections", "type": ["null", "long"]},
        {"name": "exaMaxRowsPerConnection", "type": ["null", "long"]},
        {"name": "extraParameters",
         "type": ["null",   {"type":"map", "values": "string"}]
        },
        {"name": "srcRecordCount", "type": ["null", "long"]},
        {"name": "sinkRecordCount", "type": ["null", "long"]}
    ]
}cat: ./src/main/resources/coherence: Is a directory
<?xml version="1.0"?>

<pof-config xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
            xmlns="http://xmlns.oracle.com/coherence/coherence-pof-config"
            xsi:schemaLocation="http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd">

    <user-type-list>
        <include>coherence-pof-config.xml</include>
        <include>dsl-domain-pof-config.xml</include>
    </user-type-list>
</pof-config>dev.ecs.uid1 =uk-dsl-ss-1-s3-nonprod-u
dev.ecs.host1=http://ecs-storage.it.global.hsbc:9020
dev.ecs.secret1 =SehPN3OKoc4ikmUz4VCR6x71lBBnyXiN1L8ATyFI0q1LCFf9qev0yja1IHFYfKsT2We/Mnc1+sY=
dev.ecs.bucketName1 =uk-dsl-ss-1-s3-nonprod
dev.ecs.s3bucketName1 =s3a://uk-dsl-ss-1-s3-nonprod/

dev.gcs.HSBC_11007943_TRDS_DEV=hsbc-11007943-trds-dev
dev.gcs.GCP_BUCKET_NAME =control-event-bucket-uk-1
dev.gcs.GCP_GS_BUCKET_NAME =gs://control-event-bucket-uk-1/
dev.gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL =storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com
dev.gcs.GCP_GCS_PRIVATE_KEY_ID =23ba198aad06dfaf039323ceda2104468dd690ee
dev.gcs.GCP_GCS_PROXY_ADDRESS =googleapis-dev.gcp.cloud.uk.hsbc:3128
dev.gcs.GCP_GCS_SECRET_KEY =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

dev.exa.user=DSL_PROD
dev.exa.password=cTSGTCAGcxuooAYqw04o0TEmLbV7CQBW
dev.exa.url=jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSLXDS_003.systems.uk.hsbc
dev.exa.driver=oracle.jdbc.OracleDriver
dev.exa.query_POFNONPOF_ECS = SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dslprod/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME
dev.exa.query_POF_ECS = SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME
dev.exa.query_NONPOF_ECS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME
dev.exa.query_NONPOF_GCS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME
dev.exa.query_NONPOF_GCS_AUG = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME


oat_wg.ecs.uid1 =uk-dsl-ss-1-s3-nonprod-u
oat_wg.ecs.host1=http://ecs-storage.it.global.hsbc:9020
oat_wg.ecs.secret1 =SehPN3OKoc4ikmUz4VCR6x71lBBnyXiN1L8ATyFI0q1LCFf9qev0yja1IHFYfKsT2We/Mnc1+sY=
oat_wg.ecs.bucketName1 =uk-dsl-ss-1-s3-nonprod
oat_wg.ecs.s3bucketName1 =s3a://uk-dsl-ss-1-s3-nonprod/

oat_wg.gcs.HSBC_11007943_TRDS_DEV=hsbc-11007943-trds-dev
oat_wg.gcs.GCP_BUCKET_NAME =historical_control_event_migration
oat_wg.gcs.GCP_GS_BUCKET_NAME =gs://historical_control_event_migration/
oat_wg.gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL =bigquery-storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com
oat_wg.gcs.GCP_GCS_PRIVATE_KEY_ID =7b8784dad00424932c5dc73e7b4661e826f7c390
oat_wg.gcs.GCP_GCS_PROXY_ADDRESS =googleapis-dev.gcp.cloud.uk.hsbc:3128
oat_wg.gcs.GCP_GCS_SECRET_KEY =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

oat_wg.exa.user=HCE_MIG_USER
oat_wg.exa.password=7JME0n0Yn7pxyp5dsD2rIeQ8qWUPi8VP
oat_wg.exa.url=jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/OUKDSL01_RW1.systems.uk.hsbc
oat_wg.exa.driver=oracle.jdbc.OracleDriver
oat_wg.exa.query_POFNONPOF_ECS = SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dsloat/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_wg.exa.query_POF_ECS = SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_wg.exa.query_NONPOF_ECS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_wg.exa.query_NONPOF_GCS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_wg.exa.query_NONPOF_GCS_AUG = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)


oat_sy.ecs.uid1 =uk-dsl-ss-1-s3-nonprod-u
oat_sy.ecs.host1=http://ecs-storage.it.global.hsbc:9020
oat_sy.ecs.secret1 =SehPN3OKoc4ikmUz4VCR6x71lBBnyXiN1L8ATyFI0q1LCFf9qev0yja1IHFYfKsT2We/Mnc1+sY=
oat_sy.ecs.bucketName1 =uk-dsl-ss-1-s3-nonprod
oat_sy.ecs.s3bucketName1 =s3a://uk-dsl-ss-1-s3-nonprod/

oat_sy.gcs.HSBC_11007943_TRDS_DEV=hsbc-11007943-trds-dev
oat_sy.gcs.GCP_BUCKET_NAME =control-event-bucket-uk-1
oat_sy.gcs.GCP_GS_BUCKET_NAME =gs://control-event-bucket-uk-1/
oat_sy.gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL =storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com
oat_sy.gcs.GCP_GCS_PRIVATE_KEY_ID =7d578f7053333010557337201dc76539f368ae4f
oat_sy.gcs.GCP_GCS_PROXY_ADDRESS =googleapis-dev.gcp.cloud.uk.hsbc:3128
oat_sy.gcs.GCP_GCS_SECRET_KEY =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

oat_sy.exa.user=HCE_MIG_USER
oat_sy.exa.password=7JME0n0Yn7pxyp5dsD2rIeQ8qWUPi8VP
oat_sy.exa.url=jdbc:oracle:thin:@gbl19140n2-scan.systems.uk.hsbc:2001/OUKDSL02.systems.uk.hsbc
oat_sy.exa.driver=oracle.jdbc.OracleDriver
oat_sy.exa.query_POFNONPOF_ECS = SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dsloat/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_sy.exa.query_POF_ECS = SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_sy.exa.query_NONPOF_ECS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_sy.exa.query_NONPOF_GCS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
oat_sy.exa.query_NONPOF_GCS_AUG = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)


prod_wg.ecs.uid1 =uk_motdr-s3-prod-u
prod_wg.ecs.host1=http://ecs-storage.it.global.hsbc:9020
prod_wg.ecs.secret1 =IJnwGG+SjgMJdlSg+Gpdc+QYQ4iw0TQV+aCCJyAm9ol86zGSHCKoaQtKSCayO8IFr7tkZg4bl7o=
prod_wg.ecs.bucketName1 =uk-motdr-s3-prod
prod_wg.ecs.s3bucketName1 =s3a://uk-motdr-s3-prod/

prod_wg.gcs.HSBC_11007943_TRDS_DEV=hsbc-11007943-trds-prod
prod_wg.gcs.GCP_BUCKET_NAME =trds-prod-bucket
prod_wg.gcs.GCP_GS_BUCKET_NAME =gs://trds-prod-bucket/
prod_wg.gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL =storage-admin@hsbc-11007943-trds-prod.iam.gserviceaccount.com
prod_wg.gcs.GCP_GCS_PRIVATE_KEY_ID =81da7afef380e95b37fe22bae0dff07a8c1061f5
prod_wg.gcs.GCP_GCS_PROXY_ADDRESS =googleapis-prod.gcp.cloud.uk.hsbc:3128
prod_wg.gcs.GCP_GCS_SECRET_KEY =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

prod_wg.exa.user=DSL_PROD_HCEMIG_USER
prod_wg.exa.password=DjjCOrPUm3hsXfAYQp0YRjWRfskSmDmB
prod_wg.exa.url=jdbc:oracle:thin:@gbx03-scan.systems.uk.hsbc:2010/PUKDSL01_RW2.systems.uk.hsbc
prod_wg.exa.driver=oracle.jdbc.OracleDriver
prod_wg.exa.query_POFNONPOF_ECS = SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dslprod/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_wg.exa.query_POF_ECS = SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_wg.exa.query_NONPOF_ECS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_wg.exa.query_NONPOF_GCS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_wg.exa.query_NONPOF_GCS_AUG = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)


prod_sy.ecs.uid1 =uk_motdr-s3-prod-u
prod_sy.ecs.host1=http://ecs-storage.it.global.hsbc:9020
prod_sy.ecs.secret1 =IJnwGG+SjgMJdlSg+Gpdc+QYQ4iw0TQV+aCCJyAm9ol86zGSHCKoaQtKSCayO8IFr7tkZg4bl7o=
prod_sy.ecs.bucketName1 =uk-motdr-s3-prod
prod_sy.ecs.s3bucketName1 =s3a://uk-motdr-s3-prod/

prod_sy.gcs.HSBC_11007943_TRDS_DEV=hsbc-11007943-trds-prod
prod_sy.gcs.GCP_BUCKET_NAME =trds-prod-bucket
prod_sy.gcs.GCP_GS_BUCKET_NAME =gs://trds-prod-bucket/
prod_sy.gcs.GCP_GCS_SERVICE_ACCOUNT_EMAIL =storage-admin@hsbc-11007943-trds-prod.iam.gserviceaccount.com
prod_sy.gcs.GCP_GCS_PRIVATE_KEY_ID =81da7afef380e95b37fe22bae0dff07a8c1061f5
prod_sy.gcs.GCP_GCS_PROXY_ADDRESS =googleapis-prod.gcp.cloud.uk.hsbc:3128
prod_sy.gcs.GCP_GCS_SECRET_KEY =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

prod_sy.exa.user=DSL_PROD_HCEMIG_USER
prod_sy.exa.password=DjjCOrPUm3hsXfAYQp0YRjWRfskSmDmB
prod_sy.exa.url=jdbc:oracle:thin:@gbx05-scan.systems.uk.hsbc:2010/PUKDSL02_RW2.systems.uk.hsbc
prod_sy.exa.driver=oracle.jdbc.OracleDriver
prod_sy.exa.query_POFNONPOF_ECS = SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dslprod/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(trds_tradeid_hash(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_sy.exa.query_POF_ECS = SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_sy.exa.query_NONPOF_ECS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_sy.exa.query_NONPOF_GCS = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
prod_sy.exa.query_NONPOF_GCS_AUG = SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) tradeIdHash,cast(mod(trds_tradeid_hash(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)
dev{
    ecs{
        uid1 = "uk-dsl-ss-1-s3-nonprod-u"
        host1= "http://ecs-storage.it.global.hsbc:9020"
        secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
        bucketName1 = "uk-dsl-ss-1-s3-nonprod"
        s3bucketName1 = "s3a://"${dev.ecs.bucketName1}"/"
    }
    gcs{
        HSBC_11007943_TRDS_DEV= "hsbc-11007943-trds-dev"
        GCP_BUCKET_NAME = "control-event-bucket-uk-1"
        GCP_GS_BUCKET_NAME = "gs://"${dev.gcs.GCP_BUCKET_NAME}"/"
        GCP_GCS_SERVICE_ACCOUNT_EMAIL = "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com"
    }

    exa{
        user="DSL_PROD"
        password="S4nC4rl0"
        url="jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSLXDS_003.systems.uk.hsbc"
        driver="oracle.jdbc.OracleDriver"
        query_POFNONPOF_ECS = "SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dslprod/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME"
        query_POF_ECS = "SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME"
        query_NONPOF_ECS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME"
        query_NONPOF_GCS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME"
        query_NONPOF_GCS_AUG = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_PROD.HCE_PARTITION_NAME"
    }


}

oat_wg{
    ecs{
        uid1 = "uk-dsl-ss-1-s3-nonprod-u"
        host1= "http://ecs-storage.it.global.hsbc:9020"
        secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
        bucketName1 = "uk-dsl-ss-1-s3-nonprod"
        s3bucketName1 = "s3a://"${dev.ecs.bucketName1}"/"
    }
    gcs{
        HSBC_11007943_TRDS_DEV= "hsbc-11007943-trds-dev"
        GCP_BUCKET_NAME = "control-event-bucket-uk-1"
        GCP_GS_BUCKET_NAME = "gs://"${dev.gcs.GCP_BUCKET_NAME}"/"
        GCP_GCS_SERVICE_ACCOUNT_EMAIL = "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com"
    }
    exa{
        user="dsl_oat"
        password="dsloatnov"
        url="jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/OUKDSL01_RW1.systems.uk.hsbc"
        driver="oracle.jdbc.OracleDriver"
        query_POFNONPOF_ECS = "SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dsloat/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_POF_ECS = "SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_ECS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_GCS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_GCS_AUG = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
    }
}

oat_sy{
    ecs{
        uid1 = "uk-dsl-ss-1-s3-nonprod-u"
        host1= "http://ecs-storage.it.global.hsbc:9020"
        secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
        bucketName1 = "uk-dsl-ss-1-s3-nonprod"
        s3bucketName1 = "s3a://"${dev.ecs.bucketName1}"/"
    }
    gcs{
        HSBC_11007943_TRDS_DEV= "hsbc-11007943-trds-dev"
        GCP_BUCKET_NAME = "control-event-bucket-uk-1"
        GCP_GS_BUCKET_NAME = "gs://"${dev.gcs.GCP_BUCKET_NAME}"/"
        GCP_GCS_SERVICE_ACCOUNT_EMAIL = "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com"
    }
    exa{
        user="dsl_oat"
        password="dsloatnov"
        url="jdbc:oracle:thin:@gbl19140n2-scan.systems.uk.hsbc:2001/OUKDSL02.systems.uk.hsbc"
        driver="oracle.jdbc.OracleDriver"
        query_POFNONPOF_ECS = "SELECT UUID,POF_BINARY,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/'|| TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD')|| '/dsloat/'|| source_system|| '/'|| region|| '/'|| trade_id|| '/'||trade_version|| '/DSL/'|| dsl_object_type AS DOCPATH,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)tradeIdHash,  CAST(mod(ORA_HASH(trade_id),100) AS INTEGER)thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_POF_ECS = "SELECT UUID, TRADE_ID, TRADE_VERSION, DSL_OBJECT_TYPE, POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash, 'HCE_PARTITION_NAME'AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_ECS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_GCS = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
        query_NONPOF_GCS_AUG = "SELECT UUID,CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE,DSL_OBJECT_TYPE,TRADE_ID,TRADE_VERSION,SOURCE_SYSTEM,REGION,EXTERNAL_ID,CACHE_STATUS,AUDIT_DATE,PARENT_ID,LINKED_PARENT_ID,EVICTION_DATE,IMPL_VERSION,XML_DOCUMENT,POF_BINARY,'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dsloat/' || source_system || '/' || region || '/' || trade_id || '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH,cast(mod(ORA_HASH(trade_id),100) as INTEGER) tradeIdHash,cast(mod(ORA_HASH(trade_id),100) as INTEGER) thash,'HCE_PARTITION_NAME' AS PARTITION_ID FROM DSL_OAT.T_CONTROL_EVENT_ARCHIVE_OLD PARTITION(HCE_PARTITION_NAME)"
    }
}# aws cli works only on windows machine
# Be sure to set access key andsecret key by running aws configure before running this file

echo "Transferring files...";
echo src : $1
echo dest: $2

aws --endpoint-url http://ecs-storage.it.global.hsbc:9020 s3 mv $1 $2 --recursive

echo "Completed Transferring from " $1 " to " $2
appName = trds-prototype

dev{
  ecs {
    uid = "uslab-middle-office-s3-nonprod-u";
    host="http://ecs-walden-lab.us.hsbc:9020";
    secret="LOMWus6RzNGZINZVHAfwhm6SCQt4XkV5BrnLBzhz";
    bucketName="uslab-middle-office-s3-nonprod";
  }

  group1{

  }
}

sit{
  group2{

  }
  group3{

  }
}echo "Transferring files...";
echo src : $1
echo dest: $2

gsutil -m cp -r $1 $2
echo "Completed Transferring from " $1 " to " $2

gsutil -m rm -r $1
echo "Deleted partition " $1{
  "type": "service_account",
  "project_id": "hsbc-11007943-trds-dev",
  "private_key_id": "e3ae9cc24c214657eba0d8e703f39924a1334453",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC1EZGny943tKFy\nyM823QE7pR//5BEPexdWw6fMqXwNH+qzoL84iegTFmjbOFCy8lIQ+h02zMSSUDR0\nmYYGoR/SqYCH2LPVkSRtMxBDuzmhRGRap1Q+3I/IlSiAJK9cUdJTeVfFVUncDAMZ\ndGgGCPXctcG5mAp8qBPXRBBGZgpahHxXpIiYp2yCDdH98pQ2P/ol94php9D7KdyV\n224bEDsRo/RJF07zKpib44l00Iw4eO3acLWdBRAMDzDQAp3xe2yC2bGHBgXbkqd5\nwNfF97lDlsYbDJtXSp/KmtvAMLuSeWzcGZjsWZL1Q6WQ/VeMC5cdSaPrxPVwzU/3\nWeSQFObxAgMBAAECggEAEcZXP5seReaoAK4g1Vhtsg68LPEVEal6OYy8cLVklZ8/\n+YE6tFCpZRJKmdaV6XMrRjpfWs1QMLLJ63ZUt1ri1lnIYc2OndUBN+0O2C9NcXzW\nwyVw+jjJrp2h265+PnhTxtj/GkKMnRraxQlt2sXPaOq4yUDZKrKPPA8U7+w9cPQt\nNrKyFUNTnNnUzQjud9rINuQBl4pF7PRR+xZSn85aGoNsyu3GlBvVFFObgP0OXJlB\n/xFY5R+Ce8pxftuDjS3ywd8aaUDW3UHA5WdVtB9lC7rJHd8WXc91IQGKRcoKI80I\nhAiEMguPqi3SSD4ZMZaxzYZmss8HYoPSRguNTTSTLwKBgQDhEtWxKJrDPvlpj1J4\nr2U5/h8H4YpRewv44MkNsWyV/MIz9IdWN2vEpd93KZ7ZK0bhEPaJSN1xo+aTdHbz\nJ51Nqiy7jjXllUi0PmZTE6CxsUvvCHSmLK6582JR5O89XZCwkMORHN0KSRLClMk6\ne6S11HvspNQHWUmZuGjbf1E8CwKBgQDN8tIc+bFN6b/iYREs8bSHJasWlV6PEeJI\nuOZMgvzNQbTBK/jcvpgVUARvs3IwF0IP9Rhd2cd1NeJ9gyNRJmc+yVJS7luSSzz9\n6OBGIDiK2/kxkphs+73Iaor+6Rxewfa/33WdtBRj/rgzLs2xM4CrW53v5HKVag3g\njL3FF0uKcwKBgGmvA2JFU648QrbbUnlSunBpr75unBk4ct2xBxcD6Z3f6nyk5GuK\naHMVBAIbK/iDRqvl4C2EZl62/Eze0f/I6ScPsiN7WjlsSJBCAKAfxkPJoYMi8Esb\nDgxIiTE1E1U4Ovl9cCcSa4Qp3cI6RObOKgArPulWWCz/Mv0YRzxR4x4TAoGAa40g\nfzNCjc7Bf4ZzgsYjIeThCSUuQYb0ZkfxNQm+3a4vqCW/jAAYyiCEgJT0z/qFbHHx\nrKlbiXF/e05ttiZZCqf2TwrdVXPnQS5JWTXgcVvZMjM9WMTK4owJVIAGadHfe2pG\nMPpnH6VZKpmJn72mt1ZcD3h9AlxlLNwzCmdU8tMCgYEArETEkmQ8pQrvn0m/wlLL\n78UoZvGoi5PJqvJLX9durV6qlDW7nqMr6tKCQgqt7mKKrG5HDpZAwrJHFuwQlBiD\nvN4fQBK9+ntONWWGFGT8S9VCGuIkcsquoVFG4djmDEokFb7gvkignJSp7A78UsXa\nxcQVRk3F13mcBA1miwG4B8k=\n-----END PRIVATE KEY-----\n",
  "client_email": "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com",
  "client_id": "104365758285129631244",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/storage-admin%40hsbc-11007943-trds-dev.iam.gserviceaccount.com"
}
# Set everything to be logged to the file bagel/target/unit-tests.log
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Ignore messages below warning level from Jetty, because it's a bit verbose
# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.spark=WARN
log4j.logger.parquet=ERROR
log4j.logger.org.apache.spark.sql.execution.datasources.parquet=ERROR
log4j.logger.org.apache.spark.sql.execution.datasources.FileScanRDD=ERROR
log4j.logger.org.apache.hadoop.io.compress.CodecPool=ERROR#!/bin/bash

BASEDIR=$(cd "$( dirname "${BASH_SOURCE[0]}" )"/../ && pwd )
echo "BaseDir: , " $BASEDIR

LIB_JARS=$(cd ${BASEDIR}; ls lib/ | awk -v prefix="${BASEDIR}/lib/" '{print prefix $1}' | paste -sd ',')
EXEC_JARS=$(cd ${BASEDIR}; ls lib/ | awk -v prefix="${BASEDIR}/lib/" '{print prefix $1}' | paste -sd ':')

cp -rf $BASEDIR/conf/* .

spark-submit --conf spark.driver.extraJavaOptions="-Dhttps.proxyHost=googleapis-dev.gcp.cloud.uk.hsbc -Dhttps.proxyPort=3128 -Denv=dev -DkeyFile=./hsbc-11007943-trds-dev-storage.json -Dconfig.file=./default.conf" \
             --conf spark.executor.extraJavaOptions="-Dhttps.proxyHost=googleapis-dev.gcp.cloud.uk.hsbc -Dhttps.proxyPort=3128 -Denv=dev -DkeyFile=./hsbc-11007943-trds-dev-storage.json -Dconfig.file=./default.conf" \
             --conf spark.driver.extraClassPath="$EXEC_JARS" \
             --conf spark.executor.extraClassPath="$EXEC_JARS" \
             --conf spark.executor.memoryOverhead="4096" \
             --conf  spark.hadoop.fs.s3a.multiobjectdelete.enable="false" \
             --conf  spark.hadoop.fs.s3a.fast.upload="true" \
             --conf  spark.sql.parquet.filterPushdown="true" \
             --conf  spark.sql.parquet.mergeSchema="false" \
             --conf  spark.sql.parquet.fs.optimized.committer.optimization-enabled="true" \
             --conf  spark.speculation="false" \
             --conf spark.yarn.submit.waitAppCompletion="false" \
            --conf spark.cleaner.referenceTracking="false" \
            --conf spark.cleaner.referenceTracking.blocking="false" \
            --conf spark.cleaner.referenceTracking.blocking.shuffle="false" \
            --conf spark.cleaner.referenceTracking.cleanCheckpoints="false" \
             --files ./hsbc-11007943-trds-dev-storage.json,./default.conf \
             --master yarn \
             --deploy-mode cluster \
             --driver-memory 4G \
             --executor-memory 8G \
             --driver-cores 4 \
             --num-executors 4 \
             --executor-cores 2 \
             --class com.hsbc.trds.driver.HCEMigrationSparkDriverImpl \
             --jars $LIB_JARS \
             --name "TrdsPrototype-merge-pof-$1" \
             /dsl/app/dslt12/sam/trds-1.0-SNAPSHOT/trds-1.0-SNAPSHOT.jar $@
cat: ./src/main/scala: Is a directory
cat: ./src/main/scala/com: Is a directory
cat: ./src/main/scala/com/hsbc: Is a directory
cat: ./src/main/scala/com/hsbc/trds: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala/config: Is a directory
package com.hsbc.trds.scala.config

import org.apache.hadoop.conf.Configuration
import org.apache.spark.sql.SparkSession

object EcsConfig {
  val ECS = "ECG"
  val G_CS = "G_CS"
  val GCS_HDFS = "GCS_HDFS"

  val POF_NONPOF_ECS = "POF-NONPOF-ECS"
  val POF_ECS = "POF-ECS"
  val NONPOF_ECS = "NONPOF-ECS"
  val NONPOF_GCS = "NONPOF_GCS"
  val POFECS_NONPOFGCS = "POFECS_NONPOFGCS"
  val ECS_POF_BIN = "ECS-POF-BIN"
  val NONPOF_AUG_GCS = "NONPOF-AUG-GCS"

  val uid1 = "uk-dsl-ss-1-s3-nonprod-u"
  val host1 = "http://ecs-storage.it.global.hsbc:9020"
  val secret1 = "2pHXEwQu2bE09xP0WbGdFgDjhCp85SNMBuQJrdv0"
  val bucketName1 = "uk-dsl-ss-1-s3-nonprod"
  val s3bucketName1: String = "s3a://" + bucketName1 + "/"

  val TRADE_ID_HASH = "tradeIdHash"
  val SCHEMA_NAME = "DSL_PROD"
  val PARALLEL_LOAD_AID_COLUMN = " cast(mod(ORA_HASH(trade_id),100) as INTEGER)  "

  //DataModel

  val SELECT_HCE_MIG_COLUMNLIST_POF = "select UUID, TRADE_ID, POF_BINARY "

  val SELECT_HCE_MIG_AUG_COLUMLIST = "select UUID, CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE," +
    "DSL_OBJECT_TYPE, TRADE_ID , POF_BINARY, TRADE_VERSION ,SOURCE_SYSTEM ," +
    "REGION , EXTERNAL_ID , CACHE_STATUS, AUDIT_DATE , PARENT_ID," +
    "LINKED_PARENT_ID, EVICTION_DATE, IMPL_VERSION , XML_DOCUMENT "


  val  SELECT_HCE_MIG_COLUMLIST = "select UUID, CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE," +
    "DSL_OBJECT_TYPE, TRADE_ID , TRADE_VERSION ,SOURCE_SYSTEM ," +
    "REGION , EXTERNAL_ID , CACHE_STATUS, AUDIT_DATE , PARENT_ID," +
    "LINKED_PARENT_ID, EVICTION_DATE, IMPL_VERSION , XML_DOCUMENT "

  val  SELECT_HCE_MIG_COLUMLIST_POF_BIN_NO_PARTITION = "select UUID, TRADE_ID, TRADE_VERSION, POF_BINARY"


  val  SELECT_HCE_MIG_COLUMLIST1 = "select UUID, CONTROL_EVENT_STATUS,ERROR_STATE,FDSL_BUSINESS_DATE," +
    "DSL_OBJECT_TYPE, TRADE_ID , TRADE_VERSION ,SOURCE_SYSTEM ," +
    "REGION , EXTERNAL_ID , CACHE_STATUS, AUDIT_DATE , PARENT_ID," +
    "LINKED_PARENT_ID, EVICTION_DATE, IMPL_VERSION , XML_DOCUMENT, "+
    "'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/'|| source_system ||'/'||region ||'/'|| trade_id||'/'||trade_version||'/DSL/'||dsl_object_type as DOCPATH "
    //"'DSLObject/official/' || TO_CHAR(to_date(FDSL_BUSINESS_DATE, 'dd-mon-yy', 'nls_date_language = english'), 'yyyymmdd') || '/dslprod/'|| source_system ||'/'||region ||'/'|| trade_id||'/'||trade_version||'/DSL/'||dsl_object_type as DOCPATH "


//
//  def hadoopConfiguration(): Configuration ={
//    val conf = new Configuration()
//    conf.set("fs.s3a.access.key", uid1)
//    conf.set("fs.s3a.secret.key", secret1)
//    conf.set("fs.s3a.endpoint", host1)
//
//
//
//    return conf
//  }


  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    // ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", uid1)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret1)
    // end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", host1)
  }


}
package com.hsbc.trds.scala.config

import org.apache.spark.sql.SparkSession

object GcsConfig {
  val HSBC_11007943_TRDS_DEV = "hsbc-11007943-trds-dev";
  val GCP_GCS_SERVICE_ACCOUNT_EMAIL = "storage-admin@hsbc-11007943-trds-dev.iam.gserviceaccount.com"
  val GCP_BUCKET_NAME = "control-event-bucket-uk-1"
  val GCP_GS_BUCKET_ENDPOINT = "gs://"+GCP_BUCKET_NAME +"/"

  def primeSparkSessionForGCSConnection(spark: SparkSession): Unit = {
    spark.sparkContext.hadoopConfiguration.set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
    spark.sparkContext.hadoopConfiguration.set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
    spark.sparkContext.hadoopConfiguration.set("fs.gs.project.id", HSBC_11007943_TRDS_DEV)
    spark.sparkContext.hadoopConfiguration.set("google.cloud.auth.service.account.enable", "true")
    //spark.sparkContext().hadoopConfiguration().set("google.cloud.auth.service.account.email", GCP_GCS_SERVICE_ACCOUNT_EMAIL);
    //"/dsl/app/dslt12/gcs/hsbc-11007943-trds-dev-storage.json"
    val keyFile = System.getProperty("keyFile")
    spark.sparkContext.hadoopConfiguration.set("google.cloud.auth.service.account.json.keyfile", keyFile)
  }
}
package com.hsbc.trds.scala.config

import java.io.File
import java.util.Properties

import com.typesafe.config.{Config, ConfigFactory}

class TrdsConfig(fileNameOption: Option[String] = None) extends java.io.Serializable{

  def baseConfig: Config = {
    ConfigFactory.load()
  }

  def parseFile(file: File): Config = {
    ConfigFactory.parseFile(file).withFallback(baseConfig)
  }

  def parseResources(fileName: String): Config = {
    ConfigFactory.parseResources(fileName).withFallback(baseConfig)
  }

  val config: Config = {
    if (fileNameOption.getOrElse("").isEmpty)
      ConfigFactory.load
    else
      ConfigFactory
        .systemProperties
      .withFallback(ConfigFactory.systemEnvironment.withFallback
        (ConfigFactory.systemEnvironment().withFallback(ConfigFactory.parseFile(new File(fileNameOption.getOrElse("")))
        .resolve
        )))

  }

  /**
    * returns Config object for specified path.
    *
    * This object contains all the original APIs
    *
    * @param path - json path to a specific node
    * @return - a Config object
    */

   def getConfig(path: String): Config = {
     config.getConfig(path)
   }

  def getString(path: String): String = {
    config.getString(path)
  }

  /**
    * returns a string object based on the json path
    *
    * @param path
    * @return
    */
  def getProperties(path: String): Properties = {
    import scala.collection.JavaConversions._

    val properties: Properties = new Properties()

    config.getConfig(path)
      .entrySet()
      .map({
        entry => properties.setProperty(entry.getKey, entry.getValue.unwrapped().toString)
      })
    properties
  }


}
cat: ./src/main/scala/com/hsbc/trds/scala/driver: Is a directory
package com.hsbc.trds.scala.driver

import com.hsbc.trds.scala.config.TrdsConfig
import org.apache.spark.internal.Logging

object ConfigReader extends Logging {

  def main(args: Array[String]): Unit = {

    val fileName = System.getProperty("config.file")
    val environment = System.getProperty("env")

    val config = new TrdsConfig(Option(fileName))
    val properties = config.getProperties(environment+".ecs")

    log.info("uid is :: " + config.getString(environment+".ecs.uid"))
    log.info("props are :: " + properties.entrySet())

  }

}
package com.hsbc.trds.scala.driver

import com.hsbc.gbm.dsl.domain.ControlEvent
import com.hsbc.trds.endpoints.exa.pof.converter.{PofBytesConverterInput, PofBytesToControlEventConverter}
import com.hsbc.trds.scala.endpoints.ecs.{AvroHelper, EcsUtil}
import com.hsbc.trds.scala.endpoints.exa.ExaReader
import org.apache.avro.Schema
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.io.DecoderFactory
import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql._
import org.apache.spark.sql.functions.col


object ExaPofToEcsDriver extends Logging{

  val historicControlEventWithPofViewAlias = "Historic_Control_Event_POF"
 // val historicArchiveTblName = "P_20200523"
  val schemaName = "DSL_PROD"
  val pof_parquet = "historic_control_events_id_and_pof_1.parquet"
 //val pof_parquet = "abc1.parquet"
  val schema =
    """
      |{
      |    "type": "record",
      |    "name": "GenericControlEvent",
      |    "namespace": "com.hsbc.trds.avro",
      |
      |    "fields": [
      |        {"name": "Id", "type": "string"},
      |        {"name": "pofString", "type": "string"}
      |    ]
      |}
    """.stripMargin
  val messageSchema = new Schema.Parser().parse(schema)
  //val reader = new GenericDatumReader[GenericControlEvent](messageSchema)
  val avroDecoderFactory = DecoderFactory.get()

  implicit val encoder: Encoder[ControlEvent] = org.apache.spark.sql.Encoders.kryo[ControlEvent]

  implicit val encoder1: Encoder[GenericRecord] = org.apache.spark.sql.Encoders.kryo[GenericRecord]

  //implicit def kryoEncoder: Encoder[GenericControlEvent] = Encoders.kryo

  def main(args: Array[String]): Unit = {
    val historicArchiveTblName = args(0)
    val sparkConf = new SparkConf().setAppName("trds-protp-pof" + historicArchiveTblName).setMaster("yarn");
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    execute(sparkSession, historicArchiveTblName)
  }

  def execute(sparkSession: SparkSession, historicArchiveTblName: String ): Unit = {
    log.info("Extracting data from EXA")
    ExaReader.registerHistoricControlEventView(sparkSession, historicControlEventWithPofViewAlias, historicArchiveTblName)
    //val converter = new PofBytesToControlEventConverter
    val sdf = sparkSession.sql("select uuid, pof_binary from global_temp."+ historicControlEventWithPofViewAlias)
    val sdfPof = sdf.map(convert(_))(Encoders.BINARY)//.alias("data")

    //val sdfPofData = sdfPof.select(avro.from_avro(sdfPof.col("value"),schema).as("pofdata"))

    //val sdfPofData = sdfPof.select(avro.from_avro(sdfPof.col("value"),schema).as("pofdata"))

    //sdfPof.collect.foreach(println(_))
  //  log.info("sdfpof data schema" + sdfPofData.schema)
    //sdfPof.show(2)
    log.info("writing to ECS with pof extraction")
    EcsUtil.writePof(sdfPof)
    //EcsUtil.writePof(sdfPofData)
   // log.info("Reading Count from ECS " + pof_parquet)
   // EcsUtil.sqlOnParquet(sparkSession, "select count(*) from " + historicArchiveTblName,pof_parquet,historicArchiveTblName).show()
    //EcsUtil.sqlOnParquet(sparkSession, "select version, parentId.id, parentId.tradeId from " + historicArchiveTblName,pof_parquet,historicArchiveTblName).show()
   // log.info("Reading top 10 Records from ECS " + pof_parquet)

   //val result = EcsUtil.sqlOnParquet(sparkSession, "select * from " + historicArchiveTblName, pof_parquet, historicArchiveTblName)//.show(10)
   val finalResult = queryParquet(sparkSession,  pof_parquet, historicArchiveTblName)
   finalResult.show(2)
    //finalResult.collect.foreach(println(_))
  }

  def queryParquet(sparkSession: SparkSession,parquet: String, historicArchiveTblName : String): Dataset[GenericRecord] = {
    //primeSparkSessionForECSConnection(sparkSession)
    val historicTable = sparkSession.read.parquet("s3a://uslab-middle-office-s3-nonprod/"+ parquet)
    log.info("historicTable schema in: " + historicTable.schema)
    val finalResult = historicTable.select(col("value").as[Array[Byte]](Encoders.BINARY))
      .map(bytesData => {
          val helper = new AvroHelper
          val result = helper.read(bytesData, messageSchema)
          println("the bytes data decode : " + result.toString )
          result
      })(encoder1)
    //val finalDs = historicTable.select(avro.from_avro(historicTable.col("value"), schema).as("pf"))
     log.info("finalDs schema: " + finalResult.schema)
     finalResult
    //historicTable.select("Id").select("pofString")
  }


  def execute1(sparkSession: SparkSession, historicArchiveTblName: String ): Unit = {
    log.info("Extracting data from EXA")
    ExaReader.registerHistoricControlEventView(sparkSession, historicControlEventWithPofViewAlias, historicArchiveTblName)
    //val converter = new PofBytesToControlEventConverter
    val ds = EcsUtil.sqlOnParquet(sparkSession, "select * from " + historicArchiveTblName,pof_parquet,historicArchiveTblName)
    log.info("schema: " + ds.schema)
    ds.show()
    // log.info("Reading top 10 Records from ECS " + pof_parquet)
    //EcsUtil.sqlOnParquet(sparkSession, "select * from " + historicArchiveTblName, pof_parquet, historicArchiveTblName).show(10)
  }

 // def decode[T : TypeTag](bytes: Array[Byte]): T = ???

  def convert(row: Row): Array[Byte] = {
    //println(row.schema)
   // log.info("row is : " , row.getAs("uuid"))
   // log.info("control event " + row.toString())
    val converter = new PofBytesToControlEventConverter
    val pofBytesConverterInput =
      new PofBytesConverterInput(row.getAs("uuid").toString,row.getAs("pof_binary"))
    val result = converter.convert(pofBytesConverterInput)
//    val controlEvent: GenericControlEvent = GenericControlEvent.newBuilder()
//      .setId(result.getId.toString)
//      //.setPofString(result.toString)
//      .setPofString("test string")
//      .build()

    val genericControlEvent = new GenericData.Record(messageSchema)
    genericControlEvent.put("Id", result.getId.toString)
    genericControlEvent.put("pofString", result.toString)
    //genericControlEvent.put("pofString", "test coooo string")
    val helper = new AvroHelper
    helper.write(genericControlEvent, messageSchema)



    //log.info("control convert schema: " + controlEvent.getSchema)
    //val controlEvent: GenericControlEvent = new GenericControlEvent
    //controlEvent.setId(result.getId.toString)
    //controlEvent.setPofString(result.toString)


    //log.info("convert result : " + result.getBusinessDate + "--" + result.getRawCreatedOnAsDate)
   // log.info("convert result : " + result.toString)
    //genericControlEvent
    //result
  }

}
package com.hsbc.trds.scala.driver


import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

object ExaToEcsDriver {

  val historicControlEventViewAlias = "Historic_Control_Event"
  val historicArchiveTblName = "p_20130501"
  val schemaName = "DSL_PROD"
  val no_pof_parquet = "historic_control_events_no_pof_1.parquet"


  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName(("trds-without-pof"))
    val sparkSession = getSparkSession(sparkConf)
    execute(sparkSession)
  }

  def getSparkSession(conf: SparkConf): SparkSession = {
    SparkSession.builder().config(conf).getOrCreate()
  }

  def execute(sparkSession: SparkSession) = {

  }

}
package com.hsbc.trds.scala.driver

import java.util.Properties

import com.hsbc.gbm.dsl.domain.ControlEvent
import com.hsbc.trds.context.TRDSContext
import com.hsbc.trds.endpoints.exa.pof.converter.{PofBytesConverterInput, PofBytesToControlEventConverter}
import com.hsbc.trds.model.ExaDataModel
import com.hsbc.trds.scala.config.EcsConfig
import com.hsbc.trds.scala.endpoints.ecs.AvroHelper
import com.hsbc.trds.scala.endpoints.exa.ExaReader
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.{Schema, SchemaBuilder}
import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{DataFrameReader, Dataset, Encoder, Encoders, Row, SaveMode, SparkSession}
import org.apache.spark.sql.functions.{col,expr, udf}

object HCEMigrationMain extends Logging{

  implicit val encoder: Encoder[ControlEvent] = org.apache.spark.sql.Encoders.kryo[ControlEvent]
  implicit val encoder1: Encoder[GenericRecord] = org.apache.spark.sql.Encoders.kryo[GenericRecord]
  def main(args: Array[String]): Unit = {

    val context = new TRDSContext
    val partitionColumns = Array(ExaDataModel.TRADE_ID_HASH)
    context.setPartitionColumns(partitionColumns)
    context.setPartitionTblName(args(1))
    context.setObjectStorageRootLocation(args(2))
    context.setExaConnectionCount(args(3).toLong)
    context.setExaMaxFetchRows(args(4).toLong)

    //val pofExtraction: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
     // extractPof(uuid,pofBinary)
   // }
   // val pofExtractionFunc = udf(pofExtraction)

    //val getControlEvent: ((String, String) => String) = (uuid: String, tradeId: String) => {s"{$uuid}_{$tradeId}"}
    //val controlEventFunc = udf(getControlEvent)

//    val augmented: ((String, Row) => Row) = (uuid: String, pofRow: Row)=> {
//      null
//
//    }
//    val augmentedPofFunc = udf(augmented)
//    val deserPof: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
//      //extractPof(uuid,pofBinary).getCreatedOn.toString
//      null
//    }
//    val deserPofFunc = udf(deserPof)


    val sparkConf = new SparkConf().setAppName("trds-test- ".concat(context.getPartitionTblName)
      .concat(" -> ").concat("args(2)")).setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    EcsConfig.primeSparkSessionForECSConnection(sparkSession)

   val result = ExaReader.getHistoricControlEventRowDatasetForPartition(sparkSession,context)

//    val result = getEXADataFrameReader(sparkSession, getConnectionProperties())
//      .option("dbtable", s"( {$EcsConfig.SELECT_HCE_MIG_COLUMNLIST_POF} , {$EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash from ${EcsConfig.SCHEMA_NAME}.{$args(1)})")
//      .option("partitionColumn",  "thash")
//      .option("lowerBound", 0l)
//      .option("upperBound", 99l)
//      .option("numPartitions", 100)
//      .option("fetchSize", 1000)
//      .load()

    result.createOrReplaceTempView("tradeHashTbl")
    val sqlQuery = s"${EcsConfig.SELECT_HCE_MIG_COLUMNLIST_POF}, abs(mod(HASH(TRADE_ID),100)) as ${EcsConfig.TRADE_ID_HASH} from tradeHashTbl"
    val trdDS = result.sparkSession.sql(sqlQuery)
    trdDS.printSchema

//    val udfDataSet = trdDS.withColumn("ControlEventId", controlEventFunc(col("UUID"),col("TRADE_ID")))
//      .withColumn("PofString", pofExtractionFunc(col("UUID"),col("POF_BINARY")))
//    udfDataSet.printSchema
//    udfDataSet.show(2)


    val pofRecDS = processPofData(trdDS)

    //val deserDS = pofRecDS.select(from_avro(col("value"),pofSchema.toString(true))as 'data)
    //val deserDS = pofRecDS.select(from_avro(col("value"),pofSchema.toString(true))as 'data)
    log.info("deserializeDS schema: ")
    pofRecDS.printSchema()

    val resultDS = pofRecDS.select("data")
      .select(expr("data.uuid").as("uuid"),
        expr("data.tradeId").as("tradeId"),
        expr("data.controlEventId").as("controlEventId"),
        expr("data.pofString").as("pofString"),
        expr("data.tradeIdHash").as("tradeIdHash")
      )

    log.info("finalDS schema: ")
    resultDS.printSchema()

   // new EndPointWriter().write(finalDS, EcsConfig.ECS, context, partitionColumns)

    //val queryResult = queryParquet(sparkSession, EcsConfig )

    resultDS.write.partitionBy(partitionColumns: _*)
      .mode(SaveMode.Overwrite)
      .parquet(EcsConfig.s3bucketName1.concat(context.getObjectStorageRootLocation))

    log.info("the resultant data : ")
    val readDS = sparkSession.read.parquet(EcsConfig.s3bucketName1.concat(context.getObjectStorageRootLocation))
    readDS.show(10,false)

  }

  def processPofData(ds: Dataset[Row]): Dataset[Array[Byte]] = ds.map(convert(_))(Encoders.BINARY)

  def convert(row: Row): Array[Byte] = {
    val converter = new PofBytesToControlEventConverter

    //controlevent - custom object
    val pofBytesConverterInput =
      new PofBytesConverterInput(row.getAs("UUID").toString,row.getAs("POF_BINARY"))
    val result = converter.convert(pofBytesConverterInput)

    val pofRecord = new GenericData.Record(pofSchema)
    pofRecord.put("uuid", result.getId.toString)
    pofRecord.put("tradeId", result.getTradeId)
    pofRecord.put("controlEventId", result.getId.toString.concat("_").concat(result.getTradeId))

    pofRecord.put("pofString", result.toString)
    // pofRecord.put("pofBinary", row.getAs[Array[Byte]]("POF_BINARY"))
    pofRecord.put("tradeIdHash", row.getAs[String]("tradeIdHash"))
    //pofRecord
    val helper = new AvroHelper
    helper.write(pofRecord, pofSchema)
  }

  def pofSchema: Schema = {
    SchemaBuilder.record("POFRecord").fields()
      .name("uuid").`type`().nullable().stringType().noDefault()
      .name("tradeId").`type`().nullable().stringType().noDefault()
      .name("controlEventId").`type`().nullable().stringType().noDefault()
      .name("pofString").`type`().nullable().stringType().noDefault()
      //.name("pofBinary").`type`().nullable().bytesBuilder().endBytes().noDefault()
      .name("tradeIdHash").`type`().nullable().stringType().noDefault()
      .endRecord()
  }


  def getEXADataFrameReader(sparkSession: SparkSession, connectionProps: Properties): DataFrameReader = {
    sparkSession.read
      .format("jdbc")
      .option("url", connectionProps.getProperty("url"))
      .option("password", connectionProps.getProperty("password"))
      .option("driver", connectionProps.getProperty("driver"))
      .option("user", connectionProps.getProperty("user"))

  }

  def extractPof(uuid:String, pofBinary: Array[Byte]) : String = {
    val converter = new PofBytesToControlEventConverter
    val pofBytesConverterInput = new PofBytesConverterInput(uuid,pofBinary)
    val result = converter.convert(pofBytesConverterInput)

    val pofRecord = new GenericData.Record(pofSchema)
    pofRecord.put("uuid", result.getId.toString)
    pofRecord.put("tradeId", result.getTradeId)
    pofRecord.put("controlEventId", result.getId.toString.concat("_").concat(result.getTradeId))
    pofRecord.put("pofString", result.toString)
    // pofRecord.put("pofBinary", row.getAs[Array[Byte]]("POF_BINARY"))
    //pofRecord.put("tradeIdHash", row.getAs[String]("tradeIdHash"))
    result.toString
  }

}
package com.hsbc.trds.scala.driver

import com.hsbc.trds.context.TRDSContext
import org.apache.avro.Schema
import org.apache.avro.generic.GenericRecord
import org.apache.spark.sql.{Dataset, Row, SparkSession}

trait HCEMigrationSparkDriver {

  def registerSQLModel(exaData: Dataset[Row], tblName: String) : Boolean
  def addTradeIdHash(exaData: Dataset[Row], queryType: String) : Dataset[Row]
  def extractExaData(spark: SparkSession, context: TRDSContext, runType: String): Dataset[Row]
  def writePofAndNonPofIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema)
  def writePofIntoECS(data: Dataset[Array[Byte]], context: TRDSContext, filePath: String, schema: Schema)
  def writeNonPofIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema)
  def writeNonPofIntoGCS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema)
  def writePofIntoECSAndNonPofIntoGCS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema)
  def writePofBinaryIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema)

}
package com.hsbc.trds.scala.driver
import java.net.URI
import java.text.SimpleDateFormat
import java.time.LocalDateTime
import java.util.UUID

import com.hsbc.gbm.dsl.domain.ControlEvent
import com.hsbc.trds.context.TRDSContext
import com.hsbc.trds.endpoints.exa.pof.converter.{PofBytesConverterInput, PofBytesToControlEventConverter}
import com.hsbc.trds.endpoints.GCPConfig
import com.hsbc.trds.model.ExaDataModel
import com.hsbc.trds.scala.config.{EcsConfig, GcsConfig}
import com.hsbc.trds.scala.endpoints.EndPointWriter
import com.hsbc.trds.scala.endpoints.ecs.AvroHelper
import com.hsbc.trds.scala.endpoints.exa.ExaReader
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.{Schema, SchemaBuilder}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path
import org.apache.parquet.avro.AvroParquetWriter
import org.apache.parquet.hadoop.ParquetFileWriter
import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.sql.functions.{col, expr, udf}
import org.apache.spark.sql.{Dataset, Encoder, Encoders, Row, SparkSession}

object HCEMigrationSparkDriverImpl extends HCEMigrationSparkDriver with Logging{
  implicit val encoder1: Encoder[GenericRecord] = org.apache.spark.sql.Encoders.kryo[GenericRecord]

  override def registerSQLModel(exaData: Dataset[Row], tblName: String): Boolean = {
    exaData.createOrReplaceTempView(tblName)
    true
  }

  override def addTradeIdHash(exaData: Dataset[Row], selectQueryType: String): Dataset[Row] =  {
    val partitionTbl = "partitionTbl"
    registerSQLModel(exaData, partitionTbl)
    val sqlQuery = s"$selectQueryType, DOCPATH, abs(mod(HASH(trade_id),100)) as ${EcsConfig.TRADE_ID_HASH} from $partitionTbl"
    val result = exaData.sparkSession.sql(sqlQuery)
    log.info("addTradeIdHash schema: ")
    result.printSchema
    result
  }

  override def extractExaData(spark: SparkSession, context: TRDSContext, runType: String): Dataset[Row] = {
    val result = runType match {
      case EcsConfig.ECS_POF_BIN => ExaReader.getHistoricControlEventRowEcsPOFDatasetForPartition(spark, context)
      case EcsConfig.NONPOF_GCS => ExaReader.getHistoricControlEventRowDatasetForPartition(spark,context)
      case EcsConfig.NONPOF_AUG_GCS => ExaReader.getHistoricControlEventAugRowDatasetForPartition(spark,context)
    }
    result
  }

  override  def writePofBinaryIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema): Unit = {
    //data.cache()
    val writer = new EndPointWriter
    writer.writePofBinary(data, EcsConfig.ECS,context, context.getPartitionColumns)
    val ecsStasRecord = ecsInitialStatus(context, schema)
    ecsStasRecord.put("srcRecordCount",data.count())
    writeToParquet(ecsStasRecord, schema, filePath,  data.sparkSession.sparkContext.hadoopConfiguration)
  }

  override def writePofAndNonPofIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema): Unit = ???

  override def writePofIntoECS(data: Dataset[Array[Byte]], context: TRDSContext, filePath: String, schema: Schema): Unit = {
    //data.cache()
    log.info("writePofAndNonPofIntoECS schema: ")
    data.printSchema
    val writer = new EndPointWriter
    writer.write(data, EcsConfig.ECS,context, context.getPartitionColumns)
    val ecsStasRecord = ecsInitialStatus(context, schema)
    ecsStasRecord.put("srcRecordCount",data.count)
    writeToParquet(ecsStasRecord, schema, filePath,  data.sparkSession.sparkContext.hadoopConfiguration)
  }

  override def writeNonPofIntoECS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema): Unit = ???

  override def writeNonPofIntoGCS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema): Unit = {
  //  data.cache()
    new EndPointWriter().gcsWrite(data, EcsConfig.G_CS, context, context.getPartitionColumns)
    val gcsStasRecord = gcsInitialStatus(context, schema)
    gcsStasRecord.put("srcRecordCount", data.count)
    writeToParquet(gcsStasRecord, schema, filePath,  data.sparkSession.sparkContext.hadoopConfiguration)
  }

  override def writePofIntoECSAndNonPofIntoGCS(data: Dataset[Row], context: TRDSContext, filePath: String, schema: Schema): Unit = ???

  def ecsInitialStatus(context : TRDSContext, schema: Schema) : GenericData.Record = {
    val ecsStatRecord = new GenericData.Record(schema)
    ecsStatRecord.put("partitionTableName", context.getPartitionTblName());
    ecsStatRecord.put("startTime", LocalDateTime.now().toString());
    ecsStatRecord.put("objectStorageRootLocation", context.getObjectStorageRootLocation());
    ecsStatRecord.put("objectStorageEndpoint", EcsConfig.host1);
    ecsStatRecord.put("bucket", EcsConfig.bucketName1);
    ecsStatRecord.put("exaEndPointUrl", ExaReader.getConnectionProperties().getProperty("url"));
    ecsStatRecord.put("exaParallelConnections", context.getExaConnectionCount());
    ecsStatRecord.put("exaMaxRowsPerConnection", context.getExaMaxFetchRows());
    ecsStatRecord.put("extraParameters", context.getExtraParameters());

    ecsStatRecord
  }

  def gcsInitialStatus(context : TRDSContext, schema: Schema) : GenericData.Record = {
    val gcsStatRecord = new GenericData.Record(schema)
    gcsStatRecord.put("partitionTableName", context.getPartitionTblName());
    gcsStatRecord.put("startTime", LocalDateTime.now().toString());
    gcsStatRecord.put("objectStorageRootLocation", context.getObjectStorageRootLocation());
    gcsStatRecord.put("objectStorageEndpoint", GCPConfig.HSBC_11007943_TRDS_DEV);
    gcsStatRecord.put("bucket", GCPConfig.GCP_BUCKET_NAME);
    gcsStatRecord.put("exaEndPointUrl", ExaReader.getConnectionProperties().getProperty("url"));
    gcsStatRecord.put("exaParallelConnections", context.getExaConnectionCount());
    gcsStatRecord.put("exaMaxRowsPerConnection", context.getExaMaxFetchRows());
    gcsStatRecord.put("extraParameters", context.getExtraParameters());

    gcsStatRecord
  }

  def writeToParquet(record: GenericData.Record, schema: Schema, filePath: String, conf: Configuration): Unit = {
    val uri = new URI(filePath)
    val path = new Path(uri)
   // val conf = EcsConfig.hadoopConfiguration()
    val writer = AvroParquetWriter.builder[GenericData.Record](path)
      .withWriteMode(ParquetFileWriter.Mode.OVERWRITE).withSchema(schema).withConf(conf).build()
    writer.write(record)
    writer.close
  }


  def processEcsPofBin(sparkSession: SparkSession, context: TRDSContext) ={
   // val dataset: Dataset[Row] = addTradeIdHash(extractExaData(sparkSession,context,EcsConfig.ECS_POF_BIN),EcsConfig.SELECT_HCE_MIG_COLUMNLIST_POF)

    val dataset: Dataset[Row] = extractExaData(sparkSession,context,EcsConfig.ECS_POF_BIN)
    val getControlEvent: ((String, String) => String) = (uuid: String, tradeId: String) => {uuid.concat("_").concat(tradeId)}
    val controlEventFunc = udf(getControlEvent)

    sparkSession.udf.register("controlEventFunc",controlEventFunc)
    log.info("processEcsPofBin- schema of dataset: ")
    dataset.printSchema

    val udfDataset = dataset.withColumn("ControlEventID",controlEventFunc(col("UUID"),col("TRADE_ID")))

    log.debug("processEcsPofBin - schema of udfDataset: ")
    udfDataset.printSchema

    val  filePath = EcsConfig.s3bucketName1.concat("/ecs_stats_pof_2/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet")
    writePofBinaryIntoECS(udfDataset, context, filePath, statsSchema)
  }

  def docPath(row: Row): String = {
    //log.info("row schema: " + row.schema.printTreeString())
    val fdslBusDate = row.getAs[String]("FDSL_BUSINESS_DATE")
    val sourceSystem = row.getAs[String]("SOURCE_SYSTEM")
    val region = row.getAs[String]("REGION")
    val tradeId = row.getAs[String]("TRADE_ID")
    val tradeVersion = row.getAs[String]("TRADE_VERSION")
    val dslObjectType = row.getAs[String]("DSL_OBJECT_TYPE")
    val fdslParString: String = fdslParDate(fdslBusDate)
    s"DSLObject/official/$fdslParString/dslprod/$sourceSystem/$region/$tradeId/$tradeVersion/DSL/$dslObjectType"
  }

  private def fdslParDate(fdslBusDate: String) = {
    val newDateFormat = new SimpleDateFormat("dd-MMM-yy")
    val parsedDate = newDateFormat.parse(fdslBusDate)
    newDateFormat.applyPattern("yyyyMMdd")
    val fdslParString = newDateFormat.format(parsedDate)
    fdslParString
  }

  def processNonPofGCS(sparkSession: SparkSession, context: TRDSContext) = {
    val dataset: Dataset[Row] = addTradeIdHash(extractExaData(sparkSession,context,EcsConfig.NONPOF_GCS),EcsConfig.SELECT_HCE_MIG_COLUMLIST)

//    val getControlEvent: ((String, String) => String) = (uuid: String, tradeId: String) => {s"{$uuid}_{$tradeId}"}
//    val controlEventFunc = udf(getControlEvent)

   // val getDocPath = udf((row: Row) => docPath(row))

    //log.info("processNonPofGCS- schema of dataset: ")
   // dataset.printSchema

    //sparkSession.udf.register("controlEventFunc",controlEventFunc)
   // sparkSession.udf.register("docPathFun",getDocPath)

   // val udfDataset = dataset //.withColumn("ControlEventID",controlEventFunc(col("UUID"),col("TRADE_ID")))
     //                     .withColumn("DOCPATH",getDocPath(struct(dataset.columns.map(col): _*)))

    log.info("processNonPofGCS - schema of udfDataset: ")
    dataset.printSchema

    //val noPofDataSet = udfDataset.drop(col("POF_BINARY"))
    //noPofDataSet.printSchema

    val  filePath = GcsConfig.GCP_GS_BUCKET_ENDPOINT.concat("gcs_stats_pof_1/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet")
    writeNonPofIntoGCS(dataset, context, filePath, statsSchema)

  }

  def processNonPofAugGCS(sparkSession: SparkSession, context: TRDSContext) = {
    //val dataset: Dataset[Row] = addTradeIdHash(extractExaData(sparkSession,context,EcsConfig.NONPOF_GCS),EcsConfig.SELECT_HCE_MIG_AUG_COLUMLIST)
    val dataset: Dataset[Row] = extractExaData(sparkSession,context,EcsConfig.NONPOF_AUG_GCS)

    val getControlEvent: ((String, String) => String) = (uuid: String, tradeId: String) => {uuid.concat("_").concat(tradeId)}
    val controlEventFunc = udf(getControlEvent)

    log.info("processNonPofGCS- schema of dataset: ")
    dataset.printSchema


//    val pofString: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
//      extractPof(uuid,pofBinary).toString
//    }
//    val pofStringFunc = udf(pofString)

    val createdOn: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
          extractPof(uuid,pofBinary).getCreatedOn.toString
    }
    val createOnFunc = udf(createdOn)

    val productType: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
      extractPof(uuid,pofBinary).getEventParameters.getParameter("productType").asInstanceOf[String]
    }
    val productTypeFunc = udf(productType)

    val assetClass: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
      extractPof(uuid,pofBinary).getEventParameters.getAssetClass
    }
    val assetClassFunc = udf(assetClass)

    val externalId: ((String, Array[Byte]) => String) = (uuid: String, pofBinary: Array[Byte]) => {
      extractPof(uuid,pofBinary).getEventParameters.getExternalId
    }
    val externalIdFunc = udf(externalId)

    sparkSession.udf.register("controlEventFunc",controlEventFunc)
    sparkSession.udf.register("createdOn",createOnFunc)
    sparkSession.udf.register("productType",productTypeFunc)
    sparkSession.udf.register("assetClass",assetClassFunc)
    sparkSession.udf.register("externalId",externalIdFunc)

    log.info("processEcsPofBin- schema of dataset: ")
    dataset.printSchema

    val udfDataset = dataset.withColumn("ControlEventID",controlEventFunc(col("UUID"),col("TRADE_ID")))
        .withColumn("CreatedOn",createOnFunc(col("UUID"),col("POF_BINARY")))
        .withColumn("ProductType", productTypeFunc(col("UUID"),col("POF_BINARY")))
        .withColumn("AssetClass", assetClassFunc(col("UUID"),col("POF_BINARY")))
        .withColumn("ExternalId", externalIdFunc(col("UUID"),col("POF_BINARY")))


    log.info("processNonPofGCS - schema of udfDataset: ")
    udfDataset.printSchema

    val noPofDataSet = udfDataset.drop(col("POF_BINARY"))
    noPofDataSet.printSchema


    val  filePath = GcsConfig.GCP_GS_BUCKET_ENDPOINT.concat("gcs_stats_nonpof_aug/").concat(context.getPartitionTblName()).concat("_"+ UUID.randomUUID().toString()+".parquet")
    writeNonPofIntoGCS(noPofDataSet, context, filePath, statsSchema)

  }

  def extractPof(uuid:String, pofBinary: Array[Byte]) : ControlEvent = {
    val converter = new PofBytesToControlEventConverter
    val pofBytesConverterInput = new PofBytesConverterInput(uuid,pofBinary)
    val result = converter.convert(pofBytesConverterInput)

    result
  }


  def fillContext(args: Array[String]): TRDSContext = {
    val context = new TRDSContext
    //TODO - need to reconsider for ECS
    val partitionColumns = Array(ExaDataModel.DSL_OBJECT_TYPE,ExaDataModel.TRADE_ID_HASH)
    context.setPartitionColumns(partitionColumns)
    context.setPartitionTblName(args(1))
    context.setObjectStorageRootLocation(args(2))
    context.setExaConnectionCount(args(3).toLong)
    context.setExaMaxFetchRows(args(4).toLong)
  }

  def main(args: Array[String]): Unit = {

    val context = fillContext(args)
    val sparkSession: SparkSession = getSparkSession(context)

    args(0) match {
      case "POF-NONPOF-ECS" => println("POF-NONPOF-ECS")
      case "POF-ECS" => println("POF-ECS")
      case "NONPOF-ECS" => println("NONPOF-ECS")
      case "NONPOF-GCS" => processNonPofGCS(sparkSession,context)
      case "NONPOF-GCS-AUG" => processNonPofAugGCS(sparkSession,context)
      case "POFECS-NONPOFGCS" => println("POFECS-NONPOFGCS")
      case "ECS-POF-BIN" => processEcsPofBin(sparkSession,context)
      case _ => throw new IllegalArgumentException("The first argument should be in the list of options" +
        "POF-NONPOF-ECS|POF-ECS|NONPOF-ECS|NONPOF-GCS|POFECS-NONPOFGCS")
    }

  }

  def readParquet(pofDataSet: Dataset[Array[Byte]]): Dataset[GenericRecord] = {
    val deserializeData = pofDataSet.map(bytesData => {
      val helper = new AvroHelper
      val result =  helper.read(bytesData, pofSchema)
      println("the bytes data decode : " + result.toString )
      result
    })(encoder1)
    deserializeData
  }

  def queryParquet(sparkSession: SparkSession,fileParquetPath: String): Dataset[GenericRecord] = {
    //primeSparkSessionForECSConnection(sparkSession)
    val historicTable = sparkSession.read.parquet(fileParquetPath)
    log.info("historicTable schema in: " + historicTable.schema)
    val finalResult = historicTable.select(col("value").as[Array[Byte]](Encoders.BINARY))
      .map(bytesData => {
        val helper = new AvroHelper
        val result = helper.read(bytesData, pofSchema)
        println("the bytes data decode : " + result.toString )
        result
      })(encoder1)
    //val finalDs = historicTable.select(avro.from_avro(historicTable.col("value"), schema).as("pf"))
    log.info("finalDs schema: " + finalResult.schema)
    finalResult
    //historicTable.select("Id").select("pofString")
  }

  def processPofData(ds: Dataset[Row]): Dataset[Array[Byte]] = ds.map(convert(_))(Encoders.BINARY)

  def processPofData1(ds: Dataset[Row]): Dataset[Row] = {
    val convertedDS = ds.map(convert(_))(Encoders.BINARY)
    log.info("convertedDS -->")
    convertedDS.printSchema

    val deserializeData = readParquet(convertedDS)

    log.info("deserializeData -->")
    deserializeData.printSchema
    convertedDS.select(col("value"))
    val result = deserializeData.select("value").select(expr("UUID"),expr("TRADE_ID"),expr("POF_BINARY"),expr("tradeIdHash"))
    result

  }



  def getPof(uuid: String, value: Array[Byte]) : String = {

    val converter = new PofBytesToControlEventConverter

    val pofBytesConverterInput =
      new PofBytesConverterInput(uuid, value)
    val result = converter.convert(pofBytesConverterInput)
    result.toString

  }


  def convert(row: Row): Array[Byte] = {
    val converter = new PofBytesToControlEventConverter

    //controlevent - custom object
    val pofBytesConverterInput =
      new PofBytesConverterInput(row.getAs("UUID").toString,row.getAs("POF_BINARY"))
    val result = converter.convert(pofBytesConverterInput)

    val pofRecord = new GenericData.Record(pofSchema)
    pofRecord.put("uuid", result.getId.toString)
    pofRecord.put("tradeId", result.getTradeId)
    pofRecord.put("controlEventId", result.getId.toString.concat("_").concat(result.getTradeId))
    pofRecord.put("pofString", result.toString)
   // pofRecord.put("pofBinary", row.getAs[Array[Byte]]("POF_BINARY"))
    //pofRecord.put("tradeIdHash", row.getAs[String]("tradeIdHash"))
    val helper = new AvroHelper
    helper.write(pofRecord, pofSchema)
  }


  def getSparkSession(context: TRDSContext):SparkSession = {
    val sparkConf = new SparkConf().setAppName(s"trds-proto_exa-to-ecs- ${context.getPartitionTblName} -> " +
      s"${context.getObjectStorageRootLocation}").setMaster("yarn")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    EcsConfig.primeSparkSessionForECSConnection(sparkSession)
    GcsConfig.primeSparkSessionForGCSConnection(sparkSession)
    sparkSession
  }

  def pofSchema: Schema = {
    SchemaBuilder.record("POFRecord").fields()
      .name("uuid").`type`().nullable().stringType().noDefault()
      .name("tradeId").`type`().nullable().stringType().noDefault()
      .name("controlEventId").`type`().nullable().stringType().noDefault()
      .name("pofString").`type`().nullable().stringType().noDefault()
      //.name("pofBinary").`type`().nullable().bytesBuilder().endBytes().noDefault()
      //.name("tradeIdHash").`type`().nullable().stringType().noDefault()
      .endRecord()
  }

  def statsSchema: Schema = {
    SchemaBuilder.record("HCEWriteStat").fields()
      .name("partitionTableName").`type`().nullable().stringType().noDefault()
      .name("startTime").`type`().nullable().stringType().noDefault()
      .name("endTime").`type`().nullable().stringType().noDefault()
      .name("objectStorageFootPrintMB").`type`().nullable().floatType().floatDefault(0.0f)
      .name("objectStorageRootLocation").`type`().nullable().stringType().noDefault()
      .name("bucket").`type`().nullable().stringType().noDefault()
      .name("objectStorageEndpoint").`type`().nullable().stringType().noDefault()
      .name("exaEndPointUrl").`type`().nullable().stringType().noDefault()
      .name("exaParallelConnections").`type`().nullable().longType().longDefault(0l)
      .name("exaMaxRowsPerConnection").`type`().nullable().longType().longDefault(0l)
      .name("extraParameters").`type`().nullable().map().values().stringType().noDefault()
      .name("srcRecordCount").`type`().nullable().longType().longDefault(0l)
      .name("sinkRecordCount").`type`().nullable().longType().longDefault(0l)
      .endRecord
  }

}
cat: ./src/main/scala/com/hsbc/trds/scala/endpoints: Is a directory
cat: ./src/main/scala/com/hsbc/trds/scala/endpoints/ecs: Is a directory
package com.hsbc.trds.scala.endpoints.ecs

import java.io.{ByteArrayOutputStream, File}
import java.util

import org.apache.avro.Schema
import org.apache.avro.file.SeekableByteArrayInput
import org.apache.avro.generic.{GenericData, GenericDatumReader, GenericDatumWriter, GenericRecord}
import org.apache.avro.io.{DecoderFactory, EncoderFactory}
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.types.StructType

class AvroHelper {
  def write(record: org.apache.avro.generic.GenericData.Record, schema: org.apache.avro.Schema): Array[Byte] = {
    val outputStream = new ByteArrayOutputStream()
    val datumWriter = new GenericDatumWriter[GenericRecord](schema)
    val encoder = EncoderFactory.get.binaryEncoder(outputStream, null)
   //val encoder = EncoderFactory.get.jsonEncoder(schema, outputStream)
    datumWriter.write(record, encoder)
    encoder.flush

    outputStream.toByteArray
  }

  def writeGen(record: org.apache.avro.generic.GenericData.Record, schema: org.apache.avro.Schema): GenericRecord = {
    val outputStream = new ByteArrayOutputStream()
    val datumWriter = new GenericDatumWriter[GenericRecord](schema)
    val encoder = EncoderFactory.get.binaryEncoder(outputStream, null)
    //val encoder = EncoderFactory.get.jsonEncoder(schema, outputStream)
    datumWriter.write(record, encoder)
    encoder.flush

    read(outputStream.toByteArray,schema)
  }

  def read(bytes: Array[Byte], schema: org.apache.avro.Schema): org.apache.avro.generic.GenericRecord = {
    val datumReader = new GenericDatumReader[GenericRecord](schema)
    val inputStream = new SeekableByteArrayInput(bytes)
    val decoder = DecoderFactory.get.binaryDecoder(inputStream, null)
   // val decoder = DecoderFactory.get.jsonDecoder(schema, inputStream)
    val record = new GenericData.Record(schema)
    datumReader.read(record, decoder)
  }

  def avroToJson(jsonString: String,schema: Schema): Array[Byte] = {
    val datumReader = new GenericDatumReader[GenericRecord](schema)
    val datumWriter = new GenericDatumWriter[GenericRecord](schema)
    val output = new ByteArrayOutputStream
    val decoder = DecoderFactory.get().jsonDecoder(schema, jsonString)
    val encoder = EncoderFactory.get.binaryEncoder(output, null)
    val datum = datumReader.read(null, decoder)
    datumWriter.write(datum, encoder)
    encoder.flush
    output.toByteArray
  }
}

object AvroHelper {

  implicit val encoder: Encoder[GenericRecord] = org.apache.spark.sql.Encoders.kryo[GenericRecord]

  def main(args: Array[String]): Unit = {
    val schema =
      """
        |{
        |    "type": "record",
        |    "name": "GenericControlEvent",
        |    "namespace": "com.hsbc.trds.avro",
        |
        |    "fields": [
        |        {"name": "Id", "type": "string"},
        |        {"name": "pofString", "type": "string"}
        |    ]
        |}
      """.stripMargin

    val schemaObj = new org.apache.avro.Schema.Parser().parse(schema)

    val rec1 = new GenericData.Record(schemaObj)
    rec1.put("Id", "0039000022330456_0x0000013E6108F2CC800BDC1602D9D55A6E33FBF41B76BA0AC67478464C0D1DAC")
    rec1.put("pofString", "xxxxxxxxxxxxxxx")

    val avroHelper = new AvroHelper()

    val bytes = avroHelper.write(rec1, schemaObj)

    println(avroHelper.read(bytes, schemaObj).toString)

    val schemanew = new org.apache.avro.Schema.Parser().parse(new File("C:\\trds\\TRDS_PROTOTYPING\\src\\main\\resources\\avro\\stat.avsc"))

    val newrec = new GenericData.Record(schemanew)

    newrec.put("partitionTableName", "p_20130501")
    newrec.put("startTime", "2020-10-03T19:54:28.385")
    newrec.put("endTime", "2020-12-03T19:54:28.385")
    newrec.put("objectStorageFootPrintMB", 400.4f)
    newrec.put("objectStorageRootLocation", "s3://data-bucket//")
    newrec.put("bucket", "history-bucket")
    newrec.put("exaEndPointUrl", "s3://data-url//")
    newrec.put("exaParallelConnections", 20L)
    newrec.put("exaMaxRowsPerConnection", 100L)

//    val record1 = new StructType()
//    record1.add("JMSMessageID","ID:414d51205047424c485353414732202048a26951ac11b124")
//    record1.add("JMS_IBM_Character_Set","UTF-8")
//    record1.add("businessDateTime","2013-05-01T18:42:24+00:00")
//
//    val record2 = new StructType()
//    record2.add("JMSXAppID","WebSphere MQ Client for Java")
//    record2.add("JMS_IBM_PutDate","20130501")
//    record2.add("region","HBUS")

    val extraParameters = new util.HashMap[String,String]()
    extraParameters.put("JMSMessageID", "ID:414d51205047424c485353414732202048a26951ac11b124")
    extraParameters.put("JMS_IBM_PutDate", "20130501")
    extraParameters.put("businessDateTime", "2013-05-01T18:42:24+00:00")


    newrec.put("extraParameters",extraParameters)

    newrec.put("srcRecordCount", 50L)
    newrec.put("sinkRecordCount", 150L)

    val bytes2 = avroHelper.write(newrec, schemanew)

    println(avroHelper.read(bytes2, schemanew).toString)


  }

}
package com.hsbc.trds.scala.endpoints.ecs

import java.io.File

import com.amazonaws.auth.BasicAWSCredentials
import com.amazonaws.services.s3.model.S3Object
import com.amazonaws.services.s3.{AmazonS3Client, S3ClientOptions}
import com.hsbc.trds.scala.driver.ExaPofToEcsDriver.pof_parquet
import com.hsbc.trds.scala.driver.ExaToEcsDriver
import com.hsbc.trds.scala.driver.ExaToEcsDriver.no_pof_parquet
import org.apache.avro.generic.GenericRecord
import org.apache.spark.internal.Logging
import org.apache.spark.sql._


object EcsUtil extends Logging {
  private val UID = "uslab-middle-office-s3-nonprod-u"
  private val HOST = "http://ecs-walden-lab.us.hsbc:9020"
  private val SECRET = "LOMWus6RzNGZINZVHAfwhm6SCQt4XkV5BrnLBzhz"
  private val BUCKETNAME = "uslab-middle-office-s3-nonprod"

//  dsl.cloud.ecs.host=http://ecs-storage.it.global.hsbc:9020
//  dsl.cloud.ecs.uid=uk_dsl-ss-1_s3_nonprod_u
//  dsl.cloud.ecs.secretKey=ApiDuvw7HEoU9royVZlY2NVrL2+jmwlqjt6+Yykf
//  dsl.cloud.ecs.bucketName=uk_dsl-ss-1_s3_nonprod_test

  //implicit val encoder: Encoder[GenericControlEvent] = org.apache.spark.sql.Encoders.kryo[GenericControlEvent]
  implicit def kryoEncoder: Encoder[GenericRecord] = Encoders.kryo

  def main(args: Array[String]): Unit = {
    val client = getS3ECSClient()
    val l = client.listObjects("uslab-middle-office-s3-nonprod")
    println(l)
  }

  def getS3ECSClient(): AmazonS3Client = {
    val client = new AmazonS3Client(new BasicAWSCredentials(UID, SECRET))
    //needed as otherwise it will attempt to connect to Amazon
    client.setEndpoint(HOST)
    val options = new S3ClientOptions()
    options.setPathStyleAccess(true)
    client.setS3ClientOptions(options)
    client
  }

  def putObject(key: String, file: File)  = {
    getS3ECSClient().putObject(BUCKETNAME,key,file)
  }

  def getObject(key: String): S3Object = {
    getS3ECSClient().getObject(BUCKETNAME,key)
  }

  def deleteObject(key: String) = {
    getS3ECSClient().deleteObject(BUCKETNAME, key)
  }

  def primeSparkSessionForECSConnection(spark: SparkSession): Unit = {
    //Spark S3- ECS
    //ECS S3 key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", UID)
    // Secret key
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", SECRET)
    //end point
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", HOST)
  }

  /**
    *
    * @param data
    * @param partitionCols - NOT using this as yet, hoping the read time partition will stick as the task save down data.
    */
    def write(data: Dataset[Row], partitionCols: String*) = {
      primeSparkSessionForECSConnection(data.sparkSession)
      data.write.partitionBy(partitionCols: _*).mode(SaveMode.Overwrite).parquet("s3a://uslab-middle-office-s3-nonprod/"+ no_pof_parquet);
    }

  /**
    * Assumes there exists a set of tables on which to execute the sql over
    *
    * @param sqlQuery
    */
    def sql(spark: SparkSession, sqlQuery: String): Unit = {
      registerTables(spark)
      spark.sql(sqlQuery)
    }

    private def registerTables(sparkSession: SparkSession) = {
      val histroricTable = sparkSession.read.parquet("s3a://uslab-middle-office-s3-nonprod/"+ no_pof_parquet);
      histroricTable.createOrReplaceTempView(ExaToEcsDriver.historicArchiveTblName)
    }

    def writePof(data: Dataset[Array[Byte]]) =  {

      println("control event schema: " + data.schema)
      primeSparkSessionForECSConnection(data.sparkSession)
      log.info("security details: " + data.sparkSession.sparkContext.hadoopConfiguration.get("fs.s3a.access.key")
                                     + " " + data.sparkSession.sparkContext.hadoopConfiguration.get("fs.s3a.secret.key")
                                     + " " + data.sparkSession.sparkContext.hadoopConfiguration.get("fs.s3a.endpoint"))
      //data.write.mode(SaveMode.Overwrite).parquet("s3a://uslab-middle-office-s3-nonprod/abc.parquet");
      data.write.mode(SaveMode.Overwrite).parquet("s3a://uslab-middle-office-s3-nonprod/"+ pof_parquet)
     // data.write.mode(SaveMode.Overwrite).parquet("c:\\output\\data1")
    }

    def sqlOnParquet(sparkSession: SparkSession, sqlQuery: String, parquet: String, historicArchiveTblName : String): Dataset[Row] = {
      //primeSparkSessionForECSConnection(sparkSession)
      registerTablesOnParquet(sparkSession, parquet, historicArchiveTblName)
      sparkSession.sql(sqlQuery)
    }

    private def registerTablesOnParquet(sparkSession: SparkSession, parquet: String, historicArchiveTblName: String): Unit = {
      val historicTable = sparkSession.read.parquet("s3a://uslab-middle-office-s3-nonprod/"+ parquet)
      historicTable.createOrReplaceTempView(historicArchiveTblName)
    }

}
package com.hsbc.trds.scala.endpoints

import com.hsbc.trds.context.TRDSContext
import com.hsbc.trds.endpoints.GCPConfig
import com.hsbc.trds.scala.config.EcsConfig
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{Dataset, Row, SaveMode}

class EndPointWriter extends Logging{

  def write(data: Dataset[Array[Byte]], endPt: String, context: TRDSContext, parColumns: Array[String]) = {
    val endpointUrl: String = endPointUrl(endPt)
    log.info("endpointUrl: " + endpointUrl)
    data.write
      .partitionBy(parColumns: _*)
      .mode(SaveMode.Overwrite)
      .parquet(endpointUrl.concat(context.getObjectStorageRootLocation))
  }

  def gcsWrite(data: Dataset[Row], endPt: String, context: TRDSContext, parColumns: Array[String]) = {
    val endpointUrl: String = endPointUrl(endPt).concat(context.getPartitionTblName).concat("gcs_pof_aug").concat(".parquet")
    log.info("endpointUrl: " + endpointUrl)
    data.write
      .partitionBy(parColumns: _*)
      .mode(SaveMode.Overwrite)
      //.parquet("gs://control-event-bucket-1/".concat(context.getPartitionTblName).concat("/").concat(context.getObjectStorageRootLocation))
      .parquet(endpointUrl)
      //concat(context.getPartitionTblName).
      //concat("/").
      //concat(context.getObjectStorageRootLocation))


  }


  def writePofBinary(data: Dataset[Row], endPt: String, context: TRDSContext, parColumns: Array[String]) = {
    val endpointUrl: String = endPointUrl(endPt).concat(context.getPartitionTblName).concat("ecs_pof_no_par").concat(".parquet")
    log.info("endpointUrl: " + endpointUrl)
    data.write
      .partitionBy(parColumns: _*)
      .mode(SaveMode.Overwrite)
      .parquet(endpointUrl)
  }

  def endPointUrl(endPt : String) = endPt match {
      case EcsConfig.ECS => EcsConfig.s3bucketName1
      case EcsConfig.G_CS => GCPConfig.GCP_GS_BUCKET_NAME
      case _ => null
  }
}


cat: ./src/main/scala/com/hsbc/trds/scala/endpoints/exa: Is a directory
package com.hsbc.trds.scala.endpoints.exa

import java.sql.{Connection, DriverManager, SQLException}
import java.util.Properties

import com.hsbc.trds.context.TRDSContext
import com.hsbc.trds.model.ExaDataModel
import com.hsbc.trds.scala.config.EcsConfig
import org.apache.spark.internal.Logging
import org.apache.spark.sql.{DataFrameReader, Dataset, Row, SparkSession}

object ExaReader extends Logging{

  def getConnection() : Connection = DriverManager.getConnection("jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSLXDS_003.systems.uk.hsbc", "DSL_PROD", "S4nC4rl0")

  def closeConnection(con: Connection) = {
    try {
      if (con == null && con.isClosed)
        con.close
    } catch {
      case e1: SQLException => {
        e1.printStackTrace()
      }
    }
  }

  def registerHistoricControlEventView(sparkSession: SparkSession, historicControlEventViewAlias: String, tblName: String): Unit = {
    val connectionProps = getConnectionProperties()
    val jdbcDF = getHistoricControlEventRowDataset(sparkSession, connectionProps, tblName)
    jdbcDF.createOrReplaceGlobalTempView(historicControlEventViewAlias)
  }

  def getHistoricControlEventRowDataset(sparkSession: SparkSession, connectionProps: Properties, tblName: String): Dataset[Row] = {
    val datFrameReader = getEXADataFrameReader(sparkSession, connectionProps)
    datFrameReader.option("dbtable", tblName).load()

  }

  def getHistoricControlEventRowEcsPOFDatasetForPartition(spark: SparkSession, context: TRDSContext): Dataset[Row] = {
    val lowerBound = 0l
    val upperBound = 99l

    val query = s"(${EcsConfig.SELECT_HCE_MIG_COLUMLIST} , POF_BINARY, 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id " +
      s"|| '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH, ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash, " +
      s"${EcsConfig.PARALLEL_LOAD_AID_COLUMN} ${ExaDataModel.TRADE_ID_HASH}, " +
      s" '${context.getPartitionTblName}' as PARTITION_ID from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})"

   // val query = s"(${EcsConfig.SELECT_HCE_MIG_COLUMLIST_POF_BIN_NO_PARTITION} , ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName} )"
    log.info ("HistoricControlEventRowEcsPOFDatasetForPartition query: " + query)

    val result = getEXADataFrameReader(spark, getConnectionProperties())
      //.option("dbtable", s"( ${EcsConfig.SELECT_HCE_MIG_COLUMNLIST_POF} , ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})")
      .option("dbtable",query)
      .option("partitionColumn",  "thash")
      .option("lowerBound", lowerBound)
      .option("upperBound", upperBound)
      .option("numPartitions", context.getExaConnectionCount)
      .option("fetchSize", context.getExaMaxFetchRows)
      .load()

    log.debug("getHistoricControlEventRowEcsPOFDatasetForPartition schema")
    result.printSchema()

    result
  }

  def getHistoricControlEventAugRowDatasetForPartition(spark: SparkSession, context: TRDSContext): Dataset[Row] = {
    val lowerBound = 0l
    val upperBound = 99l

    val query = s"(${EcsConfig.SELECT_HCE_MIG_AUG_COLUMLIST} , 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id " +
      s"|| '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH, ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash, " +
      s"${EcsConfig.PARALLEL_LOAD_AID_COLUMN} ${ExaDataModel.TRADE_ID_HASH}, " +
      s" '${context.getPartitionTblName}' as PARTITION_ID from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})"

    log.info("query is: " + query)
    //options must all be specified if any of them is specified. In addition, numPartitions must be specified.
    // They describe how to partition the table when reading in parallel from multiple workers.
    // partitionColumn must be a numeric, date, or timestamp column from the table in question.
    // Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.
    //partitionColumn, lowerBound, upperBound	These

    //The maximum number of partitions that can be used for parallelism in table reading and writing.
    // This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit,
    // we decrease it to this limit by calling coalesce(numPartitions) before writing.
    //--numPartitions

    //The JDBC fetch size, which determines how many rows to fetch per round trip.
    // This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).
    // This option applies only to reading.
    //--fetchsize = 1000000

    val result = getEXADataFrameReader(spark, getConnectionProperties())
      //.option("dbtable", s"( ${EcsConfig.SELECT_HCE_MIG_AUG_COLUMLIST} , ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash, '${context.getPartitionTblName}' PARTITION_ID from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})")
      .option("dbtable", query)
      .option("partitionColumn",  "thash")
      .option("lowerBound", lowerBound)
      .option("upperBound", upperBound)
      .option("numPartitions", context.getExaConnectionCount)
      .option("fetchSize", context.getExaMaxFetchRows)
      .load()

    log.debug("getHistoricControlEventRowEcsPOFDatasetForPartition schema")
    result.printSchema()

    result
  }

  def getHistoricControlEventRowDatasetForPartition(spark: SparkSession, context: TRDSContext): Dataset[Row] = {
    val lowerBound = 0l
    val upperBound = 99l

    //val query = EcsConfig.SELECT_HCE_MIG_COLUMLIST + "," + EcsConfig.PARALLEL_LOAD_AID_COLUMN + " thash, '"  + context.getPartitionTblName + "' as PARTITION_ID " + " from " + EcsConfig.SCHEMA_NAME + "." + context.getPartitionTblName
    //cast(mod(ORA_HASH(trade_id),100) as INTEGER) "+ "thash, " +  "


   val query = s"(${EcsConfig.SELECT_HCE_MIG_COLUMLIST} , 'DSLObject/official/' || TO_CHAR(FDSL_BUSINESS_DATE, 'YYYYMMDD') || '/dslprod/' || source_system || '/' || region || '/' || trade_id " +
     s"|| '/' || trade_version || '/DSL/' || dsl_object_type as DOCPATH, ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash, " +
     s" '${context.getPartitionTblName}' as PARTITION_ID from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})"
    log.info ("Historic query: " + query)

    //val query1 = s"(select TO_CHAR(fdsl_business_date,'YYYYMMDD') || '/dsl' as docpath from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})"
    //log.info("Historic query: " + query1)

    val result = getEXADataFrameReader(spark, getConnectionProperties())
      //.option("dbtable", s"( ${EcsConfig.SELECT_HCE_MIG_COLUMLIST} , ${EcsConfig.PARALLEL_LOAD_AID_COLUMN} thash, '${context.getPartitionTblName}' PARTITION_ID from ${EcsConfig.SCHEMA_NAME}.${context.getPartitionTblName})")
      //.option("dbtable", "(".concat(query).concat(")") )
      .option("dbtable", query)
      .option("partitionColumn",  "thash")
      .option("lowerBound", lowerBound)
      .option("upperBound", upperBound)
      .option("numPartitions", context.getExaConnectionCount)
      .option("fetchSize", context.getExaMaxFetchRows)
      .load()

    //log.info("getHistoricControlEventRowEcsPOFDatasetForPartition schema")
    //result.printSchema()

    result

  }

  private def getEXADataFrameReader(sparkSession: SparkSession, connectionProps: Properties): DataFrameReader = {
    sparkSession.read
                .format("jdbc")
                .option("url", connectionProps.getProperty("url"))
                .option("password", connectionProps.getProperty("password"))
                .option("driver", connectionProps.getProperty("driver"))
                .option("user", connectionProps.getProperty("user"))

  }

  def getConnectionProperties(): Properties = {
    val connectionProps = new Properties()
    connectionProps.setProperty("user","DSL_PROD")
    connectionProps.setProperty("password","S4nC4rl0")
    connectionProps.setProperty("driver","oracle.jdbc.OracleDriver")
    connectionProps.setProperty("url","jdbc:oracle:thin:@gbx01-scan.systems.uk.hsbc:2010/DSLXDS_003.systems.uk.hsbc")
    connectionProps
  }



}
cat: ./src/main/sql: Is a directory
create or replace function trds_tradeid_hash
(
  tradeid in varchar2
)
RETURN NUMBER
IS
    asciisum NUMBER := 0;
BEGIN
  FOR i IN 1..length(tradeid) LOOP
    asciisum:= asciisum + ascii(substr(tradeid,i,1));
  END LOOP;
  return asciisum;
end trds_tradeid_hash;cat: ./src/test: Is a directory
cat: ./src/test/java: Is a directory
cat: ./src/test/java/com: Is a directory
cat: ./src/test/java/com/hsbc: Is a directory
cat: ./src/test/java/com/hsbc/trds: Is a directory
cat: ./src/test/java/com/hsbc/trds/context: Is a directory
package com.hsbc.trds.context;

import org.junit.Ignore;
import org.junit.Test;

import static org.junit.Assert.*;

public class TRDSContextTest {

    @Test
    @Ignore
    public void testUUIDisReturned(){
        TRDSContext context = new TRDSContext().setControlEventIds("tradid_uuid,tradid1_uuid1");
        String uuids[] = context.getUUIDs();

        assertEquals(new String[]{"uuid","uuid1"}, context.getUUIDs());
    }

    @Test
    @Ignore
    public void testTradeId(){
        TRDSContext context = new TRDSContext().setControlEventIds("tradid_uuid,tradid1_uuid1");
        assertEquals(new String[]{"tradid","tradid1"}, context.getTradeIds());


    }
    @Test
    @Ignore
    public void testNoUUIDs(){
        TRDSContext context = new TRDSContext().setControlEventIds("tradid,tradid1");
        assertTrue(context.noUuIDS());

        TRDSContext context1 = new TRDSContext().setControlEventIds("tradid_uuid,tradid1_uuid1");
        assertFalse(context1.noUuIDS());
    }
}cat: ./src/test/java/com/hsbc/trds/driver: Is a directory
package com.hsbc.trds.driver;

import com.hsbc.trds.TRDSSparkTest;
import com.hsbc.trds.endpoints.TRDSConstants;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.junit.Ignore;
import org.junit.Test;

import static com.hsbc.trds.endpoints.ExaReader.historicArchiveTblName;
import static com.hsbc.trds.endpoints.ExaReader.getConnectionProperties;
import static com.hsbc.trds.endpoints.ExaReader.getHistoricControlEventRowDataset;
import static org.junit.Assert.assertTrue;

public class ExaToEcsDriverTest extends TRDSSparkTest {

    @Test
    @Ignore
    public void testExaConnectionThroughSpark(){
        Dataset<Row> ds = getHistoricControlEventRowDataset(spark, getConnectionProperties("",""), TRDSConstants.SCHEMA_NAME+"."+historicArchiveTblName);
        assertTrue(ds.columns().length > 0);
    }

}package com.hsbc.trds.driver;

import org.junit.Ignore;
import org.junit.Test;

import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

public class HashUtilTest {
    String algorithm  = "MD5"; // other values "SHA-1" or "SHA-256"

    @Test
    @Ignore
    public void testHash() throws NoSuchAlgorithmException {
        MessageDigest digest = MessageDigest.getInstance(algorithm);
        String code1 = "0030000032976272";
        String code2 = "1323632L";
        System.out.println(digest.digest(code1.getBytes()));
    }
}package com.hsbc.trds;

import org.apache.spark.SparkConf;
import org.apache.spark.sql.SparkSession;

/**
 * To be extended by all Spark tests
 */
public class TRDSSparkTest {

    public static SparkSession spark = null;
    static {
        System.setProperty("hadoop.home.dir","C://sandbox//devtools//hadoop//winutils//hadoop-2.7.1");
        SparkConf conf = new SparkConf().setMaster("local[2]");
        //conf = conf.set("spark.driver.memory","5G");
        spark = SparkSession.builder().appName("ExaToEcs").config(conf).getOrCreate();
    }
}
<?xml version="1.0" encoding="UTF-8"?>
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">
  <component name="NewModuleRootManager" LANGUAGE_LEVEL="JDK_1_8">
    <output url="file://$MODULE_DIR$/target/classes" />
    <output-test url="file://$MODULE_DIR$/target/test-classes" />
    <content url="file://$MODULE_DIR$">
      <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />
      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
      <excludeFolder url="file://$MODULE_DIR$/target" />
    </content>
    <orderEntry type="inheritedJdk" />
    <orderEntry type="sourceFolder" forTests="false" />
  </component>
</module>
